1235 also at which structural position such a para is situated. The importance of sentence and paragraph position information for identifying topics (in this case article topics) was pointed out e.g. in , and using the structural markup in our corpus we are able to provide structural position information for all elements of the markup. Thus, by means of an XSLT counting script we have enriched the
763 occur at a fixed rate, for instance every 10 ms, and are based on a prediction of the processing requirements (workload) until the next decision point. Although many algorithms have been proposed , DVFS (or even DFS) is rarely implemented in general purpose commercial handheld systems. Instead, handheld devices that support frequency scaling have taken a more static approach, changing
773 occur at a fixed rate, for instance every 10 ms, and are based on a prediction of the processing requirements (workload) until the next decision point. Although many algorithms have been proposed , DVFS (or even DFS) is rarely implemented in general purpose commercial handheld systems. Instead, handheld devices that support frequency scaling have taken a more static approach, changing
90 Specification Maintenance M&S Functional Requirements V&V Overall Requirements Specification M&S Requirements Specification Document Credibility AssessmentsThe following V&V techniques (Balci 1998) can be used for M&S requirements evaluation: • Desk Checking • Documentation Checking • Face Validation • Inspections • Reviews • Walkthroughs Balci Figure 4: M&S Acceptability Assessment using
90 Subject matter experts can be assigned to evaluate leaf indicators. Some leaf indicators can be evaluated by performing V&V, analysis, or direct measurement. The following V&V techniques (Balci 1998) can be used for M&S design evaluation: • • • • • ??? Audit Desk Checking Documentation Checking Face Validation Inspections Reviewss• • • • • • • • • • • • • • • • • • • • • • • • • Turing Test
90 An M&S module may be implemented by a team, group, or subcontractor. The implementation process creates executable modules, which can be evaluated by using the following dynamic testing techniques (Balci 1998): Acceptance Testing Alpha Testing Assertion Checking Beta Testing Bottom-Up Testing Comparison Testing Compliance Testing ? Authorization Testing ? Performance Testing ? Security Testing ?
1957 classification engines. 1 Introduction and Previous Work Classifier combination has become a common approach to improve classification performance. Various combination methods have been proposed (). Classifiers can be categorized based on their output information levels () into three types: (i) those that return a unique class label indicating the most probable class to which the input
795 an ad-hoc observer implementation can be a direct interface to other specific services. Indeed, our DGMonitor can easily interoperate with existing services, e.g., resource location services  or forecasting services . It can also be combined with the Round Robin Database Tool  and web-based interfaces to maintain and visualize performance data. To locate networked resources
1028 Databases, Bipartite Graph 1 Introduction The World Wide Web was originally built for human consumption, and although everything on it is machine-readable, the data is not machine-understandable . The Resource Description Framework, RDF , is a language to express metadata about information resources on the Web proposed by the WWW Consortium (W3C). It is intended that this information
1721 substances, processes 1. Introduction Ontologies are recognized as being of importance in almost all parts of computer science and engineering. They are critical at least for the Semantic Web , for data exchange among information systems , and for communication between software agents . An important aspect of such applications is that ontologies need to be represented by means of
1102 in . This single-view approach can be extended to 3D by considering multiple simultaneous views of features. Shape models in several views can be separately estimated to match object appearance ; this approach was able to learn a mapping between the low-dimensional shape parameters in each view. With multi-view contours from cameras at known locations, a visual hull can be recovered to
318 mining techniques can be applied to the clickstream or Web application data in the pattern discovery phase, such as clustering, association rule mining , and sequential pattern discovery . The recommendation engine considers the active user session in conjunction with the discovered patterns to provide personalized content. The personalized content can take the form of recommended
789 on-the-fly during the synthesis of the test case through the use of APIs at different levels , as depicted in Figure 4. The algorithms are mainly based on adaptations of Tarjan’s algorithm , computing the strongly connected components of a graph via a depth-first search. The complexity is linear in the size of the state graph (though the graph itself may be exponential in the size of
1726 appropriate knowledge representation  languages, which remains an active area of research. Prominent examples of such languages include the Resource Description Framework, RDF , the DAML+OIL  language, which integrates the US DARPA Agent Markup Language and the European OIL effort and is an extension of the RDF Schema, and DAML+OIL’s successor, the World Wide Web Consortium’s Web
2308 last longer. Several protocols have been recently proposed for sensor networks with sleeping nodes. A complete survey is outside the scope of this paper, but the interested reader is referred to  and the references therein. There are two distinct categories of such protocols: (1) Nodes are awaken according to a deterministic rendezvous schedule, and (2) Nodes cycle on-an-off at random. This
2308 supported by the Office of Naval Research under grant N00014-00-0655. information to guide the routing mechanism. A recent protocol that uses this concept is Geographic Random Forwarding (GeRaF) . With GeRaF, each message is broadcast to all nodes within range and the one node that both decodes the message and is closest to the destination forwards it. This has the benefit of not requiring
2308 is that the relaying node must be selected in a distributed fashion, without being guided by a genie. This implies that the routing and MAC protocol must be designed jointly. A key contribution of  is the development of a practical MAC protocol that allows the most geographically advantaged relay to be chosen after the source has transmitted. If no node is within range of the source, GeRaF
2308 coherence time ? is the duration that the topology remains fixed. The first network coherence interval (NCI) is the range t : {0 ? t < ?} while the i th NCI is t : {(i ? 1)? ? t < i??}. As in , we assume each node knows its own position and has a circular coverage area. Nodes within the coverage circle successfully decode the initial transmission, while those outside the circle do not.
2308 range, i.e. the value of dm that satisfies (2) with equality. For analytical convenience, we assume R1 = 1 and normalize {Rm} with respect to R1, resulting in Rm = ? 2r 2 ? 1 22r/m ?1/µ ? 1 As in , the source is located at coordinates (D, 0) and the destination at (0, 0). In GeRaF, the coverage area of the source is represented by a single circle of unit radius centered at (D, 0). In
3019 assume that (1) there are n groups, each with a weight wi and group QoS space {Qi1,Qi2, ..., Qimi }, (2) each joint QoS level Qij =(qij1, ..., qijti ) in the space has a utility ui(Qij), and (3) each process QoS level qijk in the joint QoS level has a stochastic CPU demand of C(qijk) cycles per period P (qijk). We can represent the global coordination as the following constrained
3019 maximize n? wiui(Qij) (system utility) (1) i=1 minimize f ? ? f : f ? subject to n? ti? ? C(qijk) P (qijk) i=1 k=1 (CPU speed/energy) (2) n? ti? C(qijk) P (qijk) i=1 k=1 ? fmax (EDF schedulability) (3) Qij =(qij1, ..., qijti ) ?{Qi1,Qi2, ..., Qimi } i =1, 2, ..., n (4) f ?{f1, ..., fmax} (5) where Equation (3) is the CPU resource constraint to guarantee the schedulability of the earliest deadline
3019 that the total CPU utilization of all groups is no more than one; i.e., the aggregate CPU bandwidth demand of all groups is below the coordinated CPU performance, as illustrated in Equation (2) and (3). With this schedulability constraint, the VS-CBS based scheduling algorithm provides temporal isolation among different servers (and hence different groups), as analyzed in. 19 Further, it allows
773 ? subject to n? ti? ? C(qijk) P (qijk) i=1 k=1 (CPU speed/energy) (2) n? ti? C(qijk) P (qijk) i=1 k=1 ? fmax (EDF schedulability) (3) Qij =(qij1, ..., qijti ) ?{Qi1,Qi2, ..., Qimi } i =1, 2, ..., n (4) f ?{f1, ..., fmax} (5) where Equation (3) is the CPU resource constraint to guarantee the schedulability of the earliest deadline first (EDF) based scheduling algorithm, which will be discussed in
1495 is 0.22, 0.35, 0.47, 0.6, 0.74, and 1.0 for the speed 300, 500, 600, 700, 800, and 1000 MHz, respectively. If the CPU runs for T time units, its normalized energy consumption is ? T 0 p(f(t)) dt (9) where f(t) and p(f(t)) are the CPU speed and relative power, respectively, at time t, 0 ? t ? T . Run the hyper-video player. Wefirst run a hyper-video player under GRACE-grp. The hyper-video
3258 et Notations 1) Définitions: Nous introduisons la notion du vecteur d’allocation équitable R, dont les coordonnées donnent dans l’ordre lexicographique les taux d’allocation de routes (voir ). Mathématiquement, cela peut être exprimé comme suivante : Soit P est l’ensemble de chemins (ou connexions identifiées par des chemins ) p dans un graphe G(N, L) et xp est son taux associé. Une
192 sums. The main bottleneck in implementing such an algorithm is computing the union of pairwise Minkowski sums. Given m polyhedral primitives, their union can have combinatorial complexity O(m 3 )  and m can be high in the context of Minkowski sum computation (e.g. a few thousand). Furthermore, robust computation of the boundary of the union and handling all degeneracies remains a major issue
192 the union of pairwise Minkowski sums. After the second step, there can be O(n 2 ) pairwise Minkowski sums. The pairwise convex Minkowski sums are convex. Their union can have O(n 6 ) complexity . Our algorithm for Minkowski sum computation is based on the above framework. We now discuss each of the above steps in detail. 3.3 Convex Decomposition The problem of computing an optimal convex
195 to general polyhedral models. Minkowski sum of two convex polyhedra can have O(n 2 ) complexity. However, for non-convex polyhedra in 3D, the Minkowski sum can have O(n 6 ) worst-case complexity . One common approach for computing Minkowski sum of general polyhedra is based on convex decomposition . It uses the following property of Minkowski sum. If P = P1 ? P2, then P ? Q = (P1 ? Q) ?
196 polyhedron. However, no robust implementation of this algorithm is known. Most practical algorithms for convex decomposition perform surface decomposition or tetrahedral volumetric decomposition . Typically, these methods can generate O(n) convex parts and each of them has a few faces. We used a modification of the convex decomposition scheme available in a public collision detection
200 (with n features) can have O(n 2 ) combinatorial complexity and is relatively simple to compute. On the other hand, the Minkowski sum of nonconvex polyhedra can have complexity as high as O(n 6 ) . One of the commonly used approach to compute Minkowski sums decomposes the two non-convex polyhedra into convex pieces, computes their pairwise Minkowski sums and finally the union of the pairwise
200 and m can be high in the context of Minkowski sum computation (e.g. a few thousand). Furthermore, robust computation of the boundary of the union and handling all degeneracies remains a major issue . As a result, no practical algorithms are known for robust computation of exact Minkowski sum of complex polyhedral models.sMain Results: We present a novel algorithm to approximate the Minkowski
200 based on distance fields. 2.1 Minkowski Sum Many algorithms have been proposed for Minkowski sum computation in computational geometry and solid modeling . A survey can be found in . Guibas and Seidel  proposed an output-sensitive algorithm for Minkowski sum of convex polytopes. They defined an operation, called convolution, on 2D planar tracings. Basch et al.  extended
2249 the F&D-approach by utilizing very recent machine learning techniques and a larger number of EEG-electrodes. First of all, we could increase the transfer rate by using Support Vector Machines (SVM)  for classification. Inspired by a recent approach to learning of discriminative densities  we utilized the values of the SVM classification function as a measure of confidence which we
2249 of one target symbol. For the P300-detection, we utilized two model-based methods which had been proposed by F&D, and one completely data-driven method based on Support Vector Machines (SVMs) . For training of the classifiers, we built up a sets of epochs containing an equal number of positive and negative examples, i.e. epochs with and without a P300 component. 1 OL denotes the position
1189 Based on this, a hybrid strategy for VE in peer mediators is proposed. 1. Introduction There has been substantial interest in using the mediator/wrapper approach for integrating heterogeneous data . Most mediator systems integrate data through a central mediator server accessing one or several data sources through a number of ’wrapper’ interfaces that translate data to a common data model
416 arbitrary changes of the kernel were allowed, no generalization bounds would hold . If adaptation is restricted for example to convex combinations of fixed kernels, bounds have been established . Arguments along this line could also be applied to SRNG. However, another more direct argumentation is presented in  which derives a large margin generalization bound for the basic weighted
3260 endeavor to accommodate more automation into their network infrastructures, targeting more diverse and flexible services, such as short-lived or highly dynamic VPNs. QoS (constraint-based) routing , with routers themselves searching for eligible network routes with sufficient resources to meet the QoS requirements in a distributed manner, can be a potential complement to the VPN router
2305 flow when in fact this neighbor is an alternate identity of the adversary. 7.3. Geographic routing Geographic and energy aware routing (GEAR)  and greedy perimeter stateless routing (GPSR)  leverage nodesÕ positions and explicit geographic packet destinations to efficiently disseminate queries and route replies. GPSR uses greedy forwarding at each hop, routing each packet to the
483 using LEACH to form hierarchical clusters. In this case, it is in the adversaryÕs best interest to use the above techniques against the top-most layer of clustering. Other clustering protocols  and protocols optimizing or extending LEACH such as TEEN  and PEGASIS  are also susceptible to attacks similar to those described above. 7.6. Rumor routing Rumor routing  is a
3154 factorization methods for orthographic and affine cameras . Algebraic approaches based on polynomial and tensor factorization have been proposed in the case of multiple translating objects  and in the case of two  and multiple  rigid-body motions. Our contribution. In this paper, we address the initialization of iterative approaches to motion estimation and segmentation by
3154 pn evaluated at a collection of n image measurements. This new approach offers two important technical advantages over previously known algebraic solutions to the segmentation of 3-D translational  and rigid-body motions (fundamental matrices)  based on homogeneous polynomial factorization: 1. It is based on polynomial differentiation rather than polynomial factorization, which greatly
3154 of a vector of coefficients c ? RMn(K) or CMn(K) as pn(z) = c T ?n(z) = ? cn1,n2,··· ,nK zn1 1 zn2 2 · · · znK K , (3) where ?n : R K (C K )?R Mn(K) (C Mn(K) ) is the Veronese map of degree n  defined as ?n :  T ?? T with I chosen in the degree-lexicographic order. The Veronese map is also known as the polynomial embedding in the machine learning
3154 (calibrated case) or the epipole (uncalibrated case) of object i relative to the camera between the two frames. A solution to this problem based on polynomial factorization was proposed in . Here we present a much simpler solution based on polynomial differentiation. Given the images x1 ? P 2 and x2 ? P 2 of a point in object i in the first and second frame, they must satisfy the
3154 with zero-mean Gaussian noise with s.t.d. between 0 and 1 pixels for an image size of 500 × 500 pixels. For comparison purposes, we also implemented the polynomial factorization algorithm (PFA) of  and a variation of the Expectation Maximization algorithm (EM) for clustering hyperplanes in R 3 . Figures 3(c) and (d) show the performance of all the algorithms as a function of the level of
1069 that represent a current state of the art of these objects. I. INTRODUCTION Probabilistic finite-state machines such as probabilistic finite-state automata (PFA) , hidden Markov models (HMMs) , , stochastic regular grammars , Markov chains , n-grams , , probabilistic suffix trees , deterministic stochastic or probabilistic automata (DPFA) , weighted automata  are
1069 also terms and trees. Their successes in a wide amount of fields ranging from computational linguistics  to pattern recognition –, and including language modeling in speech recognition , , , bioinformatics –, music modeling , machine translation , –, circuit testing  or time series analysis  make these objects very valuable indeed. But as more
2703 contributes to most of the total energy dissipation in micrometer-scale technologies, but static energy dissipation will contribute an increasing portion of energy in nanometer-scale technologies  . We consider both types of energy in our evaluation. Energy consumption due to accessing off-chip memory should not be disregarded, since fetching instruction and data from off-chip memory
2703 on dynamic power, under the assumption that static energy is a small fraction of total energy – perhaps less than 10%. However, for deep submicron, the fraction is increasing. For example, Agarwal  reports that leakage energy accounts for 30% of L1 cache energy for a 0.13-micron process technology. To consider this CMOS technology trend, we evaluate the situations where k_static is 30% and
2703 the transistor, resulting in increased static power consumption. Thus, static power is becoming a greater concern. Some researchers are thus working on leakage power reductions, such as DRG-Cache . We observe in Figure 6(a) and (b) that although way shutdown increases the miss rate for some benchmarks, for other benchmarks, way shutdown has negligible impact. Such negligible impact means
1939 energy_static = cycles * energy_static_per_cycle The underlined terms are those we obtain through measurements or simulations. We compute cache_hits and cache_misses by running SimpleScalar  simulations for each cache configuration. We compute energy_hit of each cache configuration through simulation of circuits extracted from our layout using Cadence  (which happened to reasonably
1939 compensated for by the reduction in time and power that would have been caused by misses. Figure 2(a) shows the miss rate for two MediaBench benchmarks, epic and mpeg2, measured using SimpleScalar  and configured with an 8 Kbyte data cache having one, two or four-way set-associativity, with a line size of 32 bytes. Notice that the hit rates for both are better with two ways than with one way,
1804 already been partially implemented. Moreover, the solution is not unique, the implementation may change according to certain trade-offs. The third approach is very close to Opdyke’s refactorings , i.e. operations that modify the source code of an application without changing its behavior. One of the many difficulties of this approach is that design patterns specify a set of solutions to a
672 allowing to verify the syntax of the JML specifications, – the jmldoc tool, that is similar to JavaDoc, but adds the JML specification to the generated html documentation and – the jmlc tool , that uses the JML annotation in order to add runtime assertions in the generated code. The assertion checking allows running the code with dynamic tests checking for the correctness of the
673 problems can be found early, as a specification violation will generate false assertions, potentially before introducing a visible runtime error. jmlc is integrated with Junit 3 giving jmljunit . This tool generates an oracle and skeletons used by Junit to run test cases. Static validation tools The main tool in this category is the ESC/Java  static checker for Java. It performs a
684 proofs. Those proof tools are targeted to Java Card, which does not contain complex Java features which would be difficult to handle such as, for example, multi-threading. – The LOOP tool  is a tool converting Java annotated sources to PVS models. It treats the complete Java Card language and now proposes an automated proof obligation generation using weakest precondition calculus. –
688 tool: the Java Applet Correctness Kit (or JACK). This tool, already briefly described in , is a formal tool that allows one to prove properties on Java programs using the Java Modeling Language  (JML). Its application domain is, at the moment, smart card applets, but one can consider that it can be useful in many development contexts. It generates proof obligations allowing to prove that
688 been collected. Section 6 presents research perspectives and Section 7 concludes. 2 Java Modeling Language This section briefly presents JML and the tools that have been developed around it. JML  is a language that allows one to specify Java classes by formally expressing properties and requirements on those classes and their methods. Some keywords and logical constructions have been added
499 we also address this issue at the subproblem level by adopting a symbolic representation. The logic synthesis and model checking communities have been using Ordered Binary Decision Diagrams (OBDD)  for compact state space encoding. An OBDD-based model checking technique has proven particularly successful in dealing with the state explosion problem . Recognizing the similarities between
175 and set attribute. We assume that the percentage of read, write, get attribute and set attribute operations are 67.1, 20.8, 0.3¤ 11.8 and respectively (loosely based on SPECsfs 3.0). Recent studies  suggest that the reference probability for Web documents requested by a client does not follow Zipf’s Law precisely, but instead follows a Zipf-like distribution with the exponent varying from
175 frequencies, then the reference probability for a document with ranksis propor ¨§ ?¦¥ tional to . Note ? ? ? that for , the request distribution strictly follows Zipf’s law. However, as shown in , the distribution of Web requests follows a Zipf-like distri¦ ? ©???? ? ??¦ ? ? ? bution, with . In our experiments, a client requests objects according to a Zipf-like distribution, where the
789 This means that no error has been detected so far and that the system is structurally sound. This component can now be decomposed further by using the strong components algorithm described in . This algorithm aims in our case to find cycles in the bipartite graph, which is equal to finding blocks of equations that can be solved independently. An explanation of the decomposition,
789 will be applied. It makes use of both algorithms for enumerating perfect matchings from  or the improved algorithm from  and the strong components algorithm by .s30 5.1. Debugging Over-Constrained systems After computing the safe set of equations, there will be a number of sets of equations that are considered safe for removal. The number of sets will be
940 slow convergence and high bandwidth utilization , . 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sf) DHT Mesh: Structured overlays like distributed hash tables (DHTs) ??? view the overlay as a distributed data structure that dictates both the network topology and message routing. This integrated view has been shown to be massively scalable, requiring O(log n)
2358 (or its variants such as NewReno or SACK) is modeled by the following relation between the equilibrium source rate xi, the overall dropping probability pi, and the round trip time di + ? (see e.g., ): pi = 2 2 + x2 i (di + ?)2 This is equivalent to the well-known square-root-p formula when the loss probability is small. For TCP Vegas, the source rate xi is related to the round trip propagation
1717 advection equations. 4 Numerical Method 4.1 Level Set Equations One way to solve equations (23) and (25) is to use, for example, RK3 in the z-direction and WENO5 upwind scheme in the x-y-?-? space . This is a typical Eulerian approach, where grid points are fixed in space. Computational complexity of this method is therefore O(N 5 LogN). This approach is not efficient in this high dimensional
2243 in the face of large feedback delays. This has led to a more robust REM presented in . C. Related works An extensive literature exists on flow control, including the original TCP flow control  and recent enhancement in , the binary feedback schemes of, e.g., , , two– bit feedback scheme of , the control theoretic approach of, e.g., , , , etc. Also see a recent
2354 two– bit feedback scheme of , the control theoretic approach of, e.g., , , , etc. Also see a recent review in . A key premise of optimization based flow control , , , , , , , , ,  is that sources with different valuation of bandwidth should react differently to network congestion. All these works motivate flow control by an optimization
2354 of objective functions or their solution approaches, and result in rather different flow control mechanisms to be implemented at the sources and the network links. Our model is closest to that in , . Indeed both their work and ours have the same objective of maximizing aggregate source utility. In ,  this objective is decomposed into optimization subproblems for the network and
2354 denote source rate at time t under Algorithm A1, and as a function of price given by (6). The meaning should be clear from the context. 3. Our work is closely connected to Kelly’s as described in , , . Both solve the same optimization problem (1–2), but they differ in the solution approach which leads to different flow control algorithms, which in turn lead to different marking
2354 order to maximize its benefit, and the network subproblem is to choose source rates (xs, s ? S) given users’ willingness–to–pay vector (ws, s ? S) in order to maximize ? s ws log xs. It is shown in  that there exist path prices (ps , s ? S), source rates x = (xs, s ? S) and willingness–to–pay (ws, s ? S) with ws = psxs such that ws solves user s’s subproblem, and x solves the network
2354 in the Appendix II). V. Fairness, quasi–stationarity, and pricing In this section we comment on some fairness and implementation issues. A. Fairness A proportionally fair rate vector is defined in  as a feasible rate vector (ˆxs, s ? S) such that for any other feasible vector (xs, s ? S), the aggregate of proportional changes is nonpositive: ? xs ? ˆxs s?S ˆxs ? 0 The primal optimal solution
1028 effective retrieval, reuse of content, or adaptation of content to a user’s interest. In parallel with research concerning the Semantic Web, a variety of semantic standards have emerged, e.g. RDF  or Topic Maps , which focus on the semantic modeling of multimedia content, i.e. not the presentation of media but their semantic interrelationships are described. As a novel approach to
1029 presentation of media but their semantic interrelationships are described. As a novel approach to semantic multimedia content modeling, we have developed Enhanced Multimedia Meta Objects (EMMOs)  in the context of the EU-project CULTOS 1 . An EMMO constitutes a self-contained piece of multimedia content that indivisibly unites three of the content’s aspects: the media aspect, i.e. the media
1029 of an EMMO can be versioned, thereby paving the way for the distributed, collaborative construction of EMMOs. The formal basis for the EMMO model are entities which are defined as 13-tuples  with the 13 values covering the following information: • Each entity is globally and uniquely identified by its OID and carries a human readable name. • The entity’s kind attribute determines which
382 are impractical for large environments. One of the attempts to solve this problem is presented Myron Hattig Intel Corporation Open Source Robotics Hillsboro, Oregon Email: myron.hattig@intel.com in  where a sampling-based technique is used. Rather than storing and updating a complex probability distribution, a number of samples are drawn from it. The other approaches utilize partially
1349 approaches have shown interesting behavior of RNNs that re¯ ect some aspects of performance and some aspects of theoretical capabilities in processing natural and arti® cial languages (e.g. Elman, 1991; Giles et al., 1992; Servan-Shrieber et al., 1988). Theoretical approaches have been guided by traditional automata theories and have formally shown that a dynamical system can be constructed as an
1349 a simple recurrent network (SRN) in a prediction task to model sentence processing capabilities of RNNs. For example, Elman reports an RNN that can learn up to three levels of center-embeddings (Elman, 1991). Stolcke reports an RNN that can learn up to two levels of center-embeddings, and up to ® ve levels for tailrecursion (Stolcke, 1990). Other work has shown that an RNN can process temporal
173 (P2P) systems rely on cooperation among selfinterested users. For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources . In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others . Further examples are file preservation , discussion
173 this context is the “tragedy of the commons”  where resources are under-provisioned due to selfish users who free-ride on the system’s resources, and is especially common in large networks  . The problem has been extensively studied adopting a game theoretic approach. The prisoners’ dilemma model provides a natural framework to study the effectiveness of different strategies in
176 of our work) is consistent with the objective of incentivizing the desired behavior among selfish players. The unique challenges imposed by peer-to-peer systems inspired additional body of work  , mainly in the context of packet forwarding in wireless ad-hoc routing    , and file sharing  . Friedman and Resnick  consider the problem of zero-cost identities in
177 and loss rate increase when nodes refuse to forward packets on behalf of others . Further examples are file preservation , discussion boards , online auctions , and overlay routing . In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance. As a result, each user’s attempt to
177 all newcomers is inevitable. Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm.  and  show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system. Some commercial file sharing clients
178 among selfish players. The unique challenges imposed by peer-to-peer systems inspired additional body of work  , mainly in the context of packet forwarding in wireless ad-hoc routing    , and file sharing  . Friedman and Resnick  consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is
181 whitewashing can be nearly eliminated from the system. The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting   . • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.
181 effective with shared history. This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them. This is consistent with previous work  showing that punishing strangers deals with whitewashers. However, Figure 12 shows that “Stranger Defect” is not effective with private history. This is because Reciprocative requires some initial
181 systems inspired additional body of work  , mainly in the context of packet forwarding in wireless ad-hoc routing    , and file sharing  . Friedman and Resnick  consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable. Using a theoretical model, they demonstrate that such a
182 challenges imposed by peer-to-peer systems inspired additional body of work  , mainly in the context of packet forwarding in wireless ad-hoc routing    , and file sharing  . Friedman and Resnick  consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable. Using a theoretical model,
183 for complexity of computation. We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work  . • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped. We show
183 player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest. Some examples of shared history schemes are   . Figure 7 shows the effectiveness of shared history under high turnover rates. In this figure, we fix the population size and vary the turnover rate. While selective players with private
185 among selfish players. The unique challenges imposed by peer-to-peer systems inspired additional body of work  , mainly in the context of packet forwarding in wireless ad-hoc routing    , and file sharing  . Friedman and Resnick  consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is
186 or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously. Several studies   of repeated Prisoner’s Dilemma games use an evolutionary model   of population dynamics. An evolutionary model is not suitable for P2P systems because it only specifies the global behavior
186 shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection. Previous decision functions such as Tit-for-Tat and Image (see Section 5) do not satisfy these criteria. For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections . In
186 for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest. Some examples of shared history schemes are   . Figure 7 shows the effectiveness of shared history under high turnover rates. In this figure, we fix the population size and vary the turnover rate. While selective players with private history
186 decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy . The effect of collusion is mag250 300 350 400 nified in systems with zero-cost identities, where users can create fake identities that report false statements. Instead, to deal with collusion,
186 shows that the Tit-for-Tat strategy dominates. Our model assumes growth follows local learning rather than evolutionary dynamics , and also allows for more kinds of attacks. Nowak and Sigmund  introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history. Players using Image cooperate
188 • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes . • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest. In the example in Figure 1, A wants service from B, B wants service from C, and C
189 selfish players. The unique challenges imposed by peer-to-peer systems inspired additional body of work  , mainly in the context of packet forwarding in wireless ad-hoc routing    , and file sharing  . Friedman and Resnick  consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable. Using
1755 information valid at a certain point of time, but lack the ability to preserve the history of information. An evolving information system, on the contrary, must be conservative or temporal ( , , ) in the sense that it should not forget anything ever fed to the system, unless explicitly asked for, thus allowing for the formulation of queries about the history of the
1755 - the time at which an event occurs in the universe of discourse - and recording time - the time at which the event is recorded in the information system - is of great importance (, , , ). Traditional systems can thus be regarded as degenerations of these evolving information systems (). For a more elaborate discussion on the difference between traditional and
1755 needed. These point of times are called the recording time of events. Our notion of event time and recording time is identical to the notions of valid time, and transaction time, respectively, in . (The reason for this renaming is that the new names correspond better to the three level architecture we will introduce in section 6.) The classification made in  is based on the support of
1755 of state in the information system due to an update (addition, deletion or modification), the former state cannot be remembered by the information system. Such systems are called snapshot systems (), due to the fact that these information systems can only model and remember (single) snapshots of an organisation’s evolution. For evolving information systems, as well as for temporal or
3263 The first topology is given in Figure 4. This topology is also used in ,  and considered to be typical of a large ISP’s network. ( This topology closely resembles the MCI backbone topology .) Each link has a bandwidth of 20 Mbps. Packet size is selected to be 500 bytes. Nodes 1 and 5 are selected as multicast sources. In the first set of simulations, each source has 6 receivers. In
2354 rate xi, denoted Ui(xi). The function value can be understood as the perceived quality, user satisfaction, etc. Meanwhile, various fairness objectives can be achieved when U(.) takes certain forms. Especially, when Ui(xi) = ?(? log xi) ? , maxmin allocation can be achieved 4 . In our previous work, we used the utility-based approach to address the problem of optimal rate allocation
2358 Rx ? c. Indeed, various congestion control algorithms in Transmission Control Protocol (TCP) can be interpreted as solving the same utility maximization problem with different utility functions . The authors of  advocate proportional fairness, characterized by Ui(xi) = log xi. In , an allocation policy called minimum potential delay is proposed with Ui(xi) =?1/xi, which is shown in
2360 = log xi. In , an allocation policy called minimum potential delay is proposed with Ui(xi) =?1/xi, which is shown in  to approximate the fairness of the TCP on the current Internet. In , the following class of utility functions is proposed ? ?1 1?? (1 ? ?) x Ui(xi,?)= i if ? ?= 1 (1) log xi if ? =1 for ? ? 0. This includes all the previously considered allocation policies –
2360 (one with larger ?) always less efficient (has a smaller aggregate throughput)? This conjecture is prompted by the various examples in resource allocation in the literature in wired networks , , , in wireless networks, , , in economics, , etc. These examples seem to illustrate (quoted from ) ‘‘the fundamental conflict between achieving flow fairness and maximizing overall
2360 (L = 1), T (?) is strictly decreasing in ? for the linear network with uniform link capacity. Example 2: Linear network with nonuniform capacity The linear network of Example 1 is considered in  with L =2, but with different link capacities c1 <c2. The authors calculated the source rates under max-min fairness: x0(?) =x1(?) = c1 2 , x2(?) =c2 ? c1 2 and pointed out that source rate x0 will
1104 flow generates a large number of correspondences, the inversion is more likely to lead to a solution . It can be combined with edge-adjustment , or with analysis-by-synthesis technique . Face images depend on head motion, illumination conditions and facial movements. In presence of large image changes, tracking can take great advantage of an appearance-variations model, which is
1107 the speaker before tracking his/her facial movements. Even then, it is not guaranteed that they could be fairly adapted to every facial configurations. Data driven models can cope with this problem . We will describe below a methodology that lets control parameters of a speakerspecific model emerge from a statistical analysis of finegrained 3D data. With only a 3D model, low-level
2148 competing objectives: providing a fair share of capacity to each subnet node while maximizing the total capacity of the network. One approach for balancing performance and fairness is outlined in . Karn, et al. Best Current Practice sRFC 3819 Advice for Internet Subnetwork Designers July 2004 11. Delay Characteristics The TCP sender bases its retransmission timeout (RTO) on
3028 of the challenges in architecting QoS for the Internet. There are presently two architectural approaches to providing mechanisms for QoS support in the Internet. IP Integrated Services (Intserv)  provides fine-grained service guarantees to individual flows. Flows are identified by a flow specification (flowspec), which creates a stateful association between individual packets by matching
2136 is the core protocol of the Internet. IP defines a simple &quot;connectionless&quot; packet-switched network. The success of the Internet is largely attributed to IP’s simplicity, the &quot;end-to-end principle&quot;  on which the Internet is based, and the resulting ease of carrying IP on a wide variety of subnetworks, not necessarily designed with IP in mind. A subnetwork refers to any network operating
2136 of any layered communication network is the appropriate layer(s) in which to implement a given function. This issue was first addressed in the seminal paper, &quot;End-to-End Arguments in System Design&quot; . That paper argued that many functions can be implemented properly *only* on an end-to-end basis, i.e., at the highest protocol layers, outside the subnetwork. These functions include ensuring the
2136 an understanding of system-level implications, including possible interactions with higher-layer mechanisms. The original architecture of the Internet was influenced by the end-to-end principle , and has been, in our view, part of the reason for the Internet’s success. The remainder of this document discusses the various subnetwork design issues that the authors consider relevant to
2136 the TCP sender is prone to resending a segment prematurely. 8. Reliability and Error Control In the Internet architecture, the ultimate responsibility for error recovery is at the end points . The Internet may occasionally drop, corrupt, duplicate, or reorder packets, and the transport protocol (e.g., TCP) or application (e.g., if UDP is used as the transport protocol) must recover from
2136 transmission errors, it will not (and need not) achieve a very low packet loss rate as the Internet protocols are better suited to dealing with lost packets than to dealing with corrupted packets . Packet corruption may be, and is, also caused by bugs in host and router hardware and software. Even if every subnetwork implemented strong error detection, it is still essential that end-to-end
1755 it cannot take advantage of well-known temporal implementations techniques such as temporal indexes , temporal storage structures , and temporal join (e.g., ) and coalescing algorithms . Further, it seems that there has been an implicit assumption (e.g., in ) that the performance of temporal DBMSs should be similar to that of conventional DBMSs, even when a temporal DBMS
1755 and experiment with temporal query languages, and it also allows some experimentation with parts of the back end of a database, e.g., query evaluation and special temporal operator implementations . The experience gained in using the stratum approach can be used in the long-term goal of building temporal functionality directly into the DBMS. We list eight criteria that a stratum should
2151 be measured ????? ? ??? ? ??? as , ? where is the size of the packet, is the time-stamp when the packet is ready to be sent ??? at the MAC layer, and ??? is the time-stamp when an ACK is received , . Note that the time interval includes the channel busy and contention time. ????????? We keep separate throughput estimates to different neighboring nodes because the channel conditions may
2722 packets at slow speed to see whether a new path is available. The probing speed should take into consideration the route discovery behavior of the underlying routing protocol. For instance, in DSR , a packet without a route will be kept in a buffer at the routing agent, while the routing agent keeps trying to find a route for the packet. If no route can be found within a certain time limit,
2722 standard TCP implementations. Below we discuss two sets of simulations: one in a medium size one-hop MANET, and the other in a large size multi-hop scenario. We use ns-2 (v2.1b8a)  with DSR  in our simulations. A. One-Hop Scenario In this set of simulations, all mobile nodes are within one-hop of transmission range to each other inside a 170m by 170m space. The nodes use the “random
695 that represents a person. This says something about the concept of author. A concept may have different types of relations with other concepts. As in typical object-oriented modeling techniques , the parent relation (specialization/generalization), the part-of relation, and non-hierarchical relations between concepts (e.g., in the form of predicate frames) are used for defining concepts in
2243 TCP for many distributions of inter-loss times. I. INTRODUCTION We analyze in this paper the performance of TCP (Transmission Control Protocol), the widely-used transport protocol of the Internet , . TCP is a reliable window-based flow control protocol where the window is increased until a packet loss is detected. Here, the source assumes that the network is congested and reduces its
3260 with mean ? . When a connection request arrives at nodes, it begins to setup the reservation. At first, the node tries to find the path for the request using a QoS routing algorithm (see ). Suppose the link ¢ is selected to get to the next hop. Then the resource manager at nodesexamines the resource availability (residual bandwidth for link ¢ ) at this time. If there is enough
3255 rate at a low cost. I. INTRODUCTION In the Internet, Integrated Service (Intserv) and Differentiated Service (Diffserv) are two well-known models which provide QoS to end users (, , ). In the Intserv, Resource reservation plays a key role. Many efforts have been put into the resource reservation research (, , ). In connection-oriented networks, such as ATM, a
868 and maintenance. Grid user essentially sees a single, large virtual computer despite of the fact that the pool of resources can be geographically-distributed and connected over world-wide networks . At its core, Grid Computing is based on an open set of standards and protocols (i.e., Open Grid Services Architecture: OGSA ) that enable communication across heterogeneous,
318 main approaches to discover these sequential patterns are based on the well-known apriori algorithm , and they essentially differ on the data portions considered to generate pattern candidates , , . However, the number of discovered rules is usually high, and the interest of most of them doesn’t fulfill user expectations. Filtering them after the fact, i.e., after the generation
318 research and on patients monitoring . Finally, in finance, applications on the analysis of product sales, client behaviors or inventory consumptions are essential for today’s business planning . Another important application of sequential pattern mining is in bioinformatics, where different characteristics of proteins and other biologically significant structures are to be inferred from
318 , and allows for reducing significantly the number of candidates for which the support counting is done, reducing the time needed to discover the set of all frequent itemsets. The AprioriAll  and its improvement GSP  are apriori adaptations for mining sequential patterns. The main difference to apriori is that candidates are sequences instead of itemsets, which means that the order
318 the entity, and there aren’t more than T elements between two consecutive pattern elements. Stated in this form, an entity could only contribute once to increment the support of each pattern . However, like apriori, the algorithm suffers from one main drawback: the lack of user-controlled focus. SPIRIT Algorithms. SPIRIT is a family of apriori-based algorithms that uses a regular
1177 process is illustrated in Figure 1. The local models of the legacy systems are not integrated into a common global model (which would be the ‘federated schema’ in federated database systems ) as it would be the case with the traditional bottom-up approach. Instead, an integration of the given reference model with each individual local model is constructed via a linking mechanism (a
1177 constructs found in the legacy models (see below). In summary, the Meta-CDL model serves as a shared description (could be compared to the ‘canonical data model’ in federated database systems ) to which the forward and the reverse engineered CDL models will be linked in order to ascertain which (portions of) legacy elements can be linked to the reference model level. In this way, it is
2050 component types (see section 4), describing the services a component offers and the services it wants to use from other components. This is opposed to other works in the field of type systems  , which only view types as specifying allowed operations on entities of the type. Automatic adaptions and extensions are in our approach reflected by dynamic changes 1 accepted at OOPSLA 99
2050 computed by the compiler by control-flow analysis . The description of the call protocol can be given by simple pre- and postconditions of functions and extend the idea of contracts . The VideoMail component of our example has the C-Automaton shown in figure 2. The function play just uses the play functions of theVideoPlayer and the SoundPlayer components, thus only having a
687 for method invocations. Described in detail in the ESC/Java User’s Manual , these heuristics are a compromise between flexibility and likelihood of errors and do not guarantee soundness. JML  provides static invariants to express properties of static fields and objects referenced from static fields. Static invariants correspond to our module invariants. Analogously to our methodology,
688 global variables) and instance variables (also called fields). The data values in each can be constrained by programmer-declared invariants, as they are in, for example, the Java Modeling Language . The correctness of a program relies on these invariants. Consequently, any tool or technique for checking the correctness of the program—manual or automatic, static Peter Müller ETH Zurich
688 for method invocations. Described in detail in the ESC/Java User’s Manual , these heuristics are a compromise between flexibility and likelihood of errors and do not guarantee soundness. JML  provides static invariants to express properties of static fields and objects referenced from static fields. Static invariants correspond to our module invariants. Analogously to our methodology,
3195 servers. As a result, state replication is an integral part of SimMud, but has not been considered in previous systems. Replication is an integral part in peer-to-peer file sharing , ,  for both availability and performance. However, those are read-only file systems, while SimMud applies frequent update to shared data. As a result, SimMud must maintain data consistency while
939 avatar what to do, e.g. “pick up object” or “attack monster”, rather than how to do it. III. Peer-to-Peer Infrastructure A number of peer-to-peer overlays have recently been proposed, including CAN , Chord , Tapestry  and Pastry . These self-organizing, decentralized systems provide the functionality of a scalable distributed hash table, by reliably mapping a given object key to a
3198 to the duration of the player’s game play. Games are different from previous P2P applications that focus on the harnessing of idle storage and network bandwidth, including storage systems , , , content distribution ,  and instant messaging . Games utilize the memory and CPU cycles of peers to maintain the shared game state. Three potential problems must be addressed to
3198 of dedicated servers. As a result, state replication is an integral part of SimMud, but has not been considered in previous systems. Replication is an integral part in peer-to-peer file sharing , ,  for both availability and performance. However, those are read-only file systems, while SimMud applies frequent update to shared data. As a result, SimMud must maintain data consistency
188 new game. Lacking hard data on RPGs, we have instead used real life measurement of the Gnutella P2P system. It has been found that over a period of 60 hours, the average session time was 2.3 hours . We realize that file sharing and online gaming session lengths are apples and oranges, but given our anecdotal data, 2.3 hours seems conservative, if anything. Equating failure and exits, this
943 to do, e.g. “pick up object” or “attack monster”, rather than how to do it. III. Peer-to-Peer Infrastructure A number of peer-to-peer overlays have recently been proposed, including CAN , Chord , Tapestry  and Pastry . These self-organizing, decentralized systems provide the functionality of a scalable distributed hash table, by reliably mapping a given object key to a unique live
943 on which we implemented SimMud. Our algorithm can, however, readily be extended to other hashing functions, and it can even be simplified for a more deterministic routing algorithm like Chord . A. Mapping games states on to peers We group players and objects by regions, and distribute the game regions onto different peers by mapping them to the Pastry key space. Each region is assigned
943 that a key will almost always be routed to the node whose ID is numerically closest to key. P2P systems such as Chord have demonstrated this property even when half of the nodes fail simultaneously . With a much lower failure frequency, it is reasonable to assume that messages eventually reach the correct node. C. Shared state replication We design a lightweight primary-backup mechanism to
2729 by a node failure. There are two cases: (a) If a cluster head fails, only this cluster has to be re-organized. This procedure does depend on the clustering algorithm. Based on the analysis of , formation of Dhop clusters requires 2D runs of flooding over a cluster. The theoretical minimum number of routing packets to re-organize a cluster due to the cluster head failure is 2M ? (N(M) +
2729 result, among others, is validated in the following section. IV. NUMERICAL RESULTS AND SIMULATIONS To verify our analysis, we build a simulator by MATLAB. We use D-hop Max-min clustering2 algorithm  in our simulations. N nodes are randomly distributed over a square area of side length L. Though our analysis is based on a regular grid, we run simulations for random network topologies in order
1445 feature of chats, namely social relations between the chatters, to filter out probably irrelevant parts ofsdiscussion. The idea is analogous to web-graph analysis, like Hubs and Authorities  and PageRank , which try to discriminate highly relevant web sites from uninteresting ones. However, instead of aiming at ranking we use the relevance scores to enhance the topic models. This
1445 structure, thus helping to alleviate cyberspace information overload. Previous approaches in this area have typically analyzed Web hyperlinks directly to determine page relationships , or have relied on measures of similarity that only consider joint referencing of pairs of pages. The approach proposed in this work relies instead on measures of similarity that exploit sets of
317 standard tools developed in other areas of science, such as cluster analysis . Similarity among objects by common reference has recently received some attention in the form of association mining . While they are not usually recognized as such, what are defined as itemsets in association mining can be interpreted as generalized cocitations. Similarities between pairs of documents in
317 of the numerical attribute. 1 Introduction Analyzing categorical data is an important field in data mining. Examples include market-basket data as well as census data. The Apriori-framework  is widely used in this field. But, most datasets do not only contain categorical attributes, they also contain numerical attributes. The straightforward approach in order to use the
317 Starting with a database of transactions, which in turn consist of items sold together, algorithms were developed to find associations between the items. The basic principles were developed in , and the well-known Apriori-algorithm was proposed in . Many extensions were developed, and the concept of frequent itemsets and association rules is applied in many areas. The Apriori-framework
939 Our proposal thus requires a name resolution infrastructure that can scalably resolve flat names. Distributed hash tables (DHTs) represent one possible solution to this resolution problem (see  for background on DHTs), and so we borrow from that literature as well. Thus, this work is a pastiche of borrowed elements; our contribution is both the distillation of some basic principles and
940 Our proposal thus requires a name resolution infrastructure that can scalably resolve flat names. Distributed hash tables (DHTs) represent one possible solution to this resolution problem (see  for background on DHTs), and so we borrow from that literature as well. Thus, this work is a pastiche of borrowed elements; our contribution is both the distillation of some basic principles and
3198 ensures each name is unique and controlled by the relevant authority; flat names make these goals harder, but not impossible, to achieve. Several mechanisms exist for global uniqueness (see  for example). Data integrity (i.e., ensuring that no one else can change the resolution of an entity’s name) is also challenging but possible (see, e.g. ). DHTs’ typical resolution
939 tremendous popularity in the Internet. P2P systems are classified either as unstructured (such as Napster, Gnutella, and Freenet) or as structured (such as Distributed Hash Table (DHT) systems , ) based on the type of the overlay topologies they create. We focus on the use of a distributed rating scheme for tackling the free-rider phenomenon that has been observed in P2P systems such
939 methods are cheat-proof, or whether the scheme can be distributed. P2P architectures have been extensively studied in the recent past. Example systems are Napster, Gnutella (unstructured) and CAN  and Chord  (structured). A survey of P2P research is beyond the scope of this paper. There has been a recent trend toward incentive compatible distributed system design . In , the
943 tremendous popularity in the Internet. P2P systems are classified either as unstructured (such as Napster, Gnutella, and Freenet) or as structured (such as Distributed Hash Table (DHT) systems , ) based on the type of the overlay topologies they create. We focus on the use of a distributed rating scheme for tackling the free-rider phenomenon that has been observed in P2P systems such as
943 refreshes the rating information from the corresponding main supervisor. However, we can always choose k > 2 and give equal responsibility to all supervisors when considering malice. Chord  is an ideal infrastructure for setting up and maintaining such ring based supervisory overlays. Chord 3sorganizes users on a ring, and each user is mapped into a fixed point on the ring by hashing
943 cheat-proof, or whether the scheme can be distributed. P2P architectures have been extensively studied in the recent past. Example systems are Napster, Gnutella (unstructured) and CAN  and Chord  (structured). A survey of P2P research is beyond the scope of this paper. There has been a recent trend toward incentive compatible distributed system design . In , the authors present a
173 of the overlay topologies they create. We focus on the use of a distributed rating scheme for tackling the free-rider phenomenon that has been observed in P2P systems such as Napster and Gnutella ,  . We present (Section IV) one distributed rating mechanism along with two different ratings validation schemes. In the first validation scheme, we present a Structured Verification Scheme
173 the need for a distributed rating system is the free-rider phenomenon observed in file-sharing P2P systems. This phenomenon has the following characteristics: • Most files (e.g., 98% reported in ) belong to a small percentage of the users (20%, respectively). The ratio of the number of queries to the number of users that respond to queries is similar. Hence, the quality and availability of
188 the overlay topologies they create. We focus on the use of a distributed rating scheme for tackling the free-rider phenomenon that has been observed in P2P systems such as Napster and Gnutella ,  . We present (Section IV) one distributed rating mechanism along with two different ratings validation schemes. In the first validation scheme, we present a Structured Verification Scheme (SVS)
181 the services a user has provided to the community. When there is no negative rating and users start at zero, there is no incentive for these users to change their names or identifiers. See  for a discussion of the social cost of cheap pseudonyms. A user might be assigned negative ratings for bad or fraudulent transactions. We might want to reprimand users in such cases. This is also a
2354 not exceed the link capacity. (b) Fairness: The available network bandwidth should be shared fairly among users. In recent years, price-based rate control schemes have received much attention , , , , . Under these schemes, the network charges users a price per unit transmission rate that depends on the current traffic load at individual links. Users then adapt their
2354 (fairness) criteria such as proportional fair, max-min fair, and socially optimal allocations. Much of the work on price-based rate control has been motivated by Kelly et al. who proposed in ,  a particularly elegant price-based rate allocation mechanism. Kelly et al. show that this mechanism converges (under suitable assumptions) to a socially optimal rate allocation . To
2354 where U ? ?1 i of Ui. :  ?? ?+ is the inverse of the derivative We note that utility functions with these characteristics are commonly used in the pricing literature (see for example ). Assumption 2 does not require all users have the same utility function. We assume that the utility function Ui is only known to user i, and unknown to all other users and the link provider.
2354 ? ? ) . i=1 Furthermore, the transmission rate of user i converges to Di(p ? ), i.e. limt?? xi(t) = Di(p ? ), i = 1,...,N. We provide a proof of Proposition 1 in Section IV. As pointed in , , , Proposition 1 implies that the above price-based allocation scheme approximates a socially optimal rate allocation. For a more detailed discussion of the connection between the above price-based
2360 the link capacity. (b) Fairness: The available network bandwidth should be shared fairly among users. In recent years, price-based rate control schemes have received much attention , , , , . Under these schemes, the network charges users a price per unit transmission rate that depends on the current traffic load at individual links. Users then adapt their transmission rates to
2249 example with respect to a given vector? weight is defined ? ? ??? ??? ? ? as . A positive margin corresponds to a correct classification and the more positive the margin the greater the confidence  that the classification is correct. The margin has been frequently used in the context of Support Vector Machines (SVMs)  and Boosting (e.g. ). However, for the definition of
2249 ??? , and then we would need to normalize function??? the . Here, we use ? ? the norm for the normalization.sgin over all¨ examples, i.e. ? ? ??? ? ?? ??? ????? ? ? ?£¢ (2) A reasonable choice  for a convex combination is to maximize the minimum margin of the examples, i.e.s? ? ? such thats? ?s? ? ¡¥¤§¦ ?©¨????s??????¢ (3) choose? Roughly speaking, the larger the margin the better the
2249 data, as all patterns are classified with some non-negative margin . Several approaches have been proposed to deal with this situation. For SVMs slack variables have been frequently used  to allow for some soft margin, i.e. violations of the margin constraints similar to (7). In the dual domain the introduction of slack variables leads to an ? ? - norm constraint on the dual
2249 ? ? ? ??? ? ? . To show how to mini???? ???? ??? ? ? ¢ (38) P? Note that it fulfills the assumptions made in Lemma 3. Furthermore, as frequently and successfully used for regression with SVMs , we the§ consider -insensitive loss: §?? ? ? ????? ? ????? ?? ¡?¤?¦ ? ? ?©? ? ????? ? ? ? ? ? ? §???¢ (39)s-insensitive loss has appealing properties, as it will usually lead to sparse solutions of
3197 Bayeux to forward each join and leave request to the root, thus preventing it from handling large, dynamic groups efficiently. The object location approach used in Tapestry and in Plaxton’s work  can be viewed as a special case of our anycast mechanism. In these systems, the tree formed by the overlay routes from each replica holder of an object to the object’s root is annotated with
939 multicast achieves low delay and induces low link and node stress . Scribe’s approach to group management could be implemented on top of other structured peer-to-peer overlay networks like CAN , Chord , or Tapestry . In fact, Scribe has been implemented on top of CAN . Next, we describe Pastry and the construction of group trees using Scribe in more detail. 2.1 Peer-to-peer
939 to large and highly dynamic groups. Like Scribe, Bayeux , and CAN-Multicast  are group communication systems built on top of structured peer-to-peer overlays, namely Tapestry  and CAN . Neither Bayeux nor CAN-multicast support an anycast primitive.sWhile such a primitive could be added, neither protocol can support highly dynamic groups efficiently. CAN-multicast builds a
940 of each group. Scribe can handle group sizes varying from one to millions, and it supports rapidschanges in group membership efficiently. All nodes join Pastry, a structured peer-to-peer overlay . Subsequently, nodes may join and leave potentially many groups many times to amortize the cost of building the Pastry overlay. This, combined with Pastry’s already scalable algorithms to join the
940 fact, Scribe has been implemented on top of CAN . Next, we describe Pastry and the construction of group trees using Scribe in more detail. 2.1 Peer-to-peer overlay Pastry is fully described in . Here, we provide a brief overview to understand how we implement group management and anycast. In Pastry, numeric keys represent application objects and are chosen from a large identifier space.
3198 the same node. Storage capacity: Storage systems often need to locate a node that has the spare capacity to store a replica of an object. For example, in an archival file storage system like PAST  a node with high storage utilization may wish to find an alternate node to store a replica on its behalf. In such a system, a group can be created consisting of nodes with spare disk capacity. A
943 achieves low delay and induces low link and node stress . Scribe’s approach to group management could be implemented on top of other structured peer-to-peer overlay networks like CAN , Chord , or Tapestry . In fact, Scribe has been implemented on top of CAN . Next, we describe Pastry and the construction of group trees using Scribe in more detail. 2.1 Peer-to-peer overlay Pastry
2351 in  and present a stability result using a drift analysis. This result which was first established in  using fluid limits and a special case of this result was presented earlier in . Finally, concluding remarks are presented in Section 6. The survey presented here covers only a small portion of the considerable research on Internet Congestion Control that has taken place over
2243 Science Lab University of Illinois at Urbana-Champaign Urbana, IL 61801 rsrikant@uiuc.edu 1 Introduction Congestion control in the Internet was introduced in the late 1980s by Van Jacobson . Jacobson’s algorithm which is implemented in the transport layer protocol called TCP (Transmission Control Protocol) has served the Internet well during a time of unprecedented growth. However,
2354 the results in  which show that TCP-Reno is not stable when the capacity per user is large or if the feedback delay is large. In Section 3, we introduce the Kelly resource allocation model  and the associated congestion control algorithms . A stability analysis of the congestion control algorithm in the presence of feedback delays is presented in Section 4. This stability
2354 compare the value of a global performance metric at the Nash equilibrium to the optimal value of that metric. Our work is most closely related to the network pricing model proposed by Kelly et al. , . In that model, users choose the total amount they are willing to pay, rather than their desired rate. Kelly established that this model maximizes aggregate surplus if link capacities are
2249 any feature selection. We next describe SVM learning and the combined knowledge sources adopted. Much of the description follows that of Lee and Ng (2002). 2 Support Vector Machines (SVM) The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes. A test example is classified depending on the side of the hyperplane it lies
2249 splice sites from pseudo sites which wouldn’t be spliced. Support Vector Machine (SVM) is known as an efficient machine learning method used in many scientific disciplines including bioinformatics . As several researchers have shown, DNA sequence data can be treated with adequate design of kernel functions i.e. core parts of SVM . We focused on patterns of nucleotide doublets of DNA
175 can be characterized at both the requestlevel and session-level. In this paper, we consider request-level characterization. Much work has been done on characterizing web workloads (for example , among others too numerous to mention), and there have been many different perspectives taken. Traces were taken from web servers, web proxies, and web browsers. What makes an E-commerce workload
175 it is not clear if they provide typical data for all types of web sites.sMost published studies on workload characterization on web servers use information servers in the most traditional sense . Characteristics considered important are: file size distribution, file popularity, request arrival process, etc. Some of the important results are as following: 1) HTML and image files account for
2249 2 SVMs classification and regression The SVMs give a simple way to obtain good classification results with a reduced knowledge of the problem. The principles of SVMs have been developed by Vapnik  and have been presented in several works like . The classification task is reduced to find a decision frontier that divide the data into the groups that we want to separate. The simplest
2249 to that in equation (6) but using as error measure the ?-insensitive function and taking into account the fact that y values are real. For further information about this optimization problem,  can be consulted. (6)s3 Noise detection and substitution In this work the first problem is to detect the noisy pixels. This task is treated as a classification procedure where the pixels are
318 notion of support is central to the definition of frequent and maximal itemsets, association rules, sequential patterns, and other ideas in the area of data mining known as association analysis . Nonetheless, few efforts to extend association analysis to handle non-traditional types of patterns and non-binary data do so by modifying the notion of support, and those efforts that do have
2281 as well. While individual rare items in the tail may not be requested frequently, these queries represents a substantial fraction of the workload, and are worth optimizing. A separate study  has shown that the popularity distribution of a file-sharing workload is flatter than what we would expect from a Zipfian distribution. The most popular items were found to be significantly less
1253 the expensive human intervention to archive and annotate contents. Many researchers are currently investigating methods to automatically analyze, organize, index and retrieve video information . This effort is further stressed by the emerging MPEG-7 standard that provides a rich and common description tool of multimedia contents. It is also encouraged by Video-TREC 1 which aims at
1257 the expensive human intervention to archive and annotate contents. Many researchers are currently investigating methods to automatically analyze, organize, index and retrieve video information . This effort is further stressed by the emerging MPEG-7 standard that provides a rich and common description tool of multimedia contents. It is also encouraged by Video-TREC 1 which aims at
1445 Alternate between the following two steps until convergence, 1. Compute and normalize u = Wv, u = u/?u?, 2 Similar ideas have also been used to find the hub and authority web pages in a link graph .s2. Compute and normalize v = W T u, v = v/?v?, where the vector norm ?·?canbechosentotheEuclidean norm, and ? can be computed as ? = u T Wv upon convergence. For a detailed analysis of the
350 multiple path options. However, the router delay for deterministic routers, and consequently their corresponding clock cycles, can be signi cantly lower than adaptive routers as pointed out in . This di erence in router delays is due to two main reasons: Number of virtual channels. Two virtual channels are su cient to avoid deadlock in dimension ordered routing ; while adaptive routing
350 policy depends also on the state of the router (i.e the occupancy of various virtual channels) causing increased router complexity and thereby higher router delays. The results reported in  show that the router delays for adaptive routers are about one and a half to more than twice as long as the dimension-order router for worm-hole routing. The advantage of adaptive routing in
350 blocking. 2.3 Modeling Router Delay In this section we describe a router delay model for the virtual cut-though deterministic and adaptive routers. The model is based on the ones described in . These models account for both the logic complexity of the routers as well as the size of the crossbar as determined by the number of virtual channels that are multiplexed on one physical channel.
350 only when very large bu er sizes are used. All of these added delays result in adaptive routers that are 15 to 16 % slower than deterministic routers. These results are similar to the results in  where 15% to 60% improvement is required for f- at routers with similar number of virtual channels and under worm-hole routing. 3 Hybrid Routing This section describes the mechanism of the hybrid
350 same dimension and uses direction-order routing for its deterministic routing algorithm. Comparisons of adaptive and deterministic router implementations, for worm-hole routing, are described in  and . However, the comparison in  does not account for the reduced queuing delay in adaptive routing. In  the reduction in queuing delay for worm-hole routing is taken into account
352 multiple path options. However, the router delay for deterministic routers, and consequently their corresponding clock cycles, can be signi cantly lower than adaptive routers as pointed out in . This di erence in router delays is due to two main reasons: Number of virtual channels. Two virtual channels are su cient to avoid deadlock in dimension ordered routing ; while adaptive routing
352 policy depends also on the state of the router (i.e the occupancy of various virtual channels) causing increased router complexity and thereby higher router delays. The results reported in  show that the router delays for adaptive routers are about one and a half to more than twice as long as the dimension-order router for worm-hole routing. The advantage of adaptive routing in
352 blocking. 2.3 Modeling Router Delay In this section we describe a router delay model for the virtual cut-though deterministic and adaptive routers. The model is based on the ones described in . These models account for both the logic complexity of the routers as well as the size of the crossbar as determined by the number of virtual channels that are multiplexed on one physical channel.
352 avoid deadlock). Note that this relationship includes the delivery port. Delay equations for the routers are derived, using the above parameters. The constants in these equations were obtained in  using router designs along with gate-level timing 8sestimates based on a 0.8 micron CMOS gate array process. Three main operations are used in all of the routers simulated here which contribute to
352 for the Symult 2010  and the Intel Paragon . The Paragon router also uses the 0.8 micron CMOS gate array technology. It gives comparable performance to those routers found in  with a delay of 40 nanoseconds and channel bandwidth of 200 megabytes/second. The Triplex routing algorithm is an example of a multi-class routing algorithm in which the dynamic selection of
355 Because of the e ect router complexity has on clock cycle time, issues concerning this topic needed to be researched. Some of the earliest work in this area involved the torus routing chip . In this chip deadlock-free worm-hole routing techniques were used along with an implementation based on a full crossbar architecture. Later implementations greatly increased router performance by
356 router delays is due to two main reasons: Number of virtual channels. Two virtual channels are su cient to avoid deadlock in dimension ordered routing ; while adaptive routing (as described in ) requires a minimum of three virtual channels in k-ary n-cube networks. Output channel selection. In dimension-ordered routing, the output channel selection 2sMessage Latency (ns) 800.0 600.0 400.0
356 router. This routing scheme is deadlock free: for any given message, the choice of paths selected is always a true subset of those that could be selected by the adaptive algorithm described in . Since the adaptive algorithm has been proven deadlock free, the hybrid is also deadlock free. 13s3.2 Pipelined Implementations The Pipelined Hybrid Router implementation is shown in Figure 4. It
360 Several studies have demonstrated that adaptive routing can achieve a lower latency, for the same load, than deterministic routing when measured by a constant clock cycle for both routers . The delay experienced by a message, at each node, can be broken down into: router delay and queuing (or waiting) delay. The former is determined primarily by the complexity of the router. The
360 plane A 0, then plane A 1, and so on. Since at each clock cycle, adaptivity is limited to two dimensions, resources are minimized while gaining adaptivity. The Turn Model proposed by Chien and Ni  is another example of a partially adaptive routing algorithm. In this algorithm deadlock freedom is accomplished by preventing turns and does not require virtual channels. Once again adaptivity is
361 and concluding remarks are given in Section 6. 2 Deterministic and Adaptive Routing The interconnection network model considered in this study is a k-ary n-cube using virtual cut-through switching : message advancement is similar to worm-hole routing , except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu ered
364 and Adaptive Routing The interconnection network model considered in this study is a k-ary n-cube using virtual cut-through switching : message advancement is similar to worm-hole routing , except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu ered at a single node. Note that a header it can progress to a
365 and non-minimal fully adaptive routing is possible . The Cray T3E router is also a hybrid router: Messages can be routed deterministically or adaptively simply by setting a bit in the header . The router also supports a shortcut for messages that continue traveling in the same dimension and uses direction-order routing for its deterministic routing algorithm. Comparisons of adaptive and
2249 model in just one pass of the training data. Their accuracy is however relatively modest. At the other end of the spectrum lie SVMs based on elegant foundations of statistical learning theory . Their training time is quadratic to the number of training examples, but they are known to be the most accurate. The simplest task in text classification is to determine whether a document belongs
2289 energy end-to-end reliable paths can be calculated and implemented for reactive (on-demand) routing protocols. We have experimented with the Ad-hoc On-demand Distance Vector Routing protocol (AODV) . It should, however, become obvious from our description that our technique can be generalized to alternative on-demand routing protocols (e.g., DSR  ? T. Nadeem A. Agrawala are with the Dept.
2289 nodes set up forwarding pointers to the destination. Once the source node receives the RREP, it may begin to forward data packets to the destination. Details of the AODV protocol can be found in . Our proposed modifications adhere to the on-demand philosophy, i.e. paths are still computed on-demand and as long as an existing path is valid, we do not actively change the path. Clearly other
2722 Distance Vector Routing protocol (AODV) . It should, however, become obvious from our description that our technique can be generalized to alternative on-demand routing protocols (e.g., DSR  ??? T. Nadeem A. Agrawala are with the Dept. of Computer Science, University of Maryland, College Park, MD 20742, USA. Emails: {nadeem,suman,agrawala}@cs.umd.edu. S. Banerjee is currently with the
2722 over the region. • Mobile Random: Initially, nodes are distributed uniformly at random over the region. During the simulation, nodes move around the region using the random waypoint model  with zero pause time. In all our simulations we had a set of 12 flows that were active over the duration of the experiment. We use both TCP and UDP flows for different experiments. For the TCP
21199 Berlin Heidelberg 2003sPost-supervised Template Induction for Dynamic Web Sources 269 has been an active area of research in recent years . The first wrapper induction system, WIEN  is a supervised learning agent, i.e. it requires manually labelled examples with output information, to learn patterns. A recent wrapper induction algorithm, STALKER, generates high accuracy (80%)
1673 a histogram of these scores for all sixteen clusters; clearly, these observers tend to agree quite strongly on what the clusters are “about”. (1) structure, landscape (2) horse (3) tree (4) war (5) people (6) people (7) people (8)figure,animal,porcelain (9) mountain, nature (10) book (11) cup (12) people (13) plate (14) portrait (15)people, religion (16)people, art, letter Figure 4. Each
1673 can be used to predict words based on image segments. Specifically, if B is the set of segments in the image under consideration, then Pw ( | Bd , ) ? PBw ( , | d) =? Pw ( | cPBcdPc ) ( | , ) ( ) (5) c In the case of Model I completing the above calculation is not completely straightforward because the aspect model is properly expressed in terms in of the documents of the training set, and we
757 clusters are “about”. (1) structure, landscape (2) horse (3) tree (4) war (5) people (6) people (7) people (8)figure,animal,porcelain (9) mountain, nature (10) book (11) cup (12) people (13) plate (14) portrait (15)people, religion (16)people, art, letter Figure 4. Each histogram corresponds to a cluster and shows the score (described in the text) for the 10 words with highest score used to
3193 security. Our system focuses on all three of these aspects, particularly security. The majority of security research in peer-to-peer has focused on providing anonymity to the user and data (e.g. , ), and automating the construction of trust relationships in systems utilizing untrusted identities (e.g. , ). Our system allows users to participate without having to prove their
3194 Our system focuses on all three of these aspects, particularly security. The majority of security research in peer-to-peer has focused on providing anonymity to the user and data (e.g. , ), and automating the construction of trust relationships in systems utilizing untrusted identities (e.g. , ). Our system allows users to participate without having to prove their identity,
2906 parameters, and uses sub-Nyquist uniform sampling and welldeveloped algorithmic solutions. Specifically, we extend some of our recent sampling results for certain classes of non-bandlimited signals  to the problem of channel estimation in ultra-wideband systems     , where unknown channel parameters are estimated from a low-dimensional signal subspace. Our approach leads to
2906 coefficients Y  are again given by a sum of weighted exponentials, yet the weights depend on n (through the term n r ). In the following, we will present a method based on annihilating filters   , which allows for joint estimation of all the unknown parameters (cl,r and tl) fromasetofatleast2RL+1 coefficients Y . The main idea behind this approach is to find the socalled
2906 can be then estimated by solving for the coefficients cl,r in (13). In the following, we give an outline of the algorithm, while a more detailed analysis of the annihilating filters can be found in   . B. Algorithm outline 1. Compute the spectral coefficients Y  fromthesetof samples yn =< hb(t ? nTs),y(t) >, n =1,...,N (16) where Ts =1/Rs and N ? 2L.s4 2. Find the coefficients H
1496 hosts. In the present work, we propose an on-line energy aware algorithm for distributed heterogeneous hard real time systems based on a modification of the Earliest Deadline First (EDF) . Each host executes a specific task set minimizing energy consumption ensuring the precedence order on the chain of tasks while meeting all time constraints. The rest of this paper is organized as
1496 we do not seek for schedulability bounds, but for an heuristic that will allow to save energy, without compromising deadlines. Moreover, we will consider a dynamic priority scheduling based on EDF . In this case, the schedulability has been determined by simulating the execution of the system over the whole hyper-period at maximum speed, assuming that tasks consume its WCET. During this
578 exhibit a considerable amount of structure. In response, several algorithmic techniques have been developed which exploit database structure to accelerate search time or reduce storage requirements . Consequently, the performance of these approaches are subject to the structure or statistical characteristics of the database. Ternary Content Addressable Memories (TCAMs) provide a
578 filter databases through confidentiality agreements . Gupta and 3sMcKeown obtained access to 40 real filter databases and extracted a number of useful statistics which have been widely cited . In , Gupta and McKeown generated synthetic two-dimensional databases consisting of source-destination address prefix pairs by randomly selecting destination address prefixes from publicly
578 to filter properties in terms of a tuple specification. To be specific, we define the filter 5-tuple as vector containing the following fields. • t, source address prefix length,  ??? t, destination address prefix length,  • t, source port range width, the number of port numbers covered by the range,  • t, destination port range width, the number of port
578 database measurements and synthetic database generation, we define a new metric, scope, to be lg(# of possible packet headers covered by the filter). Specifically, scope = lg{(2 32?t ) × (2 32?t ) × t × t ×(2 8(1?t) )} = (32 ? t) + (32 ? t) + (lg t) + (lg t) 71 78 85 +8(1 ? t) (1) Scope translates the filter tuple into a measure of filter specificity and is
578 define a smoothing parameter r which limits the maximum radius of deviation from a target tuple. Radius is measured in units of scope, or the lg(5–d distance). For example, if the target tuple is , then  is a tuple at radius five from the target. A database generated with r set to zero would have a tuple distribution whose set of unique tuples and relative weights would be
586 considerable amount of structure. In response, several algorithmic techniques have been developed which exploit database structure to accelerate search time or reduce storage requirements . Consequently, the performance of these approaches are subject to the structure or statistical characteristics of the database. Ternary Content Addressable Memories (TCAMs) provide a mechanism for
586 and prefix nesting . A simple technique for appending port ranges and protocols from real databases in order to generate synthetic five-dimensional databases is also described in . In , Baboescu and Varghese introduce a simple scheme for using a sample database to generate a larger synthetic five-dimensional database. This technique replicates filters by changing the IP prefixes
586 • t, source port range width, the number of port numbers covered by the range,  • t, destination port range width, the number of port numbers covered by the range,  • t, protocol specification, Boolean value denoting whether or not a protocol is specified,  4sTable 1: 5-tuple scope measurements for firewall databases. Database # filters µscope ?scope rules1
586 database generation, we define a new metric, scope, to be lg(# of possible packet headers covered by the filter). Specifically, scope = lg{(2 32?t ) × (2 32?t ) × t × t ×(2 8(1?t) )} = (32 ? t) + (32 ? t) + (lg t) + (lg t) 71 78 85 +8(1 ? t) (1) Scope translates the filter tuple into a measure of filter specificity and is essentially just the logarithm of the
595 have exerted much effort in order to generate realistic performance tests for new algorithms. Several research groups obtained access to real filter databases through confidentiality agreements . Gupta and 3sMcKeown obtained access to 40 real filter databases and extracted a number of useful statistics which have been widely cited . In , Gupta and McKeown generated synthetic
182 hurt the capability of the user to consume files due to asymmetric bandwidth. Several papers have appeared that argue that game theory is the correct tool for incentive modelling in P2P networks (Golle et al. 2001; Buragohain, Agrawal, & Suri 2003). We agree with that stance, and apply it to the setting of discretionary databases in general. Kalman et al. (Kalman, Fulk, & Monge 2000) present a treatise on
182 within a ? ?-factor of vfirst(f).sUse in P2P incentivizing The use of the integral of content shared over time as a measure of contribution has been proposed by several authors in the literature (Golle et al. 2001; Buragohain, Agrawal, & Suri 2003). Golle notes that this provides an incentive for agents to share unpopular files and to do so during times of low demand, and suggest scaling factors to remedy
499 is a function of, we need to monitor its size in order to ensure that it does not get very large. We estimate how large the precomputation logic is by computing the size of its corresponding ROBDD . In the sequel, we describe a branching algorithm that selects the “best” subset of inputs. The pseudocode is shown in Fig. 7. The procedure SELECT LOGIC receives as arguments the function ?, the
499 the Logic: The Boolean operations of OR and cofactoring required in the input selection procedure can be carried out efficiently using reduced, ordered binary decision diagrams (ROBDD’s) . In the pseudocode of Fig. 7, we show how to obtain the ?I C ?P function. We also need to compute ?I and ?P independently. We do this in exactly the same way, by including in ?I the cofactors
1168 to define pertinent correspondences among the representation databases. Traditional methods of defining correspondences among databases—used, e.g., by Rusinkiewicz et al.  and Ceri and Widom ???employ data dependency descriptors to describe how objects of a source class are related to objects of a target class in another database. An r-class is typically defined in the context of one
1168 rules among the i-class and the r-classes at two levels: the object correspondence level and the value correspondence level. These are similar to the existence dependency and value dependency . 4.3 Object Correspondence An object correspondence (OC) specifies how an object of one class should be related to an object of another class. Here object correspondences define existence
1177 The main purpose of the MRMS is to actively maintain multiply represented entities with respect to a set of consistency rules. The MRMS can be seen as a loosely coupled, federated database system , as it is a collection of cooperating, but autonomous component database systems. The component database systems encapsulate the representation databases. We use the term representation object
1804 the AS reflectively to models and metamodels, we have been able to show how designers can carry on, within the UML notational context, activities such as behaviour-preserving transformations  (see § 4.2), design pattern application  (§ 4.3) and design aspects weaving  (§ 4.4). The rest of the paper is structured as follows. Section 2 recalls the principles of the UML metalevel
1804 the implementation domain, thus falling into both categories. These transformations perform the “weaving” of the two aspects into a single implementation model. 4.2 Design Refactorings Refactorings  are behaviour-preserving transformations, used to improve the design of object-oriented programs. We believe that refactorings are an important artifact to software development, and we are
176 good formulation is presented in . The inefficiency of p2p systems has been pointed out in   and designing incentives for contribution using reciprocity concepts is discussed in     among others. There exist several real p2p applications which use reciprocity-based (e.g., ) or minimum contribution (see Direct Connect —http://www.neo-modus.com) mechanisms to provide the
182 public good formulation is presented in . The inefficiency of p2p systems has been pointed out in   and designing incentives for contribution using reciprocity concepts is discussed in     among others. There exist several real p2p applications which use reciprocity-based (e.g., ) or minimum contribution (see Direct Connect —http://www.neo-modus.com) mechanisms to
188 comparison of complete information and incomplete information schemes in the context of our public good formulation is presented in . The inefficiency of p2p systems has been pointed out in   and designing incentives for contribution using reciprocity concepts is discussed in     among others. There exist several real p2p applications which use reciprocity-based (e.g., )
317 our approach bears some similarities with those association rule based classification algorithms , our approach avoids the Apriori-based frequent rule set finding , which requires to transform the training data into to transactional database and to scan the data repeatedly. The performance comparison with CBA  indicates that our approach is more than
2868 in various fields, such as statistics, machine learning, and neural networks . During the recent surge of KDD research, classification becomes one of the most studied data mining problems . Techniques developed earlier were re-examined in the new context . Since classification algorithms developed in machine learning and statistics assume
2868 investigates the execution speed and the scalability with respect to the number training samples. This set of experiments used the synthetic data and classification functions defined in 381 . Each record in the data set consists of 9 attributes, including salary, commission, age, elevel, car, zipcode, hvalue, hyears and loan.. There are 10 classification functions defined on these 9
2722 . Directed diffusion borrows heavily from the literature on adhoc unicast routing. Specifically, it is a close kin of the class of several reactive routing protocols proposed in the literature . Of these, it is possibly closest to  in its attempt to localize repair of node failures, and its deemphasis of optimal routes. The differences between ad-hoc routing and directed diffusion
795 Grids to gain acceptance, particularly in industry. The purpose of this paper is to scope these security issues. The Grid Security Infrastructure (GSI), a component of the Globus Toolkit , creates Grid credentials for every user and resource. We describe how this may be extended to securely set up an interactive session on a remote host, and the additional security issues associated
795 Grids to gain acceptance, particularly in industry. The purpose of this paper is to scope these security issues. The Grid Security Infrastructure (GSI), a component of the Globus Toolkit , creates Grid credentials for every user and resource. We describe how this may be extended to securely set up an interactive session on a remote host, and the additional security issues associated
868 partial trust and mutual distrust. New security issues arise when the user may not be trusted, or the user and the host computer's owner are mutually suspicious. I. INTRODUCTION Grid computing    allows computing, storage and other resources that are geographically distributed and belong to different administrative domains to participate in a virtual organization. Resources are
868 The initial request for service must be sent to this service access point accessible outside the ASP’s firewall, and this communication should be compliant with the Open Grid Services Architecture . The ASP site has several computing resources that are available for interactive use. Together, they constitute the resource pool. A firewall F1 protects the ASP's resources from denial-of-service
1189 many sources is accessed and combined “on-the-fly” in a coherent view through a network of mediator components, each specialized in some knowledge domain. Most existing mediator systems, such as , reduce the general mediation concept to centralized architectures that consist of one mediator that integrates many data sources into a coherent view. Such data integration solutions are suitable
1189 the prepare facility in ODBC and JDBC. 3.5.2 Related approaches Query shipping is widely used for query processing in DDBMS ; multidatabase systems, e.g., ; and mediator systems, e.g., , to name a few. Large number of projects have considered query processing issues related to query shipping both for distributed DBMS and centralized mediators. A PMS adds additional complexity to
1177 either an SDPF or an MDPF. 3.3.2 Related approaches The idea of equivalent proxy functions relates to the concept of location transparency in distributed DBMS (DDBMS) , federated DBMS (FDBMS) , and mediator systems . In a DDBMS, for performance and reliability, (partially) replicated relations are distributed among many nodes. A DDBMS provides full location transparency by choosing
2050 services of this environment). The concept “the component offers some quality of service, if the component’s expecations are met by its context” naturally leads to contract-principle of B. Meyer  with pre- and postconditions. Hence, to specify quality of service for components, we first review the term “contractual use” of software components in section 3 and translate Meyer’s
3193 deployment, and a striking visibility over the past few years. There are currently several P2P systems in operation, and many more are under development. Gnutella , Napster  and Freenet  has been among the most prominent peer-to-peer file sharing systems. In these systems, files are stored at the end user machines rather than at a central server, and as opposed to the conventional
3193 anteed a definite answer in a bounded number of network hops. The second generation of routing and location schemes is also considered more scalable. Surprisingly, most of the P2P protocols  to date make the assumption that all nodes tend to participate and contribute equally to the system. Thus, these protocols distribute tasks and place data to peers based on this assumption. However
3193 initialization module works when a new peer joins the network and how the departure or failure of existing peers is handled.s3.1 Overview Similar to most of the second generation of P2P protocols , PeerCQ provides a fast and distributed computation of a hash function, mapping information monitoring requests (in form of continual queries) to nodes responsible for them. It extends consistent
3197 initialization module works when a new peer joins the network and how the departure or failure of existing peers is handled.s3.1 Overview Similar to most of the second generation of P2P protocols , PeerCQ provides a fast and distributed computation of a hash function, mapping information monitoring requests (in form of continual queries) to nodes responsible for them. It extends consistent
3197 the PeerCQ service partitioning scheme extends a randomized partition algorithm, commonly used in most of the current P2P protocols, with both peeraware and CQ-aware capability. As demonstrated in , randomized partitioning schemes are easy to implement in decentralized systems. However they do not perform well in terms of load balancing in heterogonous peer-to-peer environments.We implement
3197 PeerCQ’s ability to efficiently find peers to execute a CQ from a potentially huge number of peers. Inspired by the lookup operations described in Pastry , Tapestry , Plaxton Routing , and Chord , the PeerCQ P2P lookup service is designed to find peers that are most appropriate to execute a CQ in a PeerCQ network in terms of good load balance and better system utilization.
3197 to work done in Chord , Tapestry , and Pastry , the P2P protocol described in this paper is built based on distributed hash table and ideas originated from Plaxton’s routing algorithm . 6 Conclusion We have described PeerCQ, a decentralized peer-topeer Continual Query system for distributed information Monitoring at Internet-scale. PeerCQ is highly scalable, self-configurable and
939 between peers. However, they differ from one another in terms of their lookup services, and the current P2P protocols in these systems are not scalable. Chord , Pastry , Tapestry , CAN  are examples of a second generation of peer-to-peer systems. Their routing and location schemes are based on distributed hash tables. In contrast to the first generation P2P systems such as
939 anteed a definite answer in a bounded number of network hops. The second generation of routing and location schemes is also considered more scalable. Surprisingly, most of the P2P protocols  to date make the assumption that all nodes tend to participate and contribute equally to the system. Thus, these protocols distribute tasks and place data to peers based on this assumption. However
939 and event notification system that demonstrates the benefits of the PeerCQ protocol in building a scalable information monitoring application. There are several P2P protocols proposed so far . Similar to work done in Chord , Tapestry , and Pastry , the P2P protocol described in this paper is built based on distributed hash table and ideas originated from Plaxton’s routing
188 these protocols distribute tasks and place data to peers based on this assumption. However P2P applications should respect the peer heterogeneity and user characteristics in order to be more robust . Another common weakness of the second generation P2P protocols is the lack of flexibility in optimizing the key to peer matching algorithms to incorporate important performance metrics such as
943 model. Files are transferred directly between peers. However, they differ from one another in terms of their lookup services, and the current P2P protocols in these systems are not scalable. Chord , Pastry , Tapestry , CAN  are examples of a second generation of peer-to-peer systems. Their routing and location schemes are based on distributed hash tables. In contrast to the first
943 anteed a definite answer in a bounded number of network hops. The second generation of routing and location schemes is also considered more scalable. Surprisingly, most of the P2P protocols  to date make the assumption that all nodes tend to participate and contribute equally to the system. Thus, these protocols distribute tasks and place data to peers based on this assumption. However
943 initialization module works when a new peer joins the network and how the departure or failure of existing peers is handled.s3.1 Overview Similar to most of the second generation of P2P protocols , PeerCQ provides a fast and distributed computation of a hash function, mapping information monitoring requests (in form of continual queries) to nodes responsible for them. It extends consistent
943 list is 2r +1 and we call r the neighbor list parameter. 3.2 Capability-Sensitive Service Partitioning The PeerCQ protocol extends the existing routedquery based P2P protocols, such as Chord  or Pastry , to include a capability-sensitive service partitioning scheme. Service partitioning can be described as the assignment of CQs to peers. By capability-sensitive, we mean that the
943 ability to efficiently find peers to execute a CQ from a potentially huge number of peers. Inspired by the lookup operations described in Pastry , Tapestry , Plaxton Routing , and Chord , the PeerCQ P2P lookup service is designed to find peers that are most appropriate to execute a CQ in a PeerCQ network in terms of good load balance and better system utilization. It provides two
914 on the shared information in the presence of limited and unreliable information exchange and dynamically changing interaction topologies. Consensus problems have recently been addressed in , to name a few. In , sufficient conditions are given for consensus of the heading angles of a group of agents under undirected switching interaction topologies. In , average consensus
893 L?i] T , where R(·) is the rotation matrix. The simplified kinematic equations can be represented as ?zi = ui. The first example will be based on the virtual leader/virtual structure approach (c.f. ). We simulate the case where the geometric center of the formation follows a trajectory of a circle with radius 5 meters while the whole group preserves the desired hexagon formation shape with
911 turn be defined (s), where R(·) as zd i (s) =  T + R(??0(s))˜z d i0 is the rotation matrix and ˜z d i0 (s) is the specified desired deviation of each robot from the formation center . For example, the desired deviation for the first robot is given by ˜z10 =  T . Note that in this case the parameter s represents the minimum amount of information needed
2248 feature combination can be very expensive. For this reason, our technique here is based on Recursive Feature Elimination (RFE) for feature selection with SVMs. More details of RFE can be found in . The idea of RFE is to train the classifier, compute the ranking criterion for all features, and remove the feature with smallest ranking criterion. By iterating this procedure, we generate nested
751 in a text collection, ignoring punctuation and case. 2. Word stemming. We reduce each word to word-stem form. This is done by using a very large trie-structured morphological lexicon for English (Karp et al, 1992), that contains the standard inflections for nouns (singular, plural, singular genitive, plural genitive), verbs (infinitive, third person singular, past tense, past participle, progressive form),
1755 1, A.D. 1). Relative time differs from distance in that the former has a direction, e.g., one could envision a relative time of -9 hours, whereas a distance is unsigned. Time is multi-dimensional . Valid time concerns the time when a fact is true in reality. The valid time of an event is the wall clock time at which the event occurred in the modeled reality, independent of the recording of
1755 taken by a student varies over time, and the attribute Course is time varying. The value of an attribute may be drawn from a temporal domain. Such temporal domains are termed user-defined time ; other than being able to be read in, displayed, and perhaps compared, no special semantics is associated with such domains. Interestingly, most such attributes are time-invariant. The attribute
1755 a valid-time database . Subsequent proposals, notably Ahn, Ariav, Clifford-1 and McKenzie, have emphasized this fruitful “cubic” analogy. 6sData Model Citation Time Identifier Dimension(s) —  both Ahn Temporally Oriented Data Model  valid Ariav Time Relational Model  both Ben-Zvi —  valid Brooks Historical Data Model  valid Clifford-1 Historical Relational Data
1957 a second level inference mechanism which produces a final decision. The second approach applies to most systems since 1993, and is reffered to as classifier combination. According to Kittler et al , there are two reasons for combining classifiers - efficiency and accuracy. Simple classification tasks may be achieved by using multistage classifiers, such that classification stops once an
3193 then proceeds to build up a neighborhood of more peers by exchanging and forwarding identifying messages. A peer’s neighborhood forms a pool of searchable and downloadable files. Spores and Freenet  fall into this camp. Freenet also promotes the propagation of popular files by caching them along retrieval paths. In this way, popular files becomes more numerous and more readily available to
3195 efficient algorithmic lookup capability. Publius  is an example of a static arrangement that features anonymous file storage through the use of encryption and disbursed partial keys. Oceanstore , Past , and CAN  are examples of file sharing and storage systems based on dynamic naming and lookup techniques such as Tapestry  and Chord . For these, a file name is used to derive a
943 and disbursed partial keys. Oceanstore , Past , and CAN  are examples of file sharing and storage systems based on dynamic naming and lookup techniques such as Tapestry  and Chord . For these, a file name is used to derive a key that is systematically routed toward nodes where the file might reside. The second major type of overlay network may be classified as ad hoc and is
3201 Illinois State University portegys@ilstu.edu network, either by a static topology or by using a naming convention for files and nodes that allows an efficient algorithmic lookup capability. Publius  is an example of a static arrangement that features anonymous file storage through the use of encryption and disbursed partial keys. Oceanstore , Past , and CAN  are examples of file
2313 In this new mobility model, a node moves according to the Random Waypoint Mobility Model . (The nodes are initialized in the steady state distribution of the Random Waypoint Mobility Model .) Then as a node moves into the area of congestion, the node slows down. We use this circular overlay in iNSpect to represent the congested area, validating that the nodes slow in that area. This
2313 histograms and other charts about the node’s positions. For example, a histogram of the volume of nodes at each x-coordinate and y-coordinate allows comparison with what the steady state model  predicts for node position. V. CONCLUSIONS With the increase in wireless network research, visualization and animation of the node behavior, simulations, and results are a must. The iNSpect program
2315 to depict real-world movement , manual diagrams are (almost) impossible to generate. Many researchers currently base node location and movement on the mobility model generated mobility files . These mobility models use mathematical algorithms and statistics to randomly or selectively position nodes and assign speeds throughout the simulation area. As the number of nodes in a simulation
1485 a large part of the modern approaches to obtain positive results for ML=DM algorithms draws its roots in two fundamental bodies, the so-called Probably Approximately Correct (PAC) learning model of , and the Statistical Learning Theory, fathered by Vapnik . The principle is that the examples are drawn from some UNCORRECTED PROOFs1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 TCS
1485 i.e. the inexistence of a ordable practical algorithms to solve these problems. The second is the extension of these results to negative results for learning in models derived from the PAC model of . Some of our results can be extended to negative results on the PAC-derived robust learning model of . This is described later. We now introduce our formalism for the examples and the
1485 framework. 4.4. Beyond computational complexity and ILP It is well known since  that negative results on such problems can sometimes be extended to negative results for PAC-type learning models . Such a model typically brings a statistical and a computational constraint for an algorithm to be quali ed as a learning algorithm. Consider for example De nition 4, and the following learning
1485 any restriction on the size of the formulas, a very seldom result in the huge quantity of theoretical ML results on DNF. Indeed, DNF is one of the most central classes to the PAC learning model of , studied early by Valiant himself , and still raising some of the most important problems in computational learning theory , in particular for its learnability or approximability
2249 for ML=DM algorithms draws its roots in two fundamental bodies, the so-called Probably Approximately Correct (PAC) learning model of , and the Statistical Learning Theory, fathered by Vapnik . The principle is that the examples are drawn from some UNCORRECTED PROOFs1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 TCS 4562 ARTICLE IN PRESS R. Nock / Theoretical Computer Science
2249 before. We have only access to its estimator over the sample collected. This raises statistical issues to evaluate the quality of this estimator (and h), issues discussed in many papers (see e.g. ). The objective of our paper is not to discuss the statistical burden of the theory, but its computational issues. Proving most negative complexity-theoretic results for ML/DM follows a quite
2673 as fast as possible. Several methods for measuring the long-range dependence parameter ? have been proposed (see Appendix A for details of each one). The most well-known methods are the R/S method , variance-time analysis , the periodogram method , the Whittle estimator , and the wavelet method . All these methods except Whittle’s are graphical. Only the last two
2673 traffic, this can be viewed as “zooming out” in time, i.e., since many increments of Xi are juxtaposed in time, zooming out makes the juxtaposed increments appear as one increment, as shown in . Let ? (m) (k) denote the autocorrelation function of X (m) n . Then, Xi is said to be asymptotically second-order self-similar if for k ? 1 lim m?? ?(m) (k) = 1 2 ((k + 1)2H ? 2k 2H + (k ? 1) 2H
2673 with H = 1 2 , the Xi’s are uncorrelated. Hence, throughout this thesis, we consider the range 0 < H < 1 only. The presence of self-similarity in 9snetwork traffic was observed by Leland et al. . Since then, second-order self-similarity has become a dominant framework for modeling network traffic. An example of the self-similar process Yt is fractional Brownian motion, first introduced by
2673 method acknowledges the fact that this process is second-order self-similar. Figure 3.4 displays Ethernet measurements for a local area network traffic at Bellcore, Morristown, New Jersey . It was collected on August 29, 1989 and lasted for about fourteen minutes. Each observation represents the number of packets sent over the Ethernet per 100 ms. This data was considered by Leland
939 of control messages and also significant overhead for member joins/leaves. B. Multicast over P2P Overlays The algorithm described in  describes a broadcast mechanism that operates over CAN . Nodes forward to their neighbors in the d-dimensional space, as this is defined in CAN. There are also provisions made to eliminate duplicate messages and prevent looping of the packets around the
3026 runs. We present results for both random and powerlaw graphs. There has been strong evidence  that connects large-scale unstructured P2P networks to a power-law topology. We utilize the BRITE  and Inet-3.0  topology generators to create the random and power-law graphs respectively. We consider 10K node graphs with average node degrees around 4 (similar to gnutella snapshots ).
1496 algorithm, which has the desirable property that given any set of real-time processes whose total resource usage is less than or equal to 100% of the CPU, it will find a feasible schedule . So EDF is optimal for scheduling processes with total resource usage less than or equal to 1. However EDF does not work well when the resource usage is more than 1. In this case, an extended
1496 achieved based on EDF algorithm. EDF is a dynamic fixedpriority scheduling algorithm, which is optimal of scheduling processes on one non-idle and preemptable CPU with load less than or equal to 1 . In RBED, the EDF-based scheduling algorithm always picks the process with the earliest deadline to run on the CPU and controls the length of the run according to the worst case execution time
2358 Rx ? c. Indeed, various congestion control algorithms in Transmission Control Protocol (TCP) can be interpreted as solving the same utility maximization problem with different utility functions . The authors of  advocate proportional fairness, characterized by Ui(xi) = log xi. In , an allocation policy called minimum potential delay is proposed with Ui(xi) = ?1/xi, which is shown in
2360 = log xi. In , an allocation policy called minimum potential delay is proposed with Ui(xi) = ?1/xi, which is shown in  to approximate the fairness of the TCP on the current Internet. In , the following class of utility functions is proposed Ui(xi, ?) = ? (1 ? ?) ?1 x 1?? i log xi if ? ?= 1 if ? = 1 for ? ? 0. This includes all the previously considered allocation policies – maximum
2360 (one with larger ?) always less efficient (has a smaller aggregate throughput)? This conjecture is prompted by the various examples in resource allocation in the literature in wired networks , , , in wireless Appears in Proceedings of IEEE Infocom, Hong Kong, March 2004. (1) networks, , , in economics, , etc. These examples seem to illustrate (quoted from ) ‘‘the
2360 (L = 1), T (?) is strictly decreasing in ? for the linear network with uniform link capacity. Example 2: Linear network with nonuniform capacity The linear network of Example 1 is considered in  with L = 2, but with different link capacities c1 < c2. The authors calculated the source rates under max-min fairness: x0(?) = x1(?) = c1 2 , x2(?) = c2 ? c1 2 and pointed out that source rate x0
1258 using unlabeled data can be detrimental to the classification accuracy; a phenomenon that has been generally ignored or misinterpreted by previous researchers who observed it empirically before . This new result emphasizes the importance of using correct modeling assumption when learning with unlabeled data. We also present in this paper an analysis of semi-supervised learning for
1258 3–5, 28–32]. Overall, these publications advance an optimistic view of the labeled-unlabeled data problem, where unlabeled data can be profitably used whenever available. 1 This is different from  and , where ? is a parameter that can be set. ?sProbability of error 0.13 0.12 0.11 0.1 0.09 0.08 0.07 10 0 0.06 300 Labeled 3000 Labeled 30 Labeled 10 1 10 2 10 3 Number of Unlabeled records
1258 to the case in which unlabeled data is not used. These cases were not specific to one type of data, but for different kinds, such as sensory data , computer vision , and text classification . To explain the phenomenon, we began by performing extensive experiments providing empirical evidence that degradation of performance is directly related to incorrect modeling assumptions .
1258 much larger, pool of unlabeled data can eventually add enough bias so as to increase classification error. Such a situation is likely to have happened in some of the results reported by Nigam et al , where classification errors go up and down as more unlabeled samples are added. In summary, semi-supervised learning displays an odd failure of robustness: for certain modeling errors, more
1995 their orientation with the neighboring agents in order to better track a target. From the analysis of the cooperative negotiation paradigms for resource allocation presented in literature (see also ), it emerges that they are usually non formal and that the agents have information about others agents; moreover, the negotiation can be with or without explicit social utility information.
1440 second domain model DMtf?idf resulted from statistical analysis of Dataset 1 (described in Section 3.1). In this case, we computed the term frequency - inverse document frequency (tf*idf) score (Salton and Buckley, 1988) of each concept for individual domains. In the case of human annotations, we deal with binary values, whereas tf*idf scores range over the interval . 3 Data and Annotation Experiments We
394 sequences is proposed by Bregler and Malik . Their method is based on twists and exponential maps to recover high degree-of-freedom of motion configurations. The I/CONDENSATION algorithms ,  by Isard and Blake are used to track curves in clutter using stochastic analysis and importance/factored sampling. The methods are superior to previous Kalman filter based approaches, and achieves
868 To address some of the shortcomings in promulgating security-related information on Grids, Grid researchers are working to adopt Web Services technologies as fundamental building blocks . This includes the use of SOAP and WS-Security (www.oasis-open.org) as mechanisms to augment TLS and allow for the attachment and transport of arbitrary security-related information, and WSDL
3010 HTTP request modifications, HTML filtering, user interface improvements especially for small screens, remote caching, and support for disconnected operation and user-selected background retrieval . Other systems have made use of the two-proxy (local and remote) concept for such customizations as filtering, prefetching, and intelligent cache management at the local proxy . In
3018 issue is adaptability, where information is provided to the client application, typically from the operating system, to help it adapt to changes in resource availability and network connectivity . Some of these systems include applications using an adaptable interface, including adaptable protocols. The Internet Content Adaptation Protocol (ICAP)  is a solution developed by an industry
2289 in wireless ad-hoc networks, dominant pruning. 1sI. INTRODUCTION Route discovery in on-demand routing protocols is based on route request (RREQ) and route reply (RREP) messages (e.g., AODV  and DSR ). A request is relayed until it reaches a node with a valid route to the destination or the destination itself, which triggers a reply message sent back to the originator. RREQs are
2722 wireless ad-hoc networks, dominant pruning. 1sI. INTRODUCTION Route discovery in on-demand routing protocols is based on route request (RREQ) and route reply (RREP) messages (e.g., AODV  and DSR ). A request is relayed until it reaches a node with a valid route to the destination or the destination itself, which triggers a reply message sent back to the originator. RREQs are propagated
2734 it has a route to the destination. 6sIV. SIMULATIONS AND PERFORMANCE RESULTS To compare AODV-EDP against other protocols, we use traffic and mobility models similar to those previously reported in . We implemented AODV-EDP in Qualnets, and compare it against AODV-DP ¡¤£ (AODV with Dominant Pruning), and standard AODV with no hello messages. In the simulation scenarios, we vary both the number
2734 AODV. AODV-DP shows that DP alone can improve AODV, but it also shows that there is room for more improvement (i.e., there is some more redundancy that can be eliminated). As pointed out in , the possibility of link failures is low with low mobility, but due to the node move8sPacket delivery ratio Normalized routing load 1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 8 7 6 5 4 3 2 1 0 -1
2050 interfaces need not only be done with finite automata. Many other approaches are equally or more expressive. The notion of pre- and postconditions is used e.g., in the design by contract concept  and in Larch . This powerful concept could easily also be used to define call sequences implicitly. The problem of these predicate based approaches is, that checking protocol compatibility
940 Scribe, a large-scale, decentralized event notification infrastructure built upon Pastry, a scalable, self-organizing peer-to-peer location and routing substrate with good locality properties . Scribe provides efficient applicationlevel multicast and is capable of scaling to a large number of subscribers, publishers and topics. Scribe and Pastry adopt a fully decentralized peer-to-peer
940 of the Pastry routing and object location infrastructure. Section 3 describes the basic design of Scribe and we discuss related work in Section 4. 2 Pastry In this section we briefly sketch Pastry . Pastry forms a secure, robust, self-organizing overlay network in the Internet. Any Internet-connected host that runs the Pastry software and has proper credentials can participate in the overlay
940 routing tables can be restored by exchanging O(log 2 bN) messages. In the following, we briefly sketch the Pastry routing scheme. A full description and evaluation of Pastry can be found in . For the purposes of routing, nodeIds and keys are thought of as a sequence of digits with base 2 b . A node’s routing table is organized into ?log 2 bN? rows with 2 b ? 1 entries each. The 2 b ? 1
940 the node state, i.e., the routing table, leaf set and neighborhood sets, in the presence of node failures, node recoveries, and new node arrivals. The protocol is described and evaluated in . Briefly, an arriving node with the newly chosen nodeId X can initialize its state by contacting a nearby node A (according to the proximity metric) and asking A to route a special message using X
940 sets, updates its own leaf set and then notifies the members of its new leaf set of its presence. Routing table entries that refer to failed nodes are repaired lazily; the details are described in . 2.3 Pastry API In this section, we briefly describe the application programming interface (API) exported by Pastry which is used in the Scribe implementation. The presented API is slightly
3198 local node. In the following section, we will describe how Scribe is layered on top of the Pastry API. Other applications built on top of Pastry include PAST, a persistent, global storage utility .s3 Scribe Any Scribe node may create a topic; other nodes can then register their interest in the topic and become a subscriber to the topic. Any Scribe node with the appropriate credentials for
1350 system can have domainrelevant architecture, i.e. the architecture will be more suitable to processing a certain type of input than others (say, a recurrent network for processing sequential input, Elman, 1993), although the architecture is capable of processing non-sequential input. However, given competing inputs and competing architectural constraints, domain relevance may force the network into
1350 prototype effect)? Although such a simulation has not been run, we already know enough about the general properties of connectionist systems and the specific results of incremental learning (Elman, 1993; Plunkett and Marchman, 1993) to predict what would happen in the case of vocabulary development. Starting out with a small set of image-label pairs will result in strong learning of those pairs,
939 network traffic for two-word and three-word queries. Department of Computer Sciences The University of Texas at Austin (karu,simha,browne)@cs.utexas.edu 1 Introduction “Peer to peer” (P2P) networks  are emerging as potentially important structures for information and content management and distribution. Effective keyword search is a crucial method for information and content management. Recent
939 stored in peer to peer file systems also have an equivalent 2 link structure. The documents make references to other documents in the file system. In systems with bounded search such as CAN , Pastry  or Chord  the GUID (Global Unique Identifier) implements a pointer to each document. All these systems are distributed hash table (DHT) based systems. Systems like Freenet
940 network traffic for two-word and three-word queries. Department of Computer Sciences The University of Texas at Austin (karu,simha,browne)@cs.utexas.edu 1 Introduction “Peer to peer” (P2P) networks  are emerging as potentially important structures for information and content management and distribution. Effective keyword search is a crucial method for information and content management. Recent
940 in peer to peer file systems also have an equivalent 2 link structure. The documents make references to other documents in the file system. In systems with bounded search such as CAN , Pastry  or Chord  the GUID (Global Unique Identifier) implements a pointer to each document. All these systems are distributed hash table (DHT) based systems. Systems like Freenet  while providing
943 network traffic for two-word and three-word queries. Department of Computer Sciences The University of Texas at Austin (karu,simha,browne)@cs.utexas.edu 1 Introduction “Peer to peer” (P2P) networks  are emerging as potentially important structures for information and content management and distribution. Effective keyword search is a crucial method for information and content management. Recent
943 file systems also have an equivalent 2 link structure. The documents make references to other documents in the file system. In systems with bounded search such as CAN , Pastry  or Chord  the GUID (Global Unique Identifier) implements a pointer to each document. All these systems are distributed hash table (DHT) based systems. Systems like Freenet  while providing no bounded
1551 estimating such high-dimensional motions models is that each flow vector has to be estimated from a relatively small number of pixels. This is in contrast to more global, parametric motion models  which remain more reliable as long as the underlying scene motion can be well approximated. For example, inter-frame motions were modeled as homographies in , which estimated their parameters
499 of our approach. I. INTRODUCTION Reduced ordered Binary Decision Diagrams (BDDs) are a data structure for efficient representation and manipulation of Boolean functions. They were introduced in  and are frequently used in formal verification and logic synthesis. In many verification tools methods for functional simulation based on BDDs are used. A crucial point here is the time needed to
499 (denoted by Latin letters) are bound to values in B :? £ 0¤ 1¥ . It is well-known that a Boolean function f : B n ¦ B over the variable set Xn can be represented by a Binary Decision Diagram (BDD) , i.e. a directed acyclic graph where a Shannon decomposition ? f fxi? 1¨ xi fxi? xi § 0 ? 1 ? n© i is carried out in each node. In the following, only reduced, ordered BDDs are considered and for
499 nodes are assumed to be eliminated and variables are encountered at most once and in the same order (the “variable ordering”) on every path from the root to a terminal node. For more details see . Formally, this fixed ordering can be expressed with a mapping £ 1¤???????¤ n¥ ¦ £ x1¤???????¤ xn¥ ?: of each BDD level to a variable. Thus we write ? ?§ k© xi , if variable xi is the k-th element
1028 semantics for the data on the Web. In 1998 the W3C issued a recommendation of a metadata model and language to serve as the basis for such infrastructure, the Resource Description Framework (RDF) . As RDF evolves, it is increasingly gaining attraction from both researchers and practitioners, and is being implemented in world-wide initiatives such as the Open Directory Project, Dublin Core,
1028 graphs. This approach permits to view RDF specifications as data while keeping their knowledge-base semantics. 1.3 Related Work The RDF model was introduced five years ago as a W3C recommendation . Its formal semantics, however, is still an ongoing work . From a logical point of view, Yang and Kifer in , present an F-logic version of RDF. They define two notions of entailment for RDF
1028 languages, this feature appears as a specification of a schema to be used when processing the query . 2. PRELIMINARIES In this section we present the RDF model following the W3C documents . We use the abstract representation of the model as graphs, and do not discuss any serialization of the model (e.g. its XML-based syntax). 2.1 RDF graphs Assume there is an infinite set U (RDF URI
1623 real-time communication services to multimedia applications or “subscribed” Internet users, the proposed network QoS (Quality of Service) infrastructure like DiffServ (Differentiated Service)  reserves network resources for real-time traffic. Within the network QoS architecture, however, the reserved network resources will become a conspicuous target to potential adversaries, and will be
1623 network resources, instead of the server’s resources. III. VULNERABILITY OF RESERVED NETWORK RESOURCE We will now show that the reserved network resource in the QoS infrastructure like DiffServ  is vulnerable to spoofed traffic, and the premium service  provided by an ISP is susceptible to flooding attacks. Two entities associated with the reserved network resource are a given end-host
1350 an artificial language which possessed a number of characteristics that are presumably problematic (in the above sense), and attempted to teach a simple recurrent network to process them (see Elman, 1993 for full account). The artificial language had the following characteristics: 1. grammatical categories: words belonged to different categories (e.g., noun, verb, etc.);sElman Page 8 2. basic
3192 they reach their destinations. The location-independent routing problem has spawned a host of proposals, many of them in the context of data sharing infrastructures such as OceanStore , FarSite , CFS , and PAST . To permit locality optimizations, it is important that the routing process use as few network hops as possible and that these hops be as short as possible. ? This research
3193 and departing nodes while maintaining the above properties. Although clearly desirable, the first property is not guaranteed by existing peer-to-peer systems such as Gnutella  and FreeNet . A simple object location and routing scheme would employ a centralized directory of object locations. Servers would publish the existence of objects by inserting entries into the directory.
3193 the bounds in this table. Recent peer-to-peer applications can locate objects in a dynamic network. Gnutella  utilizes a bounded broadcast mechanism to search neighbors for documents. FreeNet  utilizes a chaotic routing scheme in which objects are published to a set of nearest neighbors and queries follow gradients generated by object pointers; the behavior of FreeNet appears to converge
3195 to node until they reach their destinations. The location-independent routing problem has spawned a host of proposals, many of them in the context of data sharing infrastructures such as OceanStore , FarSite , CFS , and PAST . To permit locality optimizations, it is important that the routing process use as few network hops as possible and that these hops be as short as possible. ?
3195 approach in .sOF6 K. Hildrum, J. D. Kubiatowicz, S. Rao, and B. Y. Zhao 2. The Tapestry Infrastructure Tapestry ,  is the wide-area location and routing infrastructure of OceanStore . Tapestry assumes that nodes and objects in the system can be identified with unique identifiers (names), represented as strings of digits. Digits are drawn from an alphabet of radix b. Identifiers
3197 can solve the nearest-neighbor problem for these networks. We describe our solution in the context of Tapestry, an overlay network infrastructure that employs techniques proposed by Plaxton et al. . 1. Introduction In today’s chaotic network, data and services are mobile and replicated widely for availability, durability, and locality. 1 This has lead to a renewed interest in techniques for
3197 our results in the context of the Tapestry overlay routing and location infrastructure , . Tapestry uses as a starting point the distributed data structure of Plaxton, Rajaraman, and Richa , which we refer to as the PRR scheme. Their proposal yields routing locality with balanced storage and computational load. However, it does not 2 Stretch is the ratio between the distance traveled
3197 as node failures. This paper extends their algorithms to a dynamic network. 1.1. Related Work Several existing object location schemes exhibit routing locality, including those by Plaxton et al. , Awerbuch and Peleg , and Rajaraman et al. . All of these provide the publication and deletion of objects with only a logarithmic number of messages and guarantee a low stretch, where
3197 (Tapestry) O(log 2 n) O(n log n) – O(log n) Yes Awerbuch and Peleg  – O(n log 3 n) O(log 2 n), general O(log 2 n) No RRVV  O(log 3 n) O(n log 3 n) O(log 3 n), general O(log 2 n) Yes PRR  – O(n log n) O(1), special O(log n) Yes PRR + this paper O(log 2 n) O(n log n) O(1), special O(log n) Yes PRR v.0 + this paper – O(n log 2 n) O(log 3 n), general O(log 2 n) No ? In this table n is
3197 identifiers as globally unique identifiers (GUIDs). For a string of digits ?, let |?| be the number of digits in that string. Tapestry inherits its basic structure from the data location scheme PRR . As with the PRR scheme, each Tapestry node contains pointers to other nodes (neighbor links) as well as mappings between object GUIDs and the node-IDs of storage servers (object pointers). Queries
939 actual distance to the object. Worse, it is neither fault tolerant nor scalable, since the directory becomes a single point of both failure and contention. Several recent proposals, Chord , CAN , Pastry , and Viceroy , address the load aspect of this problem by distributing the directory information over a large number of nodes. In particular, they can find an object with a
939 and successor nodes, as well as a logarithmic number of “chords” which cross greater distances within the circle. Queries are forwarded along chords until they reach their destination. CAN  places objects into a virtual, high-dimensional space. Queries are routed along axes in this virtual space until they reach their destination. Pastry  is loosely based on the PRR scheme,
939 D. Kubiatowicz, S. Rao, and B. Y. Zhao Table 1. Comparison of Object Location Systems. ? Scheme Insert cost Space Stretch, metric Hops Balanced? CHORD  O(log 2 n) O(n log n) – O(log n) Yes CAN  O(rn 1/r ) nr – rn 1/r Yes Pastry  O(log 2 n) O(n log n) – O(log n) Yes Viceroy  O(log n) O(n) – O(log n) Yes This paper (Tapestry) O(log 2 n) O(n log n) – O(log n) Yes Awerbuch and Peleg
940 to the object. Worse, it is neither fault tolerant nor scalable, since the directory becomes a single point of both failure and contention. Several recent proposals, Chord , CAN , Pastry , and Viceroy , address the load aspect of this problem by distributing the directory information over a large number of nodes. In particular, they can find an object with a polylogarithmic
940 until they reach their destination. CAN  places objects into a virtual, high-dimensional space. Queries are routed along axes in this virtual space until they reach their destination. Pastry  is loosely based on the PRR scheme, routing queries via successive resolution of digits in a high-dimensional name space. While its overlay construction leverages network proximity metrics, it does
940 Table 1. Comparison of Object Location Systems. ? Scheme Insert cost Space Stretch, metric Hops Balanced? CHORD  O(log 2 n) O(n log n) – O(log n) Yes CAN  O(rn 1/r ) nr – rn 1/r Yes Pastry  O(log 2 n) O(n log n) – O(log n) Yes Viceroy  O(log n) O(n) – O(log n) Yes This paper (Tapestry) O(log 2 n) O(n log n) – O(log n) Yes Awerbuch and Peleg  – O(n log 3 n) O(log 2 n), general
940 repeat distance measurements to compare those neighbors with its own, replacing further away nodes where appropriate. This is the same idea as the heuristic neighbor table building algorithms in  and . In all these cases, when a new primary neighbor has been chosen, the node needs to move some object pointers. This can be done efficiently as described in Section 4.2. Note that such
3198 The location-independent routing problem has spawned a host of proposals, many of them in the context of data sharing infrastructures such as OceanStore , FarSite , CFS , and PAST . To permit locality optimizations, it is important that the routing process use as few network hops as possible and that these hops be as short as possible. ? This research was supported by NSF
943 of the actual distance to the object. Worse, it is neither fault tolerant nor scalable, since the directory becomes a single point of both failure and contention. Several recent proposals, Chord , CAN , Pastry , and Viceroy , address the load aspect of this problem by distributing the directory information over a large number of nodes. In particular, they can find an object with
943 could be used to find small stretch routing tables and/or answer approximate distance queries in arbitrary metric spaces. 3 Most of the recent work on peer-to-peer networks ignore stretch. Chord  constructs a distributed lookup service using a routing table of logarithmic size. Nodes are arranged into a large virtual circle. Each node maintains pointers to predecessor and successor nodes,
943 gives a metric space.sOF4 K. Hildrum, J. D. Kubiatowicz, S. Rao, and B. Y. Zhao Table 1. Comparison of Object Location Systems. ? Scheme Insert cost Space Stretch, metric Hops Balanced? CHORD  O(log 2 n) O(n log n) – O(log n) Yes CAN  O(rn 1/r ) nr – rn 1/r Yes Pastry  O(log 2 n) O(n log n) – O(log n) Yes Viceroy  O(log n) O(n) – O(log n) Yes This paper (Tapestry) O(log 2 n)
3195 applications. The web content harvested by a distributed crawler can be indexed by decentralized search infrastructures, or archived using a persistent storage infrastructure such as OceanStore. Here, we focus our discussion on crawling and do not address the orthogonal issues of persistent storage and indexing. The distributed crawler harnesses the excess bandwidth and computing
3195 distributed graph structures over the Internet . Such a generalized crawler can be used to run queries over the link structure or meta-data of P2P networks such as Gnutella  and OceanStore . The rest of this paper is organized as follows. Section 2 provides a high-level overview of the distributed crawler. In Section 3, we describe a detailed description of the crawl query execution
943 they require a central coordinator, they are not truly P2P, and the crawl process is entirely under the control of Grub.sOf most relevance to our work are DHT-based web crawlers built using Chord . These crawlers propose a single partition scheme, and do not examine the tradeoffs between different schemes, nor do they investigate rate throttling of sites. Since they experiment with only a
2243 units that alert peers of new nodes joining the system, and of who has which blocks. The rate of updates passed along each link per second, ?, is subject to an AIMD flow control algorithm ,  which additively increases and multiplicatively decreases update rates depending on available bandwidth estimates. The intuition behind controlling the update rate in this manner is the
943 the Slurpie system grows to the point where this is insufficient, the topology server functionality could be distributed. Specifically, a number of hosts that provide this service could form a DHT , , , and the file name could be used to look up the server responsible for the specific file. However, we do not believe the scalability of the topology server will be the limiting factor
939 system grows to the point where this is insufficient, the topology server functionality could be distributed. Specifically, a number of hosts that provide this service could form a DHT , , , and the file name could be used to look up the server responsible for the specific file. However, we do not believe the scalability of the topology server will be the limiting factor in the
940 system grows to the point where this is insufficient, the topology server functionality could be distributed. Specifically, a number of hosts that provide this service could form a DHT , , , and the file name could be used to look up the server responsible for the specific file. However, we do not believe the scalability of the topology server will be the limiting factor in the
177 characteristics and requirements. Our solution generalizes existing protocols with a single id space, thus leveraging prior work on all aspects of structured p2p overlays, including secure routing . The rest of this paper is organized as follows. Section 2 describes in detail the design of our system and explains how messages are routed across multiple rings. Section 3 discusses the costs,
177 system does not require the use of a specific, new structured overlay protocol. This allows us to leverage existing work, for instance on secure routing in the presence of malicious participants . The nodeId certificates used in this work can be extended to bind a node’s IP address to both its nodeId and ringId. When a node joins an anycast group or offers to forward a request into a
939 Pastry form a ring. However, we emphasize that our design can be equally applied to structured overlay protocols whose identifier spaces do not form a ring, including CAN, Tapestry, and Kademlia . A multi-ring protocol stitches together the rings and implements global routing and lookup. To applications, the entire hierarchy appears as a single instance of a structured overlay network that
940 overlay networks provide a decentralized, self-organizing substrate for distributed applications and support powerful abstractions such as distributed hash tables (DHTs) and group communication . Most of these systems use randomized object keys and node identifiers, which yields good load balancing and robustness to failures. However, in such overlays, applications cannot ensure that a key
943 overlay networks provide a decentralized, self-organizing substrate for distributed applications and support powerful abstractions such as distributed hash tables (DHTs) and group communication . Most of these systems use randomized object keys and node identifiers, which yields good load balancing and robustness to failures. However, in such overlays, applications cannot ensure that a key
375 short term, while allowing configuration to evolve gradually over the longer term. The importance of policy-based configuration management, to this task, has been expounded since the early 1990???s. Policy is an important tool for ensuring the security and consistency of configured systems. It involves a specification of essentially arbitrary decisions associated with a configuration, using a
375 sources, their model encapsulates such changes also to a certain degree. The use of declarative languages for the purpose of configuration description has been considered by many authors. Convergence to a dynamic equilibrium is central to cfengine behaviour. Health is defined as a matter of policy combined with machine-learned normality. In recent work, cfengine has been combined
1069 set up and maintain, and often needs adjustments for new data sets. We have recently developed (and implemented within Febrl) new probabilistic techniques  based on hidden Markov models (HMMs)  which achieved better standardisation accuracy and are easier to set-up and maintain compared to popular commercial linkage software. A HMM is a probabilisticsnite-state machine consisting of a set
1069 set of discrete, hidden (unobserved) states, a matrix of transition probabilities between those hidden states, and a matrix of probabilities with which each hidden state emits an observation symbol  (this emission matrix is also called an observation matrix ). In our case, the hidden states of the HMM will correspond to the outputselds of the standardised addresses. The Febrl approach to
1069 symbols in the HMM used in step 3. 7 3. The list of tags is given to a HMM, and assuming that each tag (observation symbol) has been emitted by one of the hidden states, the Viterbi algorithm  willsnd the most likely path through the HMM, and the corresponding hidden states will give the assignment of the element from the input list to the outputselds. Consider for example the address
943 spawned. This precludes some higher-order uses of cords used in later examples. We consider this a defect and intend to implement global lookup using standard distributed hash table technology , and foresee no problems doing so—in fact, it will simplify several aspects of our current implementation.) Cords can’t produce effects, so the final piece of our network is the concept of a
868 the concepts described in OGSA, including Grid Services. The version 3 of the Globus Toolkit (GT3) has a new philosophy of Grid services and implements the Open Grid Service Infrastructure (OGSI) . Similar to the GT3 philosophy is the Semantic Grid proposal . This architecture adopts a serviceoriented perspective in which distinct stakeholders insthe scientific process, represented as
173 amount of files to share with other users. Thus we did not see the general free-riding behavior seen in other P2P populations, where most users download files and only a few users share and upload . Surprisingly, with another P2P application, Kazaa, which does not enforce sharing, we saw more outbound than inbound traffic. The reasons for this result are unclear, but it may be the presence of
187 In fact, of the 147 cards that saw more than 1MB of P2P traffic, a mere 10 cards (6.8% of the population) were responsible for over 50% of the traffic. This behavior has been observed elsewhere . 5.3 Streaming media The proportion of wireless streaming audio/video traffic increased by 405% between 2001 and 2003/4, and we saw over 129GB of streaming traffic in our 2003/4 trace. Most, but
187 and user persistence. As they used SNMP, with a five minute poll period, their data lack the precision of our syslog trace, and Section 6 shows that our results were very different. Saroiu et al.  traced all HTTP and P2P traffic at the University of Washington border routers for nine days in 2002. P2P dominates, accounting for 43% of the traffic, compared to 14% for web traffic. We found
939 (P2P) systems. Various application-level multicast implementations have been proposed , most of which are directly implemented on top of P2P infrastructures (Chord , CAN  or Pastry ). The good scalability of the underlying P2P networks give these application-level multicast one of the properties of the original IP Multicast service, that of serving content to a
940 Various application-level multicast implementations have been proposed , most of which are directly implemented on top of P2P infrastructures (Chord , CAN  or Pastry ). The good scalability of the underlying P2P networks give these application-level multicast one of the properties of the original IP Multicast service, that of serving content to a virtually
943 Peer-to-Peer (P2P) systems. Various application-level multicast implementations have been proposed , most of which are directly implemented on top of P2P infrastructures (Chord , CAN  or Pastry ). The good scalability of the underlying P2P networks give these application-level multicast one of the properties of the original IP Multicast service, that of serving
1496 imposed by the OSEK standard  for automotive operating systems. Rate monotonicity simply asserts that tasks with smaller periods are assigned higher priorities than tasks with greater periods . 5.2 Variables and Message Slots The operational system abstraction/view, as sketched in § 3, contains the transition from the hierarchic and connected SSD components to a clustered system view
2679 queuing model, where both job arrival and job size distribution are not Poisson, and there are n servers serving a queue with limited capacity m. A web server is an example. Previous research  has shown that Poisson processes are valid only for modeling the arrival of user-initiated TCP sessions such as the arrival of TELNET connections and FTP connections. HTTP arrivals are not Poisson.
317 into the control flow of the program and assist in the design of cache prefetching strategies. 3.3 RDF Mining Market-basket analysis. We apply data mining techniques for Market-Basket Analysis  to solve the above pattern discovery problems. These techniques are developed to find the association rules  or frequent item set sequences  in a database of customer transactions to
317 must contain the items in an association rule and ÓÒ????Ò ? for rules (of the form “A implies B”) which specifies the fraction of baskets that contain B, from among the baskets that contain A. See  for further details on association rule mining. The input data for sequential pattern mining is a collection of customer-sequences. Each customer-sequence is an ordered sequence of transactions
318 techniques for Market-Basket Analysis  to solve the above pattern discovery problems. These techniques are developed to find the association rules  or frequent item set sequences  in a database of customer transactions to answer the question of which items are frequently purchased together. A savvy marketer can then use the knowledge of frequent item sets for marketing
318 looks for sequences supported by at least ÒÙÑ?Ù×ØÓÑ?Ö× £ × customers. A customer ×ÙÔÔÓÖØ× a sequence ×?Õ if ×?Õ is contained in the customer-sequence for this customer. The readers are referred to  for further details. Problem Mapping. We can easily map our problems into a Market-Basket Analysis (MBA) framework. Problems PD1, PD2 and PD3 are mapped into finding frequent item sets in MBA, and
318 pattern uses the FPgrowth algorithm , a very fast algorithm for finding the large frequent item sets. Sequential property query pattern discovery uses the AprioriAll algorithm introduced in . 4 Experiments 4.1 Initial Experiment: Mining MusicBrainz A preliminary experiment with Stor-Mine was performed on a subset of MusicBrainz , an open-source database about music compact discs.
1069 (7)sIEEE TRANSACTION PAMI 12 C. Hidden Markov models Nowadays, hidden Markov models (HMMs) are basic components of the most successful natural language processing tasks, including speech , ,  and handwritten text recognition , , speech translation ,  and shallow parsing , to name but a few. HMMs have also proved useful in many other pattern recognition and
1069 financial returns modeling . There exist many variants of Markov models, including differences as to whether the symbols are emitted at the states or at the transitions. See for example , , , . Definition 2: AHMM is a 6-tuple M = ?Q, ?, I, qf, T, E?, where • Q is a finite set of states, • ? is a finite alphabet of symbols, • T:(Q?{qf}) × Q ? R + is a state to state transition
1069 have attempted to learn, infer, identify or approximate PFA from a given set of data. This task, often called language modeling  is seen as essential when considering pattern recognition , machine learning , computational linguistics  or biology . The general goal is to construct a PFA (or some alternative device) given data assumed to have been generated from this
1485 condition for learning a given class of model. If this condition is not met, that means that some target is not learnable. A second learning paradigm was proposed by Valiant and extended later ???. This paradigm, called probably approximatively correct (PAC) learning, requires that the learner returns a good approximation of the target with high probability. The words good and high are
317 can be e.g. local regularities in the data, or parsimonious summaries of subsets of data . The most prominent examples of interesting patterns are frequent itemsets and association rules . Let D be a bag (i.e., a multi-set) consisting of finite subsets (called transactions) of a set I of items. The bag D called a transaction database. An itemset X is a subset of I. The frequency fr
317 as the actual end product, most of the attention has been devoted for finding collection of frequent itemsets  since association rules can be produced from them as a simple post-processing step . The proposed inductive databases framework fits perfectly for mining frequent itemsets and association rules: Itemsets and association rules can be seen as queries. The query evaluation result for
592 property prohibits a routing service in sensor networks from being constructed in a highly sophisticated way such as the ones in extensible routing architectures for traditional networks (, , , , and ). Sensor networks should favor a simple structure for a programmable routing service. Figure 1 shows the sensor node architecture with the proposed programmable routing
592 protocol together with the flooding protocol described in Section 4.1 complete an automatic configuration for a routing service in a sensor network. 5. Related Work Scout , Router-Plugins , Click  and Internet Exchangeable Architecture  study extensible router architectures  to meet the needs of sophisticated services in the Internet. These routers follow powerful and
483 (air, water, soil, chemistry), precision agriculture, transportation, factory instrumentation and inventory tracking, condition based maintenance, smart spaces, and military surveillance (, , , , and ). The goal of a sensor network is to collect, process, and forward sensed data to other sensor nodes and/or base stations. Therefore, a routing service is essential to
483 of a sensor field. If there are no base stations available, one natural replacement is beacon nodes used in self-localization systems  or header nodes in any other self-clustering system . These nodes are dynamically maintained so that each sensor node is close to at least one such node. Deployment of routing services can be accomplished by deploying these high-level nodes first. In
2305 routing service is essential to sensor network applications. Several non-traditional routing services have been proposed for sensor networks. They include Greedy Perimeter Stateless Routing (GPSR) , Geographical Energy Aware Routing (GEAR) , Trajectory Based Routing (TBF) , Directed-diffusion , TwoTier Data-Dissemination (TTDD) , Content-Based MulSteven Berson, Bob Braden
497 write their own in-network processing modules and add them to the network in the form of filters. One good example of such application-specific filters is the information-driven tracking filter ( and ) that uses tracking information to conserve energy. Our proposed framework is related to this architecture. For example, the universal routing service can be considered a routing filter in
183 evolving social constructs of the web. Indeed, social network theory and economics have considered a variety of facets of this general subject . 1.2 Introducing distrust Recent work  give a mathematical approach to the propagation of trust, but does not extend to the case in which users may also express distrust. However, experience with real-world implemented trust systems
183 looking at the problem of propagating trust through networks. Yu and Singh () propose a framework which, inscontrast to our work, assumes symmetry and arbitrary transitivity. Kamvar, et. al.  consider trust propagation in a peer-to-peer environment and provide an approach that is close to ours, without the incorporation of distrust. In general, most of the work on trust propagation has
1445 and that the other basis elements would perhaps in some limited circumstances provide value. However, the value of co-citation has been proven for web pages by the success of the HITS algorithm , so we included it and the other basis elements. 3 We insist that i make a Boolean decision about j. This is so that we can measure the efficacy of our algorithms against real data and does not
3026 Tseq ( L) = . MaximalEventRateOnEachNode 4.2 Experimental Setup: Topologies, Traffic load, and Simulation Engines We generate network topologies for our experiments with an adapted BRITE tool , a degree-based Internet topology generator following the Power-Law. The flat network topology includes 20,000 routers and 10,000 hosts, which are spread over a geographic area of
3026 in use, hierarchical and degreebased. Hierarchical generator like Tiers and Transit-Stub are more close to the logical structure of the Internet. The degreebased generators like Inet, BRITE, and PLRG generate graph that follow the Power-Law and are more close to the physical connectivity of real Internet, but have less clear hierarchical structure. The latest GridG generator
2722 in hazardous locations, for example in disaster areas to aid rescue efforts, for mineral or oil prospecting, in defense applications in the battlefield etc. Most of the previous routing protocols ( ???) for wireless ad-hoc networks concentrate on finding and maintaining routes in the face of changing topology caused by mobility or other environmental changes. Typical protocols use shortest
2289 locations, for example in disaster areas to aid rescue efforts, for mineral or oil prospecting, in defense applications in the battlefield etc. Most of the previous routing protocols ( ???) for wireless ad-hoc networks concentrate on finding and maintaining routes in the face of changing topology caused by mobility or other environmental changes. Typical protocols use shortest path
1755 in the relational model. As a result, queries on time-varying data are often hard to formulate in SQL . None of the models have one of the many time-extended relational models proposed  as their implementation model. The temporal relational models have data-de nition and query-language capabilities that better support the management of temporal data and would thus constitute
1255 has to decide whether to request the corresponding true labels for sequentially presented single examples (stream-based model). The well-studied Bayesian query-by-committee (Seung et al., 1992; Freund et al., 1997) approach considers the latter scenario and decides to request class labels based on the disagreement within a set of Gibbs classifiers. 6. Conclusion We introduced novel extensions of pool-based
939 overlay network. The effort on application-specific overlay networks has targeted on widely usable applications such as multicasting, , , , ,  and peer-to-peer file sharing. Several other work has been dedicated to proposing a general overlay service networks that can be used to provide value-added service for a variety of application-layer services, such as
943 of overlay network. The effort on application-specific overlay networks has targeted on widely usable applications such as multicasting, , , , ,  and peer-to-peer file sharing. Several other work has been dedicated to proposing a general overlay service networks that can be used to provide value-added service for a variety of application-layer services, such as
943 – intelligent query routing and network topologies are required to be able to route queries to a relevant subset of peers that are able to answer the queries. Modern routing protocols like Chord  and CAN  allow for sophisticated query routing based on distributed indices. More recently, in the Semantic Web context, schema based Peer-to-Peer networks such as the one described in  have
939 query routing and network topologies are required to be able to route queries to a relevant subset of peers that are able to answer the queries. Modern routing protocols like Chord  and CAN  allow for sophisticated query routing based on distributed indices. More recently, in the Semantic Web context, schema based Peer-to-Peer networks such as the one described in  have emerged
2289 daemons on his laptop and PDA that detect when no infrastructure-mode WiFi access point or DHCP service is available, and automatically switch into ad-hoc mode using a routing protocol such as AODV . Only the most dedicated, desperate, or geeky will go to this trouble, however. To most users, having a “working” network means being able to get to Google, CNN, and Amazon.com. Any “ubiquitous”
2289 remains the same in IPv6. 2.3 Ad-Hoc Networks Do Not Scale Classic distance-vector  and link-state  routing protocols, as well as ad-hoc routing variants such as DSR  and AODV , require every node to store and regularly exchange information about every other node in the network. This linear per-node storage and/or bandwidth overhead limits the scalability of these
2289 as well as resilient. Ad-hoc routing protocols such as DSR  address the management problem at the local level but are not scalable to Internet-wide adhoc networks. Landmark  and AODV  offer scalability under localized traffic patterns, but not under the global traffic patterns of the Internet. 5 UIP node identities are similar to those of Moskowitz’s proposed Host Identity
2249 features extracted from the dataset, we employed the following machine learning approaches: k-nearest neighbor rule , decision tree , neural network , and SVM (Support Vector Machine) . We performed 10-fold cross validation on each learning algorithm with a particular parameter setting and measured precision, recall, and F-measure to verify the effectiveness of each machine
695 engineering, the (re-)use of software modules () illustrates the strategy of reusability. The reusability of (software) objects is also the major claim of the object-oriented approach (eg ). Most approaches to information systems development, including the ones which support the reuse of products, demand for an actual replacement of the information system by another information
1755 point of times are called the recording time of events. Our notion of event time and recording time is identical to the notions of valid time and transaction time, respectively, as discussed in . (The reason for this renaming is that the new names correspond better to the level architecture we will introduce in section 6.) The classification which is made in  is based on the basis of
1755 if there is a change of state in the information system due to an update, the former state cannot be ‘remembered’ by the information system. Such systems are refered to as snapshot systems (). Starting from a particular state, update in snapshot systems is performed by an addition, deletion or modification of (pieces of) information in that state, resulting in a new state without
2722 the same path, they should encounter the same network conditions. Sharing a path also shields the potential discrepancy of route discovery for different paths. We use Dynamic Source Routing (DSR ) as the underlying routing protocol. 2 There are many existing studies in enhancing TCP performance in MANET, e.g. TCP-ELFN . Similar techniques may be applied to TFRC as well. In this paper,
2151 For instance, Figure 6 shows the measured 10-second averaged link bandwidth from node 1 to 2 in the 5-node chain scenario with 50Kbps background traffic (using the bandwidth measurement method in , ). Unlike wireline networks where a physical link’s bandwidth is constant, in MANET, a wireless link’s effective bandwidth is time-varying, depending on channel contention and signal fading.
2154 instance, Figure 6 shows the measured 10-second averaged link bandwidth from node 1 to 2 in the 5-node chain scenario with 50Kbps background traffic (using the bandwidth measurement method in , ). Unlike wireline networks where a physical link’s bandwidth is constant, in MANET, a wireless link’s effective bandwidth is time-varying, depending on channel contention and signal fading. This
2142 There are two general approaches of multimedia streaming over MANET. The first approach adopts softstate reservation to protect multimedia traffic from besteffort traffic, e.g., INSIGNIA , SWAN  and dRSVP . INSIGNIA is a resource signaling protocol to support end-to-end adaptive services, such as multimedia flows with a base layer and an enhanced layer. The bandwidth reservation status
2312 in Figure 2. GPSR routes perimeter mode packets on a planar subgraph of the network connectivity graph, in which there are no crossing edges. A perimeter is a face of this planar graph. Bose et al.  also present an algorithm that uses planar network subgraphs to recover from greedy forwarding failure. GPSR originates packets in greedy mode, but changes them to perimeter mode when no neighbor
940 that meets those criteria, the Geographic Hash Table (GHT). GHT is inspired by the new generation of Internet-scale Distributed Hash Table (DHT) systems such as Chord, CAN, Pastry, and Tapestry . In these systems, a data object is associated with a key and each node in the system is responsible for storing a certain range of keys. A name-based routing algorithm allows any node in the
940 scalable DCS system must meet, and our geographic hashing approach to DCS architecture that meets these design goals. 3.1 Storage Abstraction Like the many distributed hash table systems before it , DCS provides a (key, value)-based associative memory. Events are named with keys. Both the storage of an event and its retrieval are performed using these keys. DCS is naming-agnostic; any naming
940 and query load for distinct k values evenly across the area covered by a network. The service provided by GHT is similar in character to those offered by other distributed hash table systems . However, as is the case with those systems, much of the nuance to the GHT system design arises specifically to ensure robustness and scalability in the face of the many sorts of failures possible
940 Results are the means of eight simulations. the network by GPSR in our simulations; we are evaluating GHT, not the underlying routing system, as is the practice in the evaluation of DHT systems . GPSR generates a constant volume of routing protocol traffic (beacons) per node, regardless of system size in nodes ; this load is of lower order than that generated by GHT, which sends
940 on landmark routing, GHT uses GPSR, a flat routing algorithm wherein nodes are addressed with geographic coordinates. Although GHT was inspired by Distributed Hash Table systems like Chord and CAN , we did not adopt the routing algorithms used in these systems. These algorithms require nodes to be interconnected in a fairly rigid manner. On the Internet, node neighbor relationships are at the
2305 the storage node for an arbitrary key. This enables nodes toput andgetsfiles based on their key, thereby supporting a hash-table-like interface. GHT uses the GPSR geographic routing algorithm  as the underlying routing system to provide a similar hash-table-like functionality in sensornets. Our paper has 7 sections. We start with a brief discussion of data dissemination in sensornets in
2305 replication, whereby events that hash to the same home node can be divided among multiple mirrors. 4. ALGORITHMS We proceed now to describe the algorithms that comprise GHT. GHT is built atop GPSR , a geographic routing system for multi-hop wireless networks. After briefly reviewing the features of GPSR’s design relevant to GHT, we identify a previously unexploited characteristic of GPSR that
2305 at the home node, as no neighbor of the home node can be closer to the destination. The packet then traverses the entire perimeter that encloses the destination, before returning to the home node . We name this perimeter the home perimeter. Under GHT, the home node knows to consume the packet when it returns after this tour of the home perimeter.sx y Figure 1: Greedy Forwarding Example: x
2293 wishes to transmit packets to a destination node with a known non-geographic address; a sender must map the destination’s identifier to its current location using a location database, such as GLS . Under GHT, however, the originator of a Put() or Get() packet does not know the identifier of the node that is the eventual destination of the packet. As sketched in Section 3.3, the originator of
2293 structured replication (SR) to address this scaling problem. In SR we augment event names with a hierarchy depth and use a hierarchical decomposition of the key space (similar to that used in GLS ). Let us name the single location GHT hashes a key name into the root of that key. Now, for a given root r and a given hierarchy depth d, one can compute 4 d ? 1 mirror images of r; d = 0 refers to
2293 regardless of system size in nodes ; this load is of lower order than that generated by GHT, which sends packets on paths of length O( ? n). Moreover, there is no location database like GLS  used with GHT, as GHT sends no traffic to node IDs. 5.1.1 Stable and Static Nodes As one would expect, on static networks, where the topology doesn’t change, GHT offers very nearly perfect
2293 routed to the requestor, it may be aggregated by intermediate nodes). Directed diffusion doesn’t require geographic information; it uses flooding. The Geographic Location System (GLS) used in GRID  can be augmented to provide a DCS-like service. Geographic routing delivers packets to locations, not addresses; thus, a packet sender must be able to map a destination’s identifier to its
939 that meets those criteria, the Geographic Hash Table (GHT). GHT is inspired by the new generation of Internet-scale Distributed Hash Table (DHT) systems such as Chord, CAN, Pastry, and Tapestry . In these systems, a data object is associated with a key and each node in the system is responsible for storing a certain range of keys. A name-based routing algorithm allows any node in the
939 scalable DCS system must meet, and our geographic hashing approach to DCS architecture that meets these design goals. 3.1 Storage Abstraction Like the many distributed hash table systems before it , DCS provides a (key, value)-based associative memory. Events are named with keys. Both the storage of an event and its retrieval are performed using these keys. DCS is naming-agnostic; any naming
939 and query load for distinct k values evenly across the area covered by a network. The service provided by GHT is similar in character to those offered by other distributed hash table systems . However, as is the case with those systems, much of the nuance to the GHT system design arises specifically to ensure robustness and scalability in the face of the many sorts of failures possible
939 Results are the means of eight simulations. the network by GPSR in our simulations; we are evaluating GHT, not the underlying routing system, as is the practice in the evaluation of DHT systems . GPSR generates a constant volume of routing protocol traffic (beacons) per node, regardless of system size in nodes ; this load is of lower order than that generated by GHT, which sends
939 on landmark routing, GHT uses GPSR, a flat routing algorithm wherein nodes are addressed with geographic coordinates. Although GHT was inspired by Distributed Hash Table systems like Chord and CAN , we did not adopt the routing algorithms used in these systems. These algorithms require nodes to be interconnected in a fairly rigid manner. On the Internet, node neighbor relationships are at the
943 scalable DCS system must meet, and our geographic hashing approach to DCS architecture that meets these design goals. 3.1 Storage Abstraction Like the many distributed hash table systems before it , DCS provides a (key, value)-based associative memory. Events are named with keys. Both the storage of an event and its retrieval are performed using these keys. DCS is naming-agnostic; any naming
943 and query load for distinct k values evenly across the area covered by a network. The service provided by GHT is similar in character to those offered by other distributed hash table systems . However, as is the case with those systems, much of the nuance to the GHT system design arises specifically to ensure robustness and scalability in the face of the many sorts of failures possible
943 Results are the means of eight simulations. the network by GPSR in our simulations; we are evaluating GHT, not the underlying routing system, as is the practice in the evaluation of DHT systems . GPSR generates a constant volume of routing protocol traffic (beacons) per node, regardless of system size in nodes ; this load is of lower order than that generated by GHT, which sends
943 on landmark routing, GHT uses GPSR, a flat routing algorithm wherein nodes are addressed with geographic coordinates. Although GHT was inspired by Distributed Hash Table systems like Chord and CAN , we did not adopt the routing algorithms used in these systems. These algorithms require nodes to be interconnected in a fairly rigid manner. On the Internet, node neighbor relationships are at the
3019 behalf, thus freeing programmers from this burden. To make sound fidelity decisions, it exploits history-based prediction of application resource usage. Our implementation is based on Odyssey , which originally supported the concept of fidelity for stored data. This work extends that concept to the broader notion of computational fidelity and demonstrates its applicability to a new class
3019 server, receiving results from it, round trip time of one or more RPCs, and server-side computation. Network bandwidth and round trip time estimates are provided by the base Odyssey infrastructure . The predictor also computes the effects of VM paging and remote file access ; for brevity, we do not discuss these. The default generic latency predictor can be overridden at runtime by an
3019 latency to our resource predictors); and by replacing the middleware server with a library implementation. 5 Related work This work is most closely related to previous work on fidelity adaptation . We have generalized these previous notions of fidelity, which only measured data degradation, to include arbitrary runtime parameters of an application. Our system and API also move the burden of
743 al, 1993), while most resource discovery systems in use in the Internet, such as WAIS, Gopher and WWW, are based on some simple form of combination of the two retrieval strategies (see for instance Bowman et al, 1994). A principled integration of these techniques seems indeed to be one major factor involved in the construction of more general retrieval systems, although other knowledge sources and interaction
743 and Jauslin, 1983; Pedersen, 1993). However, this is still far from a tight and principled combination. ULYSSES takes one step further, similar to the systems described in Godin et al, 1993 and Bowman et al, 1994. The different retrieval methods work on a single retrieval space and share their intermediate results, so that the user does not have to commit himself to one particular method and does not have
1633 of single server nodes and established necessary and sufficient conditions for this heuristic to be asymptotically efficient. The balanced likelihood ratio approach to importance sampling (see Alexopoulos and Shultes 1998, 2001) was developed for the analysis of rare events in fault-tolerant repairable systems. In that context, importance sampling estimators have been shown to guarantee variance reduction and yield
1633 Ratios The proposed importance sampling approach is an extension of the balanced likelihood ratio approach developed for estimating the reliability of fault-tolerant repairable systems (see Alexopoulos and Shultes 1998, 2001). In that case, all system events are classified into two classes: events that move the system “closer” to system failure, i.e., component failure events, and events that move the system
795 1 Introduction Grid Computing  envisions a future where heterogeneous resources could be shared by users across geographical and administrative boundaries, and as a utility. Several efforts  are underway to architect and deploy a middleware infrastructure for Grid Computing. Commercial acceptance of Grid Computing technology is also steadily increasing. Traditional use of Grid
795 High level overview of the Interactive Grid computing system Our architecture is proposed as an extension to the existing Grid middleware infrastructure. Our implementation uses Globus Toolkit 2.0  as this Grid middleware infrastructure. We believe our proposed solution provides a comprehensive access and admission control methodology for graphical interactive sessions in a Grid context. The
795 running Red Hat Linux 7.3, as the remote nodes. The end-user can request a graphical interactive session to these remote nodes from any machine supporting a web browser. We use Globus Toolkit 2.0  as the Grid middleware platform, and VNC  as the remote display technology for remote graphical sessions. GPDK  is used to provide a web portal to the end-user for submitting job requests. We
2305 printing job to the nearest printer. In sensor networks, the sensor nodes need to know their locations in order to detect and record events, or to route packets using geometric-aware routing, e.g., . One method to determine the location of a node is manual configuration. However, this is unlikely to be feasible for any large-scale deployment or when nodes move often. Another possibility is GPS
483 in connectedness above the phase transition at which the radius is minimal for total localizability. VII. RELATED WORK Network localization is an active research field, e.g., , , , , , , , , , , , , , , , , . The previous approaches can be classified into two types: coarse-grained and fine-grained. percentage localized nodes 1
415 the generalized eigenvector problem. Tsuda et al.  considered the kernel matrix completion problem as a missing data problem and developed a parametric approach by using the em algorithm  based on the information geometry of positive definite matrices. Recently, Kandola et al.  extended the alignment optimization method from the transductive setting to the inductive setting.
415 . , K, and also that at least one of the ?k’s is nonzero. If ? is defined by (13) or (14), then ? ? n. The proof is in the appendix. As in traditional mixture models , we shall assume that ?k ?  and ? K k=1 ?k = 1 in the sequel. Moreover, in the extreme case when ?j = 1 (for j ? {1, . . . , K}) and ?k = 0 for k ?= j, it is easy to see that (10) and (13) (or (14)) degenerate to ? = ?j and ?
416 solving convex programming problems such as a semi-definite programming (SDP) problem are still very computationally expensive on large data sets. Thus, instead of using SDP, Bousquet and Herrmann  proposed a simple, efficient gradient-descent algorithm that can be orders of magnitude faster than a typical SDP solver. Besides, Crammer et al.  also proposed another kernel matrix learning
416 kernel learning problem is then reduced to an optimization problem. Their method can work in the inductive setting. 1.3 Research Agenda of this Paper Many existing kernel matrix learning methods  constrain the target kernel to a weighted combination of some fixed base kernels. The learning problem can thus be simplified to the estimation of the weighting coefficients only. Motivated by
416 Learning a Generative Model of the Kernel Matrix In this Section, we briefly review the probabilistic approach on kernel matrix learning proposed in . As in other kernel matrix learning methods , we will also focus on the transductive setting. 3.1 Probabilistic Generative Model Denote the input parts of the training and test sets by I = {xi} n1 i=1 with n1 n2 input vectors and ? = {˜xi}
424 in the spectral decomposition of the full kernel matrix on both training and test data. However, there is no direct connection between the alignment and the generalization error. Lanckriet et al.  derived a generalization bound for choosing the kernel and formulated the kernel matrix learning problem as a convex optimization problem that is not prone to local minima. Nevertheless, even with
424 kernel learning problem is then reduced to an optimization problem. Their method can work in the inductive setting. 1.3 Research Agenda of this Paper Many existing kernel matrix learning methods  constrain the target kernel to a weighted combination of some fixed base kernels. The learning problem can thus be simplified to the estimation of the weighting coefficients only. Motivated by
424 Learning a Generative Model of the Kernel Matrix In this Section, we briefly review the probabilistic approach on kernel matrix learning proposed in . As in other kernel matrix learning methods , we will also focus on the transductive setting. 3.1 Probabilistic Generative Model Denote the input parts of the training and test sets by I = {xi} n1 i=1 with n1 n2 input vectors and ? = {˜xi}
2249 2: Mixtures of Same Type of Kernels . . . . . . . . . . . . . . . . . . . . . . . . . 18 7 Concluding Remarks 22 is1 Introduction 1.1 Kernel Methods Over the last few years, kernel methods  have been increasingly popular in machine learning due to their conceptual simplicity and strong theoretical foundations. Kernel-based learning models and algorithms, such as support vector
2249 (Section 5), it is worth noting that kernel learning forms an integral part of the entire classifier training process. On the contrary, traditional kernel-based classification methods, such as SVM  and kernel FDA , treat the model selection (i.e., kernel selection) problem and the classifier training problem separately as two successive processes. They typically start by selecting an
2249 problem, our focus in this paper is to use the classification problem to illustrate how kernel matrix learning can be performed. Typically, kernel-based classification methods such as SVM  and kernel FDA  first select or learn an appropriate kernel from data empirically, and then train a classifier with the kernel. The two problems, namely, kernel selection and classifier
2249 Like other kernel methods, KNNC works over the feature space without explicit use of the feature vectors ?(x)’s themselves. However, unlike other kernel-based classification methods such as SVM  and kernel FDA , KNNC does not require solving a quadratic programming problem or an eigen decomposition problem. The consequence of this is that our method is simpler and significantly more
2982 has minimum functionality via transmission of acknowledgements to the sender. Yet, it is becoming evident that increasing the functionality of receivers can significantly improve TCP performance . Indeed, a key breakthrough in this design philosophy is represented by fully receiver-centric protocols in which all control functions are delegated to receivers . The benefits that are
2982 by today’s senderbased TCP. 2. Background 2.1. Delegating Control Functions to Receivers One of the first transport protocols that exploits increased receiver functionality is Clark et al.’s NETBLT , which makes error recovery more efficient by placing the data retransmission timer at the receiver. In later work, an increased set of control functions appear at the receiver, either for
3027 obtain decreased response time. The web-browsing simulation scenario consists of a pool of clients and a pool of web-servers, while the bottleneck link is 10 Mbps. We adopt the model developed in  in which clients initiate sessions from randomly chosen web sites (the server pool) with several web pages downloaded from each site. Each page consists of several objects, which are downloaded by
3192 but significant degradation with the failure of each node. In contrast, Squirrel nodes, usually being desktop machines, are expected to fail, or be rebooted or be switched off at night regularly . As seen in Section 3, unannounced node failures result in losing a fraction of the cached content, so Squirrel’s overall performance degrades gracefully with the number of failed nodes. We analyze
3192 have little effect on Squirrel performance, as they are back online within a few minutes (assuming the objects/directories being cached on the machine are stored on disk). The Farsite study  of the Microsoft Corporation internal network shows that about 15% of the machines are likely to be switched off each night or weekend; this means that on average 15% of the cached content in the
939 time of last modification, time of object fetch from the origin server, and the current time. 2.2 Pastry A number of peer-to-peer routing protocols have been recently proposed, including CAN , Chord , Tapestry  and Pastry . These self-organizing, decentralized systems provide the functionality of a scalable distributed hash-table, by reliably mapping a given object key to a
939 web caching, where caches share Internet objects among themselves , and peer-to-peer request routing, characterized as decentralized, self-organizing, and faultresilient . Cooperative web caching (where autonomous web caches coordinate and share content) is found to be useful in improving hit ratio in a group of small organizations. There are many forms, viz. (1) a
940 automatically. Squirrel uses a self-organizing, peer-to-peer routing substrate called Pastry as its object location service, to identify and route to nodes that cache copies of a requested object . Squirrel thus has the advantage of requiring almost no administration, compared to conventional cooperative caching schemes. Moreover, Pastry is resilient to concurrent node failures, and so is
940 fetch from the origin server, and the current time. 2.2 Pastry A number of peer-to-peer routing protocols have been recently proposed, including CAN , Chord , Tapestry  and Pastry . These self-organizing, decentralized systems provide the functionality of a scalable distributed hash-table, by reliably mapping a given object key to a unique live node in the network. The
940 failures, and provide efficient routing of queries. Squirrel uses Pastry to store cached Web objects and directory information, and to efficiently locate them. Pastry is described and evaluated in ; for continuity and containment, we present a brief description here. Pastry is a peer-to-peer location and routing substrate that is efficient, scalable, fault resilient, and self organizing. A
940 web caching, where caches share Internet objects among themselves , and peer-to-peer request routing, characterized as decentralized, self-organizing, and faultresilient . Cooperative web caching (where autonomous web caches coordinate and share content) is found to be useful in improving hit ratio in a group of small organizations. There are many forms, viz. (1) a
3198 routing substrate that is efficient, scalable, fault resilient, and self organizing. A number of other applications have been built on top of Pastry, including an archival storage utility (PAST)  and an event notification system (Scribe) . Pastry assigns random, uniformly distributed nodeIds to participating nodes (say N in number), from a circular 128-bit namespace. Similarly,
3198 of these additional cached copies absorbs a significant fraction of client requests, and that these copies are placed in proximity of interested clients. This technique is proposed and evaluated in ; it dynamically adapts the number of cached copies to match demand, and is found to effectively disperse load arising from popular objects. The benefits of web caches are significant enough to
3198 on the complementary problem of shielding servers from load spikes resulting from flash crowds. Distributed storage systems come in many flavors, like persistent peer-to-peer stores such as PAST  and CFS , distributed filesystems, distributed shared memory systems, etc. with different objectives and tradeoffs. From a conceptual standpoint, Squirrel explores a combination of tradeoffs
943 of last modification, time of object fetch from the origin server, and the current time. 2.2 Pastry A number of peer-to-peer routing protocols have been recently proposed, including CAN , Chord , Tapestry  and Pastry . These self-organizing, decentralized systems provide the functionality of a scalable distributed hash-table, by reliably mapping a given object key to a unique live
943 web caching, where caches share Internet objects among themselves , and peer-to-peer request routing, characterized as decentralized, self-organizing, and faultresilient . Cooperative web caching (where autonomous web caches coordinate and share content) is found to be useful in improving hit ratio in a group of small organizations. There are many forms, viz. (1) a
1933 technique to reduce the latency observed by web browsers, decrease the aggregate bandwidth consumption of an organization’s network, and reduce the load incident on web servers on the Internet . Web caches are often deployed on dedicated machines at the boundary of corporate networks, and at Internet service providers. This paper presents an alternative for the former case, in which
1933 origin web server. The response contains either the entire object (sometimes with a header specifying that the object is uncacheable), or a not-modified message if the cached object is unchanged . Freshness of an object is determined by a web cache using an expiration policy. This is generally based on a time-to-live (ttl) field, either specified by the origin server, or computed by the web
1933 hash a request to one of multiple dedicated caches, (3) directorybased schemes that maintain a centralized table of references for redirecting every client request, and (4) multicast-based schemes . In view of the problems arising due to weak integration between separate cache nodes in the above methods, a Distributed WWW cache has been proposed, with an approach analogous to the xFS file
2243 packet loss as a congestion signal and an additive-increase-multiplicativedecrease-type congestion control mechanism at the end points was not part of the original TCP protocol but was proposed in  and was subsequently added in the late 1980s in response to observed congestion collapse episodes in the Internet; see also Section 3.2.1 below. 11sas the Internet has scaled up several orders of
2243 in October of 1986, when the network had the first of what became a series of “congestion collapses”— completely unexpected, sudden and drastic (e.g., factor-of-thousand) drops in throughput ; while the network continued to transmit packets, the majority of them were retransmission, thereby lowering the effective throughput over the affected links. Subsequently, Van Jacobson
2243 a stable mode with a full window of data in transit, either because they are too short or because they restarted after a packet loss. To make TCP robust to this new fragility, he also proposed in  a number of engineering solutions that were soon thereafter incorporated into RFC-1122 , which lists the requirements for a conformant implementation of TCP as of 1989 12 . The proposed
2673 a segment of the traffic rate process measured at some time scale looks or behaves just like an appropriately scaled version of the traffic rate process measured over a different time scale; see  and the important follow-up studies  and . More precisely, these and numerous subsequent empirical studies described pertinent statistical characteristics of the temporal dynamics of
2679 measured at some time scale looks or behaves just like an appropriately scaled version of the traffic rate process measured over a different time scale; see  and the important follow-up studies  and . More precisely, these and numerous subsequent empirical studies described pertinent statistical characteristics of the temporal dynamics of measured traffic rate processes and provided
2679 of individual traffic components as an invariant of Internet traffic for the past 10 or so years, despite the sometime drastic changes the network has undergone during that period; see for example . The implications for this networking-based explanation of the self-similarity phenomenon are farreaching and unexpected. On the one hand, the fact that we can explain self-similar scaling in terms
2136 have written extensively about the thought process behind the early DARPA Internet architecture and about the design philosophy that shaped the design of the Internet protocols (see for example, ). Another valuable source that provides a historical perspective about the evolution of the Internet architecture over time is the archive of Requests for Comments (RFCs) of the Internet
2136 of the Internet relied on a class of arguments, called the end-to-end arguments, that expressed a clear bias against low-level function implementation. The principle was first described in  and later reviewed in , from where the following definition is taken: 8s“The basic argument is that, as a first principle, certain required end-to-end functions can only be performed correctly
2136 at some statistically determined rate. The best way to cope with this is to accept it, and give responsibility for the integrity of communication to the end systems.  To quote from ], ‘The function in question can completely and correctly be implemented only with the knowledge and help of the application standing at the endpoints of the communication system. Therefore,
3039 the traffic variables of interest; here, an IP flow is made up of successive IP packets that satisfy certain common features). With the exception of some session-related variables (see for example , these layer-specific measurements can be readily extracted from the packet-level traces that are typically collected from individual links within the network. Indeed, while heavy-tailed
872 proof typically re-establishes typing of data, for example, distinguishing Integers from Booleans and from pointers. Interestingly, current research on PCC (such as Foundational Proof Carrying Code ) appears to be directed solely at reducing the size of the trusted computing base on thestarget platform. Unfortunately, this increases the volume of the proofs that are required even further. We
881 It does not provide any type safety and supports unsafe languages.Since the VM is very close to the real architecture it is able to achieve near to native speeds. Typed Assembly Language(TAL) is another framework for verifying the safety of a program for a low level representation. TAL uses the type system of the source language to prove the safety of the program. It achieves this by
882 resident on desktop computers (outside of a small hardware-secured trusted computing base) have been developed. They are, in turn, virtual machines with code verification , proof-carrying code , and inherently safe code formats . In virtual machines with code verification, the code is examined to ensure that the semantic gap between the source language and the virtual machine
882 programs comply with a safety policy defined by the system where the programs will execute. Typical policies are type, memory and control-flow safety, but in the framework described by Necula , any property of the program that can be expressed in first order logic constitutes a valid safety policy. Upon reception of an untrusted program, the code consumer examines the code and emits a
1069 sketches a control curve. When only the observations are available, learning is most commonly performed by algorithms such as the Baum-Welch algorithm or generalized ExpectationMaximization methods . In our case, we explicitly provide data for both the observation and hidden layers by a suitably normalized set of coupled curves. Therefore, we can estimate a HMM by the statistics of the
499 can be established. Using property languages such as the modal mu-calculus , much more advanced properties can be formulated. Using spectacular techniques, such as Binary Decision Diagrams , partial order reduction , bit-hashing , supported by the even more spectacular increase of the speed of computers and the use of networks of computers it is now possible to effectively and
868 Instead, they provide functionality similar to a traditional distributed system (remote object invocation, naming, and so forth). The benefits of aggregation have been recognized in Grid Computing  at the infrastructure level with respect to aggregation of processing, storage, and virtual wiring, as well as in a number of pervasive computing efforts  where data, I/O, and
1957 way of combining the outputs of all classifiers to get a final decision. To do this, we have used the average, which is a simple and effective scheme of combining predictions of the neural networks . Other combination rules such as product, min, and max have been tested but the simple average has produced better results. In order to evaluate the objective functions described above we have used
82 as a multiple phased system (MPS). Each phase is associated to the configuration of the system during some time interval (a part of the -or the entire- system being actually maintained or operative ), while the scheduled maintenance program drives phase changes. A complex stochastic process that includes failure processes and maintenance actions governs the behavior of the system. Under
3195 table with size constant or logarithmic in the number of participating nodes. Structured p2p overlays can be used as a platform for a variety of distributed services, including archival stores , content distribution  and application-level multicast . Service advertisement, discovery and binding are common problems in distributed systems . Service advertisement and
939 overlays providing different functionality. It provides mechanisms to advertise services and to discover services, contact nodes, and service code. 1. Introduction Recent systems such as CAN , Chord , Kademlia , Pastry  and Tapestry  provide a selforganizing structured peer-to-peer (p2p) overlay network that can serve as a substrate for large-scale peer-to-peer
940 It provides mechanisms to advertise services and to discover services, contact nodes, and service code. 1. Introduction Recent systems such as CAN , Chord , Kademlia , Pastry  and Tapestry  provide a selforganizing structured peer-to-peer (p2p) overlay network that can serve as a substrate for large-scale peer-to-peer applications. One of the abstractions that these
940 node. Additionally, each cached copy of the list can be updated independently, as described above, to ensure its freshness and to prevent overloading of the contact nodes. P2p overlays like Pastry  and Tapestry  exploit network locality to provide better performance. They require that the contact node be close to the joining node in the underlying network topology in order to achieve
3198 table with size constant or logarithmic in the number of participating nodes. Structured p2p overlays can be used as a platform for a variety of distributed services, including archival stores , content distribution  and application-level multicast . Service advertisement, discovery and binding are common problems in distributed systems . Service advertisement and
3198 All stored files are immutable except contact lists, which do not require strong consistency semantics. The functionality provided by the persistent store is similar to the one offered by PAST . All files stored in the universal ring must be signed using a private key associated with a valid nodeId certificate. A key-file pair is inserted in the store by using Pastry to route to the node
943 providing different functionality. It provides mechanisms to advertise services and to discover services, contact nodes, and service code. 1. Introduction Recent systems such as CAN , Chord , Kademlia , Pastry  and Tapestry  provide a selforganizing structured peer-to-peer (p2p) overlay network that can serve as a substrate for large-scale peer-to-peer applications. One of
2961 executed in parallel, and their results accepted if in agreement, otherwise the third variant will be executed and voted with the other two. The modelling approach adopted here was introduced by  using a &quot;reliability submodel&quot; - in the terminology of  - to represent the probabilities of the different outcomes at each execution of the redundant component: correct result,
1348 external input. The current study evaluated the extent to which a dynamic recurrent network, similar to a widely studied class of artificial neural network models known as recurrent neural networks , fulfils these requirements. An advantage of a recurrent network representation is that it enables the model to express a complex range of gene interactions whilst generalising away from the
2293 equal to the shortest path problem if the hop count is used as the global performance metric or the shortest weighted path if power  or cost  link metrics are used. It has been shown in  that the routing protocols which do not use geographical location information are not scalable, e.g, AODV (Ad hoc on-demand Distance Vector), DSDV (Destination Sequenced Distance Vector) or DSR
2293 information for all nodes using Angle of Arrival (AOA) capabilities, when only a fraction of the nodes have positioning capabilities. Finally a distributed location service (GLS) is described in , where a node sends its position updates to its location servers without knowing their actual identities. This information is then used by the other nodes in the network to perform geographical
2305 is a feasible next hop. Usually this occurs because the node observes a void region between itself and the destination. For example the Greedy Perimeter Stateless Routing (GPSR), introduced in , makes greedy forwarding decisions (as GRS in Section II.A). When a packet reaches a concave node, the GPSR tries to recover by routing around the perimeter of the void region. Recovery mechanisms,
177 address malicious behavior, where a node’s goal might be to deliberately prevent distribution of the streaming media or to otherwise damage the Pastry routing or SplitStream service. Castro et al.  discuss a number of techniques that might limit the damage a malicious node can cause to a p2p network; many of those ideas could be applied here.s3 Designs In this section, we first describe a
177 defeated if nodes with poor reputations can quit the system and rejoin under new identities, an example of a Sybil attack . While we could address these attacks by requiring certified nodeIds , we can also limit the effectiveness of such attacks by putting new nodes through a probation where they experience a lower quality of service. In SplitStream, where there are k trees being used
181 et al.  surveys many such schemes for tracking nodes’ reputations. In reputation systems, if obtaining a new identity is cheap, negative reputations can be shed easily. Friedman and Resnick  study the case of cheap pseudonyms, and argue that suspicion of strangers is costly. Distributed reputation systems have been proposed in a number of contexts, including MIX-Nets  and Gnutella
3200 and Gnutella . Our system uses the notion of a probationary period, where new nodes see degraded service, yet must participate fully in the protocol. A similar concept appears in Tangler . 6 Conclusions We have demonstrated that, by regularly rebuilding multicast trees and having nodes only track their first-hand observed behavior of their peers, freeloaders would be suitably denied
2081 early-stage comparisons of different network traffic and designs. While network performance analysis based on queueing, markov and probabilistic models has been studied rigorously in the past , enabling early-stage insights into network design tradeoffs, network power analysis has not been explored, to the best of our knowledge. In this paper, we propose a framework for network power
2081 ? power ? dissipated ¤ – in a contentionless network, ¡?¤?? ¤ and in a realistic ¡£¢ ¤?? network ¡?¤ with contention, with , the average flit latency, derived by performance analysis frameworks . However, energybased units of abstraction cannot reflect the spatial and temporal variance in an actual network power profile and hence are unsuitable as abstractions of network power. ? ? ???
2082 approximation of the actual multiplexing of message bit streams in a network – either on a packet-bypacket basis in packet-switched networks, or on a flit-by-flit basis in virtual-channel networks . Following our walkthrough example, the summation of the two link utilization functions on links¡ £ goes through the following steps (summarized in Figure 2(d)): ? ? © ? ? ? ? © ? © ? ? ? ? © ? ? ?
881 security policy it enforces is the standard type and memory safety. Other frameworks, including Kozen’s efficient code certification (ECC)  and Morrisett et al.’s Typed Assembly Language (TAL) , concentrate exclusively on standard type safety properties. The main reason that certified code has been used in this restricted fashion is that automated theorem provers are not powerful enough
881 security policy it enforces is the standard type and memory safety. Other frameworks, including Kozen’s efficient code certification (ECC)  and Morrisett et al.’s Typed Assembly Language (TAL) , concentrate exclusively on standard type safety properties. The main reason that certified code has been used in this restricted fashion is that automated theorem provers are not powerful enough
881 place. We already know how to preserve many kinds of typing invariants from top to bottom, through the compilation of (higher-order) modules , closure conversion  and code generation . It is less clear how to preserve proofs represented in other formalisms through these transformations. Moreover, there has been much research on implementing efficient, decidable type checking and
882 untrusting web browser or operating system will use a mechanical checker to verify that the code is safe before executing it. For example, Necula and Lee’s proof-carrying code (PCC) implementation  uses a first-order logic and they have shown that they can check many interesting properties of hand-coded assembly language programs including access control and resource bound policies . In
3192 kinds of unavailability: (i) host availability, a binary value that indicates whether a host is reachable and the desktop grid client is up, which corresponds to the definition of availability in ; and (ii) CPU availability, a percentage value that quantifies the fraction of the CPU that can be exploited by a desktop grid application, which corresponds to the definition in [3, 12, 33, 15,
3192 While this data is interesting for applications that require hosts to be reachable for given period of time (i.e. content distribution) and could be used to confirm and extend some of the work in , it is less relevant to our probCumulative percentage 1 0.9 0.8 0.7 0.6 0.5 weekdays weekends weekday mean: .391 trillion ops weekend mean: .848 trillion ops 0 5 10 15 Interval length in number of
3192 of the temporal structure of host and CPU availability in desktop grid, which is fundamental for advanced resource management, resource selection, and application scheduling. The works in  have presented measurement and analysis of host availability in enterprise systems and in large peer-to-peer networks (where host availability is defined as the host being reachable). While these
816 this temporal structure and analyze its impact on desktop grid applications. Several other studies have measured the percentage of available CPU cycles for large collections of desktop machines . The results of these studies have the following shortcomings for characterizing the utility of desktop grid resources. First, the monitored CPU availability may not be a good model of what an
816 in ; and (ii) CPU availability, a percentage value that quantifies the fraction of the CPU that can be exploited by a desktop grid application, which corresponds to the definition in . When a host becomes unavailable (e.g., during a shutdown of the O/S), no new task can be started, and if a desktop grid application task was running on that host, it fails. When a CPU becomes
816 CPU availability during periods of host availability. A few other studies have obtained percentages of CPU cycles available for large collections of machines as part of research projects , or have made available such measurement data through a monitoring infrastructure . While the results from these studies could be used to quantify the utility of a desktop grid for a
188 kinds of unavailability: (i) host availability, a binary value that indicates whether a host is reachable and the desktop grid client is up, which corresponds to the definition of availability in ; and (ii) CPU availability, a percentage value that quantifies the fraction of the CPU that can be exploited by a desktop grid application, which corresponds to the definition in [3, 12, 33, 15,
188 While this data is interesting for applications that require hosts to be reachable for given period of time (i.e. content distribution) and could be used to confirm and extend some of the work in , it is less relevant to our probCumulative percentage 1 0.9 0.8 0.7 0.6 0.5 weekdays weekends weekday mean: .391 trillion ops weekend mean: .848 trillion ops 0 5 10 15 Interval length in number of
188 of the temporal structure of host and CPU availability in desktop grid, which is fundamental for advanced resource management, resource selection, and application scheduling. The works in  have presented measurement and analysis of host availability in enterprise systems and in large peer-to-peer networks (where host availability is defined as the host being reachable). While these
1258 EM algorithm and Naive Bayesian classifiers. Blum and Mitchell (Blum & Mitchell, 1998) introduced the co-training algorithm and applied it to the problem of web-page categorization. Nigam et al (Nigam et al., 2000) further explore the use of several EM algorithm variants again on the same dataset. Ghani (Ghani, 2001) explores several variants of the EM algorithm combined with Naive Bayes classifiers on
1258 initial models are therefore already good, unlabeled data tends to degrade performance (refer to the plots for 1,000 and 2,000 labeled files); this is in tandem with previous observations (e.g., (Nigam et al., 2000)). However, when only a small amount of labeled data is available, and the initial models are therefore relatively poor, unlabeled data is seen to give a significant improvement in performance. The
3260 to not sell more than the available capacity. To be able to buy capacity on an entire path, the capacity must be ac2squired in a bundle on the network capacity market. In previous contributions , we have shown how this market could be run, and how the trading of bundles could be executed. The idea is that the network owners sell first class transit capacity in their ASes. The capacity sold
3260 token. When revoking a token from an access node, the user gets a signed receipt from the access node. This revocation receipt is needed to be able to sell the capacity on a network capacity market . 2.6.2 Headers When the users sends data, the packets can be of three types: • best-effort, • access node routed, or • explicitly routed. The user signals the type to the access node by using a
3260 errors and omissions are entirely ours. References  J. Moy. RFC 2328: OSPF Version 2. IETF, April 1998.  Y. Rekhter and T. Li. RFC 1654: A Border Gateway Protocol 4 (BGP4). IETF, July 1994.  Lars Rasmusson and Erik Aurell. A Price Dynamics in Bandwidth Markets for Point-to-point Connections. Technical Report SICS--T2001-21-SE, SICS, Stockholm, Sweden, 2001.  Lars Rasmusson.
3260 trading, option pricing 1 Introduction Computer networks are now being used to transfer live video and other data that need reliable service from the network, in terms of latency, jitter, and loss . Bandwidth or capacity markets can help to provide the necessary end-user quality of service guarantees . Capacity trading enables users to reserve capacity in congested networks, and
3260 under path constrains. Since it is NP-hard to find one optimal multi-constrained path that satisfies a set of (more than one) constraints, approximative solutions such as hierarchical routing , or relaxing multi-constraint to single constraints  are necessary. Most approaches to combinatorial allocation use a centralized distributor that solves a global constraint problem c.f. [5, 10,
2355 and ?f ?Sn,0 (0, ¯ S) = T C e rT1 f(0, ¯ S) = N? M? vinQ i=1 ?f Sm,0 ?Sn,0 m=1 (0, ¯ S) ? T C K Q  There is no closed form for the sum of lognormal variables , which makes it difficult to reduce the Q-terms further, but since S(T ) has a closed form under the risk neutral measure Q, the option price can be approximated with Monte-Carlo simulation.
2355 System, Proc. of 1st IEEE/ACM Int. Workshop on Grid Computing, Bangalore, India, LNCS 1971, Springer Verlag, Dec. 2000. http://www.csse.monash.edu.au/˜rajkumar/Grid2000/grid2000/book/19710064.ps  Moshe Arye Milevsky, and Steven E. Posner, Asian Options, the Sum of Lognormals, and the Reciprocal Gamma Distribution, Journal of Financial and Quantitative Analysis Vol. 33, No. 3, September 1998
2355 the border routers have bounded capacity. To determine how much traffic that can be allowed at the border, the path capacity must be estimated. It can be done by active or passive measurements . Traffic engineering can be used to rebalance the load of a network so that the available capacity increases (see  for a survey and references to current methods). The amount of traffic that
2355 ACM Press, 2000.  Z. Turányi, A. Veres, and A. Oláh. A family of measurement-based admission control algorithms. Performance of Information and Communication Systems, Lund, Sweden, May 1998.  Manish Jain and Constantinos Dovrolis. Pathload: A Measurement Tool for End-to-end Available Bandwidth. In Proc. of Passive and Active Measurement Workshop, Fort Collins, CO, March 2002.  G.
2355 first collect all demands, and then distribute all allocation information before the system’s state is updated. Another relaxation of the QoS problem is to only give statistical service guarantees . This is in the spirit of service models such as DiffServ  that only aims to provide QoS within a centrally managed network domain, while it fails to provide guarantees along an entire path. In
1939 be kept small even with a large number of critical load instructions. 4. EXPERIMENTAL METHODOLOGY To study the performance of IMPT, we developed a cycleaccurate simulator based on SimpleScalar 3.0a . We added major enhancements to the simulator to implement accurate bus and memory contention. To this end, we have changed the buses and memory models of SimpleScalar into event driven models. The
795 the \inaccuracy&quot;. Wide-area Resource Accounting As we move toward global distributed computing, one goal is to account for aggregate consumed resources across multiple providers on a per-user basis. Given the scale of this problem, maintaining accurate resource usage information will incur prohibitive overhead. Allowing bounded error in usage information is a promising approach to solving
2502 the nature of our market’s volatility to derive an appropriate method of risk evaluation. We begin be adopting a simple model to track the movement of price in our system. Cox, Ross, and Rubinstein  developed the binomial options-pricing method to forecast the expected price swings of a security. The binomial model was a response to the newly derived Black-Scholes option-pricing model .
2502 the augmented binomial model — a Markov state model. One final issue remains. We must compute the probabilities and magnitudes for each transition in the new Markov model. Cox, Ross, and Rubinstein  show that for relative Brownian price movement r := e ?? T/n d := e ??? T/n pr := (8.2) (8.3) µ?d , (8.4) r?d where n is the length of each time period, T is the number of observed time periods, µ
1939 a “general-purpose” workload . A. Simulation Methodology To obtain performance data for the benchmarks, we use SimpleScalar, a cycle-accurate out-of-order superscalar processor simulator . We consider the scenario in which the operating system is invoked each time a SRAS swap is required and the scenario where the processor primarily handles SRAS swapping. The OS-managed swapping
539 This means that the density distribution of clouds should be defined in threedimensional space to create realistic images. Therefore, a lot of methods have been developed to display clouds . Using these methods, extremely realistic images can be generated. Their main purpose is, however, to create images of static clouds. Fascinating animations of clouds with changing their shapes and
539 to display photo-realistic images, however, it is desirable to use the physical model, taking into account scattering/absorption due to particles. Many such methods have therefore been developed . Some of them take into account multiple scattering of light . Additionally, Nishita et al. take into account the effect of skylight on the cloud color . Multiple scattering and
541 on. They are often created by filming them in advance and replaying the film quickly. Since generating such realistic animations by computer graphics is useful, a lot of methods have been developed . This paper proposes a new method for realistic animation of clouds. Our aim is to develop a simple method that can create realistic animation as quickly as possible, preferably in real-time. We
541 rendering are separately reviewed. SIMULATION: In computer graphics, there are two categories to simulate the gaseous motion like clouds. One is to simulate the physical process of fluid dynamics . The other is a heuristic approach . Most of the methods in the former category need a large amount of computation time. Stam, however, developed a fast simulation method by
555 on. They are often created by filming them in advance and replaying the film quickly. Since generating such realistic animations by computer graphics is useful, a lot of methods have been developed . This paper proposes a new method for realistic animation of clouds. Our aim is to develop a simple method that can create realistic animation as quickly as possible, preferably in real-time. We
555 rendering are separately reviewed. SIMULATION: In computer graphics, there are two categories to simulate the gaseous motion like clouds. One is to simulate the physical process of fluid dynamics . The other is a heuristic approach . Most of the methods in the former category need a large amount of computation time. Stam, however, developed a fast simulation method by
555 to display photo-realistic images, however, it is desirable to use the physical model, taking into account scattering/absorption due to particles. Many such methods have therefore been developed . Some of them take into account multiple scattering of light . Additionally, Nishita et al. take into account the effect of skylight on the cloud color . Multiple scattering and
555 distribution is discrete in space and time. The density at an arbitrary point, x, is then obtained as a weighted sum of a simple basis function, f. Gaussians are often used for the basis function . In this paper, however, we use a field function of metaballs proposed by Wyvill et al . The reason for this is as follows. A metaball has a parameter, an effective radius, which represents its
554 on. They are often created by filming them in advance and replaying the film quickly. Since generating such realistic animations by computer graphics is useful, a lot of methods have been developed . This paper proposes a new method for realistic animation of clouds. Our aim is to develop a simple method that can create realistic animation as quickly as possible, preferably in real-time. We
554 rendering are separately reviewed. SIMULATION: In computer graphics, there are two categories to simulate the gaseous motion like clouds. One is to simulate the physical process of fluid dynamics . The other is a heuristic approach . Most of the methods in the former category need a large amount of computation time. Stam, however, developed a fast simulation method by
554 to display photo-realistic images, however, it is desirable to use the physical model, taking into account scattering/absorption due to particles. Many such methods have therefore been developed . Some of them take into account multiple scattering of light . Additionally, Nishita et al. take into account the effect of skylight on the cloud color . Multiple scattering and
554 them as a constant ambient term. One of the major approaches to rendering the volume density similarly to clouds is to use 3D textures. Stam used 3D hardware texture mapping to display gases . With the help of the high-end graphical workstation, the method can generate realistic images in real-time by combining 3D textures and advecting cloud textures developed by Max et al. .
882 Portability is inherited from the use of an intermediate code and by a limited set of hardware primitives. Security is ensured by a code-safety checking (which uses a PCC-like algorithm ) at loading time . Extensibility is provided through a simple representation of the hardware that at the root of the system does not predefine any abstractions. Memory pages, numerical values,
1502 is a vote to accept or reject the new extension. The user could solve problems by deleting incompatible system extensions. This two levels CPU sharing process is close to hierarchical schedulers . It is this sufficient to build a RT scheduler over an exo-kernel, but it introduces sub-optimal solutions. The key points we have presented prove that the exo-kernel, real-time operation and an
1258 algorithm when only a small number of examples are available. For using Co-Training, the features in the problem domain should naturally divide into two sets. For the same task, Nigam et. al.  have used an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation Maximization (EM) and Naive Bayes classifier. Ghani  has combined the EM
2477 methods. 1 Introduction Open-domain natural language question answering (Q/A) is a challenging task in natural language processing which has received significant attention in the last few years . In the Text REtrieval Conference (TREC) question answering competition, for example, given a free form query like “What was the largest ? This research is supported by NSF grants ITR-IIS-0085836,
2722 the next hop to each destination in the network. Then, at the time of packet forwarding, a node has to only look up the next hop entry in the routing table for the destination indicated. DSR  and AODV  are examples of reactive or on-demand protocols. In these protocols, the source, at the time of sending a packet, broadcasts a query to obtain a route to the destination. The broadcast
2289 hop to each destination in the network. Then, at the time of packet forwarding, a node has to only look up the next hop entry in the routing table for the destination indicated. DSR  and AODV  are examples of reactive or on-demand protocols. In these protocols, the source, at the time of sending a packet, broadcasts a query to obtain a route to the destination. The broadcast query
175 generates the reply from the very beginning even if the client has already received part of the reply. It is not a serious problem for most of the Web requests, since previous studies ,  have shown that most web replies are less than 8K bytes. However, it might introduce considerably longer delays when the requested file is very large. Two steps are needed to speed up the recovery
1957 alphanumeric characters. For multiclassifier combination, five fusion schemes were investigated. These are sum, median, min, max, and simple majority voting. Details of these rules can be found in . All the tables presented forthwith include performance statistics averaged over the 10-fold cross validation experiments performed on the unseen test sets from the above database. Tables 1 and 2
3015 we present a novel middleware framework, which enables ubiquitous delivery of QoS-aware multimedia applications. The framework is based on our previous multimedia middleware research: (1) Agilos , a controlbased middleware for QoS adaptations; (2) 2K Q , a unified reconfigurable QoS-aware middleware; and (3) SMART , a scalable middleware. This paper focuses on addressing the new
1490 residual bandwidth. In addition to these server mechanisms, resource reservation is also a common mechanism to provide isolation between soft real-time and besteffort applications in an open system  . The reservation mechanism, typically combined with admission control and real-time scheduling, allows an application to reserve processor resource and guarantees resource
763 the authors proposed and evaluated several algorithms, which are interval-based and adjust speed accordingsto the processor utilization in previous intervals. These algorithms have been extended by  and .  and  proposed implementable algorithms, removing the requirement of the future knowledge in  and . Several approaches, including estimation of workload distributions
1500 bandwidth. In addition to these server mechanisms, resource reservation is also a common mechanism to provide isolation between soft real-time and besteffort applications in an open system  . The reservation mechanism, typically combined with admission control and real-time scheduling, allows an application to reserve processor resource and guarantees resource
768 and evaluated several algorithms, which are interval-based and adjust speed accordingsto the processor utilization in previous intervals. These algorithms have been extended by  and .  and  proposed implementable algorithms, removing the requirement of the future knowledge in  and . Several approaches, including estimation of workload distributions  and prediction
773 demands. Dynamic voltage scaling (DVS) is typically used to reduce the processor energy consumption by dynamically adjusting the speed, and hence power, according to application workload . The workload is either predicted using some heuristics or estimated from applications’ worst-case execution time (WCET). The DVS algorithms are shown to be effective for general-purpose
773 for overrun handling or/and energy saving. Dynamic voltage scaling is usually used to reduce the processor energy consumption with tradeoff between performance and power consumption . Recently, DVS has been investigated in two main areas, general-purpose operating systems (GPOS) and real-time operating systems (RTOS). Algorithms in a GPOS predict the workload using some
773 utilization in previous intervals. These algorithms have been extended by  and .  and  proposed implementable algorithms, removing the requirement of the future knowledge in  and . Several approaches, including estimation of workload distributions  and prediction of episodic interaction , have been proposed to maintain good interactive performance under DVS in
891 . In Beni et. al. , a swarm distribution is determined via a system of linear equations describing difference equations with periodic boundary conditions. Behavior-based approaches  are also very popular. They derive vector information in a fashion similar to AP. Furthermore, particular behaviors such as “aggregation” and “dispersion” are similar to the attractive and
1106 encompasses the design of a mechanism capable of representing a face realistically, but also its control. Most computer generated facial systems are based on 3D Models  or image based models  , and are parametric. Using these representations we may animate a face using only a speech signal. This is desirable for many applications such as low-bandwidth network communications and
1102 required for each node in the hierarchy. For the representation of a node we introduce a non-linear speech-appearance model. This is an extension of an appearance model introduced by Cootes et al  encoding relations between appearance parameters and speech vectors allowing the synthesis of facial configurations given new audio. For the root node the model is built using the full facial
1102 one of our labelled training images annotated with the 82 landmarks. 4 HIERARCHICAL FACIAL MODELLING Facial area synthesis for each node in our hierarchical model is based on an appearance model . Given an audio input each node is used, in turn, to synthesise a facial area. Given our training set we begin building our model by extracting landmark shape data, and shape-free texture data, for
1102 FACIAL AREA MODELLING AND NODE INITIALIZATION Given a subset of facial information from the global training set we first build an appearance model of the corresponding facial area as described in . Statistical PCA models of shape and texture are built using the training set and combined in a joint PCA model. We define this model as follows x ¤ P¥ W¦¨§ ¥ Q¥ ¡£¢x c (1) g ¡ ¢g ¤ P© Q© c (2)
3140 name a few, see Tugnait , BarShalom , Elliott et al. , Murphy , Logothetis & Krishnamurthy , Chen & Liu , Lerner et al. , Koutsoukos et al. , Doucet et al. , Hofbaur & Williams , Costa et al. , Germani et al. . Other authors have worked on the filtering problem for stochastic hybrid systems governed by continuous-time equations in both
3146 Linear Systems, often referred to as discrete-time Stochastic Hybrid Systems in the electrical engineering community. To name a few, see Tugnait , BarShalom , Elliott et al. , Murphy , Logothetis & Krishnamurthy , Chen & Liu , Lerner et al. , Koutsoukos et al. , Doucet et al. , Hofbaur & Williams , Costa et al. , Germani et al. . Other
1445 (hyper-)graphs are summarized in . Spectral techniques have been applied in a wide variety of contexts including high performance computing , image segmentation , web page ranking , information retrieval , RNA motif classification , data clustering , and dimensionality reduction . In particular, spectral clustering refers to a collection of techniques that
1445 of L + (which, as already stressed, correspond to the largest of L). They also stress the links between these techniques and web page ranking algorithms such as PageRank , HITS  and randomized HITS . 9sFinally, there is an intriguing correspondence between random walk on a graph and electrical networks theory, as popularized by Doyle and Snell in their nice book
2722 Much work in sensor networks builds on results in ad-hoc networks that address the limitations of wireless networks (low bandwidth, high error rates, low power, disconnections) . The node localization problem has been previously discussed by others and usually requires estimates of inter-node distance, a difficult problem. Simi? and Sastry  3spresent a distributed
2305 a second method we call Path Routing which enables us to “embed” one or mores paths adaptively in the sensor network. The protocol is an instance of geographic routing tailored to navigation . Hop-by-hop communication is used to identify the sensor nodes lying on the path. A message is broadcast which contains a list of coordinates. Each sensor that receives the message checks to
2305 to some extent by choosing an appropriate path width or by adding acknowledgment messages to assure the path message reaches its destination. An approach similar to greedy perimeter routing  could also be used to route around obstacles. The rest of this section presents the details of our method. A path is an array of X,Y coordinates designating waypoints along a route. A path
939 comparable to our approach, since they are also creating an aggregation of the top k results from the peers. In contrast to our ranking-algorithm they do not use any broadcast-topology, but use CAN  in combination with the vector space model (VSM) and latent semantic indexing (LSI) to create an index which is stored in CAN using the vector representations as coordinates. PlanetP
2243 Since wireless access to the Internet is the most common application of this kind of networks, it is reasonable to assume that the traffic will be carried over the well–known TCP/IP protocol suite . Hence, an important issue to address is the interaction between the 802.11 MAC and the closed–loop nature of TCP. This is accomplished in the first part of the paper, where we estimate the
2679 time increases. This is mainly due to the fact that serious congestion that causes a true timeout event can last for some time (considering the long range dependent nature of Internet traffic , ). Accordingly, even though a BTCP using BLE may treat the first true TO as a FTO, (and transmits data with half of previous congestion window), most likely it will receive another true TO, and
499 area as well as high design quality by consideration of layout aspects during synthesis . These techniques make use of ordered Binary Decision Diagrams (BDDs) as introduced in . Their size often critically depends on the chosen variable ordering. In applications like logic synthesis it is most important to determine a good ordering, since a reduction in the number of BDD
499 (denoted by Latin letters) are bound to values in B := {0, 1}. It is well-known that a Boolean function f: B n ? B over the variable set Xn can be represented by a Binary Decision Diagram (BDD) , i.e. a directed acyclic graph where a Shannon decomposition f = xifx i=1 + xifx i=0 (1 ? i ? n) into two cofactors in xi is carried out in each node, yielding a “then-successor” via a 1-edge and
499 and variables are encountered at most once and in the same order (the “variable ordering”, e.g. x1 < x2 < x3 < x4 in Figure 2) on every path from the root to a terminal node. For more details see . Variable orderings are often denoted by permutations ?. For simplicity, the author uses the notation xi = ?(k), if variable xi is the k-th element of the variable ordering, i.e. xi is in the k-th
3027 ). Self-similar (SS) scaling exists in large time scales (i.e. one second or larger), cf. , , . More recently, multi-fractal phenomena were observed and analyzed, see , , . At small time scale, the traffic still has mono-fractal behavior in the Internet backbone . Many people have also investigated into the possible causes of these long-range-dependent and
2673 parameter. We provide upper and lower bounds on the constant that determines the scale parameter of the corresponding Weibull distribution. I. INTRODUCTION Since the seminal work of Leland et al. , there has been tremendous research work on the characterization and modeling of the Internet traffic. A number of studies have confirmed the prevalence of the Long Range Dependence (LRD) and
2679 Range Dependence (LRD) and fractal behavior in LAN and WAN as well as in Web servers (see e.g. ). Self-similar (SS) scaling exists in large time scales (i.e. one second or larger), cf. , , . More recently, multi-fractal phenomena were observed and analyzed, see , , . At small time scale, the traffic still has mono-fractal behavior in the Internet backbone . Many
1626 configured a single link at 10 Mbps and packets arriving at the link were served by a WFQ scheduler. Each QVPN traffic consisted of aggregated traffic traces of recorded VoIP conversations used in , in which spurt-gap distributions were obtained using G.729 voice activity detector. Each VoIP stream had an average data rate of around 13 Kbps, peak data rate of 34 Kbps, and packet size of Lmax
1755 TapeNum Ts Te Vs Ve C101 T1234 2 UC 2 4 C102 T1245 5 7 5 now C102 T1245 8 UC 5 7 C102 T1234 9 9 9 11 C102 T1234 10 13 9 13 C102 T1234 14 15 9 now C102 T1234 16 UC 9 15 CustomerID TapeNum   C101   T1234   C102   T1245         T1234
1755 has been much work on the topic in the data warehousing context, only a few papers have considered the more general problem . Finally, Coalescing is an important operation in temporal databases . Coalescing merges value-equivalent tuples with intervals that overlap or meet. This operation may be implemented by first sorting the argument relation on the explicit attribute values as well as
187 by the recent introduction of linear-time, rateless erasure codes. 1 Introduction One of the most prominent uses of peer-to-peer systems is to download files—often very large files, such as movies . More often than not, these files are available at least in part at more than one node on the network. This observation has inspired a number of different algorithms for multi-source file download
3192 kinds of availability: (i) host availability, a binary value that indicates whether a host is reachable and the desktop grid software is up, which corresponds to the definition of availability in ; and (ii) CPU availability, a percentage value that quantifies the fraction of the CPU that can be exploited by a desktop grid application, which corresponds to the definition in [3, 12, 42, 15,
3192 CPU availability just as it would be experienced by a compute-intensive desktop grid application. As a result, our method provides more detailed information than just measuring host availability . Moreover, our traces are not susceptible to OS idiosyncrasies, and can directly measure the effect of task failures (caused by mouse or keyboard activity, for example). In contrast, lightweight
869 To model this scenario, we used the log of clock rates of machines at SDSC, which yielded a narrow normal distribution with a mean speed of 3.2GHz and standard deviation of 170MHz. Much work  in desktop grids has focused on using resources found in multiple labs. Recently,  reports the use of XtremWeb  at a student lab in LRI with nine 1.8GHz machines, and a Condor cluster in
188 kinds of availability: (i) host availability, a binary value that indicates whether a host is reachable and the desktop grid software is up, which corresponds to the definition of availability in ; and (ii) CPU availability, a percentage value that quantifies the fraction of the CPU that can be exploited by a desktop grid application, which corresponds to the definition in [3, 12, 42, 15,
188 CPU availability just as it would be experienced by a compute-intensive desktop grid application. As a result, our method provides more detailed information than just measuring host availability . Moreover, our traces are not susceptible to OS idiosyncrasies, and can directly measure the effect of task failures (caused by mouse or keyboard activity, for example). In contrast, lightweight
317 relation is used in joins. Definition 1 A single-entity basis set is identical to a set of descriptors Di ?D. This definition is equivalent to typical definitions used in association rule mining . Our goal is to mine relational basis sets that will be constructed from multiple descriptor sets that belong to the same tuple of a joined relation. Relations representing relationships have two
318 are therefore unlikely to produce meaningful results. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
321 problem of excluding classes of rules as well as their impact on support and confidence from the scope of the association rule mining algorithm. A further related research area is graph-based ARM . The relations we are considering in this paper can be viewed as graphs. Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find
322 and, thereby, do not require relational association rule mining. In fact, since their support and confidence based on the joined table differs from single table results it has been concluded  that they should not be listed for the joined table. We call such rules out-of-scope. It has been observed that in many settings items in ARM  or attributes in classification  are highly
322 be represented by RR(Tl,Tr,D (R) ) with D (R) being a set of relationship descriptors. We would split such a relation into a separate entity relation as well as a standard relation RR(Tl,Tr) as in . Joined relation basis sets are formed in multiple steps. Relationship- and entity relations are joined through a natural join operation (?). Attribute names are changed  such that they are
322 joined relation and any higher order relation. The support and confidence will however vary depending on that context, and a rule that is strong in onescontext may not be so in another. We follow  in always using the lowest order possible. Definition 6 An item set I is out-of-scope if one or more entities are not represented, i.e., if |?G(I)| <nwhere || indicates the cardinality, ? is the
323 notes the problem of what we term repetitious rules but does not resolve it. The problem of rule interest has been addressed in a variety of work on redundant rules, including closed set generation . Additional rule metrics such as support as lift and conviction have been defined . These approaches do not address the problem of excluding classes of rules as well as their impact on support
324 name a few. A variety of techniques have been developed for data mining of relational data . A typical approach is to allow maximum flexibility to the user through inductive logic programming . Although such an approach can in principle resolve any problem a user may be aware of, it may fall short of identifying even simple pitfalls. For the particular type of data of interest in this
324 set, their origin identifier differs, and their descriptor is equal. 3 Related work Relational association rule mining in general has been addressed in the context of inductive logic programming . These approaches are very flexible and leave most choices up to the user. It cannot, however, be expected that a user understands the implications of out-of-scope and repetitious rules
327 and implemented over sets of items. For our problem we need the relational algebra framework to manipulate data from multiple relations and therefore choose an extended relational model similar to  for our description to account for both requirements. Attributes within this model are allowed to be set-valued, thereby violating first normal form. We go one step further by allowing sets of
329 problem of excluding classes of rules as well as their impact on support and confidence from the scope of the association rule mining algorithm. A further related research area is graph-based ARM . The relations we are considering in this paper can be viewed as graphs. Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find
332 set, their origin identifier differs, and their descriptor is equal. 3 Related work Relational association rule mining in general has been addressed in the context of inductive logic programming . These approaches are very flexible and leave most choices up to the user. It cannot, however, be expected that a user understands the implications of out-of-scope and repetitious rules
334 Ji ? FreqSet 15. if(|?G(Ji)| == n ) 16. Apriori:Rule Gen(Ji,minconf) 5.1 Data sets YEASTP-P consists of two data tables that were gathered from the Comprehensive Yeast Genome Database at MIPS . We use yeast protein annotation data as the entity relation. Annotations are hierarchically structured with hierarchies for function, localization, protein class, complexes, phenotypes, and
338 are therefore unlikely to produce meaningful results. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
343 results. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A tuple in a joined relation can
345 problem of excluding classes of rules as well as their impact on support and confidence from the scope of the association rule mining algorithm. A further related research area is graph-based ARM . The relations we are considering in this paper can be viewed as graphs. Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find
346 are therefore unlikely to produce meaningful results. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
347 notes the problem of what we term repetitious rules but does not resolve it. The problem of rule interest has been addressed in a variety of work on redundant rules, including closed set generation . Additional rule metrics such as support as lift and conviction have been defined . These approaches do not address the problem of excluding classes of rules as well as their impact on support
347 repetitious and out-of-scope rules themselves but also their contributions to support and confidence of all rules. This approach differs from other work on redundancy in association rule mining  in which some rules are discarded and all others left unchanged. We compare UNIC with a probabilistic approach to correlation elimination in the presence of rules that are explicitly considered
348 are therefore unlikely to produce meaningful results. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
1069 model based solely on a map and a stream of non-continuous and noisy GPS sensor data. A general approach for solving such learning problems is the well-known ExpectationMaximization (EM) algorithm . In our application, EM is based on the observation that learning the model parameters would be easy if we knew the person’s true location and transportation mode at any point in time.
1069 between edges and modes, e.g., the probability of transiting from edge ? ? to edge ? ? in ????? mode. The discreteness of these transitions allows us to apply the well-known Baum-Welch algorithm , an EM algorithm for hidden Markov models (HMM). The Monte Carlo version of the Baum-Welch algorithm  performs at each iteration both a forward and a backward (in time) particle filtering 2
1069 from edge to at time £ and in mode ? ?§¦¨¡ . £?? §¤? §¤? ? ? ?§¦?¡ ? ?§¦?¡ ? ?§¦?¡ ? ?§¦?¡ ?§¦¨¡ ? at time is the probability transiting from mode to on edge ¥ ? ? . and A short derivation gives us , ¢ ? §¤? ?§¦?¡ §?? ?§¦¨¡ ???s?§¦¨¡ £?? ?§¦¨¡ §?? ?§¦¨¡ ? © ¡ £?? ? ??? ?§¦?¡ §?? ?§¦¨¡ ? ? £?? ? §¤? ?§¦?¡ ? (5) £?? ?§¦¨¡ £ ? §¤? ?§¦?¡ §¤? ?§¦?¡ ???s?§¦¨¡ £?? ?§¦?¡ §?? ?§¦¨¡ ? © ¡ £ ? ? ???
317 Dakota State University Fargo, North Dakota 58105, USA anne.denton@ndsu.nodak.edu acterized by an entire record of data that can itself be used for data mining purposes. Association Rule Mining , ARM, is a powerful technique for the discovery of patterns among sets of items that characterize an object. In traditional market basket analysis objects are shopping carts and items are the
317 the modified ARM. We close with conclusions and future expansions in the final section. 2. RELATED WORK The basic framework on which our process is built is standard ARM as it was first established . In this setting, the database can be represented as one relational table with tuples containing binary items from a basic set of items. Once we include the interaction relation among entities, the
318 tend to focus on a particular pattern size at a time. 2.2 Sequential Pattern Mining Sequential pattern or episode mining can be done in similar form to graph-based mining. Sequential pattern mining  shares the same basic framework as traditional ARM, but is extended to deal with an ordering relationship among the entities. Sequences are linear graph structures or paths. This type of mining has
321 of linear paths rather then some radius of neighbors. For relational ARM this is rarely a problem since very few hop structures are likely to be relevantas the length increases. While Graph ARM  is concerned with scaling as a function of the subgraph size, the major problem in relational ARM is scaling with respect to itemset size. Also due to the ”small world” property of scale-free
321 of the database, usually in the form of adjacency matrices, and then finds frequent substructures of the graph in Apriori fashion. Much progress has been made in this area as shown in various works . Graph-based ARM concerns itself with identifying patterns of nodes and edges, which places the focus on the structure of interactions. Most settings allow edge labels and node labels in the
322 in computational complexity between non-relational algorithms and their relational counterparts. More importantly, the naive translation of a non-relational algorithm can often lead to skewed  or irrelevant results in the relational setting. In this paper we focus on networks of objects for which interactions are predefined, corresponding to a many-to-many relationship of an entity with
322 is the additional fact that we do not apply ARM to data tables we have directly. 2.3 Multirelational and ILP Approaches including inductive logic programming (ILP) and multi-relational mining  also address the problem of extracting information from the relation or interaction schema and tables. UNIC addresses a special case in relational mining but can benefit from some of the insights
322 correct rule setting is brought up. Some works have cited that a flaw in flat table mining of relations is that rules containing items of a subset of tables are graded on the full join . Rather, they recommend that such rules are evaluated on the relevant tables of scope. Similar to this concept we introduce rules which are called out-of-scope. Such rules are made of items which
323 4. The rule (R1) is likely not to add information compared with boring and out-of-scope rules. Issues of redundant and misleading rules have extensively been discussed in the standard ARM setting . In the relational setting these problems are aggravated by the fact that commonly the strongest and simplest rules are also the least interesting ones. The problem is not limited to itemsets of
324 is the additional fact that we do not apply ARM to data tables we have directly. 2.3 Multirelational and ILP Approaches including inductive logic programming (ILP) and multi-relational mining  also address the problem of extracting information from the relation or interaction schema and tables. UNIC addresses a special case in relational mining but can benefit from some of the insights
329 of linear paths rather then some radius of neighbors. For relational ARM this is rarely a problem since very few hop structures are likely to be relevantas the length increases. While Graph ARM  is concerned with scaling as a function of the subgraph size, the major problem in relational ARM is scaling with respect to itemset size. Also due to the ”small world” property of scale-free
329 of the database, usually in the form of adjacency matrices, and then finds frequent substructures of the graph in Apriori fashion. Much progress has been made in this area as shown in various works . Graph-based ARM concerns itself with identifying patterns of nodes and edges, which places the focus on the structure of interactions. Most settings allow edge labels and node labels in the
332 is the additional fact that we do not apply ARM to data tables we have directly. 2.3 Multirelational and ILP Approaches including inductive logic programming (ILP) and multi-relational mining  also address the problem of extracting information from the relation or interaction schema and tables. UNIC addresses a special case in relational mining but can benefit from some of the insights
338 to deal with an ordering relationship among the entities. Sequences are linear graph structures or paths. This type of mining has received as much attention as association rule mining in general . It can be seen that sequential pattern mining has the capability to mine a subset of the transactions we may produce for our algorithm. The subset is that of all linear patterns as part of Figure
345 of linear paths rather then some radius of neighbors. For relational ARM this is rarely a problem since very few hop structures are likely to be relevantas the length increases. While Graph ARM  is concerned with scaling as a function of the subgraph size, the major problem in relational ARM is scaling with respect to itemset size. Also due to the ”small world” property of scale-free
345 of the database, usually in the form of adjacency matrices, and then finds frequent substructures of the graph in Apriori fashion. Much progress has been made in this area as shown in various works . Graph-based ARM concerns itself with identifying patterns of nodes and edges, which places the focus on the structure of interactions. Most settings allow edge labels and node labels in the
346 to deal with an ordering relationship among the entities. Sequences are linear graph structures or paths. This type of mining has received as much attention as association rule mining in general . It can be seen that sequential pattern mining has the capability to mine a subset of the transactions we may produce for our algorithm. The subset is that of all linear patterns as part of Figure
347 4. The rule (R1) is likely not to add information compared with boring and out-of-scope rules. Issues of redundant and misleading rules have extensively been discussed in the standard ARM setting . In the relational setting these problems are aggravated by the fact that commonly the strongest and simplest rules are also the least interesting ones. The problem is not limited to itemsets of
347 the standard ARM setting. Note that boring and out-of-scope rules themselves are trivially eliminated by our algorithm. This approach is different from other work on redundancy in association rules  in which some rules are discarded in favor of others. In the presence of rules that are explicitly irrelevant we can, in principle, go a step further in standard ARM and correct support and
348 to deal with an ordering relationship among the entities. Sequences are linear graph structures or paths. This type of mining has received as much attention as association rule mining in general . It can be seen that sequential pattern mining has the capability to mine a subset of the transactions we may produce for our algorithm. The subset is that of all linear patterns as part of Figure
1939 compared to the superscalar; (5) the memory hierarchy used for both models is identical: an LSQ and a two-level cache hierarchy 2 . The superscalar is a 4-way out-of-order SimpleScalar simulation  with the PISA instruction set, using gcc -O2 2.7.2 as 2 For this study we use a very similar LSQ for both ASH and the superscalar. As future work we are exploring the synthesis of program-specific
868 dealing with service composition, especially with computationally complex services, is the efficient routing of service requests among a set of providers. OGSA (Open Grid Services Architecture)  compliant Grid systems are rapidly emerging and are widely accepted. These Grid systems provide support to efficiently invoke and use individual services in the Grid in a request/reply style.
868 and QoS restrictions. The framework developed is based on GT3 . The core part consists of a set of classes building the central master and slave services. These are OGSAcompliant Grid Services  bundled with corresponding stubs and some supporting classes for specialized exceptions and encapsulating the input and output parameters passed around. The work left to the application programmer
483 spreading signatures, throughput I. INTRODUCTION AND MOTIVATION We consider a wireless sensor network which may contain thousands of tiny low-cost sensors scattered over a region of interest ,. The sensors (also called nodes) are battery operated (i.e., energy constrained), have limited memory and processing power, and form a randomly connected ad hoc network. Since the nodes are mostly
943 within 95% of the optimal value. I. INTRODUCTION In this work, we address the problem of load balancing in peer-to-peer (P2P) systems that provide a distributed hash table (DHT) abstraction (, , , ). In such structured systems, each data item that is stored is mapped to a unique identifier ID. The identifier space is partitioned among the nodes and each node is responsible for
943 may consist of peers that range from old desktops behind modem lines to powerful servers connected to the Internet through high-bandwidth lines. If node identifiers are chosen at random (as in , , , ), a random choice of item IDs results in an ¢¤£¦¥¨§?©???? imbalance factor in the number of items stored at a node. Furthermore, applications may associate semantics with IDs, which means
943 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Load/Target Fig. 4. The effect of dislodges on the number of rounds. The load distribution is Gaussian. V. RELATED WORK Most structured P2P systems (, , , ) assume that object IDs are uniformly distributed. Under this assumption, the number of objects per node varies within a factor of ¢¤£¦¥¨§?©???? , where ? is the number of nodes in the
943 this factor by considering a subset of existing nodes (i.e., a node along with neighbors) instead of a single node when deciding what portion of the ID space to allocate to a new node. Chord  was the first to propose the notion of virtual servers as a means of im¥ ? §?© proving load balance. By allocating virtual servers per real node, Chord ensures that with high probability the number
940 of the optimal value. I. INTRODUCTION In this work, we address the problem of load balancing in peer-to-peer (P2P) systems that provide a distributed hash table (DHT) abstraction (, , , ). In such structured systems, each data item that is stored is mapped to a unique identifier ID. The identifier space is partitioned among the nodes and each node is responsible for storing all the
940 of peers that range from old desktops behind modem lines to powerful servers connected to the Internet through high-bandwidth lines. If node identifiers are chosen at random (as in , , , ), a random choice of item IDs results in an ¢¤£¦¥¨§?©???? imbalance factor in the number of items stored at a node. Furthermore, applications may associate semantics with IDs, which means that IDs
940 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Load/Target Fig. 4. The effect of dislodges on the number of rounds. The load distribution is Gaussian. V. RELATED WORK Most structured P2P systems (, , , ) assume that object IDs are uniformly distributed. Under this assumption, the number of objects per node varies within a factor of ¢¤£¦¥¨§?©???? , where ? is the number of nodes in the system. CAN
760 a large amount of idle time across many short intervals. Occasional accesses that appear during interactive workloads can be handled adequately by previously proposed disk power management policies . In order to associate related file accesses that are genKernel Level erated by different processes (as an example consider accesses caused by the various invocations of thegcc compiler during the
760 Rambus DRAM chips. Hard Disks. The energy efficiency of hard disks is not a new topic. The cost and risks of Standby mode played a role in the early investigation of hard-disk spin-down policies . Concurrently with our own work , several groups have begun to investigate the deliberate generation of bursty access patterns. Heath et al.  and Weissel et al.  propose user-level
760 Even relativelysshort increases in the average idle interval length can lead to significant energy savings, mostly by making more efficient use of intermediate low-power states. Published studies  attribute 9-32% of the total laptop energy consumption to the hard disk. These figures imply that our prefetching algorithm may increase battery lifetime by up to 25%. The exact fraction depends on
763 and frequency scaling. The key idea is to schedule so as to “squeeze out the idle time” in rate-based applications. Several researchers have proposed voltage schedulers for general purpose systems . Lebeck et al.  explore power-aware page allocation in order to make more efficient use of memory chips supporting multiple power states, such as the Rambus DRAM chips. Hard Disks. The energy
766 a large amount of idle time across many short intervals. Occasional accesses that appear during interactive workloads can be handled adequately by previously proposed disk power management policies . In order to associate related file accesses that are genKernel Level erated by different processes (as an example consider accesses caused by the various invocations of thegcc compiler during the
766 Rambus DRAM chips. Hard Disks. The energy efficiency of hard disks is not a new topic. The cost and risks of Standby mode played a role in the early investigation of hard-disk spin-down policies . Concurrently with our own work , several groups have begun to investigate the deliberate generation of bursty access patterns. Heath et al.  and Weissel et al.  propose user-level
3019 in the design of operating systems. ECOSystem  provides a model for accounting and for fairly allocating the available energy among competing applications according to user preferences. Odyssey  provides operating system support for application-awaresresource management. The key idea is to trade quality for resource availability. Several policies have been proposed to decrease the power
773 and frequency scaling. The key idea is to schedule so as to “squeeze out the idle time” in rate-based applications. Several researchers have proposed voltage schedulers for general purpose systems . Lebeck et al.  explore power-aware page allocation in order to make more efficient use of memory chips supporting multiple power states, such as the Rambus DRAM chips. Hard Disks. The energy
763 on the idle and active cycles of the previous interval. If the idle cycles exceed a threshold, it slows down the processor. Else if the active cycles are higher, it speeds it up. As Govi et al.  point out, PAST uses a narrow window of past information to predict future workloads and changes the clock speed at every interval, increasing energy dissipation and cycles. Variations of PAST
773 results 6.1 Interval Level At the largest granularity are interval-based policies that regularly adjust processor speed based on prior workloads. The simplest algorithm of this kind is PAST . PAST adjusts CPU speed at fixed length intervals based on the idle and active cycles of the previous interval. If the idle cycles exceed a threshold, it slows down the processor. Else if the
770 (every paging area change) than in active state (every base station change). Since the power spent in updating the network is an order of magnitude greater than the power spent in standby mode , battery power consumption at the mobile host is reduced significantly . Thus, the main benefit of providing a paging service is to facilitate efficient power management at the mobile host. The
770 networks (WAN) such as General Packet Radio Service (GPRS)  and CDMA data . Wireless local area network (LAN) protocols such as IEEE 802.11 , also have the notion of a powersave state . Here the paging functionality is limited to waking the mobile host from power-save (standby) state to active state at a single base station. The paging architecture and protocols in each of these
3193 and disk storage, without any administration cost or centralized infrastructure support. The previous research work on P2P systems has mainly focused on providing scalable P2P discovery services , which enable data and resource sharing, in P2P systems. However, few have addressed the service sharing problem which is important for the user of P2P systems to utilize a wealth of application
3011 proxy network in terms of resource availability. Hence, their solution only applies to the small size proxy network, and thus does not meet the scalability requirement of P2P systems. Finally, in , we proposed a centralized approach to address the service composition and instantiation problems in ubiquitous computing environments. We achieved scalability by applying a hierarchical design to
939 and disk storage, without any administration cost or centralized infrastructure support. The previous research work on P2P systems has mainly focused on providing scalable P2P discovery services , which enable data and resource sharing, in P2P systems. However, few have addressed the service sharing problem which is important for the user of P2P systems to utilize a wealth of application
939 using application-specific QoS parameters such as frame rate, response time. • Discover service instances. Once the user request is acquired, the P2P lookup protocol, such as Chord  or CAN , is invoked to retrieve the locations (i.e., IP addresses) and QoS specifications (Q in , Q out , R) of all candidate service instances, according to the abstract service path. • Compose a QoS
939 P2P systems have drawn much research attention with the popularity of P2P file sharing applications such as Gnutella and Napster. However, most of the research work, such as Chord  and CAN , focuses on providing an efficient lookup service which enables the data (e.g., mp3 files) and resource (e.g., disk storage) sharing in P2P systems. We believe that in order to make P2P systems
188 number of application-level connections. Each hop in the service aggregation may include many network-level hops depending on the network distance between two peers. 2.2 Network Model According to , peers are highly heterogeneous and also reluctant to report their performance information to other peers or deliberately misreport the information. Hence, in order to provide efficient QoS
943 and disk storage, without any administration cost or centralized infrastructure support. The previous research work on P2P systems has mainly focused on providing scalable P2P discovery services , which enable data and resource sharing, in P2P systems. However, few have addressed the service sharing problem which is important for the user of P2P systems to utilize a wealth of application
943 output with a QoS level Q out , both of which are vectors of application-level QoS parameters. The application-level QoS parameters can be data format (e.g., MPEG, JPEG) , frame rate (e.g., fps), and others. In order to process input and generate output, a specific amount of resources R is required, which is a vector of required end-system resources (e.g., cpu, memory). The network
943 QoS requirements using application-specific QoS parameters such as frame rate, response time. • Discover service instances. Once the user request is acquired, the P2P lookup protocol, such as Chord  or CAN , is invoked to retrieve the locations (i.e., IP addresses) and QoS specifications (Q in , Q out , R) of all candidate service instances, according to the abstract service path. •
943 Work Recently, P2P systems have drawn much research attention with the popularity of P2P file sharing applications such as Gnutella and Napster. However, most of the research work, such as Chord  and CAN , focuses on providing an efficient lookup service which enables the data (e.g., mp3 files) and resource (e.g., disk storage) sharing in P2P systems. We believe that in order to make
939 robust and can route lookup queries in just one hop, thus enabling applications that cannot tolerate the delay of multi-hop routing. 1 Introduction Structured peer-to-peer overlay networks like CAN , Chord , Pastry , and Tapestry  provide a substrate for building large-scale distributed applications. These overlays allow applications to locate objects stored in the system in a
943 can route lookup queries in just one hop, thus enabling applications that cannot tolerate the delay of multi-hop routing. 1 Introduction Structured peer-to-peer overlay networks like CAN , Chord , Pastry , and Tapestry  provide a substrate for building large-scale distributed applications. These overlays allow applications to locate objects stored in the system in a limited number of
943 we associate a successor node with every 128-bit key key; this is the first node in the identifier ring clockwise from key. This mapping from keys to nodes is based on the one used in Chord , but changing our system to use other mappings is straightforward. Clients issue queries that try to reach the successor node of a particular identifier. We intend our system to satisfy a large
975 from the impact of research to produce new technology. But more than in the case of technology, the steps leading from research-derived knowledge to benefits are not easily measurable (Smith and Pardey 1997). Policy makers draw from a pool of information for defining and prioritizing problems and finding appropriate solutions; policy research merely adds to this pool of information. This makes it
1755 r2. r1: A TS a  a  a  r2: A TS a  The results, R1 through R4, of four possible definitions of the difference operator are given next, and are discussed in turn. R1: A TS a  a  R2: A TS a  a  a  R3: A TS a  a  R4: A TS a  a  a  The first result contains the times associated with value ainr1 that are not
1755 coalesced, but not over the uncoalesced, instance. The reason for this is that it is possible to derive the coalesced relation instance from an uncoalesced one, e.g., using a regular SQL statement . These considerations indicate that a model that is able to tell coalesced and uncoalesced relation instances apart, and in this sense is interval-based, is in some sense more powerful than a model
1755 database system must guarantee that the result of queries do not depend on the specific choice of timestamp values. This guarantee is met by performing coalescing operations, which can be expensive . While it is possible to sometimes eliminate coalescing during query optimization, there remain situations where coalescing has to be performed . 3 Temporal Data Models and Time Domains
1755 usage of points. To exemplify this, assume that the integers with the < order is our time point domain. Then it seems reasonable to claim that the relations r1 = fhak2i; hak3i; hak4ig and r2 = fhakig have the same information contents, i.e., that hai is valid at instants 2, 3, and 4, and nothing more. This assumption, nonetheless, is incorrect for r2 because intervals in addition to being
1755 temporal difference operator. Let the integers with the < order be the underlying time point domain, and D denote the minimum requirements for this operator. Assume r1 = fhakig and r2 = fhaki; haki; haki; hbkig. If r 00 1 = r0 1 = r1, r0 2 fhaki; hakig,then D (r1;r2;hai;f5;6;7g) D (r 0 D (r 00 = fhakig, and r00 2 1 ;r0 2 ;hai;f5;6;7;8;9;10g) ;
1957 see that the above methodology can be also used for the case of more than two matchers. Among the fusion rules which can implement eq. (2), the most simple ones are the so-called fixed fusion rules . This term derives from the observation that such rules require no training parameters. In other words, eq. (2) can be implemented by simply combining the given matching scores without other kind
2473 by using cooccurrences in lists, conjunctions, and appositives (Roark and Charniak, 1998); metabootstrapping which repeatedly finds extraction patterns and extracts words from the found patterns (Riloff and Jones, 1999); a co-training combination of three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s
1258 be easily solved using the EM framework, by assuming that the missing data is the posterior probabilities of the mixture components for only the objects ? ? ? in , i.e., the unlabeled data objects . Because of this, we only need to update the posterior probabilities of the unlabeled data objects in the expectation step. The maximization step remains unchanged. This results in a modified EM
2463 of the selectional restrictions of all the verbs, while the verbs themselves provide intensional descriptions for each concept. (ii) The second approach is based on a combination of Hearst-Patterns , WordNet  and various heuristics, which can be selected separately by the user 1 . 5.2.2 TermExtraction TermExtraction can be used to create new concepts from possibly relevant terms
2463 are to be extracted from the corpus. InstanceExtraction (see figure 5.2) supports both semi-automatic and fully automatic learning of instances by applying a combination of various patterns from  and , which can be selected separately by the user. If InstanceExtraction is performed semi-automatically each candidate term will be 1 Using WordNet requires an installation of WordNet 1.7.1
2463 a list of candidate relations from the text. Whereas the first approach is based on association rules , the second one applies a set of text patterns very similar to those defined by Hearst . The user being presented this list can select single or multiple candidates and add them to the OI-Model - either as a property or a taxonomic relation. Available parameters for RelationExtraction
317 entropy method described in e.g.  might be useful in finding topics. The method is used to answer queries about the data as follows: first, one mines frequent sets with some threshold , and then finds the maximum entropy distribution  consistent with the frequent sets. We performed experiments using simulated data to see whether the results are consistent with the topic
1445 the new image with the most “relevant” term nodes. We have many choices: electricity based approaches ; random walks (PageRank, topic-sensitive PageRank) ; hubs and authorities ; elastic springs . In this work, we propose to use random walk with restarts (“RWR”) for estimating the affinity of node “ ¡ ” with respect to the restart node “¢ ”. But, again, the specific
2358 The system is further simplified if we approximate v ? (y) = c for y > y ? . Then (41) reduces to ? = b c(2 ? r) (42) For TCP Reno TCP and RED, we use the following model for f and g (e.g., ): p1 = 2 2 + x2 1 (d + ?)2 (43) r = k(b ? b) (44) for some k and b. With c = 125 pkts/s, d = 0.1s, N = 32, b = 20 pkts, k = 10 ?3 , the asymptotic values are numerically solved to be (values in
2152 ad-hoc networks makes this scheduling problem very different from that in infrastructure networks. Interference mitigating techniques and distributed protocols have been considered in this regard ,,. Routing is the functionality of transporting data from the source to the destination across a sequence of links. In adhoc networks, routing faces a number of challenges, including the
2289 wireless link  and node mobility. These issues differ significantly from their counterparts in both cellular networks and wired networks (such as the Internet), and have been extensively studied ,,. Recently, there has been significant interest in computing the capacity of ad-hoc networks ,,,. Consider an ad-hoc wireless network, where n identical nodes on a unit area
2722 link  and node mobility. These issues differ significantly from their counterparts in both cellular networks and wired networks (such as the Internet), and have been extensively studied ,,. Recently, there has been significant interest in computing the capacity of ad-hoc networks ,,,. Consider an ad-hoc wireless network, where n identical nodes on a unit area
2249 every subset B of X there is a set F?F such that X ? F = B. F generates all subsets of X. The VC-dimension of F is defined as the supremum of the sizes of all finite sets that can be shattered by F ,. An underlying probability distribution (PD) is assumed. An i.i.d sequence X = X1...Xn is chosen with distribution ? PD. The relative frequencies of events are N(F ) = 1 n n i=1 I(Xi?F ). Sets
1823 decoder then selects an output from the joint lattice. 5.3 Experimental Results Performance of the systems was measured using the NIST scoring metric , as well as the Bleu score . In order to validate the statistical significance of the differences in NIST and Bleu scores, we applied a commonly used sampling technique over the test set: we randomly draw 258 sentences
1445 a di erent angle. They employed Foil (First Order Inductive Learner) (Quinlan, 1990), a relational learner to exploit the relational structure of the web, and a Hubs & Authorities style algorithm (Kleinberg, 1998) to exploit the hyperlink topology. By combining these two algorithms they had improved classi cation accuracy over Foil, on a corpus of university web pages in three classes. There are also
2146 Most of the assumptions made in our analysis of directional collision avoidance schemes have been used successfully in the performance evaluation of several MAC protocols , , , ,  to obtain tractable analytical models. As we have stated, to make the analysis of a multi-hop network tractable, we follow the line of modeling first used by Takagi and Kleinrock . In this
1069 employed in , . Other approaches to capturing longer term dependencies have been presented in  and functions of first order Markov chains such as the Hidden Markov Model (HMM) ,  successfully capture longer term dependencies, however, inevitably these come with an increased computational cost. DRAFT July 13, 2003sThe representation provided by such models is global in
1255 key insight is to simultaneously build several redundant functions and exploit the disagreement amongst them to discover new kinds inconsistencies amongst duplicates in the dataset. Active learning  methods also rely on a similar insight for selecting instances for labeling from a large pool of unlabeled instances. Unlike an ordinary learner that trains using a static training set, an active
1255 will likely not have much effect on the learner. Theoretical justification for approximating expected reduction in confusion (formally, version space) with prediction uncertainty appear in . The example above was for a simple case where the two classes are completely separable by the classifier. Reallife data is noisy and when picking instances based on uncertainty we need to make
1255 the notions of uncertainty (Section 3.1) and representativeness of an instance (Section 3.2). The techniques discussed are a distillation of the various methods of active learning proposed recently  along with an explanation of our particular design rationale. We will accompany each design decision with justifications of our chosen approach using experiments on two real-life datasets: a
1255 for clustering, number of clusters and the weights to tradeoff uncertainty with representativeness. A second more common approach relies on sampling to preserve the underlying data distribution . First, each candidate unlabeled instance is weighted by its uncertainty value. Then the n instances for active learning are selected from this using weighted sampling. We chose and experimented
3317 in the range of ? is denoted by vrange(?). A substitution ? is idempotent if ?? = ?. The class of idempotent substitutions exhibit some interesting properties and have been studied extensively . We say that two substitutions ? and ? are independent if dom(?) ? vrange(?) =? and dom(?) ? vrange(?) =?. It can be shown the mgu’s are unique up to renaming of variables. We will sometimes
3325 work has already been done in this direction, such as for example in the OntoWeb SIG-2 1 or by Motta et al., working on annotation formalisms and reasoning in Web environments (Motta et al., 2000; Domingue and Motta, 2000) • The Web already contains a massive amount of information that will not be rewritten to fit a knowledge encoding formalism. Furthermore the majority of people writing new texts are probably or
3331 to describe words that belong to distinct categories as synonyms for one concept. In this sense the lexicon reflects the EuroWordNet strategy to allow for cross-category listing of synonyms (Vossen, 1997). • Regular expression-like syntax can be used to combine entries with the same structure that differ for example only in the choice of a word (such as alternative prepositions expressing the same
3335 further in this section. 5.1 Annotation of Video Recordings The only textual sources that can be directly matched to the video recordings are the transcripts of the speech in those recordings (Wester et al., 2002). Even though Information Extraction results for these transcripts is possible, the transcripts themselves contain more errors than the other sources. Furthermore they contain relatively few actual
940 of flooding. The basic functionality they offer is lookup (key), which returns the identity of the node storing the object with that key. Recent proposed DHT systems include Tapestry , Pastry , Chord , CAN  and Koorde . In these DHT systems, objects are associated with a key that can be produced by hashing the object name. Nodes have identifiers that share the same space as
940 volume is also replicated. 6.3 Structured Peer-to-Peer Networks Besides Chord, there are many other structured Peer-to-Peer networks proposed in recent years, such as Tapestry , Pastry , CAN , Koorde , Skip Graphs  and SkipNet . The routing algorithms used in Tapestry and Pastry are both inspired by Plaxton  . The idea of the Plaxton algorithm is to find a
3197 in recent years, such as Tapestry , Pastry , CAN , Koorde , Skip Graphs  and SkipNet . The routing algorithms used in Tapestry and Pastry are both inspired by Plaxton  . The idea of the Plaxton algorithm is to find a neighboring node that shares the longest prefix with the key in the lookup message and to repeat this operation until a destination node is found
2734 communication devices that lack centralized routing infrastructure. A number of recent studies are related to the investigation of throughput and traffic localization issues of ad hoc networks . One of the fundamental theoretical results is published by Gupta and Kumar in . They have shown that for stationary ad hoc networks the per node throughput of the system decays as ? ? 1 O ?n
713 Another advantage of XML is the ability to query it to retrieve and manipulate data stored in the document. A number of query languages have been developed, including Lorel , Quilt , UnQL , XDuce , XML-QL , XPath , XQL , Xquery , and YaTL . XPath and Xquery are two query languages that received the W3C recommendation. XML-RPC XML-RPC (XML-Remote Procedure
2082 Output MSB MSB Tile Output MSB Tile Logic & Local Wiring South Output East Output Figure 2 West input connections to output controllers MSBsThe simple network employs virtual-channel flow control . Details of an input controller and output controller are shown in Figure 3. Each input controller has an input buffer and input state logic for each virtual channel. When a head flit arrives at
1722 formed using terms from an ontology. Unfortunately, while existing techniques for TBox reasoning (i.e., reasoning about the concepts in an ontology) seem able to cope with real world ontologies , it is not clear if existing techniques for ABox reasoning (i.e., reasoning about the individuals in an ontology) will be able to cope with realistic sets of instance data. This difficulty arises
1724 using a database lookup. (The chances of detecting equivalence via syntactic checks could be increased by transforming concepts into a syntactic normal form, as is done by optimised DL reasoners , but this additional refinement has not yet been implemented in iS.) Individuals are grouped into equivalence sets, where each individual in the set is asserted to be an instance of a syntactically
1725 formed using terms from an ontology. Unfortunately, while existing techniques for TBox reasoning (i.e., reasoning about the concepts in an ontology) seem able to cope with real world ontologies , it is not clear if existing techniques for ABox reasoning (i.e., reasoning about the individuals in an ontology) will be able to cope with realistic sets of instance data. This difficulty arises
1725 specialised ABox reasoners we have compared the performance of iS with that of RACER  (the only publicly available DL system that supports full ABox reasoning for an expressive DL) and of FaCT  (using TBox reasoning to simulate reasoning with a role-free ABox). Related Work As already mentioned, the idea of supporting DL style reasoning using databases is not new. One example is ,
1725 DL reasoners and databases. The core component is a Java application  talking to a DLsreasoner via the DIG interface  and to a relational database via JDBC. We have tested it with FaCT  and RACER reasoners and MySQL, Hypersonic, and Oracle databases. initialise(Reasoner reasoner, Database db, TBox t) addAssertion(Individual i, Concept C) retract(Individual i) retrieve(Concept Q):
1939 compiler-based swapping methods more attractive. 6 Experimental Results We have implemented the methodology described in section 4 using the sim-outorder simulator from the SimpleScalar 2.0 suite , with the default configuration of 4 IALUs, 4 FPAUs, 1 integer multiplier, and 1 floating point multiplier. SimpleScalar simulates a MIPS-like machine, 17swith 32-bit integer registers and 64-bit
3260 the commercial e-business process management. It did not address the endto-end QoS assurances for the composed service. Our work is also different from the traditional IP-layer QoS routing problem  because: (1) The goal of IP-layer QoS routing is to find a network routing path satisfying QoS constraints while QUEST addresses two mapping problems to achieve not only QoS assurances but also
3011 addressed the adaptability problem in service composition. The SPY-Net  framework addressed the problem of resource contention while finding a multimedia service path. In the Gaia project , we addressed the QoS consistency and load partition issues for composing service path in ubiquitous computing environments. However, little has been done to support generic QoS provisioning for
3011 is constrained by the user’s application-specific quality requirements and different pervasive client devices, such as PDAs and cell-phones. Mapping-1 has been addressed by several research work . It can be performed based on the application-specific QoS specifications  or using automatic composition plan tools . The mapping from the composite service template to an instantiated
676 it produces the error trace t1 = . Our algorithm uses information about correct traces in the program to localize the cause of the error. This program has only one correct trace t2 = . The only portion of t1 that does not intersect with t2 is line 4 and our algorithm highlights it as the likely cause. Indeed, thesmain() { 1 AcquireLock(); 2 if (...) 3 ReleaseLock(); else 4 ...;
676 the model checker again. The halt statement instructs the model checker to stop exploring paths through the statement at line 4. As a result, the model checker reports a different error trace t3 = . Again, by comparing t3 with the correct trace t2, line 8 is highlighted as the potential cause of the error, another halt statement is introduced at line 8, and the model checker is invoked for
676 point, it reports that there are no more error traces. In summary, our algorithm automatically produces two error traces, namely, t1 =  with line 4 as the identified cause, and t3 =  with line 8 as the identified cause. ? We present the following results in this paper: • A technique for using a model checker as a subroutine to localize the error cause in an error trace, report
676 problem. Consider the program in Figure 8. We wish to check if it returns the same value as the function foo, which is called exactly once along every path. The program has one error trace t1 = . The error cause lies on lines 3 and 4: the value returned by foo is ignored at line 3 and the value SUCCESS is unconditionally assigned to the variable status at line 4. The algorithm fails to
676 status have the value SUCCESS at line 6. Consider the program in Figure 9. We wish to check whether the function bar is called at most once along every path. The program has one error trace t1 =  in which bar is called twice. The error cause is on line 3: baz is called assuming that the return value of foo is always SUCCESS. The algorithm fails to localize this cause since it belongs to a
90 much as is dictated by the M&S intended uses and project objectives rather than trying to test the M&S application completely. Although more than 100 verification and validation (V&V) techniques (Balci 1998; Binder 2000) are available, only a limited number of techniques are used in an M&S project due to time and resource constraints. Limited testing hinders our ability to substantiate sufficient M&S
90 model execution behavior to intent in conducting VV&A. To a certain extent, however, computer-aided support can be provided to perform this comparison by using the assertion checking V&V technique (Balci 1998). An assertion is a statement that should hold true as the simulation model executes. Assertion checking is a V&V technique used to compare an execution profile against the expectations of the
1719 Activity, for example,shelps to determine an instance of Health Care Activity by contributing its “strength of evidence”. 2.2 DAML+OIL-Based Ontology of a simplified UMLS Semantic Network OilEd , the DAML+OIL editor, was used in order to specify the relationships between our three selected Semantic Types and the remaining Semantic Types of UMLS. Among all possible relationships (edges), we
3145 where ?i is the i th singular value of Li and ? is a pre-specified threshold that depends on the noise level. One can also use the geometric information criterion to estimate the rank as shown in . Even though we have derived the polynomial segmentation algorithm Polysegment in a purely algebraic setting, it turns out that it also has a probabilistic interpretation. Let {xj} N j=1 noise
3145 are hard to verify in the presence of noise. In order to estimate n and k in a robust fashion, one could for example combine our rank constraints with model selection techniques, 110ssimilarly to . Furthermore, in the case of subspaces of arbitrary dimensions, the estimation of the number of subspaces is still an open question. In fact, as mentioned in Remark 25, it is possible to
3154 Estimating a collection of subspaces is then equivalent to estimating the algebraic variety defined by such a set of polynomials. 2 Part of the results presented in this chapter were published in . 47s2. Mixtures of (K ? 1)-dimensional subspaces: We show in Section 3.3 that the union of n subspaces of dimension k = K ? 1 is defined by a unique homogeneous polynomial pn(x). The degree of
3154 In the particular case of data lying on hyperplanes, we have that Bi = bi ? R K for i = 1, . . . , n. Therefore, the objective function in (3.97) becomes EO(b1, . . . , bn) = as demonstrated in . N? j=1 ? npn(x j ) ? 2 ???Dpn(xj = )?2 N? n2 ?n ? ? ?n ? i=1 j=1 i=1 (bT i x j ) 2 ??=i (bT ? xj )bi ? ?2 , (3.98) 3.7 Initialization of iterative algorithms in the presence of noise In this
943 enhanced HTML-document. Although, GnuViz was developed for a Gnutella network, it can be used within other existing networks like Freenet  or JXTA  and future networks like in Chord , Pastry  or Tapestry  too. Due to its modular design, only the crawler has to be adapted. 2.1. The crawler To explore the connections between the nodes participating in the Gnutella network,
2285 issues (, ) and the user behaviour  and not on the network structure. Other attempts to map the network only explore the network and do not map the network to its geographical locations . They are thus not able to show the problems of Gnutella implied by the random assignment of connections. With the help of GnuViz, we want to make this world spanning network visible. Thus we are
188 between the Gnutella nodes can also reach all over the world, and can also span the whole world. Previous measurement approaches concentrated on traffic issues (, ) and the user behaviour  and not on the network structure. Other attempts to map the network only explore the network and do not map the network to its geographical locations . They are thus not able to show the
3193 favor specific requirements: in Gnutella, the emphasis is on accommodating highly volatile peers and on fast file retrieval, with no guarantees that files will always be located. In Freenet , the emphasis is on ensuring anonymity. In contrast, distributed hash tables such as CAN , Chord , Pastry , and Tapestry  guarantee that files will always be located, but do not support
939 peers and on fast file retrieval, with no guarantees that files will always be located. In Freenet , the emphasis is on ensuring anonymity. In contrast, distributed hash tables such as CAN , Chord , Pastry , and Tapestry  guarantee that files will always be located, but do not support wildcard searches. One way to optimize these tradeoffs is to understand user behavior. In
175 efficient solution design. A well known example is the relationship between file popularity in the Web and cache size. The popularity of web pages has been shown to follow a Zipf distribution , : few pages are highly popular and many pages are requested few times. As a result, the efficiency of increasing cache size is not linear: caching is useful for the popular items, but there is
2285 degree distribution) were inaccurate: the Internet topology had a power-law degree distribution . Other results followed: the web graph ,  and the Gnutella overlay (as of year 2000)  are also power-law networks. Another class of networks are the “small worlds”. Two characteristics distinguish small-world networks: first, a small average path length, typical of random graphs
2285 interval and temporal user activity, meaning that users are not uniformly active during a period, but follow some patterns (for example, downloading more music files during weekends or holidays ). Thus, we ask: Q4 Are the properties we identified in the data-sharing graph, especially the large clustering coefficient, an inherent consequence of these well-known behaviors? To answer this
1028 of annotating images in the cultural heritage domain. Ontologies are rapidly emerging as ‘engineering artifacts’ with standardized languages such as Resource Description Framework (RDF) 05=1 (Lassila and Swick, 1999) and RDF Schema (Brickley and Guha, 2000) that have been developed by the Semantic Web initiative (BernersLee et al., 1999) led by the World Wide Web Consortium (W3C). While ontologies allow for
2722 for routing protocols that exploit connectivity determined by lower level link estimations to form a routing topology. Many wireless routing protocols assume connectivity by hearing certain packets. For example, ROUTE REQUEST and ROUTE REPLY messages are often building blocks for creating a routing structure. However, Figure 1(a) suggest that long unreliable links are likely to influence
2722 to the current parent worsens, its link estimation will automatically degrade over time, allowing the selection of a new parent. This is in contrast to traditional link detection technique found in , which counts the number of transmission failures, and is better suited to handle semi-lossy links. If connectivity to the current parent is lost and no potential parents are available, the node
2722 a parent based on the source address of the first flooding messagesthat it receives in each epoch. The broadcast protocol essentially captures reverse path routing as found in protocols such as DSR . IV Destination Sequenced Distance Vector (DSDV) uses destination based sequence numbers to avoid cycle formation . We adopt DSDV into our framework and preserve the essence of the protocol;
2422 of a stakeholder. The importance of multiple views as an effective means of separating the concerns during architectural design and analysis has been realised by SA researchers and practitioners in . However, there are different opinions on the number and nature of the architectural views (for example Hofmeiser et al.  suggest conceptual, module, execution, and code views; Kruchten
1069 LOCALLY TESTABLE TREE LANGUAGES A number of algorithms have been proposed in the past to build automata from stochastic samples. In some cases the result is a non-deterministic automaton , ; in others the automata are deterministic , . Here, a stochastic sample ? = ?1, ?2, . . . ?|?| consists of a sequence of (possibly repeated) trees generated according to an unknown
1445 difference between an exhaustive breadthfirst crawler and a typical focused crawler. A focused crawler implements a strategy that associates a score with each link in the pages it has downloaded . The links are sorted according to the scores and inserted in a 528 queue. A best first search is performed by popping the next page to analyze from the head of the queue. This strategy ensures
1258 This classifier architecture provides reasonable performance, high speed, meets the requirement of our system that a likelihood estimate be provided for each classification, and is well studied . Assume that we have a document di represented by the vector corresponding to the reduced TF-IDF representationsrelative to the vocabulary V. Documents from class c j, defined to correspond to
1258 P wt ?c j ? N wt ?di P c j?di (4) ? ?V ? ?di D j ??V s?1 N ws?di P c j?di where N wt ?di is the number of occurrences of wt in the document di and ?V ? is the number of phrases in the vocabulary V . The parameters P c j can be calculated by estimating the number of elements in each of the layers of the merged context graph. While useful when the layers do not contain excessive numbers of
2121 attacks is bound to have a significantly large adverse impact on the economy. In the last few years researchers have been examining survivability issues in WDM networks , , , , , , , , , , , , , , . Two techniques, protection at the WDM layer and restoration at the IP layer have emerged as the two main contenders for fault management
2121 the paper. II. RELATED WORK Although several researchers in the last few years have published a significant number of papers on survivability issues in WDM optical networks , , , , , , , , , , , to the best of our knowledge, the topic of this paper, the complexity of the disjoint paths problem with wavelength continuity constraint, remains open.
1107 the speaker before tracking his/her facial movements. Even then, it is not guaranteed that they could be fairly adapted to every facial configurations. Data driven models can cope with this problem  . We will describe below a methodology that lets control parameters of a speaker-specific model emerge from a statistical analysis of fine-grained 3D data. With only a 3D model, low-level
1104 flow generates a large number of correspondences, the inversion is more likely to lead to a solution . It can be combined with edge-adjustment , or with analysis-by-synthesis technique . Face images depend on head motion, illumination conditions and facial movements. In presence of large image changes, tracking can take great advantage of an appearancevariations model, which is
2722 information between any two nodes in the network. Typically, such support can be provided by a separate layer that supports a routing protocol for ad hoc networks, like Dynamic Source Routing (DSR) , and exposes an interface to query the routing information. 2.2 Architecture Components Figure 2(A) shows a high-level view of the DFuse architecture that consists of two main runtime components:
3255 the next-generation IP routers should be QoS-capable. Limited by the Moore’s Law, one possible solution is to introduce parallelism as well as the Differentiated Service (DiffServ) scheme  into the router architecture to provide QoS provision at a high speed and a low cost. In this paper, we propose a novel architecture called the High-Performance QoS-capable IP Router (HPQR). We
2722 used for evaluating the routing quality, the most common one is probably the number of hops on the routing path. The protocols that use shortest path routing include Dynamic Source Routing (DSR) , Ad-hoc On-demand Distance Vector routing (AODV)  and many others. Please refer to the surveys  and the references therein. On the other hand, energy-aware routing algorithms, which try to
2722 al.  proposed an algorithm GAF which is designed to reduce the energy consumption by turning off unnecessary nodes. In , , the traditional energy-unaware routing protocols such as DSR  or AODV  were re-visited to take into account the energy-aware metric. All of the energy-aware protocols mentioned above are heuristics and do not provide any guarantee on the performance. The
2289 common one is probably the number of hops on the routing path. The protocols that use shortest path routing include Dynamic Source Routing (DSR) , Ad-hoc On-demand Distance Vector routing (AODV)  and many others. Please refer to the surveys  and the references therein. On the other hand, energy-aware routing algorithms, which try to maximize the network survivability, have attracted a
2289 an algorithm GAF which is designed to reduce the energy consumption by turning off unnecessary nodes. In , , the traditional energy-unaware routing protocols such as DSR  or AODV  were re-visited to take into account the energy-aware metric. All of the energy-aware protocols mentioned above are heuristics and do not provide any guarantee on the performance. The paper is
2249 successfully e.g. in splines, multilayer perceptrons, regularization networks (Poggio and Girosi, 1990), Support Vector Machines (SVM) and related methods (see e.g. (Hastie et al., 2001)). SVM (Vapnik, 1998) is a powerful methodology for solving problems in nonlinear classification, function estimation and density estimation which has also led to many other recent developments in kernel based learning
2249 with the selected components (compare to basis pursuit, see e.g. (Chen et al., 2001)). 3.2 Sparse LS-SVM substrates Sparseness is often regarded as good practice in the machine learning community (Vapnik, 1998; von Luxburg et al., 2004) as it gives an optimal and minimal representation of the solution (from the viewpoint of VC theory and compression) The primal-dual framework also provides another line
2249 of the Lagrange multipliers. The 1-norm is considered min e,?,b,c,ev ?e?22 + ????1 s.t. (10) holds and ? + c = e. (16) where ? ? R + 0 acts as a hyper-parameter. This criterion leads to sparseness (Vapnik, 1998) as already exploited under the name of ?-SVM (Chang and Lin, 2002). A similar principle is applied in the estimation of sparse parametric models known as basis pursuit (Chen et al., 2001) or LASSO
2249 sparse LS-SVM substrates) were obtained from 10-fold cross-validation (see Table 1). 5.2 Structure detection In order to test the structure detection mechanism, an artificial example is taken from (Vapnik, 1998) and the Boston housing dataset from the UCI benchmark repository was used for analyzing the practical relevance of the structure detection mechanism. This subsection considers the formulation as
939 follows from the corollary that in fact almost all nodes are at distance log d n ? O(1). Many protocols (Chord, Kademlia , Pastry , Tapestry ) offer O(log n) degree and hop count. CAN  uses degree d to achieve O(dn 1/d ) hops. These are near-optimal bounds but the lower boundsallows for (i) O(log n) hops using only constant degree and (ii) a degree of O(log n) achieving O((log
940 node is at distance at least log d n ? 1. The averagecase claim follows from the corollary that in fact almost all nodes are at distance log d n ? O(1). Many protocols (Chord, Kademlia , Pastry , Tapestry ) offer O(log n) degree and hop count. CAN  uses degree d to achieve O(dn 1/d ) hops. These are near-optimal bounds but the lower boundsallows for (i) O(log n) hops using only
1673 images, they have been largely focused on two particular aspects. One is the extraction and recognition of text contained in web images . The other is image search and retrieval on the web . There has been no previous study on functionality based image categorization. Jianying Hu Amit Bagga Avaya Labs Research 233 Mount Airy Road Basking Ridge, NJ 07920 fjianhu, baggag@avaya.com Table
1673 category S are photographic images in GIF format, while 13 of the 127 JPEG images are graphic images. Other researchers have also observed similar mixture of image classes within each single format . Thus, image format cannot be used as the definitive indication of photographic vs. graphic images. Much of previous research on image based classification in the document analysis community has
1673 of different classes of images for classification. Swain et al. proposed using features such as degree of color saturation and number of dominant colors to separate photographic and graphic images . Lopresti and Zhou  used similar features to identify text regions in web images. After investigating the two different approaches described above, it became clear to us that they are
1673 cluster histogram is computed and used as the feature representing the whole image. 3.2. Color Features Swain et al. proposed 8 color related features to distinguish graphic and photographic images . A study of those features revealed that many of them are various heuristic ways of implementing aspects of the frequency domain characteristics that are better captured by the frequency domain
1673 often contains useful information about the nature and content of the images. Much research has been carried out in the past on using the associated text for image searching and indexing on the web . For that particular task, it was found that the most relevant text fields are: image file names, image captions and alternate text (defined by the<alt> tag in HTML). The functional classification
2249 in detail in Section 5. The data set is divided into five roughly equal parts, each containing roughly equal numbers of graphic and photographic images. A Support Vector Machine (SVM) classifier  was then trained on each four of the five parts and tested onsthe remaining part. The process is rotated and the combined five part results were then pooled together to arrive at the overall
1102 to a model of a face based on decoupling transformations of the domain of the image from transformation of the intensity values, akin to so-called “active appearance” or “linear morphable models” . However, unlike traditional active appearance models, we do not require manual selection and registration of interest points, but instead perform the learning automatically. Unlike , we do not
1102 the general domain of deformable templates . Here, we do not have a general scene, but various deformations of the same face due to speech. Therefore, in the spirit of active appearance models , we assume that local variability of the domain can be modeled as linear transformations of a number of basis elements: w(x, t) = w0(x) + W (x)y(t); x ? ?, t = 1, 2, . . . (2) where W = [W1, . . .
1102 frames, sampled at 60 frames per second. About 200 points were selected in the first frame and tracked throughout the sequence. We modeled face images using shape and radiance elements as in . The shape element s = (x1, x2, ..., xl) ? R 2l is defined by vertex coordinates (trackedsModeling and Synthesis of Facial Motion Driven by Speech 9 points) of an n-point triangular mesh
1106 standard, we must devise models that can capture subtleties. While there has been remarkable progress in the area of speech content recognition and general facial motion based on speech utterances , there remains an open question of capturing dynamic complexities and interactions between facial motion and speech signals. Such subtleties are encoded largely in the dynamics of facial motion as
1106 has been the subject of considerable attention recently. A scheme for modifying emotional attributes of facial motion, such as happiness or anger, associated with utterances is discussed in . In  Ezzat et al. propose a variant of the multidimensional morphable model as a representation for images, particularly effective in describing a set of images with local variations in shape and
1106 to a model of a face based on decoupling transformations of the domain of the image from transformation of the intensity values, akin to so-called “active appearance” or “linear morphable models” . However, unlike traditional active appearance models, we do not require manual selection and registration of interest points, but instead perform the learning automatically. Unlike , we do not
188 of several peer-to-peer resource sharing communities, showing that in the absence of incentives for resource donation, most users only consume resources from the system, donating nothing back  . To provide incentives for donating resources to the grid, OurGrid implements a scheme called the network of favors  . Each site offers to the community access to its idle resources,
800 for Grid computing environments. Therefore, GridFTP is a basic Grid protocol for transferring data between Grid nodes. GridFTP extends FTP with new features and provides several advantages over FTP , such as Grid Security Infrastructure (GSI) authentication , a standard and secure authentication mechanism in the Grid environment, thirdparty control, striping, and partial file access
1102 expressions. In this paper we address the issue of face modeling, video realistic face generation and facial expression synthesis and recognition using statistical active facial appearance models . Thanks to the French Incentive Concerted Action for Young Researchers (ACI Jeunes, Ministère de la Recherche) and the european Interface project (FP5 - IST) for funding. 2. ACTIVE FACIAL
882 to speed-up bytecode verification. They therefore require a dedicated application loader, corresponding to the verification technique used. In KVMs, for example, a pre-verifier adds a PCC-like  proof to application bytecodes through a StackMap attribute that can be used only with JVMs supporting this functionality (at both the loader and verifier levels). Another use for attributes is
881 hope to discover general typing mechanisms and principles that allow greater lattitude in the design of lowlevel languages intended for systems applications or as the target of certifying compilers . In this paper, we ? This material is based on work supported in part by the AFOSR grant F49620-97-1-0013 and the National Science Foundation under Grant No. EIA 97-03470. Any opinions, findings,
881 c ::= ? | C | ? 3 For the purposes of this paper, we ignore the space required by closures. The language is powerful enough to encode closure conversion in the style of Morrisett et al. . If desired, closure environments can be represented as memory blocks and functions can be required to be closed. 4 locations ? ::= ? | ? constraints C ::= ?|C ?{? ?? ?} |C ? ? types ? ::= ? | ?
943 – intelligent query routing and networkstopologies are required to be able to route queries to a relevant subset of peers that are able to answer the queries. Modern routing protocols like Chord , CAN  and Pastry  allow for sophisticated routing based on distributed indices. More recently, in the Semantic Web context, schema based Peer-to-Peer networks such as the one described in
939 query routing and networkstopologies are required to be able to route queries to a relevant subset of peers that are able to answer the queries. Modern routing protocols like Chord , CAN  and Pastry  allow for sophisticated routing based on distributed indices. More recently, in the Semantic Web context, schema based Peer-to-Peer networks such as the one described in  have
940 and networkstopologies are required to be able to route queries to a relevant subset of peers that are able to answer the queries. Modern routing protocols like Chord , CAN  and Pastry  allow for sophisticated routing based on distributed indices. More recently, in the Semantic Web context, schema based Peer-to-Peer networks such as the one described in  have emerged that are
1028 This is not unique to RDF. It is merely a consequence of the general difficulty in rendering graphs well. It is also possible to view RDF as plain text using one of the RDF serialization syntaxes . While these can work well for small files, it can be difficult to see relationships between nodes scattered throughout the serialization. For example it is relatively easy to see the arcs from a
1939 running time of the tool is in order of seconds for the benchmarks we used. We have used a Pentium 4, Sun UltraSparcIII, PA-8500 (HP) workstations for profiling purposes. The SimpleScalar simulator  has been used for obtaining the number of misses for cache. We have used the CACTI model  for energy estimation of both the cache and scratch pad memories and also for the 128Kb off-chip memory
1939 that are powers of two while being greater or equal than the buffer size required in the scratch pad configuration. The cache line size has been selected to be the minimal allowed by the simulator  (8 bytes, which is 2 data elements in all benchmarks). In this way, we compare solely how well is the temporal locality of the data exploited without considering spatial locality issues. These
1755 the next section.s482 J. SKYT AND C. S. JENSEN As the context for P-views, this paper formalizes the append-only nature of many applications by introducing relations with transaction-time support . In these applications, conventional deletions have only a logical effect, so a new mechanism is needed for physical deletion. For this, we employ vacuuming, which is a specific approach to
2456 of each other---grammar and topicality make this so. Despite these violations, empirically the Naive Bayes classifier does a good job of classifying text documents (Lewis & Ringuette, 1994; Craven et al., 1998; Yang & Pederson, 1997; Joachims, 1997; McCallum, Rosenfeld, Mitchell, & Ng, 1998). This observation is explained in part by the fact that classification estimation is only a function of the sign
2456 600 documents. The use of each non-overlapping training set comprises a new trial of the given experiment. Results are reported as averages over all trials of the experiment. The WebKB data set (Craven et al., 1998) contains 8145 web pages gathered from university computer science departments. The collection includes the entirety of four departments, and additionally, an assortment of pages from other
2456 and project---all together containing 4199 pages. The task is to classify a web page into the appropriate one of the four categories. For consistency with previous studies with this data set (Craven et al., 1998), when tokenizing the WebKB data, numbers were converted into a time or a phone number token, if appropriate, or otherwise a sequence-of-length-n token. We did not use stemming or a stoplist; we
795 where it is advantageous to have multiple vantage points on the network. Of course, we believe that many of our goals are applicable to other distributed systems as well, in particular the Grid . We envision a bartering economy as providing the basis for decentralized growth and as a foundation for layering additional functionality at higher layers. Examples of higher level functionality
2136 bilateral exchange based on bartering. A number of successful large-scale, distributed systems similarly are rooted in simple base functionality and introduce richer functionality at higher layers . In both cases, simplicity and robustness are critical to promoting growth and providing the foundations for layering additional complexity. Given these historical precedents, we propose that a
2281 a self-organizing, self-maintaining overlay network that locates objects (possibly replicated) placed in arbitrary network locations. Recent studies of scalable content exchange networks, e.g., , indicate that up to 80% of Internet searches could be satisfied by local hosts within one’s own organization. Therefore, in order for the network to remain viable, it is crucial to consider
3197 overlay networks were formed for routing search queries in peer-to-peer applications, and exhibit no locality awareness. There are several known schemes that provide locality awareness, including . All of these solutions borrow heavily from the PRR scheme , yet they vary significantly in their assumptions and properties. Some of these solutions are designed for a uniform density space
3197 yet cope with an upper bound only on the growth rate . There is also variability in the guarantee provided on the stretch: In , there is no bound on stretch (except the network diameter). In , the stretch is an expected constant, a rather large one which depends on the growth bound. And in , the stretch can be set arbitrarily small (1 + ?). Diversity is manifested also in the node
3197 processes, we may obtain that hosting show routers increases a nodes degree only by an expected constant factor. Shadow emulation of nodes is employed in LAND . In all other algorithms, e.g., , a node’s out-degree is a priori set so that the stretch bound holds with high probability (but is not guaranteed). Hence, there is a subtle tradeoff between guaranteed out-degree and guaranteed
3197 being proportional to the network diameter (which could be rather large) to being related directly to the actual distance of the target. This is done via a technique suggested by Plaxton et al. in , that makes use of short-cut links that increase the node degree by a constant factor. With a careful choice of the short-cut links, as suggested by Abraham et al. in , this guarantees an
3197 neighborhood for publishing provides a tradeoff between out-degree and stretch. Setting it large, so as to provide an optimal stretch bound, is unique to the design of LAND . The designs in  fix the size of publish neighborhoods indepedently of the network density growth. This yields a stretch bound that depends on the density growth rate of the network. 2 Solutions that are both
940 it is shown that while hypercube and ring geometries have about (log n)! different routes from a given source to a given target, PRR like networks have only one! Thus the basic architecture of  is fragile and must be augmented with some form of robustness. The first overlay network that has both a provable low latency for paths and a high fault tolerance was presented by the authors in
2457 al. tried combining votes from several variants of a naive Bayes classifiers in a Web based application. They report that the combined classifiers were not uniformly better than their constituents . To summarize, combining several text categorizers has had mixed success. Some authors report improvements with combined systems, particularly for precision, but others report systems that give
2312 lies in the area of routing where virtual coordinates represent a simple way to embody the topology of the network. Based on virtual coordinates, geometric routing algorithm such as GFG/GPSR  or GOAFR  can be applied. Other than for routing algorithms in multi-hop radio networks, virtual coordinates have found prominent application in the context of Internet mapping . Here,
2102 two diversities in combination. In this paper, we take the HDR systems for an example, but our study applies to the HSDPA systems as well. Multiple-Input Multiple-Output (MIMO) antenna techniques , ,  have been studied extensively in the recent past. One technique is the orthogonal space-time block coding (STBC) , , which achieves both “full transmit diversity” and reliable
2102 unit is bits/s/Hz. In the remaining part we neglect the subscript k whenever possible without confusion. A. STBC- or BLAST-coded MIMO Channels For the downlink channel of each user, STBC scheme (, ) transmits an array of different data streams from multiple transmit antennas simultaneously. It then duplicates the streams in time domain and sends them out from an orthogonal arrangement of
2105 as well. Multiple-Input Multiple-Output (MIMO) antenna techniques , ,  have been studied extensively in the recent past. One technique is the orthogonal space-time block coding (STBC) , , which achieves both “full transmit diversity” and reliable communication, but fails to provide a linearly increasing channel capacity as the number of transmit and user 1 Downlink physical
2105 is bits/s/Hz. In the remaining part we neglect the subscript k whenever possible without confusion. A. STBC- or BLAST-coded MIMO Channels For the downlink channel of each user, STBC scheme (, ) transmits an array of different data streams from multiple transmit antennas simultaneously. It then duplicates the streams in time domain and sends them out from an orthogonal arrangement of the
2105 antennas. For the STBC scheme of nT = 2 and nR = 1 or 2, an orthogonal design named Alamouti code  is given with the code matrix G defined as follows: G = ? c1 c2 ?c ? 2 c ? 1 ? . (2) Based on , , the STBC/MIMO channel capacity is as follows: r(t) = R log(1 + SNRSTBC), (3) where R = 1 and 3 4 for nT = 2 and 3, respectively; SNRSTBC is the effective signal-to-noise ratio (SNR) at the
2360 ˜rj that is no bigger than ˜ri. An example to achieve near-optimum Max-Min fairness among TCP and UDP users is . Later, Kelly et al.  proposed proportional fairness criterion, and Mo et al.  extended it to (p,?)proportionally fair (see Definition 1 in ). Mapping (p,?)proportionally fair into our notations, the (w,?)-proportional fairness says that given a positive w =
382 the recent Bayesian revolution in applied statistics and related fields, including econometrics  and biometrics. Their application in other fields such as image synthesis  and mobile robotics  is more recent. Principles The principle of using Monte Carlo methods for numerical integration is to approximate the integral ? I = p(x)g(x) d d x, by estimating the expectation of the function
173 Stanford and Harvard are promoting their use among their students as a valuable mean of publishing of research ideas and results. 5 Studies have claimed Gnutella is affected by free riding  5 The technology is incredibly simple but has some disruptive characteristics. Weblogging tools create the standard HTML file for human browsing but also some semantically well defined XML files
181 question of identity is central to reputation systems. We require three properties of identity which we call persistent, unique, and distinct. We are investigating the use of expensive pseudonyms , cryptographically generated unique identifiers , and secure hardware modules  to counter identity problems such as the Sybil attack . Redemption. Our solution enforces redemption of
183 users and deprive them of the rewards. The reward can be used to query the reputation of others. Pinocchio does not intend to protect against conspiracies or bad-mouthing. The EigenTrust mechanism  aggregates trust information from peer by having them perform a distributed trust calculation approaching the Eigenvalue of the trust matrix over the peers. The algorithm relies on the presence of
483 header overhead. Address centric routing. Routing in IP networks is based on the addresses of the hosts and networks. Due to the application specific nature of sensor networks, data-centric routing  is often preferable to addresscentric routing. We use a specific form of application overlay networks to implement data-centric routing and data aggregation for TCP/IP sensor networks. Limited
688 have been proposed by . All three techniques are employed in our work. Depends clauses were introduced and refined in  and are implemented in the Java Modeling Language (JML) . The details of this notion slightly differ from ours. 9 Conclusions and Future Work We have presented a framework for the modular verification of object-oriented programs. Much needs still to be
695 objects for each class, and the values of local variables and attributes. Contracts for classes and interfaces are well known as restrictions on all objects of this type during their lifetime . An invariant contract (?, t) consists of an OCL invariant constraint ? 3sand the constrained class 1 t. We write s |= ? when the OCL formula ? is true for every object of class t in state s.
3009 concerns of a software system. Besides filters, however, Adaptive Java can be applied to components that interact in arbitrary ways, and therefore is perhaps more general. The PCL project  also focuses on language support for run-time adaptability and is perhaps most closely related to our work. PCL is intended for use directly by applications. Our concept of &quot;wrapping&quot; classes with
3015 can be implemented in different parts of the system. One approach to is to introduce a layer of adaptive communication-oriented middleware between applications and underlying transport services . An adaptive middleware framework can help to insulate application components from platform variations and changes in external conditions. On the other hand, many context-aware applications are
3015 middleware frameworks that can accommodate dynamic, heterogeneous infrastructures. Examples include Adapt , MOST , Rover , MASH , TAO , dynamicTAO , MobiWare , MCF , QuO , MPA , Odyssey , DaCapo ++ , RCSM , and Sync . In addition, several higher-level frameworks have been designed to support wearable/ubiquitous applications; examples
1496 inherits the priority of the high priority transaction. The transaction priority assignment used in all of the experiments described here is the widely-used Earliest Deadline First (EDF) policy , wherein transactions with earlier deadlines have higher priority than transactions with later deadlines. 11 Update-locks are acquired when a data page intended for modification is first read,
1995 a preference level. SemiringCSPs subsume many other notions of preferences in constraints, such as fuzzy CSPs , probabilistic CSPs , optimal CSPs , or partial constraint satisfaction . Definition 3 () A c-semiring is a tuple (A, +, ×, 0, 1) such that 1. A is a set and 0, 1 ? A; 2. + is a commutative, associative and idempotent (i.e., a ? A implies a + a = a) operation with
2004 Intelligence can be framed as optimization problems, where the task is to find a best assignment to a set of variables such that a set of constraints is satisfied. Formalisms for soft constraints  aim at more closely integrating constraint satisfaction and optimization. Soft constraints extend hard constraints by defining preference levels, such that assignments are associated with an
1485 in the Limit (Gold, 1967) requires that the learner identifies the target concept exactly and with certainty but in unbounded time, Valiant's framework of Probably Approximately Correct learning (Valiant, 1984) requires the learner to find, with high probability, a hypothesis which incurs an error of no more than &quot; from a batch of examples (preferably polynomial in size) where the error is measured in
1485 learnability classes in the identification in the limit framework (e.g., Angluin & Smith, 1983; Jain et al., 1999) and orthogonal concepts of learnability in other frameworks, such as PAC theory (Valiant, 1984), the Bayesian framework (e.g., Berger, 1985), and the Statistical Physics framework (e.g., Tishby, 1995). In the following, I will briefly survey the &quot;vanilla&quot; versions of some of these models and
1485 a detailed overview on the identification in the limit framework the reader is referred to (Jain et al., 1999). 2.2.2 The PAC and VC Models of Generalization The PAC framework of generalization (Valiant, 1984) (for an overview, see Kearns & Vazirani, 1994; Vidyasagar, 1997) is more strongly focused on efficient learning and learning from fixed-sized samples. In PAC theory, one distinguishes between a
1485 a hypothesis which is consistent with any sample in the class of target functions in polynomial time. Several function classes have been shown to be polynomially learnable: Conjunctive concepts (Valiant, 1984), linear threshold units (Blumer et al., 1989), k-DNF (Boolean disjunction with up to k literals per conjunction Valiant, 1985), k-CNF (Valiant, 1985), and k-decision lists (Rivest, 1987) are
1485 target is a Boolean function and the instances are uniformly distributed), the analysis predicts and thus explains learning curves with a precision that has not been achieved by PAC theory (e.g., Valiant, 1984), statistical physics (e.g., Tishby, 1995), or other frameworks. One reason for that is that the analysis is not a worst-case anal3.9. DISCUSSION 57 ysis; instead, it considers the distribution of
2249 Lunch In Section 1.2, I discussed the question whether any learning bias can be superior to all other learning biases informally. PAC results by Blumer et al. (1987) and similar VC style results (Vapnik, 1998) which show that we can be more certain about the error rate of a hypothesis which has been learned from a small hypothesis language than we can be about the error rate of a hypothesis which
2249 h occurs. The penalty term G(\Delta; \Delta) penalizes the size or, (more frequently) when the models are infinite, the number of free parameters (Moody & Utans, 1992; Moody, 1992) or VC dimension (Vapnik, 1998) of the model H i in which a focused hypothesis h occurs for the first time in the stratification. Frequently, G(E S (h); H i ) = E S (h) + penalty(H i ). By means of a penalization term,
3193 query plans. 1 Introduction Peer-to-peer (P2P) computing is currently attracting enormous attention, spurred by the popularity of file sharing systems such as Napster , Gnutella , Freenet , Morpheus  and Kazaa . In P2P systems a very large number of autonomous computing nodes (the peers) pool together their resources and rely on each other for data and services. P2P computing
2249 O-okayama, Meguro-ku, Tokyo, 152-8552, Japan 1 Introduction In recent years, a number of kernel-based learning algorithms such as the regularization networks , the support vector machines , and the Gaussian process regression  have been investigated. These kernel machines are shown to work very well on real-world problems, given appropriate kernel functions. For general purposes,
2249 hand, a lot of attention have been paid recently to designing kernel functions using the problem-dependent prior knowledge. Various methods for constructing suitable kernels have been proposed . In this contribution, we propose a framework for designing kernel functions for regression. 2 A Kernel Design Framework for Regression Let us consider a one-dimensional regression problem. Kernel
2570 services available, but requires providing automatic mechanisms so that the services can be linked in appropriate and meaningful ways. Automatic composition and interoperability of Web services  have been achieved through mapping, service verification, and execution monitoring. Scientific workflow  is supposed to support interoperation through semantics. It may have the potential to
2465 data £eld to be extracted, form an important part of the wrapper. Quick and ef£cient generation of extraction rules, so-called wrapper induction, has been an active area of research in recent years . The most advanced of such wrapper induction systems use machine learning techniques to learn extraction rules by example. Using a graphical user interface a user marks up data to be extracted on
21199 data £eld to be extracted, form an important part of the wrapper. Quick and ef£cient generation of extraction rules, so-called wrapper induction, has been an active area of research in recent years . The most advanced of such wrapper induction systems use machine learning techniques to learn extraction rules by example. Using a graphical user interface a user marks up data to be extracted on
1190 those information sources maintain complete autonomy but |at the same time| their locations, data models, contents and query interfaces should not burden the users posing queries. Several works, , have addressed the previous problem on the basis of a shared framework to overcome those diculties found by the users. The goal of all such works is to provide users with an interface to access
1487 is impractical, as it would result in very poor utilization. Paper Scope and Outline: This paper presents the design for an implementation of Statistical Rate Monotonic Scheduling (SRMS)  in KURT Linux . SRMS allows the scheduling of periodic tasks with highly variable execution times and statistical QoS requirements. It enforces task isolation (a.k.a. the
1487 of this paper, we use &quot;SRMS&quot; to refer to the Basic SRMS algorithm augmented with these two extensions. The following is a brief overview of these extensions. Interested readers are refered to  for more details. Time Inheritance: Time inheritance is another instance of the SRMS concept of &quot;smoothing the variability in resource usage through aggregation&quot;. In Basic SRMS, this aggregation
1495 high priority time than was guaranteed. In the Rialto OS, a graph-based scheduling algorithm was implemented to provide guaranteed periodic CPU reservations as well as to guarantee aperiodic tasks . The CPU schedule is precalculated; this calculation involves reducing all periods so that each is equal to the product of the minimum period and a power of two. Other implementation efforts have
1496 Department Boston University Boston, MA 02215 fakatlas, bestg@cs.bu.edu Abstract Statistical Rate Monotonic Scheduling (SRMS) is a generalization of the classical RMS results of Liu and Layland  for periodic tasks with highly variable execution times and statistical QoS requirements. The main tenet of SRMS is that the variability in task resource requirements could be smoothed through
1496 if every instance of every task in the set is guaranteed to meet its deadline. An optimal fixed-priority algorithm is the classical Rate Monotonic Scheduling (RMS) algorithm of Liu and Layland. To ensure the satisfaction of the hard deadlines imposed on periodic tasks, RMS requires that either the periodic resource requirement of each task be constant, or the periodic worst-case resource
2462 because domain knowledge needs to be encoded. This tends to be a cumbersome task since the domain knowledge is in general not readily available. Several approaches based on hidden Markov models  have been undertaken, but these approaches are limited by the effort required to learn the Markov model. A new approach is proposed based on the observation that information extraction can in some
2462 To retrieve low level structure, first of all a sentence need to be broken up in tokens. Then, each of the tokens need to be assigned a class. HMMs have been applied with success to the latter task . The power of HMMs lies in taking into account the whole of the set of tokens to classify. The problem with hidden Markov models is learning the transition probabilities. Also, typically only a
2462 publisher, address, and wrapper. The value wrapper is used for anything not in any of the other categories, for example text like ‘In’ to indicate a publication is part of a book like citation  in this article. Another approach is to extract authors, and year information from a citation and use that for matching against a database. This was motivation for creating another dataset with a
795 able to switch to a flat tree broadcast when network latency is high . Making the communication library aware of the precise network topology is not easy: MPICH-G2 queries the underlying Globus  environment to retrieve information about the network topology that the user may have specified through environment variables. Such network-aware libraries bring interesting results as compared to
795 results 5.1. Hardware environment Our experiment consists in the computation of 817,101 ray paths (the full set of seismic events of year 1999) on 16 processors. All machines run Globus  and we use MPICH-G2  as message passing library. Table 1 shows the resources used in the experiment. They are located at two geographically distant sites. Processors 1 to 6 (standard PCs with
2433 is organized as follows. In section 1 the training problem is formulated. In section 2 a recently proposed de ection procedure  is brie y presented. The fundamentals of simulated annealing  are presented in section 3, while genetic and evolutionary algorithms  are reviewed in section 4 and 5 respectively. Section 6 summarizes and discusses our results as well as presents
2433 AND GLOBAL OPTIMIZATION Figure 1.2 Applying the de ection procedure to the Six Hump Camel Back test function for =1:5 (left) and =10(right) 3. THE SIMULATED ANNEALING Simulated Annealing (SA)  refers to the process in which random noise in a system is systematically decreased at a constant rate so as to enhance the response of the system. In the numerical optimization framework, SA is a
939 information sufficiently up-to-date such that a large fraction (e.g., 99%) of the queries will succeed without being re-routed. 1 Introduction Structured peer-to-peer overlays like Chord , CAN , Pastry , and Tapestry  provide a substrate for building large-scale distributed applications. These overlays allow applications to locate objects stored in the system in a limited number
188 routing state – typically O(log N) – because their designers expect that system membership changes frequently. This expectation has been confirmed for successfully deployed systems. A recent study  shows that the average session time in Gnutella is only 2.9 hours. This is equivalent to saying that in a system with 100, 000 nodes, there are about 19 membership change events per second.
188 where n is a large number like 10 5 or 10 6 . We assume dynamic membership behavior as in Gnutella, which is representative of an open Internet environment. From the study of Gnutella and Napster , we deduce that systems of 10 5 and 10 6 nodes would show around 20 and 200 membership changes per second, respectively. We call this rate r. We refer to membership changes as events in the rest of
188 nodes with 10 slices and 5 units per slice. The results are shown in Figure 7. In the first 300 seconds of the simulation, all nodes join rapidly. After that the system shows Gnutella-like churn  with 24 events per minute. All nodes join by obtaining a routing table from another node; the routing tables then continue to grow as new nodes come in. After approximately the first 10 minutes,
943 routing information sufficiently up-to-date such that a large fraction (e.g., 99%) of the queries will succeed without being re-routed. 1 Introduction Structured peer-to-peer overlays like Chord , CAN , Pastry , and Tapestry  provide a substrate for building large-scale distributed applications. These overlays allow applications to locate objects stored in the system in a
943 item (e.g., providing storage for it) rests with its successor; this is the first node in the identifier ring clockwise from key. This mapping from keys to nodes is based on the one used in Chord , but changing our system to use other mappings is straightforward. Clients issue queries that try to reach the successor node of a particular identifier. We intend our system to satisfy a large
943 period when it decides that the node is unreachable or dead. A joining node contacts another system node to get its view of the current membership; this protocol is similar to the Chord protocol . The membership information enables it to get in touch with its predecessor and successor, thus informing them of its presence. To maintain correct full routing tables, notifications of membership
943 period when it decides that the node is unreachable or dead. A joining node contacts another system node to get its view of the current membership; this protocol is similar to the Chord protocol . The membership information enables it to get in touch with its predecessor and successor, thus informing them of its presence. To maintain correct full routing tables, notifications of membership
943 can argue that our successor and predecessor pointers are correct due to the fact that we essentially follow the same protocol as Chord to maintain these, and this has already been proven correct . 3.3 Scalability Slice leaders have more work to do than other nodes, and this might be a problem for a poorly provisioned node with a low bandwidth connection to the Internet. To overcome this
1685 Tensor Image (DTI), measures the microscopic diffusion of water in a tissue, such diffusion is constrained by the direction of nerve bundles. For a given voxel r, the Stejskal–Tanner equation, Sr = S0r exp(?bg T Drg), (1) shows the relationship between the signal magnitude measured without diffusion, S0, and the one attenuated by the water diffusion in the tissue, S. The unitary vector
1685 gradient is applied, Dr is a positive Figure 1: Geometric interpretation of the diffusion tensor. definite symmetric tensor and b is a constant that depends on the acquisition parameters, see  for more details. The conventional procedure for computing the tensor field D, at each voxel r, is based on a least–squares method: at least 8 diffusion images (each one corresponding to a magnetic
1685 orthogonal tensors with high–anisotropy results in a tensor with low– anisotropy. This fact increases the uncertainty of the tissue orientation and reduces the anisotropy of the observed tensor . Furthermore, the inverse problem is not well–defined: different combinations of high–anisotropic tensors may results in the same low–anisotropic tensor, see Figure 2. So, one needs to solve these
1685 trajectory through the fiber crossing, these particles should reach the low confident region with a trajectory aligned to the tracked fiber. Otherwise, the particle trajectory could be bent. In , an homogeneous Gaussian smoothing is applied to the tensor field. Although the blurring produces a denoising effect, it also increases the uncertainty in the orientation. Another approach ,
2433 the solution space increases exponentially and hence also the running time of these algorithms. Several modern heuristic methods have been applied to the problem. Examples are simulated annealing , stochastic evolution , genetic algorithms  and tabu search . In addition, the authors have successfully applied tabu search and a memetic algorithm to this problem and compared it to a
2673 are available at URL: http://www.ics.forth.gr/netgroup/msa/ 2 Courcoubetis et al. / Many sources asymptotic and effective bandwidths For example, statistical analysis of traffic measurements  has shown a self-similar or fractal behavior; such traffic exhibits long range dependence or slowly decaying autocorrelation. Although the implications of such long range dependence is still an
2673 3 Available from The Internet Traffic Archive at URL: http://www.acm.org/sigcomm/ITA/ 10 Courcoubetis et al. / Many sources asymptotic and effective bandwidths and D. Wilson . The duration of the trace is 122797.83 seconds. For voice traffic we use an on-off Markov fluid model with peak rate 64 kbps and average time spent in the &quot;on&quot; and &quot;off &quot; states 352 msec and 650
2679 are available at URL: http://www.ics.forth.gr/netgroup/msa/ 2 Courcoubetis et al. / Many sources asymptotic and effective bandwidths For example, statistical analysis of traffic measurements  has shown a self-similar or fractal behavior; such traffic exhibits long range dependence or slowly decaying autocorrelation. Although the implications of such long range dependence is still an
695 one of the documents), but the actual extraction process should require no manual interference. Documentation in separate documents is often forgotten during maintenance, and self documentation  requires less discipline from developers. In other words, it is easier to maintain trace information sources that are part of or close to the program code than maintaining additional documentation
695 For a developer it is usually easier to add a documentation directly to the design or program code than to remember to update a separate documentation (see the self documentation principle ). Details are -- in contrast to abstractions -- volatile. Because details are going to change, they should not be tangled with abstractions, but put where they will create the least amount of
795 users. Some of the issues we are currently addressing in our work with Symphony are: defining a general syntax for resource declaration and job specification, support for the Globus middleware  and its security infrastructure, support for other middleware and security architectures, providing means for more effective data routing between resources, and support for resource discovery
802 are motivated by similar goals. Simulation management also serves as a way of documenting the ‘history’ of computational runs and experiments. For example, in conducting parameterized sweeps , knowing that certain particular choices have been executed elsewhere on the Grid allows flexibility in load balancing and farming out computations to distributed resources. 1.4. GCEs for
802 only trivial support from this layer. In the parameter definition layer we locate those activities that associate model instances with models. Examples include tools that generate parameter sweeps  orothertypesofmodel instance ensembles, as well as the use of problem-oriented scripting languages to generate multiple model instances. (Note that we are using ‘parameter’ in a very broad sense
802 how our semistructured representations of models and bindings can aid in even higher level problem-solving facilities. In particular, we concentrate on facilities such as the ‘parameter sweep’ tool  andthe Copyright © 2002 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2002; 14:1241–1273s<experiment id=’diff. prop.’> WHERE <environment id=’$id’> <meta><type>urban</type></meta>
1190 to accomplish this (Figure 1). User Source 1 Source 2 Source N Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic  and Information Manifold  employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM ) for dealing with the
1190 `Smith'. Notice that the queries to s1 are now feasible because they specify the title values. Other ways to describe source capabilities in mediator systems have been proposed in the literature . For instance, the Information Manifold  uses capabilities records for encoding source capabilities. We note that the capability description language used by TSIMMIS is more powerful than the
3192 they are under consideration as part of the security architecture for providing consistency . In any case, the techniques used in OceanStore are very related to our approach. 5.3 Farsite Farsite  is a serverless distributed file system developed at Microsoft Research (http://research.microsoft.com/sn/Farsite/) relying on otherwise unused disk space in a large number of desktop machines. It
695 uses an existing T value. Statically Typed Object-Oriented Languages Statically typed object-oriented languages make use of various forms of record subtyping. SIMULA , C++ , Eiffel  and Modula-3  are typical of these languages. Such languages generally refer to a record type as a class, a routine field as a message, and an implementation of a routine field as a method .
2553 this can be done in O(log N) time by utilizing a Delaunay hierarchy , but this may not always be practical. Using a simple k-d tree, an approximate nearest neighbor can be found in O(log 3 N) . As shown in Section 5.1, all points pi influencing x can be determined in constant time by a depthfirst or breadth-first search. Overall, f(x) can be determined in O(log N) (or O(log 3 N) if using
2025 Nested states increase the amount of detail without unnecessarily cluttering the design at higher levels of abstraction. Nested state diagrams were introduced by Harel under the form of statecharts . Variants of statecharts are widely used and accepted in object-oriented analysis and design methodologies like Booch , OMT , UML  and Syntropy , and in real-times methodologies like
2025 (NSDs) 2.1 Definition To keep the presentation clear, we have chosen to present the definitions in this paper in a semi-formal way. We will use a notion of state diagrams that combines statecharts  and OMT state diagrams . In order to reduce complexity we have imposed several restrictions on these diagrams. For example, we do not allow concurrrent substates or transitions between states at
2025 M timeout Sec. S Figure 3: Digital Clock Behaviour S Time Set S S Hour Min. M Date Set M S S Month Day 1Other semantics are possible, for example by making use of the so-called history state of . This design choice however is irrelevant to the discussion in the paper. This paper does not try to capture all possible semantics and corresponding conflicts, but rather gives a methodology for
2025 have entry and exit actions that occur when leaving the source state or entering the target state of the firing transition. Finally, activities can be associated to states. • Based on the work of , history states could be used as an alternative to initial states, and concurrent states could be introduced to deal with concurrency. • A final limitation is that we excluded nondeterminism from
1028 semantic channel. We propose an enhanced type of infomediary to act as personalization server, and describe its architecture and functional modules. Given the increasing popularity of RDF/S (Lassila & Swick 1999; Brickley & Guha 2000) for exporting Portal catalogs on the Web, we rely on robust RDF tools, like RDFSuite 2 , for analyzing, storing and querying resource descriptions and schemas.
1028 P. However, M has to update periodically its cache using appropriate wrappers for each Portal. Since the term hierarchies and the Web resources of Portals are more and more exported in RDF/S (Lassila & Swick 1999; Brickley & Guha 2000) we employ RSSDB (Alexaki et al. 2001) for storing the cached copies of the catalogs as well as the user-defined semantic channels. Browsing Semantic Channels Clearly we can
795 information. It then uses numerical models to generate forecasts of what the conditions will be for a given time frame. NWS is used for various meta-computing systems such as Globus and APPLeS . MOSIX is a popular platform for supporting distributed computing. It enhances the Linux kernel with cluster computing capabilities. In a MOSIX cluster, there is no need to modify applications to
814 at Beowulf clusters, is a distributed system that periodically monitors and dynamically forecasts the performance various network and computational resources can deliver over a given time interval . The service operates a distributed set of performance sensors (network monitors, CPU monitors, etc.) from which it gathers system condition information. It then uses numerical models to generate
2673 The characteristics of the traffic have a significant impact on the queueing delay. In a ground-breaking work, Willinger et al. reported that network traffic is self-similar rather than Poisson , and much research has been done since to explore the consequences of non-Poisson traffic on queueing delay. The Fractional Brownian Motion (FBM) model has been proposed to capture the coarse time
2675 Motion (FBM) model has been proposed to capture the coarse time scale behavior of network traffic. It shows that the queueing behavior diverges from that of the Poisson traffic model significantly , . Follow-up work shows that the wide-area network traffic is multi-fractal and exhibits varying scaling behavior depending on the time scale . Recent work reveals that the queueing behavior
2675 dependent, and such traffic can be modeled as Fractional Brownian Motion (FBM). Norros shows that the queueing delay distribution of the FBM traffic is approximated by a Weibull distribution . We test the obtained queueing delay distributions against all three types identified above. To examine what tail category our delay distributions fall into, we first plot the complementary
2675 both data sets. Both distributions fit to a Weibull distribution with a shape parameter b close to 0.6. Thus the distributions of measured queueing delay are long tailed, confirming the finding in . B. Impact of Link Utilization on Queueing Delay In this section, we investigate the evolution of queueing delay with respect to link utilization in our backbone network, where link utilization
2102 MIP assumption. 1960 III. BIT ERROR PERFORMANCE OF 2D-RAKE RECEIVER IN MIMO SYSTEMS For the purpose of illustration, the simple dual transmit diversity space-time block code proposed by Alamouti  is adopted in the analysis. Alamouti  has shown that the maximum likelihood (ML) estimates of the transmitted data are identical to the ML estimates obtained in a system with a single transmit
2105 l b = ? M + ? R 2 s ? 0 l= 0 MT ?ml ?sin ? P ? I R d?. (11) It is necessary to point out that the derivation of (10) is based on the use of space-time block codes from orthogonal designs. However,  proved that for complex orthogonal designs, only M T = 2 Alamouti-code can provide the maximum possible transmission rate and full diversity. For M T > 2 , the space-time block codes can give full
3195 by large-scale and high-dynamism of their operating environment are being built. In these distributed systems, participating peers directly share resources as equals in a peer-to-peer fashion . We name them, peer-to-peer (P2P) systems. The high dynamism in P2P systems is due to two reasons mainly. First, there is the need for freedom, peers should be able to join or leave the system at
939 that it will be located when needed. Furthermore, these overlay networks tend to be ine#cient, as they mainly use flooding for search. On the other hand, there are structured overlay networks , where a peer joins the overlay network by connecting itself to some other well-defined peers, based on its logical identifier. We say that structured overlay networks are built in a controlled
939 is used in place of virtual identifier based routing. 2 1.1. Motivations Since the introduction of structured overlay networks, various such systems have been proposed. A partial list includes , and new such systems are probably to come. Unfortunately, existing structured overlay networks are presented in a fragmented way. As a consequence of this, understanding any new such systems
939 for correcting routing information that each peer maintains. Most of the existing peer-to-peer infrastructures use this technique. For instance, it is used in systems such as Chord , CAN  and Pastry . The idea here is that each peer periodically checks its neighbors, to detect any change that occurs in the vicinity of the checking peer. In Chord, this is done by periodically
939 can be derived. The designer only need to decide which division of the space to use and 24 the rule for selecting responsible peers. Many existing structured peer-to-peer overlay networks, such as , fit the presented framework. Interestingly, from our framework of embedding k-ary trees, we can derive structured overlay networks of constant degree. In this paper, we briefly shown one way to
943 as false lookup failure, in which the system returns a message saying that an item is not present in the system while the item is actually there. Our simulation-based study of systems such as Chord  that use very &quot;weak&quot; join protocol, can have significant number of false lookup failures when compared to our DKS system in which local atomic join protocol is used. The main idea behind local
943 routines for correcting routing information that each peer maintains. Most of the existing peer-to-peer infrastructures use this technique. For instance, it is used in systems such as Chord , CAN  and Pastry . The idea here is that each peer periodically checks its neighbors, to detect any change that occurs in the vicinity of the checking peer. In Chord, this is done by
3194 schemes have been designed to enable anonymous messaging (Onion Routing ), anonymous emails (Mix Nets ), anonymous web browsing (Crowds ), anonymous publishing (FreeHaven ), and anonymous indexing (Privacy Preserving Indexes ). The P4P framework allows an individual to declare the level of control she desires over speci£c information. We expect some of these
2433 the minimum energy, i.e., the states with the minimum energy are reached at lower temperatures. B. Simulated Annealing Based on the annealing process in statistical mechanics, Kirkpatrick et al.  proposed an algorithm, namely, simulated annealing (SA), for solving complicated combinatorial optimization problems. In the SA algorithm, a simulation of the annealing process is performed. The
881 on the functionalities of the underlying UNIX systems. Our scheme aims to improve the robustness of setuid programs written in C. Language-independent approaches such as code certification  and SFI  can be applied to improving the robustness of setuid programs. Code certification ensures type safety in language-independent ways. However, type safety is not enough to protect
676 invariant detector. 1 INTRODUCTION Previous research explored the use of dynamic methods for discovering likely program invariants, with a particular interest in supporting software evolution tasks . A prototype implementation, Daikon, demonstrated the feasibility of dynamically detecting invariants, or properties that hold at a particular program point. The approach is to run the program of
676 discuss eliminating undesired invariants. Section 9 briefly surveys related work. Finally, Section 10 recapitulates the results and discusses future work. 2 BACKGROUND Dynamic invariant detection  discovers likely invariants from program executions by instrumenting the source program to trace the variables of interest, running the instrumented program over a set of test cases, and then
676 the routine but was incorrectly omitted from the specification) and that the routine does not modify its arguments (which could aid understanding the program). More details are available elsewhere . 3 RELEVANCE We call an invariant relevant if it assists a programmer in a programming activity. Relevance is inherently contingent on the particular task, as well as the programmer's capabilities,
676 derived variables, return values, and (for non-entry points) variables that represent the initial values of variables in scope. Daikon did report invariants that were both useful and unexpected . It also reported some accurate but uninformative invariants. For example, for replace Daikon reports done ! pat where done is a boolean and pat is a character array. That invariant is unlikely
676 of the various rules. Larger test suites do not proportionately reduce the number of differences beyond those for 1000 test cases. In our experience, there are three reasons for this behavior . (1) Beyond a certain size, expanding test suites has little impact on the accuracy of invariant detection or the specific invariants detected. (2) For test suites smaller than that cutoff,
1445 and increase the precision among the top ranked documents. However, the weaker evidence provided from the hyperlink structure and the different types of queries (e.g. specific vs. broad queries) (Kleinberg, 1999) suggest that combining content and hyperlink analysis in a uniform way, independently from the queries, would not result in optimal retrieval effectiveness (Plachouras et al., 2003). For example,
173 the number of items reviewed/recommended per member . This kind of distribution is typical of systems where a group of users share information: a minority of the users do a majority of the work . In the context of collaborative document monitoring, this means that the majority can keep up-to-date on many changing web sites by freeriding on the good will of a few people who are willing
1023 heterogeneity, but also semantic problems. So the final resource presentation style must be comprehensible to an agent, thus overcoming any syntactic and semantic issues. The use of ontologies (Fensel, 2001) might provide a way to specify the admissible output structure of a resource.sFigure 7: Command Line Program BioAgent provides access to resources using both ontologies and wrappers. AIXO is used
21199 Oils</description> <index>Oil1</index> <index>Oil2</index> <index>Oil3</index> </set> </sets> . . . Figure 8: GAMS raw XML Document 6 Related Works According to Kushmerick (Kushmerick, 1997) we can distinguish two main categories of wrappers: Hand-Coding Wrappers and Boosted Wrappers. Boosted Wrappers can learn, after an initial learning stage, how a document is structured and how to
2722 reaches only 10m. The bandwidth is also larger, 11-54Mbps compared to less than 1Mbps for Bluetooth. Additionally, many routing protocols for mobile ad hoc networks based 802.11 already exist . The disadvantage of 802.11 is that it consumes too much energy, and consequently, it drains out the mobile devices’ batteries in a very short period of time. With the current state of the art, we
499 heuristic search in general. Introduction During the last decade, powerful search techniques using an implicit state representation based on the reduced ordered binary decision diagram (BDD, Bryant 1986) have been developed in the area of symbolic model checking (McMillan 1993). Using blind exploration strategies these techniques have been successfully applied to verify systems with very large
499 by a general function for applying Boolean operators. Due to the space limitations of this paper we will treat BDDs as a black-box. Readers interested in a thorough introduction are referred to (Bryant 1986). A search problem is a §?????????¤?? ? ? 4-tuple ??? . is a set of ????????? states. is a transition relation defining the search §?????????????? graph. iff there exists a transition leading ? from
1228 complex models. In contrast, systems of real scenario such as Web applications and large-scale information management necessitate fast classification tools. Accordingly, several studies (e.g. ) on improving accuracy of low complexity classifiers have been carried out. They are related to the designing of efficient T C models in Web scenarios: feature space reduction, probabilistic
1440 is, a vector summarizing all training documents d such as d ? Ci. Vector components are called features and refer to independent dimensions in the similarity space. Traditional techniques (e.g. ) employ words or stems as basic features. The i-th component of a vector representing a given document d is a numerical value. It is the weight that the i-th feature of the training-set assumes in
1440 ? 0, 1 |R| ? d?R ? d f ? ? | ¯ R| where ? represents the rate between the original Rocchio parameters, i.e. ? ? . 4 Several methods are used to assign weights to a feature, as widely discussed in . ? d??? ¯ R ? d f ? (3) (4)sOur hypothesis for finding good ? value is that it deeply depends on the differences among classes in term of document contents. This enables the existence of different
1440 is always obtained by microaveraging the target measure over all categories of the target corpus. The sets of features used in these experiments are all tokens that do not appear in the SMART  stop list 7 . They are 33,791 for Reuters, 42,234 for Ohsumed and 55,123 for ANSA. No feature selection has been applied. The feature weight in a document is the usual product between the logarithm
1440 at ftp://medir.ohsu.edu/pub/ohsumed. 7 No stop list was applied for Italian corpus.s(inside the document) and the associated inverse document frequency (i.e. the SMART ltc weighting scheme ). 4.2 Relationship between accuracy and ? values In this experiments we adopted the fixed split of the Reuters corpus as our test-set (RT S). The aim here is simply to study as ? influences the
381 step, i.e. in deciding whether there is an overlap between the two maps or not. To avoid this decision problem, most existing approaches assume knowledge about the robots’ relative start locations . At the minimum, these techniques require that one robot is known to start in the map already built by the other robot. If we consider the revisiting problem in a Bayesian context, then to make an
381 robot estimates the location of this robot relative to its own, partial map. Once the relative offset between the maps is determined, map merging can be performed by a mapping algorithm such as . Existing approaches to map merging assume knowledge about the robots’ relative start locations . At the minimum, these techniques require that one robot is known to start in the map
381 represents a unique match between the partial maps built by the two robots. Once a match with sufficiently high probability is found, map merging can be performed by a mapping algorithm such as . The partial map localization algorithm is highly efficient and can be computed in real time on a state-of-the-art laptop. 4.2 View extraction To test our approach using data collected by real
1102 or in combination with other face animation parameters (FAPs) to control artificial 3D head models. In this paper, we describe the use of active facial appearance models for face analysis . Such image-based parametric models are used to analyse and generate video-realistic faces, and differ from the physical 3D face models frequently used today in machine to man interaction systems.
1102 examples, can be used for face tracking, facial expression synthesis and recognition. 2. Active facial appearance models 2.1. Model description It has been shown that the active appearance model  is a powerful tool for face synthesis and tracking. It uses Principal Component Analysis to model both shape and texture variations seen in a training set of visual objects. For each image of the
1102 is illustrated by the set of white points). 3. Facial expression analysis and synthesis 3.1. Facial expression modeling The aim of this section is to study a linear model, as it is proposed in , correlating the appearance parameters to facial expression intensity according to: c = ae0 + ae1I + ? (5) where I is a scalar varying from I =0to indicate neutral expression to I =1to indicate a
2462 because domain knowledge needs to be encoded. This tends to be a cumbersome task since the domain knowledge is in general not readily available. Several approaches based on hidden Markov models  have been undertaken, but these approaches are limited by the effort required to learn the Markov model. A new approach is proposed based on the observation that information extraction can in some
2462 To retrieve low level structure, first of all a sentence need to be broken up in tokens. Then, each of the tokens need to be assigned a class. HMMs have been applied with success to the latter task . The power of HMMs lies in taking into account the whole of the set of tokens to classify. The problem with hidden Markov models is learning the transition probabilities. Also, typically only a
2462 publisher, address, and wrapper. The value wrapper is used for anything not in any of the other categories, for example text like ‘In’ to indicate a publication is part of a book like citation  in this article. Another approach is to extract authors, and year information from a citation and use that for matching against a database. This was motivation for creating another dataset with a
1440 (IR). For ? each document we construct a fea? ? ? ??? ture vector , where the components are determined by the frequency of which term ? ? ? occurs in that document. Following standard practice  we choose a term frequency £ ? inverse document frequency weighting scheme: ¡ ? ? ? £ ? ? ? ? ¢ £ ? ? £ ? ??? ??????? ??? where the term frequency £ ? £ ? ??? ? ? denotes the number of times £ ?
1804 that perform part of the tasks automatically, however. Other environments do provide automated transformations , and are based upon the concept of refactoring, first identified in . However, they do not automatically guide a developer by telling him when or where the transformations should be applied, nor do they provide support for upgrading.  propose tools
2025 Digrams are often given as a (un)directed graph. Nodes and edges can be annotated by text or program fragments. Advanced notations are based on hypergraphs or Harel's Higraphs (Hierarchical Graphs). 1 In Higraphs, nodes are combined as sets to super-nodes. Graphically, they are drawn to be contained in the super-node. An edge from/to the super-node stands for the set of edges from/to all of
939 ) (3) where Nhorizon ?1 represents the cost of querying the item using Gnutella, and CSi,DHT represents the cost of querying the item in the DHT. In a typical DHT system, CSi,DHT is log N messages  (with the InvertedCache option). Further, let Ti be the life-time of node i in the system 5 , and let CPi,DHT be the cost of publishing item i into the DHT. Then the total cost per time unit of
943 ) (3) where Nhorizon ?1 represents the cost of querying the item using Gnutella, and CSi,DHT represents the cost of querying the item in the DHT. In a typical DHT system, CSi,DHT is log N messages  (with the InvertedCache option). Further, let Ti be the life-time of node i in the system 5 , and let CPi,DHT be the cost of publishing item i into the DHT. Then the total cost per time unit of
383 a major thrust in systems and control theory, in the context of cooperative control, distributed control of multiple vehicles and formation control; see for example (Leonard and Friorelli, 2001; Cortes et al., 2002; Olfati and Murray, 2002; Reif and Wang, 1999; Fax and Murray, 2002; Liu et al., 2003; Gazi and Passino, 2003; Tabuada et al., 2001; Jadbabaie et al., 2002; Ögren et al., October 2002; Desai et
902 cooperative control, distributed control of multiple vehicles and formation control; see for example (Leonard and Friorelli, 2001; Cortes et al., 2002; Olfati and Murray, 2002; Reif and Wang, 1999; Fax and Murray, 2002; Liu et al., 2003; Gazi and Passino, 2003; Tabuada et al., 2001; Jadbabaie et al., 2002; Ögren et al., October 2002; Desai et al., 2001; Justh and Krishnaprasad, 2002; Vidal et al., 2003; Pand et
1217 information streams — including copies of the instructor’s lecture notes, boardwork, slides, videos, the audio of the lecture, and even other students’ notes — may improve the learning experience . However the view, occasionally heard, that such advances will render note-taking unnecessary is questionable at best. Rather note-taking must co-exist with such technology, as many researchers
1217 however, it is worth considering how our proposal fits in with other research advances and trends. One recent innovation is the idea of making the audio portion 23saccessible from the class notes ; this could be done equally well with notes taken using the computer. Other innovations include techniques for organizing and indexing digital notes ; these could also be used profitably with
2433 In some cases, by doing this it is possible to solve a problem declared infeasible by the heuristic. 4.3 Using Simulated Annealing to Improve the Heuristic Solution The Simulated Annealing method  is an optimization method designed for non-linear integer problems that are di cult to solve analytically. The method starts from a feasible solution, and perturbates this solution to see if it can
3013 systems other than J-Orchestra can also be classified as automatic partitioning tools. In the Java world, the closest approaches are the Addistant  and Pangaea  systems. The Coign system  has promoted the idea of automatic partitioning for applications based on COM components. Addistant  is the closest alternative to J-Orchestra in the design space. J-Orchestra has three
3013 of a class can be safely passed by-copy, etc. This information is application-specific and getting it wrong results in a partitioning that violates the original application semantics. Coign  is an automatic partitioning system for software based on Microsoft’s COM model. Although Coign is a pioneering system, it suffers from two drawbacks. First, Coign is not applicable to many
3013 it does not distribute components when they share data through memory pointers. Such components are deemed non-distributable and are located on the same machine. Practical experience with Coign  showed that this is a severe limitation for the only real-world application included in Coign’s example set (the Microsoft PhotoDraw program). The Coign approach would be impossible in the case of
743 task into localized indexing, performed by a set of gatherers, and centralized searching, performed by a set of brokers, has been suggested since the early days of the Web by the Harvest project (Bowman et al., 1994). Fish Search (De Bra & Post, 1994) was a search system proposed at the same time as InfoSpiders (Menczer, Willuhn, & Belew, 1994) and inspired by some of the same ideas from artificial life. Fish
327 after image segmentation by feature localization. 2.2. Multimedia associations with recurrent items. Association rule mining has been studied extensively in data mining research community recently . Many algorithms and approaches have been proposed for mining many types of association rules in large databases. However, typically, the databases relied upon are alphanumerical and often
2249 Information Criterion for Non-Quadratic Regularizers 2 1 Introduction Supervised learning techniques allow to estimate underlying unknown statistical inputoutput relations from given training data . In this process one has to be careful not to overfit the training data, but to estimate the underlying statistical data generation process, such that the learning machine generalizes well, i.e.
2249 the function class from which the estimators are chosen. Thus, one introduces a preference from complicated models towards simpler models for example by choosing a model with a small VC dimension  or by introducing regularization . Intuitively this amounts to selecting a smoother model. In this paper we will consider regularization for enhancing the generalization capability. Here the
2249 or approximations thereof, e.g. in a worst or average case setting. The former considers the worst generalization error achieved on all possible training sets (see e.g. methods based on VC theory ). The latter considers ensemble averages over all possible training sets, for example the Network Information Criterion (NIC)  or the Subspace Information Criterion (SIC) . Furthermore
2249 test samples are used for measuring the generalization error. The regression function is described as f  (x) = 50 # i=1 # i k(x, x i ), (45) where k is the third-order ANOVA decomposition kernel : k(x i , x j ) = # 1#k 1s2s3 #13 #(x ik 1 , x jk 1 )#(x ik 2 , x jk 2 )#(x ik 3 , x jk 3 ) (46) constructed from a linear spline kernel # : #(x i , x j ) = 1 + x i x j + x i x j min(x i , x j )
483 2 In environments such as this, for the objectives we seek to address, computation in large-scale sensor networks will require scalable coordination amongst sensors to accomplish the desired tasks , In most circumstances, the sensors must coordinate to achieve one or more global objectives, but yet must do so with only local information. Distributed algorithms to achieve global objectives
483 for better scalability, since it does not suer from message implosion inherent in the monitoring example just described or in the case of deploying thousands of small sensors in disaster areas . 3.4 Utility Functions and Objective Functions We associate each sensor domain with a monotonically non-decreasing utility function which maps the number of nodes participating in a sensory
483 algorithms for doing so in the next section. 4 Adaptive, Energy-Ecient Algorithms for Utility Maximization Our algorithm exhibits several desirable properties for routing protocols as proposed by . # Loop-freedom: All routing and communication is performed over a logical spanning tree of the network # Localization: Our algorithm is distributed and message exchanges among nodes are localized
2722 which we survey here. One aspect of our work leverages o of the considerable body of literature which has focused on improving adaptive routing protocols for communication in ad-hoc networks . In general, these protocols provide improved fault-tolerance and support for mobility, for example by establishing a routing backbone which can be updated dynamically by distributed algorithms
1496 support this framework. Finally, the paper is concluded in Section 7. 2 Design rationales There exist many analytical methods for modelling and analysis of a real-time system’s temporal behaviour . However, the analytical models and analyses found in conventional scheduling theories are often too simple and therefore a real system cannot always be modelled and analyzed using such methods.
1102 ¢ are the dimensionalities of the reduced shape space and ¥ the reduced texture space, respectively. Note that a concatenated shape and texture vector has been earlier suggested by Cootes et al . They use a diagonal weight matrix to combine the shape and texture features. Rather than calculating a weight matrix, we implement a simple normalization procedure to commensurate the lower
1719 etc. It supports DAML+OIL and RDF. We chose RACER as the ontology reasoner for our approach since it has a richer set of functionalities and is supported by graphical ontology editors such as OilEd  and RICE (RACER Interactive Client Environment), both of which can perform reasoning over DAML+OIL with RACER as a background reasoner. In our approach, RACER will be used with OilEd as a user
1939 of processor enhancements. 3) This paper, by way of illustrating the second contribution, determines the most important machine parameters in the commonly used SimpleScalar superscalar simulator . The remainder of this paper is organized as follows: Section 2 describes the statistical Plackett and Burman design. Sections 3 and 4 describe the experimental setup and the results, respectively,
1939 values and benchmark programs for simulations, and how it can provide insights into the impact of a processor enhancement. The base simulator, sim-outorder, is from the SimpleScalar tool suite  and models a superscalar processor. We modified sim-outorder to include user configurable instruction latencies and throughputs. The benchmarks that were used in this paper, shown in Table 2, were
1496 number of preemptions with respect to the cost to pay. 1. Introduction and problem description Preemptive fixed priority scheduling (FPS) has been widely studied since the work of Liu and Layland . It has gained large acceptance in a number of applications, mostly due to simple run-time scheduling and good flexibility for tasks with incompletely known attributes. However, the impact of
1102 quantitatively and qualitatively and show successful real-time face tracking on a number of image sequences containing varying degrees of occlusions. 1 Introduction Active Appearance Models (AAMs)  (and the closely related concepts of Active Blobs  and Morphable Models ) are generative parametric models commonly used to track faces in video. AAMs are normally constructed by applying
1102 collection of training images of faces with a mesh of canonical feature points (usually handmarked) on them . AAMs are then fit frame-by-frame to input videos to track the face through the video . The best fit model parameters are then used in whatever the chosen application is. A variety of video applications are possible, including dynamic pose estimation for real-time user interfaces,
1102 2.1.2 Computing the Shape Variation: si In traditional AAMs the shape vectors si are computed by first aligning every training shape vector s with the base mesh s0 using a similarity transform . The mean shape (i.e. the base mesh s0) is subtracted from each shape vector. Principal Components Analysis  is then performed on the aligned shape vectors s. In the case of occlusion only the
1102 of the face region occluded. Only non-occluded vertices are used in the AAM construction. si are then set to be the orthonormalized eigenvectors with the largest eigenvalues. As is common practice  we retain enough shape modes to explain 95% of the observed variation in the training set. 2.2 Appearance As a convenient abuse of terminology, let s0 also denote the pixels x = (x, y) T that lie
1102 AAMs the appearance vectors Ai are computed by warping all of the input images onto the base mesh using the piecewise affine warps defined between the training shape vector s and the base mesh s0 . Principal Components Analysis is then applied to the resulting images. In the case of occlusion the shape normalized input images are incomplete. If any of the vertices of a triangle are not
1102 algorithm accurately tracks the face while the project-out algorithm fails. portant to note that we achieve accurate tracking of a face across wide pose changes with a single model. In  the same task was achieved using multiple AAMs and a heuristic for switching between them. One major advantage of using only a single model is that the model parameters have the same “meaning” for
