9074080	22	and type II diabetes. Finally, experimental findings from animal studies show that the refractive state of young chicks will adapt to compensate for refractive errors induced by spectacle lenses. The combination of genetic and environmental influences on the development of myopia suggests that myopia is a complex disorder and should not be classified as a simple Mendelian trait. Further
9074107	49	matching procedure gives rise to the selection of the appropriate winning unit. In the present paper, we investigate Wittmeyer’s iteractive scheme  for solving the input reconstruction problem  and combine it with a winner-takeall method, where the winner is selected upon a least squares Goodness-Of-Fit (GOF) of the input reconstruction. At this point, the method resembles the
9074107	49	between the input and the memories), and only one neuron stays active. It is also true for the normalised memories that the neuron with the largest initial activity will be ‘selected’ as the winner . The DCR scheme that utilises overlapping memories can thus be viewed as a linear network that promotes contrast enhancement sometimes up to the extreme of WTA behaviour at the level of the
9074107	58	at the level of the internal representation via indirect inhibitory action between neurons (expressed by the term ?a e), and the inhibitory action is mediated by the reconstruction procedure . 3. Extension using the Winner-TakeAll Procedure It is straightforward to extend the DCR scheme to a categorising architecture by designing DCR subnets for each category and utilising a WTA layer
9074107	65	to the DCR net is strongly recommended. Independent component analysis is thus attractive, since it is closely related to sparse representation and prefers concentrated neural activities . In this paper, no effort was made to develop such a preprocessing stage, e.g. in the form of Gabor filters: it means that the results could be improved. If one considers how simple and fast the
9074120	80	the MPS from the initial probability vector and Vi, j t ( ) , according to the standard computation algorithms. 4 DEEM at work As already described in Section 2, DEEM possesses a GUI inspired by  and realized using an X11 installation with Motif runtime Libraries which the user employs to define his model of a MPS. We remark that while building the models, the attributes of the model
9074120	82	models a phase change.sFigure 2: Property windows associated to Transition t1 Figure 3: Property windows associated to TransitionSO1-yes Figure 4: DEEM Interface and the DSPN model of the MPS in  Each net is made dependent on the other one by marking-dependent predicates which modify transition rates, enabling conditions, reward rates, etc., to model the specific MPS features. Marking
9074121	86	to n-dependent and n-independent functions. Probably this gap can be reduced by considering the fixed points of f1,f2,..., successively, and by using a proper renormalization procedure . However, this has not yet been successfully carried out.sN. Kataoka, K. Kaneko / Physica D 181 (2003) 235–251 241 Fig. 4. Phase diagram (·-2) regarding the behavior of the attractor of fn,
7448224	90	important one. When the term user implies a team of persons, different criteria of what constitutes credibility can apply. Various formal methods of credibility assessment are then often suitable (Balci 1998). For implementation we hence need a simulation model in which the user can have faith in the sense that it is comStåhl patible with her own mental model of the studied system. In order to
7448224	94	slightly modified. Some examples are Simula (based on Algol 60), SIMSCRIPT (based on FORTRAN), MODSIM (based on Modula), CSIM19 (based on C++; see Schwetman 2001) and Silk (based on Java; see Kilgore 2000). Since they have a GPL, often object oriented, in the core, they can be used for writing in principle any kind of program, doing this in a very structured fashion. 3. Block Based Simulation
8918452	125	and structures as the works of Lefebvre, de Certeau and the Situationists might imply. Everyday life might first and foremost be constituted by the search for simplicity, comfort and order . We will argue here, that there is no need to consider the two interpretations mutually exclusive. Actually the very articulation of social friction might prove a manoeuvre that can bind them
172	173	However, individual rationality results in free-riding behavior among peers, at the expense of collective welfare. Empirical studies have shown prevalent free-riding in P2P file sharing systems . While it is possible that free-riding can be sustainable in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of
172	174	???¤£ and ????? are positive constants. Hence user benefit is an increasing function of the number of contributors, but with diminishing returns—a form widely accepted in this context (see, e.g., , , ). Thus, the performance of the system, denoted by ????????????? , is defined as the difference between the average benefit received by all users (including both contributors and
172	175	and ????? are positive constants. Hence user benefit is an increasing function of the number of contributors, but with diminishing returns—a form widely accepted in this context (see, e.g., , , ). Thus, the performance of the system, denoted by ????????????? , is defined as the difference between the average benefit received by all users (including both contributors and free-riders)
172	176	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	176	the free-riders. We can interpret the value p as the probability that a free-rider can be caught and excluded from the system. Alternatively, we can adopt a service differentiation interpretation , where the free-riders receive a reduced benefit of (1 - p)Q. Downgrading the benefit of the free-riders increases user contribution in two ways. First, the reduction in system load reduces the
172	177	are two ways to counter whitewashing attacks. The first is to require the use of free but irreplaceable pseudonyms, e.g., through the assignment of strong identities by a central trusted authority . In the absence of such mechanisms, it may be necessary to impose a penalty on all newcomers, including both legitimate newcomers and whitewashers. This results in a social cost due to cheap
172	178	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	180	the free-riders. We can interpret the value p as the probability that a free-rider can be caught and excluded from the system. Alternatively, we can adopt a service differentiation interpretation , where the free-riders receive a reduced benefit of (1 - p)Q. Downgrading the benefit of the free-riders increases user contribution in two ways. First, the reduction in system load reduces the
172	181	re-joins the network under new identities to avoid the penalty imposed on free-riders. The whitewashing attack is made feasible by the availability of low cost identities or cheap pseudonyms . There are two ways to counter whitewashing attacks. The first is to require the use of free but irreplaceable pseudonyms, e.g., through the assignment of strong identities by a central trusted
172	181	necessary to impose a penalty on all newcomers, including both legitimate newcomers and whitewashers. This results in a social cost due to cheap pseudonyms, as demonstrated by Friedman and Resnick . We develop a simple modeling framework that helps to predict the level of free-riding in P2P systems. We use this model to quantify the effect of a penalty mechanism, which gives free-riders
172	181	same time, a fraction §? ¢¡£s? §? ¤¡ ?¦¥ ? of users whitewash under FI. strategy by imposing the penalty on all newcomers. However, this results in a social cost, as shown by Friedman and Resnick . In this section, we are interested in quantifying the social cost of cheap pseudonyms in terms of system performance loss. We do so by extending our model from section 4 into a dynamic model with
172	182	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	183	peers according to their contribution level. However, imposing penalties on free-riders require some means of identifying free-riders and distinguishing them from contributors. Reputation systems  may help, but these systems may be vulnerable to the whitewashing attack, where a free-rider repeatedly re-joins the network under new identities to avoid the penalty imposed on free-riders. The
172	183	the free-riders. We can interpret the value p as the probability that a free-rider can be caught and excluded from the system. Alternatively, we can adopt a service differentiation interpretation , where the free-riders receive a reduced benefit of (1 - p)Q. Downgrading the benefit of the free-riders increases user contribution in two ways. First, the reduction in system load reduces the
172	185	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	186	peers according to their contribution level. However, imposing penalties on free-riders require some means of identifying free-riders and distinguishing them from contributors. Reputation systems  may help, but these systems may be vulnerable to the whitewashing attack, where a free-rider repeatedly re-joins the network under new identities to avoid the penalty imposed on free-riders. The
172	187	????? are positive constants. Hence user benefit is an increasing function of the number of contributors, but with diminishing returns—a form widely accepted in this context (see, e.g., , , ). Thus, the performance of the system, denoted by ????????????? , is defined as the difference between the average benefit received by all users (including both contributors and free-riders) and
172	188	However, individual rationality results in free-riding behavior among peers, at the expense of collective welfare. Empirical studies have shown prevalent free-riding in P2P file sharing systems . While it is possible that free-riding can be sustainable in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of
172	189	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
190	192	the union of pairwise Minkowski sums. After the second step, there can be O(n 2 ) pairwise Minkowski sums. The pairwise convex Minkowski sums are convex. Their union can have O(n 6 ) complexity . 3.3 Our Approach Our algorithm for computing the Minkowski sum is based on the decomposition property. We have a set of convex primitives consisting of the pairwise convex Minkowski sums whose
190	195	Motion Planning with Translation Motion 5 sums . In the worst case, k = O(n 2 ). However, for non-convex polyhedra in 3D, the Minkowski sum can have O(n 6 ) worst-case complexity . One common approach for computing Minkowski sum of general polyhedra is based on convex decomposition. It uses the following property of Minkowski sum. If P = P1 ? P2, then P ? Q = (P1 ? Q) ? (P2
190	196	the two polyhedra and compute pairwise Minkowski sums between the convex pieces. We used a modification of the convex decomposition scheme available in a public collision detection library, SWIFT++ . We used a convex hull algorithm to compute the pairwise Minkowski sums. This algorithm adds the vectors of each vertex of one polyhedron with that of every vertex of the other polyhedron to get a
190	200	for polygonal objects was also developed by Avnaim and Boissonant . Sacks  presented a practical configuration space computation algorithm for pairs of curved planar parts. Halperin  presented efficient and robust algorithms to compute the Minkowski sum of 2D polygonal objects and used them for exact motion of planning of 2D objects undergoing translation motion. Aronov and
190	201	potential field . These approaches can be resolution complete if the resolution parameters are selected properly, but not exact or complete. Other algorithms are based on probabilistic roadmaps , which have been successfully applied to many high-dof robots. However, they may not terminate in a deterministic manner when there is no collision-free path.s2 Varadhan et. al. In this paper, we
190	201	connectivity roadmap. 5.1 Sampling We construct a roadmap by performing a sampling of the free space. Unlike previous approaches such as probabilistic roadmaps (PRMs) that generate samples randomly , we construct a roadmap in a deterministic fashion. Our goal is to sample the free space sufficiently to capture its connectivity. If we do not sample the free space adequately, we may not detect
190	206	efficient algorithm for a polygonal robot among polygonal obstacles with 3-dof configuration space. A similar algorithm for polygonal objects was also developed by Avnaim and Boissonant . Sacks  presented a practical configuration space computation algorithm for pairs of curved planar parts. Halperin  presented efficient and robust algorithms to compute the Minkowski sum of 2D
190	206	the configuration space obstacle corresponds to performing a sliced Minkowski sum operation on RSV and OSV . This is related to the concept of computing critical slices in the configuration space . By using the above formulation, we can use our path planning algorithm to compute a collision-free path. 7.3 Tangential Contact The sampling algorithm presented earlier uses an octree to perform
12008184	228	with humans and while performing tasks. Parts of this architecture have already been extended to several robots designed specifically for enhanced human interaction, such as MIT’s robot Leonardo (Breazeal, 2003) (Figure 3). While Leonardo is not a humanoid, it is being developed with human-like characteristics and functionalities. We are also extending the architecture and methodology to include and study
12008184	230	high-level control mechanisms aboard Robonaut. These cognitive architectures are ACT-R/S (Harrison and Schunn, 2003) based upon the ACT-R architecture (Anderson and Lebiere, 1998) and Polyscheme (Cassimatis, 2002). ACT-R is one of the most prominent cognitive architectures to have emerged in the past two decades as a result of the information processing revolution in the cognitive sciences. Also called a
12008184	230	can be performed. We use ACT-R/S to create cognitively plausible models of human performance of tasks to be performed by the robots. Furthermore, we are using Cassimatis’ Polyscheme architecture (Cassimatis, 2002) for spatial, temporal and physical reasoning. The Polyscheme cognitive architecture enables multiple representations and algorithms (including ACT-R models), encapsulated in “specialists” to be
12008184	237	individuals who do not know the spatial reasoning capabilities and limitations of the robot provide instructions to the robot for performing various tasks where spatial referencing is required (Perzanowski, et al., 2003). The results of this study will be used to enhance the multimodal interface by establishing a common language for spatial referencing which incorporates those constructs and utterances most
12008184	241	we might want to build into an intelligent, collaborative robot. The capabilities described above have been successfully implemented and demonstrated on several mobile robotic platforms (Sofge, et al., 2004), and we are now porting them to Robonaut. We are also extending the capabilities of the cognitive architectures (both ACT-R/S and Polyscheme) and their perspective-taking cognitive models. Future
12008184	242	“north.” Interaction with a robot capable of manipulating the same representation instead of traditional real number matrices would be more natural and efficient. In (Bugajska, et al., 2002) and (Trafton, et al., 2003) we used cognitive models of human performance of the task to augment the capabilities of robotic systems. We are investigating the use of two cognitive architectures based on human cognition for
12008184	243	that communication among them will be much easier. For example, in tasks requiring direction generation, humans naturally use qualitative spatial relationships (Miller and JohnsonLaird, 1976; Tversky, 1993) such as “left,” “up”, “east,” or “north.” Interaction with a robot capable of manipulating the same representation instead of traditional real number matrices would be more natural and efficient.
8918473	288	the user to focus on the customer, rather than worrying about how to access and update the information. Empowering users with access to their sales data via a Natural Language Interface (NLI)  does just that. Rather than taking the time to learn how to navigate through extensive menus only to be forced to sit through multiple screen refreshes, users simply ask for what they want in one
8918473	289	This is all the more important in a mobile setting, where the NLI does not have the luxury of a complementing GUI. An Agent-Oriented NLI The Adaptive Agent Oriented Software Architecture (AAOSA) is an Agent Oriented Software Engineering (AOSE)  system used for Natural Language Interfaces (NLI) . To create an NLI using AAOSA, an engineer will model the application, not the language,
303	307	matrix, further denoted by ?, we have For the resulting within-class means, further denoted by ?i, we have It can be shown, but this falls outside the scope of this paper, that 1 ? = Nex ? 1 S2W. (8) ?i = U ? Wµi, i = 1, . . . , Nuser. (9) ?j,j = 1, j = 1, . . . , NPCA ? Nuser + 1, (10) (?i)j = 0, j = 1, . . . , NPCA ? Nuser + 1. (11) This means that only the last Nuser ? 1 dimensions of U ? WW
303	307	and a test set of, nearly, equal sizes. This was done in such a way that the three measurements for the same grip were kept together. The total mean, the diagonal within-class covariance matrix ? (8), the within-class means ?i (9) and the total transform matrix T (12) were estimated from the training set. Similarity scores (13) were computed for all the data in the training and in the test set.
303	309	by ?i, we have It can be shown, but this falls outside the scope of this paper, that 1 ? = Nex ? 1 S2W. (8) ?i = U ? Wµi, i = 1, . . . , Nuser. (9) ?j,j = 1, j = 1, . . . , NPCA ? Nuser + 1, (10) (?i)j = 0, j = 1, . . . , NPCA ? Nuser + 1. (11) This means that only the last Nuser ? 1 dimensions of U ? WW can contribute to the verification. Therefore, a further dimension reduction is
316	317	protein interactions, relational data mining, graph-based data mining 1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics
316	317	YMR207C Definition 1. A single-node basis set is identical to a set of descriptors Di ?D. This definition is equivalent to the basic definition of an item set used in association rule mining . Our goal is to mine relational basis sets that will be constructed from multiple descriptor sets that belong to the same tuple of a joined relation. An edge relation has two attributes RE(Tl,Tr),
316	318	protein interactions, relational data mining, graph-based data mining 1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics
316	318	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
316	8918479	the known correlations dominate all other observations either directly or indirectly. This problem has been observed when relational association rule mining is directly applied to protein networks . Excluding matching items of interacting proteins is therefore commonly advisable in the interest of getting meaningful results alone . Matching annotations can be studied by simple correlation
316	321	of annotations on an individual network is discussed in . These approaches fall short of contrasting annotations in different networks. A further related research area is graphbased ARM . Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find frequent substructures in that setting. Removal of a class of redundant
316	322	principle, have the alternate form RE(Tl,Tr,D (E) )withD (E) being a set of edge descriptors. We could split such a relation into a separate node relation as well as a standard edge relation as in . Joined-relation basis sets are formed in multiple steps. Edge and node relations are joined through a natural join operation (?). Attribute names are changed  such that they are unique. We use
316	322	join-relation and any higher order relation. The support and confidence will however vary depending on that context, and a rule that is strong in one context may not be so in another. We follow  in always using the lowest order possible. For network comparison purposes we need three entities to derive 2-node rules. See definition (6). The problems associated with multiple contexts leads us
316	323	ARM is to find frequent substructures in that setting. Removal of a class of redundant rules is an important part of differential rule mining. Redundant rules have been studied, and closed sets  have proven a successful approach to their elimination. Closed sets alone do not, however, address the problem of contrasting different nodes or networks. Since we know what kinds of rules we want
316	324	1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics problems, biologists are interested in comparing different sets of items.
316	324	notes the problem of what we term repetitious item sets but does not resolve it. Relational association rule mining has more generally been addressed in the context of inductive logic programming . These approaches are very flexible and leave most choices up to the user. This paper, on the other hand, addresses the question of what specifications allow extracting meaningful rules. It is
316	327	protein interactions, relational data mining, graph-based data mining 1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics
316	327	rule mining is commonly defined and implemented over sets of items. We combine the concept of sets with the relational algebra framework by choosing an extended relational model similar to  . Attributes within this model are allowed to be set-valued, thereby violating first normal form. We go one step further by allowing sets of tuples, i.e. relations themselves, as attribute values.
316	329	of annotations on an individual network is discussed in . These approaches fall short of contrasting annotations in different networks. A further related research area is graphbased ARM . Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find frequent substructures in that setting. Removal of a class of redundant
316	332	1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics problems, biologists are interested in comparing different sets of items.
316	332	notes the problem of what we term repetitious item sets but does not resolve it. Relational association rule mining has more generally been addressed in the context of inductive logic programming . These approaches are very flexible and leave most choices up to the user. This paper, on the other hand, addresses the question of what specifications allow extracting meaningful rules. It is
316	333	North Dakota 58105 anne.denton Ajay Yekkirala Biology Dept North Dakota State University Fargo, North Dakota 58105 ajay.yekkirala actions are detected in silico by comparing different species . Two genes in one species are labeled as interacting if they have homologs in another species and those homologs are exons of the same gene. Previous approaches to network comparison have studied
316	334	following equation defines this step for a given rule A?C: ?G(A) ? ?G(C) ==? (8) 4.1 Data sets Our data consist of one node relation gathered from the Comprehensive Yeast Genome Database at MIPS , gene orf. The gene orf node relation represents gene annotation data. Annotations are hierarchically structured, with hierarchies for function, localization, protein class, complex, enzyme
316	338	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
316	343	sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A tuple in a joined-relation can
316	345	of annotations on an individual network is discussed in . These approaches fall short of contrasting annotations in different networks. A further related research area is graphbased ARM . Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find frequent substructures in that setting. Removal of a class of redundant
316	346	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
316	347	ARM is to find frequent substructures in that setting. Removal of a class of redundant rules is an important part of differential rule mining. Redundant rules have been studied, and closed sets  have proven a successful approach to their elimination. Closed sets alone do not, however, address the problem of contrasting different nodes or networks. Since we know what kinds of rules we want
316	348	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
349	350	delay by providing multiple path options. However, the router delay for deterministic routers, and consequently their corresponding clock cycles, can be significantly lower than adaptive routers . This di erence in router delays is due to two main reasons: number of VCs and output (OP) channel selection. Two VCs are su cient to avoid deadlock in dimension ordered routing ; while adaptive
349	350	the OP channel selection policy depends also on the state of the router (i.e the occupancy of various VCs) causing increased router complexity and higher router delays. The results reported in  show that the router delays for adaptive routers are about one and a half to more than twice as long as the dimension-order router for wormhole routing. The advantage of adaptive routing in
349	350	blocking. 2.3 Modeling Router Delay In this section we describe a router delay model for the virtual cut-though deterministic and adaptive routers. The model is based on the ones described in . These models account for both the logic complexity of the routers as well as the size of the crossbar as determined by the number of VCs that are multiplexed on one PC. These models were modi ed
349	350	and dominated by Ts for large bu er sizes. All of these added delays result in adaptive routers that are 15 to 16 % slower than deterministic routers. These results are similar to the results in  where 15% to 60% improvement is required for f- at routers with similar number of VCs and under worm-hole routing. 3 Hybrid Routing This section describes the mechanism of the hybrid routing scheme
349	350	in understanding the e ects of router complexity on cycle time involved deterministic routers . Adaptive and deterministic router implementations were then compared for worm-hole routing . However, the comparison in  does not account for the reduced queuing delay in adaptive routing. In  the reduction in queuing delay for worm-hole routing is taken into account and the
349	352	delay by providing multiple path options. However, the router delay for deterministic routers, and consequently their corresponding clock cycles, can be significantly lower than adaptive routers . This di erence in router delays is due to two main reasons: number of VCs and output (OP) channel selection. Two VCs are su cient to avoid deadlock in dimension ordered routing ; while adaptive
349	352	the OP channel selection policy depends also on the state of the router (i.e the occupancy of various VCs) causing increased router complexity and higher router delays. The results reported in  show that the router delays for adaptive routers are about one and a half to more than twice as long as the dimension-order router for wormhole routing. The advantage of adaptive routing in
349	352	blocking. 2.3 Modeling Router Delay In this section we describe a router delay model for the virtual cut-though deterministic and adaptive routers. The model is based on the ones described in . These models account for both the logic complexity of the routers as well as the size of the crossbar as determined by the number of VCs that are multiplexed on one PC. These models were modi ed
349	352	channels. Note that this relationship includes the delivery port. Delay equations for the routers are derived, using the above parameters. The constants in these equations were obtained in  using router designs along with gate-level timing estimates based on a 0.8 micron CMOS gate array process. The three main operations (delays) prevalent in all of the routers simulated here are as
349	352	in understanding the e ects of router complexity on cycle time involved deterministic routers . Adaptive and deterministic router implementations were then compared for worm-hole routing . However, the comparison in  does not account for the reduced queuing delay in adaptive routing. In  the reduction in queuing delay for worm-hole routing is taken into account and the
349	355	path to be utilized more often and o sets any adaptivity loss. 5 Related Work Some of the earliest work in understanding the e ects of router complexity on cycle time involved deterministic routers . Adaptive and deterministic router implementations were then compared for worm-hole routing . However, the comparison in  does not account for the reduced queuing delay in adaptive
349	356	delays is due to two main reasons: number of VCs and output (OP) channel selection. Two VCs are su cient to avoid deadlock in dimension ordered routing ; while adaptive routing (as described in ) requires a minimum of three VCs in k-ary n-cube networks. In dimension-ordered routing, the OP channel selection policy only depends on information contained in the message header itself. In
349	356	dimensions, no cycle exists in the channeldependency graph and the algorithm is deadlock-free. The adaptive routing scheme considered in this work (Duato's or *-channels algorithm) is described in . In this algorithm, adaptive routing is obtained by using adaptive VCs along with dimension-order routing. A message is routed on any adaptive channel until it is blocked. Once blocked, a message
349	356	through a given stage. This routing scheme is deadlock free: for any given message, the selection of paths is always a true subset of those that could be selected by the adaptive algorithm in . Since the adaptive algorithm has been proven deadlock free, the hybrid is also deadlock free. 3.2 Pipelined Implementations The Pipelined Hybrid Router (PHR) implementation is shown in Figure 4.
349	360	Several studies have demonstrated that adaptive routing can achieve alower latency, for the same load, than deterministic routing when measured by a constant clock cycle for both routers . The delay experienced by a message, at each node, can be broken down into: router delay and queuing (or waiting) delay. The former is determined primarily by the complexity of the router. The
349	361	or multi-machine level implementations (NOWs). 2 Deterministic and Adaptive Routing The interconnection network model considered in this study is a k-ary n-cube using virtual cut-through switching : message advancement is similar to worm-hole routing , except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu ered
349	364	and Adaptive Routing The interconnection network model considered in this study is a k-ary n-cube using virtual cut-through switching : message advancement is similar to worm-hole routing , except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu ered at a single node. Note that a header it can progress to a
349	365	and non-minimal fully adaptive routing is possible . The Cray T3E router is also a hybrid router. Messages can be routed deterministically or adaptively by simply setting a bit in the header . The router supports a shortcut for messages that continue traveling in the same dimension and uses directionorder routing for its deterministic routing algorithm. It also implements a routing
366	369	the way RBAC can be extended practically. 3 Role Attributes The attributes associated with a role has variously been discussed in . A more sophisticated approach than that taken in  is needed as soon as the elemen3stary model is to be extended. The attributes highlighted below, lack of explicit mention so far in the literature, are mainly for their usefulness.. Why do we need
366	373	environment. ??? ???????????????????? When the responsibility and capability of a subject interacts with that of another, there is the concept of role interaction. This has been amply discussed in , under both the concept of role relationships and relationship classes. Apart from this significant concept, there is a need to recognise the relative nature of roles with respect to role owner.
366	373	matters are reported. In this case, role relativity provides distinct views according to the role owner, and adds clarity to the notion of role, something that eludes the definitions given in . The single most important application of this concept is in enforcing RBAC related to personal data privacy laws, where role owner’s identity must be taken into account. ??? ???????????????? Often
366	375	a role becomes active. This is very similar to role scope establishment which is orthogonal to the activation requirements. In the literature, role has been regarded as a collection of policies in . This treatment will subsume the requirements mentioned in this section, provided that the way policies are described is sufficiently generic, with descriptive power at least as encompassing as
366	376	another, the transferor will no longer retain the capabilities. How delegable and transferable a role is could be an attribute of the role itself. Previous discussion on this area can be found in  which looks at relatively simple scenarios. For convenience, we coin the phrase “role empowerment” to mean either role delegation or role transfer. A more complete approach to role empowerment
1500758	389	given point in time. Such an assumption is appropriate for active sensors such as pan-tilt-zoom (PTZ) cameras, which are finding significantly increased usage in the current geopolitical climate , or when the computational requirements of tracking algorithms support only a single target assignment. With this in mind, our problem can be viewed as an optimal allocation of resources for target
1500758	381	This yielded significant improvements to methods used in mobile robot navigation, localization and mapping . Thrun et al. have also contributed significant research to these areas . However, our work distinguishes itself from traditional data fusion techniques in that the sensors themselves are actively managed to improve the quality of the measurements obtained prior to the
1500758	382	This yielded significant improvements to methods used in mobile robot navigation, localization and mapping . Thrun et al. have also contributed significant research to these areas . However, our work distinguishes itself from traditional data fusion techniques in that the sensors themselves are actively managed to improve the quality of the measurements obtained prior to the
1500758	383	resulting in corresponding improvements in state estimation. There has been other related research under the heading of sensor networks. Cortes et al. investigated the issue of sensor coverage . In their model a single sensor is sufficient to cover a point, however the quality of coverage decreases with distance. This research focused on the movement of sensor networks while ensuring
1500758	389	a subset of X ×Y ×W such that every element of X ? Y ? W belongs to exactly one element of A) such that ? (i,j,k)?A c(i, j, k) is minimized. 3D-Assignment (3DA) is NP-hard  and inapproximable . It is easy to see that any instance of 3DA can be reduced to an instance of FOA just by setting cFOA(i, j, k) =c3DA(i, j, k) whenever c3DA(i, j, k) is defined and infinite otherwise. Moreover,
1500758	393	followed random trajectories, and were tracked in simulation using particle filters. The respective particle sets were employed to generate a numerical error metric for the targets as discussed in . Two algorithms were investigated for this maximization approach. The first employed a greedy assignment strategy, and the second a 2-locally optimal approach as discussed in Section 3.5. The
1500758	394	can serve as inputs to traditional Bayesian filters (i.e. Kalman or particle) which yield estimates of the target pose. The latter has shown robustness for tracking features in a cluttered image . For the arbitrary sensor placement problem, general visibility constraints (range, occlusions, etc.) can also be accommodated by 32smerely eliminating the corresponding triples from the feasible
8918487	403	is the set of text extraction rules that identify the relevant information to be extracted. IE systems have been built with different levels of success on several kinds of text domains. CRYSTAL  was a learning-based IE system that took parsed annotated sentences and found patterns for extraction in novel sentences. Webfoot  was an attempt at general IE that processed fragments by
8918487	404	levels of success on several kinds of text domains. CRYSTAL  was a learning-based IE system that took parsed annotated sentences and found patterns for extraction in novel sentences. Webfoot  was an attempt at general IE that processed fragments by looking at Hyperlink Markup Language (HTML) tags. SRV  was another learning architecture for IE. It took a user-defined feature set
8918487	405	annotated sentences and found patterns for extraction in novel sentences. Webfoot  was an attempt at general IE that processed fragments by looking at Hyperlink Markup Language (HTML) tags. SRV  was another learning architecture for IE. It took a user-defined feature set together with a set of hand tagged training documents and learned rules for extraction. Craven et al.  reported that
8918487	406	tags. SRV  was another learning architecture for IE. It took a user-defined feature set together with a set of hand tagged training documents and learned rules for extraction. Craven et al.  reported that greater accuracy could be achieved by representing each web page as a node in graph and each hyperlink an edge. Cardie  provided a list of learning-based IE problems, including the
8918487	407	documents and learned rules for extraction. Craven et al.  reported that greater accuracy could be achieved by representing each web page as a node in graph and each hyperlink an edge. Cardie  provided a list of learning-based IE problems, including the difficulty of obtaining enough training data and the lack of corpora annotated with the appropriate semantic and domainspecific
8918487	410	combinators and a markup algebra. The markup algebra extracts structured and unstructured values from pages for computation, and is based on algebraic operations on sets of markup elements. Un Yong  describes a system called DiscoTEX that combines IE and KDD methods to perform a text-mining task, discovering prediction rules from natural-language corpora. Hence by parsing the HTML formatting,
8918487	412	predefined HTML templates. The systems generate delimiter-based rules that use linguistic constraints. Wien  uses only delimiters that immediately precede and follow the actual data. SoftMealy  is a wrapper induction algorithm that generates extraction rules expressed as finite-state transducers. World Wide Web Wrapper Factory  does extraction by using an HTML parser to construct a
414	415	In the case that there are missing data in the kernel matrix, Tsuda et al. (2003) developed a parametric approach to kernel matrix completion using the em algorithm based on information geometry (Amari, 1995). In their approach, the Kullback-Leibler (KL) divergence is used for measuring the similarity between kernel matrices. Assuming that the kernel matrix is a random positive definite matrix
414	416	a spectral method (Cristianini et al., 2002), semi-definite programming (SDP) (Lanckriet et al., 2002), the Gram-Schmidt method (Kandola et al., 2002), and a gradient-based method (Bousquet & Herrmann, 2003). Crammer et al. (2003) cast the kernel matrix learning problem under the boosting paradigm for constructing an accurate kernel from simple base kernels. In the case that there are missing data in
414	418	matrix. More specifically, we need a criterion for optimizing the kernel matrix. The alignment was proposed as such a criterion defined in the form of a similarity measure between kernel matrices (Cristianini et al., 2002). Based on this criterion, several methods have been proposed for optimizing the kernel matrix, including a spectral method (Cristianini et al., 2002), semi-definite programming (SDP) (Lanckriet et
414	418	algorithm can work on distributions over random variables or random vectors (instead of random matrices). This is motivated by some existing kernel matrix learning methods (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), which constrain the target kernel matrix to a weighted combination of some fixed base kernel matrices so that the kernel matrix learning problem can be
414	418	later, we define the kernel matrix K = (A + B)/2 as the discriminant kernel (Zhang, 2003), where A = ? exp(??xi ? xj? 2 /?) ? n×n is the standard Gaussian kernel and B is the ideal kernel (Cristianini et al., 2002) based on the training set, i.e., ij = ? 1 yi = yj 0 yi ?= yj. Following the generative model formulation of the kernel matrix in (Zhang et al., 2003a), we now assume that the kernel matrix K is
414	418	algorithm. In this section, we propose a simplified Bayesian model that makes it possible to develop an efficient implementation. In the kernel matrix learning literature (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), it is common to constrain the target kernel to a weighted combination of some available base kernels so that the learning problem is simplified to the
414	418	U and µ iµ ? i ’s as the base matrix and the base kernel matrices, respectively. Now, given U, we want to estimate ?i’s and hence G to approximate some desired kernel H, such as the ideal kernel (Cristianini et al., 2002), based on some criterion like the kernel alignment or the KL divergence between G and H. This motivates us to devise a simplified Bayesian hierarchical model and then an efficient implementation
414	424	et al., 2002). Based on this criterion, several methods have been proposed for optimizing the kernel matrix, including a spectral method (Cristianini et al., 2002), semi-definite programming (SDP) (Lanckriet et al., 2002), the Gram-Schmidt method (Kandola et al., 2002), and a gradient-based method (Bousquet & Herrmann, 2003). Crammer et al. (2003) cast the kernel matrix learning problem under the boosting paradigm
414	424	over random variables or random vectors (instead of random matrices). This is motivated by some existing kernel matrix learning methods (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), which constrain the target kernel matrix to a weighted combination of some fixed base kernel matrices so that the kernel matrix learning problem can be simplified to the
414	424	we propose a simplified Bayesian model that makes it possible to develop an efficient implementation. In the kernel matrix learning literature (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), it is common to constrain the target kernel to a weighted combination of some available base kernels so that the learning problem is simplified to the estimation of the
414	428	(x, y), we define a kernel matrix K on (X × Y) × (X × Y) in this paper. Specifically, in our experiments to be presented later, we define the kernel matrix K = (A + B)/2 as the discriminant kernel (Zhang, 2003), where A = ? exp(??xi ? xj? 2 /?) ? n×n is the standard Gaussian kernel and B is the ideal kernel (Cristianini et al., 2002) based on the training set, i.e., ij = ? 1 yi = yj 0 yi ?= yj.
10549099	473	growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However, it has been noted that square or 90-degree angles have serious issues and drawbacks , including the particularly serious parity constraint, i.e. any pair of amino acids which are an odd-distance apart from each other can never lie on adjacent square lattice points. 4sDue to this,
10549099	474	simplified models suffer from computational intractability in the worst case. For example, optimizing the simple 2-D square lattice hydrophobic-hydrophilic (HP)  has been shown to be NPComplete . In the past several years, numerous algorithms and techniques have been proposed and explored for quickly determining native conformations based on models such as the HP models. Methods such as
10549099	474	2D-lattice. The standard assumption has been that the lattice is square in structure. Under this assumption, it has been proven that protein folding on the two- dimensional HP model is NP-complete . Several methods have been presented to try to solve this problem, such as the chain growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However,
10549099	476	it has been proven that protein folding on the two- dimensional HP model is NP-complete . Several methods have been presented to try to solve this problem, such as the chain growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However, it has been noted that square or 90-degree angles have serious issues and drawbacks , including the
10549099	477	2D-lattice. The standard assumption has been that the lattice is square in structure. Under this assumption, it has been proven that protein folding on the two- dimensional HP model is NP-complete . Several methods have been presented to try to solve this problem, such as the chain growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However,
10549099	480	numerous algorithms and techniques have been proposed and explored for quickly determining native conformations based on models such as the HP models. Methods such as genetic and memetic algorithms , tabu search , and ant colony optimization  use approximation and randomized search in an effort to find good solutions in a reasonable amount of time. The fundamental nature of such
10549099	480	of the extra difficulty, simple genetic algorithms (GA) and evolutionary approaches have a difficult time solving 2D triangular lattice HP problems. As researchers have empirically demonstrated , more sophisticated optimization techniques such as hybrid local/global search, multi- meme GAs, scatter/gather searching, etc. must be employed in a variety of ways to improve diversity of the
10549099	480	to compute the computational effort saved through caching. We implemented both versions in C++ on a Pentium IV 2GHz machine with 1G RAM running Linux. The testbed proteins we employ are drawn from  for triangular 2D-HP and from  which have yet to be computed in triangular 2D-HP. Table 5.1 contains the HPs used in our experiments. We note again that our goal in this experiment is to
10549099	480	-17 30 PHHHPHHHPPPHPHHPHHPPHPHHHHPHPPHHHHHPHP HHPPHHP 45 n/a -36 31 HPHHHPHHHPPHHPHPHHPHHHPHPHPHHPPHHHPPHP HPPPPHPPHPPHHPPHPPH 57 n/a -38 Table 5.1. HP Testbed Proteins. Protein ID 1-20 are from . Protein ID21-31 are from . 16sAbsolute Energy Improvement 1.80 1.60 1.40 1.20 1.00 0.80 0.60 0.40 0.20 0.00 1 4 7 10 13 16 19 Protein Id # 22 25 28 31 Fig. 5.2. Average absolute energy value
481	483	(uncontrolled) sensor networks grew out of the field of distributed sensing, estimation, and data fusion. Recent work focuses on large dynamic sensor networks in which attribute based naming  must be used instead of traditional address naming. Simulations of information-theoretic sensor selection algorithms in networks of up to a 100 nodes are reported in . This work builds on a
481	483	or • what additional information about the world to use (an observation ? or a belief b) Technically, all of these options can implemented in scalable fashion provided that attribute based naming  is used and proper communication infrastructure is available. A tree network created and maintained by the DDF algorithm is suitable for transmitting such command messages as well. The four options
481	484	(II): The research in this field to date has been limited to communication between one or several humans and a small number of robots. The field of adjustable autonomy  or collaborative control  is an active research area which aims to span the gap between teleoperation and full autonomy. Current solutions call for bidirectional communication in the form of a human-robot dialog. With
481	485	and environment as a set of continuous states, together with the use of information as payoff, allows the information acquisition problem to be formulated as a standard optimal control problem . Several classification systems for human-robot interactions have been suggested. For example, Scholtz  identifies mechanic, supervisory, and peer-to-peer levels of human-robot interaction.
481	486	three indoor robots cooperatively build an map of a building in . Both data fusion and control part of the algorithm are centralized. Decentralized control techniques are combined with DDF in . In all of these works, the issues of human-robot interface are not addressed. This research area also includes a large body of work on reactive architectures, most of which are fully decentralized
481	486	platform, the sensor and actuator models, and the current state of the world supplied by the node, the controller will issue commands to the actuator which will maximize a certain reward function . Any number of nodes, sensors, and actuators may be attached to a single platform but from the point of view of a node, a sensor, or a controller platform assignment is unique. Likewise, an
481	486	as point features by the laser range finders. The objective of the network is to find and localize the point features. An information surfing controller is implemented on the Pioneer robots . It is a zero look-ahead control law which moves the platform in the direction of the steepest descent in information space. All software components run as separate applications and communicate
481	490	interface are not addressed. This research area also includes a large body of work on reactive architectures, most of which are fully decentralized and some were applied to sensing applications . They fall outside of the scope of this work because they do not perform network-level data fusion and typically do not interact with humans. Human-robot interaction in multi-robot systems (II):
481	491	interface are not addressed. This research area also includes a large body of work on reactive architectures, most of which are fully decentralized and some were applied to sensing applications . They fall outside of the scope of this work because they do not perform network-level data fusion and typically do not interact with humans. Human-robot interaction in multi-robot systems (II):
481	492	constructed and programmed in a modular fashion. 3.1 Multi-Agent Framework To make discussion of various aspects of communication within an ASN more concrete, a formal team framework will be used . The problem is stated as two tuples. ?S, A?, ??, P, ??, O?, B?, R, T ? , ???, ?A? (1) where for each agent i in team ? S world states (terrain, features, agents, etc.) agent’s domain-level actions
481	494	the information acquisition problem to be formulated as a standard optimal control problem . Several classification systems for human-robot interactions have been suggested. For example, Scholtz  identifies mechanic, supervisory, and peer-to-peer levels of human-robot interaction. Alternatively, the potential control methods can be identified by examining Equation 1. An operator can tell a
481	495	issues are not addressed. An architecture for DDF and control with an emphasis on scalability is described in . A team of three indoor robots cooperatively build an map of a building in . Both data fusion and control part of the algorithm are centralized. Decentralized control techniques are combined with DDF in . In all of these works, the issues of human-robot interface are
481	497	attribute based naming  must be used instead of traditional address naming. Simulations of information-theoretic sensor selection algorithms in networks of up to a 100 nodes are reported in . This work builds on a decentralized data fusion (DDF) architecture , demonstrated in a network of up to 8 nodes. Active sensor networks (I): A hierarchical architecture applied to real time
498	499	are labeled with 0 or 1 and correspond to the constant Boolean functions. The root node root(Gf ) corresponds to the function f. In the following, BDD refers to a reduced ordered BDD (as defined in ) and the size of a BDD is given by the number of nodes. Definition 1 A one-path in a BDD Gf = (V, E) is a path p = (v0, ..., vl?1, vl), vi ? V, (vi, vi+1) ? E with v0 = root(Gf ) and label(vl) = 1.
498	500	BDDs in general are an efficient data structure for representation and manipulation of Boolean functions. They are well-known and widely used in logic synthesis  and formal verification  of integrated circuits. BDDs are well-suited for applications in the area of logic synthesis, because the cubes in the ON-set of a Boolean function are implicitly represented in this data
498	509	function are implicitly represented in this data structure. A hybrid approach for the minimization of DSOPs relying on BDDs in combination with structural methods has recently been introduced in . It hassbeen shown that BDDs are applicable to the problem of DSOP minimization. Given a BDD of a Boolean function, the DSOP can easily be constructed: each one-path, i.e. a path from the root to
498	509	variable is chosen. No variable is chosen twice during this process. For the evaluation of our fitness function, in the following the improved pathminimization algorithm based upon sifting from  is used. This algorithm employs structural techniques to reduce the number of cubes in the DSOP. mp, 3 Evolutionary Algorithm In this section we describe the Evolutionary Algorithm (EA) that is
498	509	a fitness that measures the quality of the variable ordering. First the BDD is constructed using the variable ordering given by the individual, then the number of reduced one-paths are counted . The selection is performed by roulette wheel selection and we also make use of steady-state reproduction : The best individuals of the old population are included in the new one of equal size.
498	509	times to the 37 benchmarks in the test suite. Each time a randomly chosen seed for the random number generator was used. In column hybrid the number of cubes resulting from the method proposed in  for each function is given. Columns EA summarize the results from the EA proposed in this paper. min. and max. show the minimal and maximal DSOP of all test runs, respectively. In column median the
498	515	used in several applications in the area of CAD, e.g. the calculation of spectra of Boolean functions  or as a starting point for the minimization of Exclusive-Or-Sum-Of-Products (ESOPs) . In  some techniques for minimization of DSOPs have been introduced. They are working on explicit representations of the cubes and therefore are only applicable to small instances of the
498	519	applications in the area of CAD, e.g. the calculation of spectra of Boolean functions  or as a starting point for the minimization of Exclusive-Or-Sum-Of-Products (ESOPs) . In  some techniques for minimization of DSOPs have been introduced. They are working on explicit representations of the cubes and therefore are only applicable to small instances of the problem. BDDs
528	534	the LBM a number of physically accurate effects, such as the buoyancy effect of hot gases, additional realism can be provided. Our framework makes efficient use of the concept of textured splats , which are associated with the macroscopic particles to represent the gaseous phenomena. The textured splats form the observable “display particles,” such as the smoke particles or dust particles,
528	534	to give periodic and chaotic vector fields that can be combined with the global motions. Another approach is to take advantage of commodity texture mapping hardware, using textured splats  as the rendering primitive. King et al.  first used this technique to achieve fluid animation based on simple and local dynamics. A drawback of their model is, however, that it lacks the
528	535	section, we present a complete framework for the LBM-based fluid simulation and how its calculation is implemented based on the fast growing GPU technology. Physically-based particle models , , , , , ,  have also been used to describe fluid behaviors. Particle systems were first introduced by Reeves  as a technique for modeling fuzzy objects, such as fire, clouds,
528	535	the interaction of particles due to the thermal energy. Terzopoulos et al.  implemented a similar approach. Particles and springs are utilized to render a series of blobbies. Desbrun and Gascue ,  developed a paradigm extended from the Smoothed Particle Hydrodynamics approach used by physicists for cosmological fluid simulation. This technique defines a type of particle system which
528	536	of particles due to the thermal energy. Terzopoulos et al.  implemented a similar approach. Particles and springs are utilized to render a series of blobbies. Desbrun and Gascue ,  developed a paradigm extended from the Smoothed Particle Hydrodynamics approach used by physicists for cosmological fluid simulation. This technique defines a type of particle system which uses
528	539	rendering speed. Finally, we outline our implementation and describe several examples in Section 8. 2 PREVIOUS WORK A common approach to simulating gaseous phenomena is procedural modeling , , , where fluid behaviors are described by procedural functions. This method is fast and easy to program, but it is difficult to find the proper parameter settings that achieve realistic
528	541	gave an analytic solution to the NS equations by using simple flow primitives. Chen and Lobo  solved a simplified NS equations in 2D using a finite difference approach. Later, Foster and Metaxas  presented a full 3D finite difference solution to simulate the turbulent rotational motion of gas. Because of the inherent instability of the finite difference method with a larger time step, the
528	547	value. 6 MAPPING LBM TO GRAPHICS HARDWARE We briefly review in this section the basic ideas of mapping LBM to graphics hardware, that is, a graphics processing unit (GPU). (For more details, see .) To compute the LBM equations on GPU, we divide the LBM grid and group the packet distributions fqi into arrays according to their velocity directions. All the packet distributions with the same
528	548	the boundary conditions become complicated, such as a fast moving boundary object or a complex geometric structure. In our work, we implemented the boundary conditions based on Mei et al.’s method  for curved boundaries. In their approach, the problem is solved in another way. Instead of directly setting the microscopic values, they calculate the macroscopic variables of density and velocity
528	553	speed. Finally, we outline our implementation and describe several examples in Section 8. 2 PREVIOUS WORK A common approach to simulating gaseous phenomena is procedural modeling , , , where fluid behaviors are described by procedural functions. This method is fast and easy to program, but it is difficult to find the proper parameter settings that achieve realistic results. The
528	554	solution to simulate the turbulent rotational motion of gas. Because of the inherent instability of the finite difference method with a larger time step, the speed of this approach is limited. Stam  devised a fluid solver using a semi-Lagrangian advection scheme and implicit solver for the NS equations. Each term of the equations is handled in turn, starting with external force, then
528	555	equally efficient way to add the small-scale turbulence details into the visual simulation and render these to the screen. One way to model the small-scale turbulence is through spectral analysis . Turbulent motion is first defined in Fourier space and then it is transformed to give periodic and chaotic vector fields that can be combined with the global motions. Another approach is to take
528	558	the fluid animation. We also plan to model the behaviors of objects in the flow, such as the leaves blowing in the wind. Besides gaseous phenomena, our model can also be used to simulate liquid , heat in a solid, and the like, and be extended to model fire . ACKNOWLEDGMENTS This work is partially supported by US Office of Naval Research grant N000140110034, US National Science
528	559	in the flow, such as the leaves blowing in the wind. Besides gaseous phenomena, our model can also be used to simulate liquid , heat in a solid, and the like, and be extended to model fire . ACKNOWLEDGMENTS This work is partially supported by US Office of Naval Research grant N000140110034, US National Science Foundation (NSF) grants IIS-0097646 and CCR-0306438 and NSF CAREER grant
8918500	563	understanding how well humans and robots are able to work together. Thus, we intend to use a variety of metrics and critical incident analysis to characterize and assess human-robot performance (Fong et al. 2004; Scholtz et al. 2004). To evaluate system performance, one metric we will examine is fan out, which measures how many robots can be effectively controlled by a human (Goodrich and Olsen 2003). To
8918500	571	interaction. Moreover, interruption is problematic because humans have cognitive limitations that restrict their ability to work during interruptions and to resume previously interrupted tasks (McFarlane 1990). Computational Cognitive Models In order to take the best possible advantage of the particular skills of humans and of robots in mixed-initiative teams, itsis important that robots be able to
8918500	574	well humans and robots are able to work together. Thus, we intend to use a variety of metrics and critical incident analysis to characterize and assess human-robot performance (Fong et al. 2004; Scholtz et al. 2004). To evaluate system performance, one metric we will examine is fan out, which measures how many robots can be effectively controlled by a human (Goodrich and Olsen 2003). To assess operator
577	578	addressing this problem: algorithmic and architectural. A few pioneering groups of researchers posed the problem, provided complexity bounds, and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the
577	578	Addressable Memory (TCAM) . Some of the most promising algorithmic research embraces the practice of leveraging the statistical structure of filter sets to improve average performance . Several algorithms in this class are amenable to high-performance hardware implementation. We discuss these observations in more detail and provide motivation for packet classification on larger
577	578	to provide significantly better average performance. Gupta and McKeown published a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set
577	578	Classification (RFC) Leveraging many of the same observations, Gupta and McKeown introduced Recursive Flow Classification (RFC) which provides high lookup rates at the cost of memory inefficiency . The authors introduced a unique high-level view of the packet classification problem. Essentially, packet classification can be viewed as the reduction of an m-bit string defined by the packet
577	578	technique like hashing. We probe a tuple for a matching filter by using the bits of the packet field specified by the tuple as the search key. For example, we construct a search key for the tuple  by concatenating the first bit of the packet source address, the first three bits of the packet destination address, the Range ID of the source port range at Nesting Level 2 covering the packet
577	580	addressing this problem: algorithmic and architectural. A few pioneering groups of researchers posed the problem, provided complexity bounds, and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the
577	580	multiple field packet classification algorithms targeted to a hardware implementation. Their seminal technique is commonly referred to as the Lucent bit-vector scheme or Parallel Bit-Vectors (BV) . The authors make the initial assumption that the filters may be sorted according to priority. Like the previously discussed “cutting” algorithms, Parallel BV utilizes a geometric view of the
577	580	technique like hashing. We probe a tuple for a matching filter by using the bits of the packet field specified by the tuple as the search key. For example, we construct a search key for the tuple  by concatenating the first bit of the packet source address, the first three bits of the packet destination address, the Range ID of the source port range at Nesting Level 2 covering the packet
577	580	0 covering the packet destination port 35sTable 5: Example filter set; address fields are 4-bits and port ranges cover 4-bit port numbers. Filter SA DA SP DP Prot Tuple a 0? 001? 2 : 2 0 : 15 TCP  b 01? 0? 0 : 15 0 : 4 UDP  c 0110 0011 0 : 4 5 : 15 TCP  d 1100 ? 5 : 15 2 : 2 UDP  e 1? 110? 2 : 2 0 : 15 UDP  f 10? 1? 0 : 15 0 : 4 TCP
577	580	to at most (2W ? 1) where W is the address length. Each filter mapping to a tuple  leaves a marker in each tuple to its left in its row. For example, a filter (110?, 0111) stored in tuple  leaves markers (11?, 0111) in  and (1?, 0111) in . For all filters and markers in a tuple , we can precompute the best matching filter from among the filters stored in less
577	581	and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the performance constraints discussed in Section 1.1, researchers in industry and academia devised architectural solutions to the problem. This
577	581	set. 5.7 Fat Inverted Segment (FIS) Trees Feldman and Muthukrishnan introduced another framework for packet classification using independent field searches on Fat Inverted Segment (FIS) Trees . Like the previously discussed “cutting” algorithms, FIS Trees utilize a geometric view of the filter set and map filters into d-dimensional space. As shown in Fig21squery 0110, 11 Index Block
577	582	and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the performance constraints discussed in Section 1.1, researchers in industry and academia devised architectural solutions to the problem. This
577	582	a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set sizes are small, ranging from tens of filters to less than 5000 filters. It is unclear if the size
577	582	with deeper pipelines and higher link rates. Linear search is a popular solution for the final stage of a lookup when the set of possible matching filters has been reduced to a bounded constant . 4.2 Ternary Content Addressable Memory (TCAM) Taking a cue from fully-associative cache memories, Ternary Content Addressable Memory (TCAM) devices perform a parallel search over all filters in
577	582	e d j Woo independently applied the same approach as HiCuts and introduced a flexible framework for packet classification based on a multi-stage search over ternary strings representing the filters . The framework contains three stages: an index jump table, search trees, and filter buckets. An example data structure for the filter set in Table 4 is shown in Figure 11. A search begins by using
577	582	the HyperCuts algorithm  improves upon the HiCuts algorithm developed by Gupta and McKeown  and also shares similarities with the Modular Packet Classification algorithms introduced by Woo . In essence, HyperCuts is a decision tree algorithm that attempts to minimize the depth of the tree by selecting multiple “cuts” in multi-dimensional space that partition the filter set into lists
577	586	Addressable Memory (TCAM) . Some of the most promising algorithmic research embraces the practice of leveraging the statistical structure of filter sets to improve average performance . Several algorithms in this class are amenable to high-performance hardware implementation. We discuss these observations in more detail and provide motivation for packet classification on larger
577	586	a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set sizes are small, ranging from tens of filters to less than 5000 filters. It is unclear if the size
577	586	5.2 Extended Grid-of-Tries (EGT) Baboescu, Singh, and Varghese proposed Extended Grid-of-Tries (EGT) that supports multiple fields searches without the need for many instances of the data structure . EGT essentially alters the switch pointers to be jump pointers that direct the search to all possible matching filters, rather than the filters with the longest matching destination and source
577	587	Addressable Memory (TCAM) . Some of the most promising algorithmic research embraces the practice of leveraging the statistical structure of filter sets to improve average performance . Several algorithms in this class are amenable to high-performance hardware implementation. We discuss these observations in more detail and provide motivation for packet classification on larger
577	587	with deeper pipelines and higher link rates. Linear search is a popular solution for the final stage of a lookup when the set of possible matching filters has been reduced to a bounded constant . 4.2 Ternary Content Addressable Memory (TCAM) Taking a cue from fully-associative cache memories, Ternary Content Addressable Memory (TCAM) devices perform a parallel search over all filters in
577	587	0 1 Figure 11: Modular packet classification using ternary strings and a three-stage search architecture. 5.5 HyperCuts Introduced by Singh, Baboescu, Varghese, and Wang, the HyperCuts algorithm  improves upon the HiCuts algorithm developed by Gupta and McKeown  and also shares similarities with the Modular Packet Classification algorithms introduced by Woo . In essence, HyperCuts is
577	589	eliminate many of the unfavorable characteristics of current TCAMs . We observe that the community appears to be converging on a combined algorithmic and architectural approach to the problem . In order to lend structure to our discussion, we develop a taxonomy in Section 3 that frames each technique according to its high-level approach to the problem. The presentation of this taxonomy
577	589	Taylor and Turner also performed a battery of analyses on real filter sets, focusing on the maximum number of unique field values and unique combinations of field values which match any packet . They found that the number of unique field values is less than the number of filters and the maximum number of unique field values matching any packet remains relatively constant for various
577	590	eliminate many of the unfavorable characteristics of current TCAMs . We observe that the community appears to be converging on a combined algorithmic and architectural approach to the problem . In order to lend structure to our discussion, we develop a taxonomy in Section 3 that frames each technique according to its high-level approach to the problem. The presentation of this taxonomy
577	590	a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set sizes are small, ranging from tens of filters to less than 5000 filters. It is unclear if the size
577	590	6.5 Parallel Packet Classification (P 2 C) The Parallel Packet Classification (P 2 C) scheme introduced by van Lunteren and Engbersen also falls into the class of techniques using decomposition . The key novelties of P 2 C are its encoding and aggregation of intermediate results. Similar to the Parallel Bit-Vector and RFC techniques, P 2 C performs parallel searches in order to identify
577	592	in Section 6.3. For filters defined by source and destination prefixes, Grid-of-Tries improves upon the directed acyclic graph (DAG) technique introduced by Decasper, Dittia, Parulkar, and Plattner . This technique is also called set pruning trees because redundant subtrees can be “pruned” from the tree by allowing multiple incoming edges at a node. 11sa b c d e f g h 10* 0* 111 11* *
577	594	search engines. The latter is achieved by a novel technique called Field Splitting which we do not discuss in this survey. Using a collection of 12 real filter sets and the ClassBench tools suite , the authors provide analyses of DCFL performance and resource requirements on filter sets of various sizes and compositions. For the 12 real filter sets, they show that the worst-case number of
577	595	is much less than the number of filters in the filter set, Srinivasan, Suri, and Varghese introduced the tuple space approach and a collection of Tuple Space Search algorithms in a seminal paper . In order to illustrate the concept of tuples, we provide an example filter set of filters classifying on five fields in Table 5. Address prefixes cover 4-bit addresses and port ranges cover 4-bit
577	595	discussed in Section 4. 7.1 Tuple Space Search & Tuple Pruning The basic Tuple Space Search technique introduced by Srinivasan, Suri, and Varghese performs an exhaustive search of the tuple space . For our example filter set in Table 5, a search would have to probe seven tuples instead of searching all 12 filters. Using a modest set of real filter sets, the authors found that Tuple Space
577	598	Conflict-Free Rectangle Search W (d?1) d! Warkhede, Suri, and Varghese provide an optimized version of Rectangle Search for the special case of packet classification on a conflict-free filter set . A filter set is defined to be conflict-free if there is no pair of overlapping filters in the filter set such that one filter is more specific than the other in one field and less specific in
577	599	authors observe that in real filter sets conflicts are rare; furthermore, techniques exist to resolve filter conflicts by inserting a small set of resolving filters that resolve filter conflicts . Conflict-Free Rectangle Search begins by mapping the filter set to the W × W tuple space. Using precomputation and markers, the authors prove that a binary search can be performed on the columns
8918535	654	when touching between characters/digits is assumed. There have been three strategies for dealing with the segmentation problem: segmentation-based (dissection), recognition-based,andholistic Daekeun You and Gyeonghwan Kim Dept. of Electronic Engineering Sogang University CPO Box 1142, Seoul 100-611 Korea gkim@ccs.sogang.ac.kr approaches. Correct segmentation is important
8918535	657	could not be considered independently when touching between characters/digits is assumed. There have been three strategies for dealing with the segmentation problem: segmentation-based (dissection), recognition-based,andholistic Daekeun You and Gyeonghwan Kim Dept. of Electronic Engineering Sogang University CPO Box 1142, Seoul 100-611 Korea gkim@ccs.sogang.ac.kr approaches.
8918535	658	characters/digits is assumed. There have been three strategies for dealing with the segmentation problem: segmentation-based (dissection), recognition-based,andholistic Daekeun You and Gyeonghwan Kim Dept. of Electronic Engineering Sogang University CPO Box 1142, Seoul 100-611 Korea gkim@ccs.sogang.ac.kr approaches. Correct segmentation is important since errors
64580	665	Java programmers to use JML specifications as practical and effective tools for debugging, testing, and design by contract. 3.3 Unit testing A formal specification can be viewed as a test oracle , and a runtime assertion checker can be used as the decision procedure for the test oracle . This idea has been implemented as a unit testing tool for Java (jmlunit), by combining JML with the
64580	666	a significant advance over the state of the art in runtime assertion checking as represented by design by contract tools such as Eiffel  or by Java tools such as iContract  or Jass . The jmlc tool also supports advances such as (stateful) interface specifications, multiple inheritance of specifications, various forms of quantifiers and set comprehension notation, support for
64580	668	than can be handled by extended static checking with ESC/Java. A recent paper describing a case study with the LOOP tool, giving the best impression of the state of the art, is now available . A similar program verification tool for JML-annotated code under development is the Krakatoa tool ; it produces proof obligations for the theorem prover Coq, but currently covers only a subset
64580	668	earlier, which is specifically designed for Java Card programs. One of the classes of the electronic purse mentioned above has provided the most serious case study to date with the LOOP tool .s5 Related Work 5.1 Other runtime assertion checkers An Overview of JML Tools and Applications 11 Many runtime assertion checkers for Java exist, for example, Jass, iContract, and Parasoft’s
64580	668	the use of JML gradually, simply by adding the odd assertion to some Java code. – JML can be used for existing (legacy) code and APIs. Indeed, most applications of JML and its tools to date (e.g., ) have involved existing APIs and code. 3. There is a (growing) availability of a wide range of tool support for JML. Unlike B, JML does not impose a particular design method on its users. Unlike
64580	669	same meaning in JML and Java, as does ==, and the same rules for overriding, overloading, and hiding apply. One cannot expect this for OCL. In fact, a semantics for OCL was only recently proposed .s12 Lilian Burdy et al. In all, we believe that a language like JML, which is tailored to Java, is better suited for recording the detailed design of a Java programs than a generic language like
64580	688	to specify and verify a component of a smartcard operating system . ESC/Java has been used with great success to verify a realistic example of an electronic purse implementation in Java Card . This case study was instrumental in convincing industrial users of the usefulness of JML and feasibility of automated program checking by ESC/Java for Java Card applets. This provided the
64580	688	the use of JML gradually, simply by adding the odd assertion to some Java code. – JML can be used for existing (legacy) code and APIs. Indeed, most applications of JML and its tools to date (e.g., ) have involved existing APIs and code. 3. There is a (growing) availability of a wide range of tool support for JML. Unlike B, JML does not impose a particular design method on its users. Unlike
64580	671	3.6 CHASE One source of unsoundness of ESC/Java is that it does not check assignable clauses. The semantics of these frame axioms are also not checked by the JML compiler. The CHASE tool  tries to remedy these problems. It performs a syntactic check on assignable clauses, which, in the spirit of ESC/Java, is neither sound nor complete, but which spots many mistakes made in
64580	672	assertion checking The JML compiler (jmlc), developed at Iowa State University, is an extension to a Java compiler and compiles Java programs annotated with JML specifications into Java bytecode . The compiled bytecode includes runtime assertion checking instructions that check JML specifications such as preconditions, normal and exceptional postconditions, invariants, and history
64580	673	and design by contract. 3.3 Unit testing A formal specification can be viewed as a test oracle , and a runtime assertion checker can be used as the decision procedure for the test oracle . This idea has been implemented as a unit testing tool for Java (jmlunit), by combining JML with the popular unit testing tool JUnit for Java . The jmlunit tool, developed at Iowa State
64580	673	When the method under test satisfies its precondition, but otherwise has an assertion violation, then the implementation failed to meet its specification, and hence the test data detects a failure . In other words, the generated test code serves as a test oracle whose behavior is derived from the specified behavior of the class being tested. The user is still responsible for generating test
64580	674	most important of these below. – To allow specifications to be abstractions of implementation details, JML provides model variables, which play the role of abstract values for abstract data types . For example, if instead of a class Purse, we were specifying an interface PurseInterface, we could introduce the balance as such a model variable. A class implementing this interface could then
64580	675	such as (stateful) interface specifications, multiple inheritance of specifications, various forms of quantifiers and set comprehension notation, support for strong and weak behavioral subtyping , and a contextual interpretation of undefinedness . In sum, the JML compiler brings “programming benefits” to formal interface specifications by allowing Java programmers to use JML
64580	675	of object-oriented systems. When exactly should invariants hold? How should concurrency properties be specified? JML’s specification inheritance forces subtypes to be behavioral subtypes , but subtyping in Java is used for implementation inheritance as well; is it practical to always weaken the specifications of supertypes enough so that their subtypes are behavioral subtypes? There
64580	676	for checking specifications, there are also tools that help a developer write JML specifications, with the aim of reducing the cost of producing JML specifications. • The Daikon tool (Section 3.4, ) infers likely invariants by observing the runtime behavior of a program. • The Houdini tool  uses ESC/Java to infer annotations for code. • The jmlspec tool can produce a skeleton of a
64580	676	then verify code against the specification. Writing the JML specification is left to a programmer. Because this task can be time-consuming, tedious, and error-prone, the Daikon invariant detector  provides assistance in creating a specification. Daikon outputs observed program properties in JML form and automatically inserts them into a target program. The Daikon tool dynamically detects
64580	677	the actual behavior of the program is not necessarily the same as its intended behavior.) However, Daikon uses static analysis, statistical tests, and other mechanisms to suppress false positives . Even if a property is not true in general, Daikon’s output provides valuable information about the test suite over which the program was run. Even with modest test suites, Daikon’s output is
64580	680	the jmldoc tool (authoreds6 Lilian Burdy et al. by David Cok) produces browsable HTML pages containing both the API and the specifications for Java code, in the style of pages generated by Javadoc . This tool reuses the parsing and checking performed by the JML checker and connects it to the doclet API underlying Javadoc. In this way, jmldoc remains consistent with the definition of JML and
64580	683	which has been extended to provide a formal semantics for a large part of JML. The verification of the proof obligations is accomplished using a Hoare Logic  and a weakest-precondition calculus  for Java and JML. Interactive theorem proving is very labor-intensive, but allows verification of more complicated properties than can be handled by extended static checking with ESC/Java. A recent
64580	684	that has has been formalized in PVS, and which has been extended to provide a formal semantics for a large part of JML. The verification of the proof obligations is accomplished using a Hoare Logic  and a weakest-precondition calculus  for Java and JML. Interactive theorem proving is very labor-intensive, but allows verification of more complicated properties than can be handled by
64580	687	smartcards written in the Java Card dialect of Java. Keywords. Formal methods, formal specification, Java, runtime assertion checking, static checking, program verification. 1 Introduction JML , which stands for “Java Modeling Language”, is useful for specifying detailed designs of Java classes and interfaces. JML is a behavioral interface specification language for Java; that is, it
64580	688	smartcards written in the Java Card dialect of Java. Keywords. Formal methods, formal specification, Java, runtime assertion checking, static checking, program verification. 1 Introduction JML , which stands for “Java Modeling Language”, is useful for specifying detailed designs of Java classes and interfaces. JML is a behavioral interface specification language for Java; that is, it
64580	688	uses Java’s expression syntax in assertions, thus JML’s notation is easier for programmers to learn than one based on a language-independent specification language like the Larch Shared Language  or OCL . Figure 1 gives an example of a JML specification that illustrates its main features. JML assertions are written as special comments in Java code, either after //@ or between /*@ ...
64580	688	of object-oriented systems. When exactly should invariants hold? How should concurrency properties be specified? JML’s specification inheritance forces subtypes to be behavioral subtypes , but subtyping in Java is used for implementation inheritance as well; is it practical to always weaken the specifications of supertypes enough so that their subtypes are behavioral subtypes? There
64580	689	uses Java’s expression syntax in assertions, thus JML’s notation is easier for programmers to learn than one based on a language-independent specification language like the Larch Shared Language  or OCL . Figure 1 gives an example of a JML specification that illustrates its main features. JML assertions are written as special comments in Java code, either after //@ or between /*@ ...
64580	689	(time and space), the behavior of the original program is unchanged. The transparency of runtime assertion checking is guaranteed, as JML assertions are not allowed to have any side-effects . The JML language provides a rich set of specification facilities to write abstract, complete behavioral specifications of Java program modules . It opens a new possibility in runtime assertion
64580	689	of specifications, various forms of quantifiers and set comprehension notation, support for strong and weak behavioral subtyping , and a contextual interpretation of undefinedness . In sum, the JML compiler brings “programming benefits” to formal interface specifications by allowing Java programmers to use JML specifications as practical and effective tools for debugging,
64580	692	such as (stateful) interface specifications, multiple inheritance of specifications, various forms of quantifiers and set comprehension notation, support for strong and weak behavioral subtyping , and a contextual interpretation of undefinedness . In sum, the JML compiler brings “programming benefits” to formal interface specifications by allowing Java programmers to use JML
64580	694	since they are used in sensitive applications such as bank cards and mobile phone SIMs. (An interesting overview of security properties that are relevant for Java Card applications is available .) JML, and several tools for JML, have been used for Java Card, especially in the context of the EU-supported project VerifiCard (www.verificard.org). JML has been used to write a formal
64580	695	Section 5 discusses some related languages and tools, such as OCL and other runtime assertion checkers, and Section 6 concludes. 2 The JML Notation JML blends Eiffel’s design-by-contract approach  with the Larch  tradition (and others that space precludes mentioning). Because JML supports quantifiers such as \forall and \exists, and because JML allows “model” (i.e., specification-only)
64580	695	fields, and model methods. Thus the JML compiler represents a significant advance over the state of the art in runtime assertion checking as represented by design by contract tools such as Eiffel  or by Java tools such as iContract  or Jass . The jmlc tool also supports advances such as (stateful) interface specifications, multiple inheritance of specifications, various forms of
64580	696	Daikon’s output provides valuable information about the test suite over which the program was run. Even with modest test suites, Daikon’s output is highly accurate. In one set of experiments , over 90% of the properties that it reported were verifiable by ESC/Java (the other properties were true, but were beyond the capabilities of ESC/Java), and it reported over 90% of the properties
64580	697	example, if Daikon generated 100 properties, users had only to delete less than 10 properties and to add another 10 properties in order to have a verifiable set of properties. In another experiment ,s8 Lilian Burdy et al. users who were provided with Daikon output (even from unrealistically bad test suites) performed statistically significantly better on a program verification task than did
64580	698	with LOOP An Overview of JML Tools and Applications 9 The University of Nijmegen’s LOOP tool  translates JML-annotated Java code into proof obligations for the theorem prover PVS . One can then try to prove these obligations, interactively, in PVS. The translation from JML to formal proof obligations builds on a formal semantics for sequential Java that has has been
64580	699	Java programmers to use JML specifications as practical and effective tools for debugging, testing, and design by contract. 3.3 Unit testing A formal specification can be viewed as a test oracle , and a runtime assertion checker can be used as the decision procedure for the test oracle . This idea has been implemented as a unit testing tool for Java (jmlunit), by combining JML with the
64580	700	has shown that JML is expressive enough to specify a non-trivial existing API. The runtime assertion checker has been used to specify and verify a component of a smartcard operating system . ESC/Java has been used with great success to verify a realistic example of an electronic purse implementation in Java Card . This case study was instrumental in convincing industrial users of
8918537	706	large surfaces by people. We mix horizontal lines and vertical lines and even write text slantingly. Most of the previous publications and systems have been assuming only horizontal lines of text  while we have been trying to relinquish any writing constraint from on-line text input. We proposed a method to recognize mixtures of horizontal, vertical and slanted lines of text with assuming
8918537	706	line direction free recognizer already developed . This recognizer employs segmentation likelihood, character recognition likelihood, context (bi-gram) likelihood and character size likelihood . 3.5. Selection of most plausible interpretation The recognition of text lines described above is applied for every CO-revised text lines for each text line element. This step selects the highest
709	711	possessing the data warehouse. Web data sources are available via the Internet. We represent Web data using a special data model called MIX (Metadata based Integration model for data X-change) . MIX is a self-describing data model in the sense Time Dimension Key Day Month Year Shipping Fact Table Book Shop Fact Table Time-key Customer-Key Book-key List-price Sold-price Units-sold
709	713	of the Third Int’l Workshop Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS’01) 1530-1354/01 $10.00 © 2001 IEEEsquery systems for semistructured/unstructured data, such as . RLTs serve as a view of MIX objects in our approach. Besides, a relational database table can be easily represented as a tree with all leaves having the same depth (fixed-depth tree). The online
709	713	changed, i.e., is application independent. Our transformation rule definition file can be written using a language for querying semi-structured or unstructured data. In our prototype, we use UnQL  as the rule definition language. The reasons are: first, that language is designed for querying semi-structured and unstructured data organized as Rooted Labeled Trees (RLTs) or Labeled Graphs
709	713	Discount Fact Table tuple ... tuple ... Time-key BookBookStore- SoldPrice Discount keykey Figure 7: RLT representation of data warehouse tables Our mapping rule definitions are written in UnQL . UnQL consists of tree constructors, function definitions and query definitions: ¯ tree constructors ??, ?l :t?, Ø ? Ø , ?Ð ? Ø ? ???? ÐÒ ? ØÒ? ¯ functions defined by structural recursion: let sfun
709	713	recursive traverse and ensures that the recursion always terminates. Therefore, the above mapping definitions can be written in the following form using structural recursion following UnQL’s syntax . We will use mapping definitions in the structural recursion form in later sections, because this form can help to understand the semantics of transformation rules, which will be discussed in
709	713	traversal on an object tree and the restructuring of its subtrees. In order to make explicit the exact meaning, i.e., the specified processing of the mapping rules, we use a calculus called UnCAL , which is UnQL’s internal algebra in the sense of lambda calculus (a formalism with variables and functions) rather than in the sense of relational calculus (a logic with variables and
709	716	the comparison of source- and target-schema and tree restructuring. First, Web data is integrated based on a common structural and semantic basis by using a selfdescribing object model, called MIX . We refer to this step as the Web data representation and integration phase, and the MIX model serves as a Web data representation model. Next is the transformation phase. In this step our source
709	720	of the Third Int’l Workshop Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS’01) 1530-1354/01 $10.00 © 2001 IEEEsquery systems for semistructured/unstructured data, such as . RLTs serve as a view of MIX objects in our approach. Besides, a relational database table can be easily represented as a tree with all leaves having the same depth (fixed-depth tree). The online
709	720	are explicitly described as mapping rules, based on which the transformation can be accomplished automatically via tree restructuring. Our approach is related to the work of Milo, Beeri and Zohar . They define common schema and data models for the source and target data. Using a rule-based method, they match components in the source schema with components in the target schema. The matching
709	720	information is already provided as a part of MIX objects, we can use RLTs not only for the schema comparing process but the data mapping task as well. Second, the TranScm system introduced in  uses rules to match schemata. Each rule uses match and descendents functions to handle schema matching, and uses translation functions to handle data translation. In contrast to their descendents
8918544	724	specially designed to increase the performance of the nearest neighbor classification rule. We have not made assumptions on the data distribution, and we don’t force our projection to be orthogonal . The only assumption we impose is that our embedding must be based on a set of simple 1D projections, which can complement each other to achieve better classification results. We have made use of
8918546	733	(35) &quot;Proper Government (PG) cannot apply over governing domains&quot; is falsified: ?BRaD &quot;beard&quot; ? ? ? '( ? ? ? > ? > ? ? ????? ? ? ? ????????????? PGs(23) alternative: CVCV syllable structure (Lowenstamm 1996) closed syllable geminate long vowel O N O N O N O N O N O N ? ? ? ? \ / ? ? \ / C V C ø C V C V (27) PG ????????? ? ? (28) ? ? ? ? ? ? ? ?????????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
740	741	of each text. Basically, there are two kinds of approaches. In the AI-based approach, natural language processing techniques (Sowa, 1984; Srihari and Burhans, 1994) or machine learning techniques (Barletta and Mark, 1988; Baudin et al, 1994) can be used to build or refine an internal representation of each text. Although most of these methods can produce deep conceptual indices, they can only work in restricted
740	742	there are two kinds of approaches. In the AI-based approach, natural language processing techniques (Sowa, 1984; Srihari and Burhans, 1994) or machine learning techniques (Barletta and Mark, 1988; Baudin et al, 1994) can be used to build or refine an internal representation of each text. Although most of these methods can produce deep conceptual indices, they can only work in restricted environments and
740	743	of structure and then let the user navigate through it. In fact, the organizing/navigating paradigm is becoming very popular (Thompson and Croft, 1989; Maarek et al, 1991; Lucarella et al, 1993; Bowman et al, 1994). One main problem of this approach is creating the atomic pieces of information and linking them, which usually requires subjective and time-consuming decisions. We argue that conceptual
740	751	words occurring in a text collection, ignoring punctuation and case. 2. Word stemming We reduce each word to word-stem form. This is done by using a very large morphological lexicon for English (Karp et al, 1992) that contains the standard inflections for nouns (singular, plural, singular genitive, plural genitive), verbs (infinitive, third person singular, past tense, past participle, progressive form),
740	757	of a text database is to identify the content of each text. Basically, there are two kinds of approaches. In the AI-based approach, natural language processing techniques (Sowa, 1984; Srihari and Burhans, 1994) or machine learning techniques (Barletta and Mark, 1988; Baudin et al, 1994) can be used to build or refine an internal representation of each text. Although most of these methods can produce deep
759	760	so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique is dynamic voltagefrequency scaling. Since the power used by a CMOS circuit is proportional to the product f · v 2 , where f
759	761	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	763	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	764	next event requiring the processor will occur is unknown. In some cases, the end of the shutdown time can be predicted, so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique
759	764	by I/O events, such as portable wireless terminals. Furthermore, they don’t implement their 2 techniques on a real system; their results are based on analysis and modeling. C.-H. Hwang’s work  focuses solely on mechanisms for predicting the length of idle periods on event driven applications (X Window System server, Netscape, telnet, and tin). J.R. Lorch and A.J. Smith’s work  is
759	766	so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique is dynamic voltagefrequency scaling. Since the power used by a CMOS circuit is proportional to the product f · v 2 , where f
759	767	work  focuses solely on mechanisms for predicting the length of idle periods on event driven applications (X Window System server, Netscape, telnet, and tin). J.R. Lorch and A.J. Smith’s work  is based on processors that can be put to sleep by turning off the clock signal, which incurs very little latency, and which preserve most of their state when they are sleeping. 1 They also focus
759	768	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	769	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	770	so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique is dynamic voltagefrequency scaling. Since the power used by a CMOS circuit is proportional to the product f · v 2 , where f
759	773	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
781	783	2 is therefore interesting to develop specific resolution techniques for these particular fragments. All previous algorithms for solving conjunctive and disjunctive classes, including those from , take at least quadratic time in the size of a boolean equation system in the worst case. For large boolean equations which are typically encountered in model checking and preorder/equivalence
781	783	Definition 3.2. Let G = (V, E, ?) be a dependency graph and k ? V . We define the graph G?k = (V, E?k, ?) by taking • E?k = {?i, j? ? E | i ? k and j ? k}. The following essential lemma comes from . Lemma 3.3. Let GE = (V, E, ?) be the dependency graph of a disjunctive Boolean equation system E. Let xi be any variable in E and let valuation v be the solution of E. Then the following are
781	783	other. Using for instance Lemma 3.22 of  a Boolean equation system can be reordered such that our notion of alternation depth and the notion of  coincide. A note about the open question in . In  the following open question was stated. Given a directed graph of which the vertices are ordered and labelled with either red or green. Is there a sub quadratic algorithm to determine
781	787	O(e 2 ) time solutions, our algorithm computes the solution of such a fixpoint equation system with size e and alternation depth d in O(e log d) time. 1 Introduction A Boolean Equation System (BES)  is a sequence of boolean equations with minimal and maximal fixpoints. It gives a useful framework for the verification of finite state concurrent systems. This is due to the fact that many
781	789	and an empirical comparison between our approach and other related algorithms are left for future work. Our algorithm combines essentially graph theoretic techniques for finding strong components  and hierarchical clustering . King, Kupferman and Vardi  recently found an algorithm in the realm of parity word automata. Their algorithm is very similar to ours and also resorts to the
781	789	and (b) GE contains a cycle of which the lowest index of a vertex on this cycle is j. Finding cycles in graphs can be done in linear time using any algorithm to detect strongly connected components . A strongly connected component (SCC) in a graph G = (V, E, ?) is a set of vertices W ? V such that for each pair of vertices k, l ? W it is possible to reach l from k by following directed edges
1033965	792	of this article is to highlight the research accomplishments of the NetSolve system; additional low-level details including complete usage instructions can be found in the NetSolve User's Guide . 10 THE NetSolve SYSTEM This section offers a high-level overview of the NetSolve system discussing, in turn, architectural, implementation and managerial issues. The architecture An instance of a
1033965	795	(GRID) Figure 4. Proxy architecture. behalf of the client means that different proxies can negotiate for different types of services. So far, we have implemented proxies to negotiate for Globus  services and, of course, the standard NetSolve services. Future work may include integrating other systems like Condor  or Legion  in a similar fashion; however, a primary motivation of the
1033965	799	to optimize the use of Grid resources, especially when bandwidth is of the essence, data sets are very large, or both. Distributed storage infrastructures A distributed storage infrastructure (DSI)  is a technology that allows a program to manage data stored remotely. The Internet backplane protocol (IBP)  is an example DSI that has been 25 incorporated into NetSolve in an effort to
1033965	800	to leverage DSI storage without modifying the standard NetSolve functions for computational requests. A complete taxonomy on the state of the art in DSIs and Data Grid technologies can be found in , which also gives full details regarding NetSolve's DSI integration efforts. Task farming 10 This interface addresses applications that have simple task-parallel structures but require a large
1033965	801	it is to be expected that the availability and load of resources within the server pool will change dynamically. The design and validation of the NetSolve task farming interface is presented in . This article also presents a preliminary adaptive scheduling algorithm for task farming. The initial task farming work in NetSolve has lead to a larger development and integration effort as part
1033965	802	also presents a preliminary adaptive scheduling algorithm for task farming. The initial task farming work in NetSolve has lead to a larger development and integration effort as part of the APST  project 25 (see the section on data persistence). Transparent algorithm selection Through NetSolve, users are given access to complex algorithms that solve a variety of types of problems, one
1033965	802	Several scheduling heuristics were studied in  and experimental results using NetSolve on a wide-area testbed for running large MCell simulations with those heuristics can be 15 found in . Nuclear engineering The goal of this project is to develop a prototype environment for the Collaborative Environment for Nuclear Technology Software (CENTS). CENTS aims to lay the foundation for a
1033965	809	PDF facility, NetSolve has integrated a variety of scientific packages. These PDFs 10 are distributed with the current implementation of NetSolve: BLAS , LAPACK , ScaLAPACK , ItPack , PETSc , AZTEC , MA28 , SuperLU  and ARPACK . When efficient, we include the actual numerical library with NetSolve; in the other cases, these packages are available for a large
1033965	811	scientific packages. These PDFs 10 are distributed with the current implementation of NetSolve: BLAS , LAPACK , ScaLAPACK , ItPack , PETSc , AZTEC , MA28 , SuperLU  and ARPACK . When efficient, we include the actual numerical library with NetSolve; in the other cases, these packages are available for a large number of platforms and are freely distributed.
1033965	814	report information regarding their CPU load so that the agent can determine which servers represent the best choice to service a request. The CPU load manager performs this task; however, the NWS  is a Grid service utility with a component, the CPU sensor, that does this as well. The following section discusses our integration of NWS forecasters 10 and motivates our use of the NWS CPU
1033965	814	is reported. 30 NWS forecasters When allocating resources, the agent's main goals is to choose the best-suited computational server for each incoming request We have employed the use of the NWS  to help us gather the information necessary to make these decisions. The NetSolve agent calls upon the services of NWS forecasters to predict future availability of server hosts based upon
1033965	816	San Diego. The use of NetSolve isolates the scheduler from the resource-management details and allows researchers to focus on the scheduler design. Several scheduling heuristics were studied in  and experimental results using NetSolve on a wide-area testbed for running large MCell simulations with those heuristics can be 15 found in . Nuclear engineering The goal of this project is to
855	861	tufts). It is natural to map the display parameters of the glyphs to the information associated with the data. Parameters that have been demonstrated include color, thickness, opacity, and scale . Multi-sensory modalities such as haptics and sound  have been utilized when the visual approaches have been exhausted—typically the case for multi-dimensional data. Modifications to geometry
855	861	interpolated surface locations. Animated surfaces  can show the range of locations over which a surface might lie as well as the range for other data types. For a more complete review see . 3 Representing Sea Bottom Information Our research investigated representations of depth measurements containing multiple dimensions of uncertainty andsFigure 1: Images show from left to right a
8918568	868	originally stored in the permanent storage, an effect that is certainly not to be desired. While the current R&D efforts have been focusing on how to aggregate (e.g., ) and make use of (e.g., ) distributed computing resources, few addressed the above issues. In this paper, we report on the InstantGrid framework, which is the result of our effort in creating a tool to facilitate the
8918568	869	stored in the permanent storage, an effect that is certainly not to be desired. While the current R&D efforts have been focusing on how to aggregate (e.g., ) and make use of (e.g., ) distributed computing resources, few addressed the above issues. In this paper, we report on the InstantGrid framework, which is the result of our effort in creating a tool to facilitate the
871	872	for trying out, tuning, and analysing different safety policies and logics for different target platforms. Our approach is different from other work in the formal foundation of PCC by Appel et al.   or Hamid et al.  in that it works with an explicit, executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical
871	873	trying out, tuning, and analysing different safety policies and logics for different target platforms. Our approach is different from other work in the formal foundation of PCC by Appel et al.   or Hamid et al.  in that it works with an explicit, executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical
871	875	6 the simplifier and a decision procedure for presburger arithmetic suffice to prove the verification condition. For the client side Isabelle provides (compressed) proof terms and a proof checker . Proofs are encoded as ? terms having a type that corresponds to the theorem they prove (Curry Howard Isomorphism). Proof Checking becomes a type checking problem, which can be handled by a small
871	876	isafeF OD (0,2). 4.4 Code Producer and Consumer The code producer can write annotated programs in Isabelle. To obtain the verification condition one can generate and execute ML code for the VCG  or use the simplifiers12 to evaluate vcg ?. Proving the verification condition is supported by powerful proof tools and a rich collection of HOL theorems. For the example in Fig. 6 the simplifier
871	877	of the PCC system. Proof checkers are relatively small standard components of many logical frameworks. The VCG on the other hand is large (several thousand lines of C code in current PCC systems  ) and complex (it handles annotations, produces complex formulae, and contains parts of the safety policy). Our framework contains a VCG with a formal proof of safety, mechanically checked in
871	877	machine language, safety policy, or safety logic. Additionally to the correctness of VCG and proof checker, we need the safety logic to be sound. As a recent bug  in the SpecialJ system  shows, this is not trivial. It is not even immediately clear, what exactly a safety logic must satisfy to be sound. Our framework makes the underlying assumptions on machine, policy, and logic
871	878	and analysing different safety policies and logics for different target platforms. Our approach is different from other work in the formal foundation of PCC by Appel et al.   or Hamid et al.  in that it works with an explicit, executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical foundations of PCC as the
871	879	our approach is related to other techniques that impose safety policies on machine code statically: Typed Assembly Language , Mobile Ressource Guarantees  or Java Bytecode Verification . There are four levels in our PCC systems. The first level, the PCC framework (§2), provides generic features and minimal assumptions. The second level is the platform (§3). Platform designers can
871	880	is not restricted to any particular machine language, safety policy, or safety logic. Additionally to the correctness of VCG and proof checker, we need the safety logic to be sound. As a recent bug  in the SpecialJ system  shows, this is not trivial. It is not even immediately clear, what exactly a safety logic must satisfy to be sound. Our framework makes the underlying assumptions on
871	881	extensions can then be applied to that sound core. On a broader scale, our approach is related to other techniques that impose safety policies on machine code statically: Typed Assembly Language , Mobile Ressource Guarantees  or Java Bytecode Verification . There are four levels in our PCC systems. The first level, the PCC framework (§2), provides generic features and minimal
871	882	systems by instantiating it to a simple assembly language with procedures and a safety policy for arithmetic overflow. 1 Introduction Proof Carrying Code (PCC), first proposed by Necula and Lee  , is a scheme for executing untrusted code safely. Fig. 1 shows the architecture of a PCC system. The code producer is on the left, the code receiver on the right. Both use a verification
871	883	by instantiating it to a simple assembly language with procedures and a safety policy for arithmetic overflow. 1 Introduction Proof Carrying Code (PCC), first proposed by Necula and Lee  , is a scheme for executing untrusted code safely. Fig. 1 shows the architecture of a PCC system. The code producer is on the left, the code receiver on the right. Both use a verification condition
871	884	the PCC system. Proof checkers are relatively small standard components of many logical frameworks. The VCG on the other hand is large (several thousand lines of C code in current PCC systems  ) and complex (it handles annotations, produces complex formulae, and contains parts of the safety policy). Our framework contains a VCG with a formal proof of safety, mechanically checked in the
871	885	executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical foundations of PCC as the one started by Necula and Schneck  and on encouraging the analysis of safety properties other than the much researched type and memory safety. Necula and Schneck  also present a framework for VCGs. They work with a small,
871	888	This enables us to optimise verification conditions after/during their construction. By now, we also have instantiated the PCC framework to a (downsized) version of the Java Virtual Machine . For this we did not have to change the framework, thus we believe that our framework’s formalisation and its requirements are reasonable, even for real life platforms.sPrototyping Proof Carrying
871	889	or recursive procedures and safety policies about time and memory consumption of programs. Moreover we have instantiated a safety logic based on first order arithmetic in form of a deep embedding . There, formulae are modelled as HOL datatype and can by analysed by other HOL functions. This enables us to optimise verification conditions after/during their construction. By now, we also have
8918569	891	McLain Brigham Young University, Provo, Utah 84602 {weiren,beard}@ee.byu.edu, mclain@byu.edu Much of the research focus in the cooperative control community has been on formation control problems . This focus may be due to the fact that the group control problem can be reduced to well-established single-agent control problems by employing a leader-follower type control strategy. For example,
8918569	891	and stij = qtij otherwise. Also note that detSt() = detQt(), j = 1, · · · , N. Then we know that detSt = = N? (?1) 1+j st1jdetSt() j=1 N? (?1) 1+j qt1jdetSt() j=1 + k1mdetSt() ? (?1) 1+m k1mdetSt() =detQt + k1m(detSt() + (?1) m detSt()). Consider a matrix E = , (i, j) = 1, · · · , N ? 1, given by adding  T to the (m ? 1)th
8918569	891	s3N ? . . . . ? . ? . sN2 sN3 · · · sNm + sN1 · · · sNN Thus e i(m?1) = s (i+1)m + s (i+1)1, i = 1, · · · , N ? 1. Using the properties of determinants, it can be verified that det(tI ? E) = detSt() + (?1) m detSt(). Obviously matrix E has zero row sum and nonpositive diagonal elements. Also matrix E is diagonally dominant. From the Gersgorin disc theorem, we know that E has at least
8918569	893	McLain Brigham Young University, Provo, Utah 84602 {weiren,beard}@ee.byu.edu, mclain@byu.edu Much of the research focus in the cooperative control community has been on formation control problems . This focus may be due to the fact that the group control problem can be reduced to well-established single-agent control problems by employing a leader-follower type control strategy. For example,
8918569	893	, the states of the leader constitute the coordination variable since the actions of the other vehicles in the formation are completely specified once the leader states are known. In , the notion of a virtual structure is used to derive formation control strategies. The motion of each vehicle is causally dependent on the dynamic states of the virtual structure, therefore the
8918569	894	the computation and communication loads. Coordination variables and functions have been applied successfully to UAV cooperative timing missions  and UAV cooperative reconnaissance problems . An illustrative example is given in the next section.s1.1 Example: Cooperative Timing Coordination Variables and Consensus 5 The application of coordination variables and functions can be
8918569	896	information variable is “pushed” toward Aj’s information variable with strength kij. In the case of ?i ? R, Eq. (5) can be written in matrix form assCoordination Variables and Consensus 7 ?? = C?, (6) where ? =  T , C = , (i, j) = 1, · · · , N, with cii = ?( ? j?=i kijGij), i = 1, · · · , N, and cij = kijGij, j ?= i. We say that C is the matrix associated with graph G. Note
8918569	896	is the dynamic state of a virtual leader. Static Consensus It has been shown in  that the group of vehicles A reach consensus asymptotically using the update scheme (5) if matrix C in Eq. (6) has exactly one zero eigenvalue and all the others are in the open left half plane. The following result computes the value of the information variable that is reached through the consensus
8918569	896	ass8 W. Ren, R. W. Beard, T. W. McLain M ? 0, if all its entries are nonnegative. A nonnegative matrix is said to be a stochastic matrix if all its row sums are 1. Lemma 1. If C is given by Eq. (6), then eCt , ?t > 0, is a stochastic matrix with positive diagonal entries. Furthermore, if C has exactly one zero eigenvalue, then eCt ? b?T and ?i(t) ? ?N i=1 (?i?i(0)) as t ? ?, where b = [1, · ·
8918569	896	eigenvalue 1. Thus it can be seen that ? = ?x for some ? ?= 0. Since ? N i=1 ?i = 1, it must be true that ? > 0, which implies that ? ? 0.sCoordination Variables and Consensus 9 The solution to Eq. (6) is given by ?(t) = eCt?(0). Therefore, it is obvious that ?i(t) ? ?N i=1 (?i?i(0)), i = 1, · · · , N, as t ? ?. Note that if we replace matrix C with ?C in Eq. (6), where ? > 0, we can increase
8918569	896	for the group of agents A. Let G2 be the communication graph by adding one more directed link from any node m to node ? to graph G1, where m ?= l. Also let Q and S be the matrices in the update law (6) associated with graphs G1 and G2 respectively. Denote pQ(t) = det(tI ? Q) and pS(t) = det(tI ? S) as the characteristic polynomial of Q and S respectively. Let Qt = tI ? Q and St = tI ? S. Given
8918569	897	by the same time-varying input u(t), which might represent an a priori known feedforward signal. The associated consensus scheme is given by ??i = ? N? kijGji(?i ? ?j) + u(t), i = 1, ?? · · , N. (7) j=1 Eq. (7) can also be written in matrix form as ?? = C? + Bu(t), (8) where C is the matrix associated with graph G and B =  T . We have the following theorem regarding consensus of
8918569	901	For example, in , cooperative task allocation is addressed. Individual vehicle behavior is dependent on the task allocation vector which becomes the coordination variable. Similarly, in , the coordination variable is the dynamic role assignment in a robot soccer scenario. Information necessary for cooperation may be shared in a variety of ways. For example, relative position
8918569	902	of ways. For example, relative position sensors may enable vehicles to construct state information for other vehicles , or knowledge may be communicated between vehicles using a wireless network , or joint knowledge might be pre-programmed into the vehicles before a mission begins . In Section 1 we offer some definitions and general principles regarding coordination variables. For
8918569	902	seeking consensus among a team of vehicles. Section 2 states some results on multi-agent consensus seeking for fixed communication topologies. The consensus problem has recently been addressed in . The work reported in  is particularly relevant to the results reported in thissCoordination Variables and Consensus 3 paper. Ref  addresses the knowledge consensus problem when teams of
8918569	910	two-level decomposition process significantly reduces the computation and communication loads. Coordination variables and functions have been applied successfully to UAV cooperative timing missions  and UAV cooperative reconnaissance problems . An illustrative example is given in the next section.s1.1 Example: Cooperative Timing Coordination Variables and Consensus 5 The application of
8918569	911	for constant coordination variables, which may not be suitable for applications where the coordination variable evolves dynamically. For example, in the context of leader-following approaches (c.f. ), the group leader’s trajectory can act as the coordination variable for the whole group. Dynamic Consensus Suppose that the information variable on each vehicle is driven by the same time-varying
8918569	914	seeking consensus among a team of vehicles. Section 2 states some results on multi-agent consensus seeking for fixed communication topologies. The consensus problem has recently been addressed in . The work reported in  is particularly relevant to the results reported in thissCoordination Variables and Consensus 3 paper. Ref  addresses the knowledge consensus problem when teams of
945	947	THE CASE OF CHINA Shenggen Fan 1 , Cheng Fang, 2 and Xiaobo Zhang 3 1. INTRODUCTION Many studies have shown that investments in agricultural research can yield favorable economic returns (Alston et al. 2000), and contribute to significant reductions in rural poverty (Kerr and Kolavalli 1999; Fan, Hazell, and Thorat 2000; Fan, Zhang, and Zhang 2000; Hazell and Haddad 2001). The links between
945	975	Institutional changes and policy reforms have made important contributions to growth in agricultural and nonagricultural production and poverty reduction in rural China (Fan 1991, and Fan and Pardey 1997). We do not need to estimate these contributions for the purposes of this study, but in order to reduce possible estimation biases that may arise from neglecting them, we added year and province
945	1023	by state governments and were allowed to vary by only a few percentage points across provinces. We therefore assume that price levels were the same for all provinces in 1984. Kanbur and Zhang (1999) and Yang and Cai (2000) have adopted similar methods to calculate real expenditure levels across regions.s12 years for a person who is illiterate and semi-illiterate, 5 years for a person with
945	1023	d. In this report, we use PDLs of degree 2. In this case, we only need to estimate three instead of i+1 parameters for the lag distribution. For more detailed information on this 6 Alston et al. (1999) argue that research lag may be much longer than previously thought, perhaps even infinite. But this argument may be less relevant for most developing countries since their national agricultural
8918572	1023	Concepts By providing a shared and common understanding of a domain that can be communicated between people and applications systems, ontologies facilitate the sharing and reuse of knowledge . An ontology suitable for the integration into the EMMO model has to distinguish between object concepts and relational concepts. Object concepts are used for labeling the nodes, relational
8918572	1028	unique approach to multimedia meta modeling. None of the standards for multimedia document models, such as SMIL  or SVG , and none of the standards for semantic media description, such as RDF  or Topic Maps , addresses all these aspects. Thus, none of the query languages for those standards can fulfil all requirements with respect to the expressiveness of a query language for EMMOs.
8918572	1029	content is to enhance multimedia resources by semantic metamodels and to integrate domain ontologies. To answer these challenges, we have developed Enhanced Multimedia Meta Objects (EMMOs) , a novel approach for semantic multimedia content modeling. EMMOs were created within the EU-project CULTOS to model InterTextualThreads (ITTs), i.e. complex knowledge structures used by the
1059	1062	motion for sustainable operation, 5) logistics capability with energy transport via the cable systems, and 6) node elevation for enhancing wireless link budget relative to surface-distributed nodes . Finally, the need for high fidelity monitoring of large environments will require the simultaneous operation of multiple NIMS nodes that exploit the infrastructure for communication and
1059	1063	the need for high fidelity monitoring of large environments will require the simultaneous operation of multiple NIMS nodes that exploit the infrastructure for communication and coordination . NIMS applications to environmental monitoring range from fundamental science objectives to public health and safety. Examples include natural environment monitoring with requirements for
1059	1069	mapping algorithms have relied primarily on probabilistic as opposed to statistical approaches. These approaches range from the Bayesian filter  to Kalman filter , hidden Markov model , to Bayesian network. These methods apply to mapping unknown environments with known robotic localization or localization with known environments. The Simultaneous Localization and Mapping (SLAM)
695740	1084	the user to explicitly layout the visual components. In this paper, we focus on the added capabilities to the original version of the scripting language AnimalScript built into the Animal system (Rößling and Freisleben, 2001). To avoid confusion, we will always refer to the new implementation as AnimalScript V2, and use AnimalScript for the original implementation. We first review the main features of interest in the
695740	1084	works on String, boolean and integer variables as well as literal Strings. 4 Conclusions and Further Work AnimalScript V2 extends the functionality of the scripting language AnimalScript (Rößling and Freisleben, 2001) by offering important base operations for simplifying animation creation: loops, conditionals, variables and expressions. The additions considerably increase the expressive power of AnimalScript,
8918581	1101	a static generic model to speaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time
8918581	1102	analysis of motion capture data. Figure 3: Shape and appearance changes associated with extreme variations along the first lip component (rounding/spreading) for two speakers. Shape-free textures  have been obtained from image data with colored beads. (a) (b) (c) Figure 4: Rendering and recovering 3D articulation. Figure 5: Subdivision of n elementary volume of the original space and new
8918581	1102	configurations used for estimating the shape model (by warping all images to the neutral configuration). Instead of combining a posteriori separate shape and appearance models as in Cootes et al , we estimate a simple multilinear model that relates RGB colors of each pixel of the shape-free images to shape parameters. Figure 3 illustrates the change of shape-free appearance accompanying the
8918581	1104	variability of articulation, there is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision  or MPEG-4/SNHC coding scheme  require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape
8918581	1106	a static generic model to speaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time
8918581	1107	raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time and with a extreme precision
8918581	1110	of the face with no implicit reference to any articulatory model (e.g. FAP3 open_jaw “does not affect mouth opening” ). FAP ease however specifying constrictions sizes and positions  supposed to be less speaker-dependent than articulatory parameters. On the contrary articulatory models are often used to specify how constrictions sizes and positions are reached by
8918581	1112	in all allophonic variations of  for carrying the tongue front and upwards . (a) (b) (c) Figure 1: (a) Gathering fleshpoint positions; (b) The generic facial mesh developed by Pighin et al. ; (c) The transformed mesh.s(a) jaw down/up (b) jaw back/front (c) lips spread/round (d) upper lip up (e) lip corners up Figure 2: Elementary speech movements extracted from statistical analysis of
8918581	1113	variability of articulation, there is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision  or MPEG-4/SNHC coding scheme  require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape
8918581	1113	raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time and with a extreme precision
8918581	1113	SPEAKERSPECIFIC DATA The deformation of a high definition 3D surface towards a set of low definition 3D data is achieved by an original 3D-to-3D matching algorithm.. The generic 3D mesh used here  has 5826 vertices connected by 11370 triangles (see Figure 1.b). The 3D articulatory model of the female speaker used here drives 304 fleshpoints : 245 beads for the face as in Figure 1.a, 30
8918581	1116	subtle and distributed all over the face, but should not be neglected since interlocutors should be quite sensitive to laws governing biological motion (e.g. the experiments of Runeson et al  with body movements when carrying imaginary versus real loads). Although its crude linear assumptions do not take into account, for now, saturation due to tissue compression, this multilinear
8918581	1117	the second deals with point-to-point distance: a set of 3D fleshpoints {qj} are identified and paired with {rj} vertices of S. The minimization is performed using the Levenberg-Marquardt algorithm . 3.2 Matching a neutral configuration The algorithm described above is applied to the articulatory configuration that provides the same neutral articulation as the static generic model. A minimal
1164	1168	and r-classes and are described at two levels: the object correspondence (OC) level and the value correspondence (VC) level. These mirror the general notions of existence and value dependencies . Finally, the restoration rules specify which actions to be taken if inconsistency occurs. They specify not only the actions needed for restoring consistency, but specify also the conditions that
1164	1164	management. The MRMS maintains consistency based on a specification, a multiple representation schema (MRSchema), formulated in a so-called multiple representation schema language (MRSL) . The MRSchema specifies matching, consistency, and restoration rules. The MRSL is based on the assumption that objects representing the same entity exhibit semantic similarities that enable us to
1164	1164	former is based on the Unified Modeling Language (UML) , and we use the Object Constraint Language (OCL)  to specify constraints. A more thorough description of the MRSL is given elsewhere . An MRSL specification, termed an MRSchema, is used to configure the MRMS. The process is depicted in Figure 1, which shows MRSchema specified in MRSL Translation ObjectRelational MRSchema
1164	1177	multiple representations with matching, consistency, and restoration rules. Finally, it should be noted that multiple representation databases relate in a more general sense to federated databases . A federated schema usually provides an integrated view of the underlying autonomous databases. In contrast, the main purpose of an MRMS is to maintain consistency. At a more general level, related
8918617	1187	mediator do not deal with decentralized systems with participants andsWordNet Exploitation through a Distributed Network of Servers 267 information sources location upredictability. As mentioned in  a P2P approach would be a more appropriate solution, since it lacks a centralized structure and promotes the equality among peers and their collaboration only for the time necessary to fulfill a
8918617	1188	where no a priori knowledge of the location of specific data is possible, the traditional mediator and federated databases approaches are not appropriate. Furthermore, approaches such as  that provide a source- and query-independent mediator do not deal with decentralized systems with participants andsWordNet Exploitation through a Distributed Network of Servers 267 information
8918617	1189	through the WMS. From its definition, WMS falls into the Data Integration framework, being able to manage a distributed, dynamic network of homogeneous data. Previous systems built for this purpose  are often characterized by a centralized system that controls and manages interactions among distributed information sources in order to serve requests for data. As a consequence, in a distributed
8918617	1190	where no a priori knowledge of the location of specific data is possible, the traditional mediator and federated databases approaches are not appropriate. Furthermore, approaches such as  that provide a source- and query-independent mediator do not deal with decentralized systems with participants andsWordNet Exploitation through a Distributed Network of Servers 267 information
8918617	1177	through the WMS. From its definition, WMS falls into the Data Integration framework, being able to manage a distributed, dynamic network of homogeneous data. Previous systems built for this purpose  are often characterized by a centralized system that controls and manages interactions among distributed information sources in order to serve requests for data. As a consequence, in a distributed
1193	1194	extracted from document images. Experimental results confirmed the validity of the proposed approach. 1. Introduction The problem of compressed pattern matching was introduced by Amir and Benson to perform pattern matching directly in a compressed text without any decompressing. For a given text Ì and pattern È , the usual approach to finding the occurrences of È in compressed Ì is to
1193	1197	pattern matching was generally faster than the method of decompressing followed by an ordinary pattern matching. For document image processing, the research of duplicate document detection and OCR on compressed images has been Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE reported recently.
1193	1199	why these documents are stored in the image format. Image based approach becomes an alternative way to directly search keywords in the document images without OCRing the entire document images. Several methods regarding this issue have been reported in the past years. Furthermore, in order to save storage space and speed up the transmission in the Internet, many document images
1193	1204	process is needed. Otherwise, È and É are further matched by the method based on the weighted Hausdorff distance. 4.2. Word Image Matching Based on Weighted Hausdorff Distance The Hausdorff distance measures the degree of mismatch between two point sets, which has been widely applied in two-dimensional image matching, especially in the area of object matching. Dubussion and Jain  presented
1193	1205	distance measures the degree of mismatch between two point sets, which has been widely applied in two-dimensional image matching, especially in the area of object matching. Dubussion and Jain  presented the modified Hausdorff distance(MHD) measure by employing the summation operator over all distance, rather than the maximum operator in the traditional Hausdorff distance. The MHD was
1206	1208	etc., all but the most motivated users will easily give up and save this task for another day. Browsing is an alternative to active search that is used by many online video retrieval interfaces . However, these methods are still limited by what can be displayed on a PC monitor. On the other hand, it’s well known that people can read three times faster than they can listen which suggests
1206	1211	listen which suggests that a paper-based interface could provide a more efficient search mechanism than an online media viewer. Video Paper is a paper-based interface for accessing digital video . The closed caption transcript that’s often provided with a television program is formatted and printed together with key frames selected from the video. Bar codes are also included so that with a
1206	1216	etc., all but the most motivated users will easily give up and save this task for another day. Browsing is an alternative to active search that is used by many online video retrieval interfaces . However, these methods are still limited by what can be displayed on a PC monitor. On the other hand, it’s well known that people can read three times faster than they can listen which suggests
1206	1217	browsing and retrieval was described. In contrast to the prior art, which relies on manual effort by the user to create a multimedia document which links a paper representation to digital data (), Video Paper is a fully automatic solution that creates a paper document which allows users to quickly access information in a long recording by reading the transcript or browsing key frames.
1206	1218	closed caption transcript to suggest key frames that could be useful to someone who browses a Video Paper document. The face detection algorithm first finds skin pixels in the normalized RG-space . Small holes in skincolored regions are removed by a morphological closing Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03
8918623	1228	result was 45% on 100 test documents.  had their best result on accuracy at 51% using Yahoo hierarchy and Yahoo web documents. The non-hierarchical web documents automatic categorization by  that used probabilistic description-oriented had worst result on their preliminary experiment with only an average precision of 2.13%. Later after they had increased the size of their sample
8918623	1229	because if we already choose the wrong node at the top, our topic node will be wrong too. This problem is faced by most of the automatic text classification using hierarchical structure , , . In this experiment we only use a small size of web ontology. This is because we are only interested in how our topic identification system performs on real data regardless of size. However, for
8918623	1232	in many ways. We try to take advantage of these semantics relationships to establish links between the words of Yahoo concepts and WordNet vocabulary. Based on our review on past related works , ,  we decide to use three types of semantics relationships found in the WordNet and they are synonym 1 , hypernym/hyponym 2 and meronym/holonym 3 . Using these three semantics relationships, we
8918623	1233	5 followed by our conclusion in section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to the
8918623	1233	the new document will be identified by computing the similarity between the document feature and each of the node categories feature or the probability that the document belongs to a node category . Other methods are like  that emphasise on representing a document with hypernym density using the WordNet hypernym hierarchy and  which combines clustering method with text categorization.
8918623	1233	mistakes because if we already choose the wrong node at the top, our topic node will be wrong too. This problem is faced by most of the automatic text classification using hierarchical structure , , . In this experiment we only use a small size of web ontology. This is because we are only interested in how our topic identification system performs on real data regardless of size.
8918623	1235	HTML tags and therefore our extraction technique is not appropriate to be used. In this case, alternative way of extracting web document like words frequency ,  , and positional policy  are more applicable. Since in this paper we are interested in extracting out information from the web document based on HTML tag, we consider extracting information from non-structured document as
8918623	1236	the WordNet hypernym hierarchy and  which combines clustering method with text categorization. Our approach of identifying the topic of a document has been inspired by the works of  and .  extends the word frequency counting (the classic way of identifying text topic) to concept counting. He uses the WordNet taxonomy to collect interesting concepts and later generalizes the
8918623	1237	section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to the node categories of the hierarchy. By
8918623	1237	some web documents maybe lack of HTML tags and therefore our extraction technique is not appropriate to be used. In this case, alternative way of extracting web document like words frequency ,  , and positional policy  are more applicable. Since in this paper we are interested in extracting out information from the web document based on HTML tag, we consider extracting
8918623	1237	concepts. Analysis. Our result is quite comparable to that result produced by the topic identification (text classification) that used machine learning techniques on web documents classification.  obtained 37% of accuracy result using Yahoo with 151 classes and 50 documents. Their best attained accuracy result was 45% on 100 test documents.  had their best result on accuracy at 51% using
8918623	1240	section 5 followed by our conclusion in section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to
8918623	1240	by computing the similarity between the document feature and each of the node categories feature or the probability that the document belongs to a node category . Other methods are like  that emphasise on representing a document with hypernym density using the WordNet hypernym hierarchy and  which combines clustering method with text categorization. Our approach of identifying
8918623	1240	way of identifying text topic) to concept counting. He uses the WordNet taxonomy to collect interesting concepts and later generalizes the concepts to identify the main topic of the target text. In , they transform the bag-of-words representation of the text into hypernym density. Using the height of generalization, they can limit the number of steps upward through the hypernym hierarchy for
8918623	1241	in section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to the node categories of the
8918623	1241	ways. We try to take advantage of these semantics relationships to establish links between the words of Yahoo concepts and WordNet vocabulary. Based on our review on past related works , ,  we decide to use three types of semantics relationships found in the WordNet and they are synonym 1 , hypernym/hyponym 2 and meronym/holonym 3 . Using these three semantics relationships, we can
8918625	1249	M' B profit margin common standard workstation console This course is derived from the PhD thesis “CAFCR: A Multi-view Method for Embedded Systems Architecting; Balancing Genericity and Specificity”. 1.2 Program The program purposefully alternates process, business and technology views. The table below shows the program of the stakeholder part. Normally this part of the course is given in a
1251	1252	proposed selection methods, such as query by committee  or uncertainty sampling . Applications of active learning techniques are now emerging in the field of multimedia databasesannotation . In the following section we propose a new uncertainty sampling strategy, called partition sampling, that allows to select multiple samples as opposed to classical approaches. 3. PROPOSED APPROACH
1251	1252	that close elements are similar. Thus the knowledge of one sample should induce the knowledge of its neighbors. This is implicitly used in the greedy-like active learning and it is emphasized in , where they proposed to weight the selection function value of a sample with an estimation of its probability density function to increase learning speed. However most ambiguous points are likely
1251	1253	the expensive human intervention to archive and annotate contents. Many researchers are currently investigating methods to automatically analyze, organize, index and retrieve video information . This effort is further stressed by the emerging Mpeg-7 standard that provides a rich and common description tool of multimedia contents. It is also encouraged by Video-TREC which aims at
1251	1255	digit. They can also be selected from a unlabeled set. This approach, called selective sampling, is the most common, and many researchers proposed selection methods, such as query by committee  or uncertainty sampling . Applications of active learning techniques are now emerging in the field of multimedia databasesannotation . In the following section we propose a new uncertainty
1251	1256	to find optimal elements since we do expect teachers to have a rest between rounds. Figure (2) presents results on a real database. It is composed of 40,000 annotated shots from news sequences . Shots are described by HS color histograms and Gabor’s energies of their key frame. We use Latent Semantic Analysis, as described in , to capture local information, remove noise and emphasize
1251	1257	the expensive human intervention to archive and annotate contents. Many researchers are currently investigating methods to automatically analyze, organize, index and retrieve video information . This effort is further stressed by the emerging Mpeg-7 standard that provides a rich and common description tool of multimedia contents. It is also encouraged by Video-TREC which aims at
1251	1258	approaches were proposed to this problem, semi-supervised and active learning. On one hand, a semi-supervised learner combines a small set of labeled samples with a large set of unlabeled samples . The latter set does not provide any direct information but their distribution can be used to boost the performance of the classifier. On the other hand, an active learner starts from a very small
1251	1259	proposed selection methods, such as query by committee  or uncertainty sampling . Applications of active learning techniques are now emerging in the field of multimedia databasesannotation . In the following section we propose a new uncertainty sampling strategy, called partition sampling, that allows to select multiple samples as opposed to classical approaches. 3. PROPOSED APPROACH
1251	1259	fL(x),y)] ? EY |X (4) P To learn the hypothesis f L +(.) of equation (2) for each possible query sample S is now the major problem to compute the estimated error reduction. In , the authors first assume that all losses for any x ? P\L have an equal influence. Hence, the sum over P is reduced over S. Then they can neglect C( f L +(x),y) over C( fL(x),y) since the new
11121203	1293	over long time-scales. Understanding the nature of such fluctuations has important consequences and can reveal the underlying laws of ecosystem dynamics. The biosphere is a complex adaptive system (Levin 1998). As such, it displays some universal features common to other far from equilibrium systems. In this context, beyond the plethora of fine-scale details present in any ecosystem, simple laws can
11121203	1328	Self-organized critical systems require a number of strong constraints in order to operate (Jensen 1998), and it is not clear whether such constraints can operate in a broad range of situations (Solé et al. 1999; Solé & Goodwin 2001). However, partially inspired by ideas of this kind, we propose a mechanism of ecosystem organization in which the observed patterns result from the spontaneous driving of
1337	1339	and utterance types. Recent acquisition research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable
1337	1343	was structured, or the network’s memory was initially limited, and developed gradually. 4 An SRN’s performance with such recursive structures has also been shown to fit well to the human data (Christiansen and Chater, 1999).sSuch networks have also been shown to go beyond the data in interesting ways. Elman (1998) and Morris et al. (2000) showed that SRNs induce abstract grammatical categories which allow both
1337	1348	above sort of evidence, the stochastic information in data uncontroversially available to children is sufficient to allow for learning. Building on recent work with simple recurrent networks (SRNs; Elman 1990), we show that the correct generalization emerges from the statistical structure of the data. Figure 1 shows the general structure of an SRN. The network comprises a three-layer feed-forward
1337	1349	speech that appear to be important for language acquisition, and particularly for the issue at hand. Complexity increases over time which has been shown to be a determinant of learnability (e.g. Elman, 1991, 1993) and there are also arguably meaningful shifts in the distribution of types, and the limitations on forms. The increasing complexity of the child’s input is especially relevant to the problem
1337	1350	i.e., ‘is the boy who is smoking is . . . ’; in fact an auxiliary is predicted as a possible 8 The SRN responsible for these results incorporates a variant of the developmental mechanism from (Elman, 1993). That version reset the context layer at increasing intervals; the version used here is similar, but does not reset the context units unless the network’s prediction error is greater than a set
1337	1358	research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable from positive examples alone, while their
8918655	1440	quadrant-based tree structure, the P-tree . 2.1 Data Representation The vector space model is a simple ubiquitous approach used to represent text documents in machine understandable formats   . It is characterized by being relatively computationally efficient and by having conceptual simplicity; however, it suffers from loss of information related to the original document
8918655	1443	value given to each document, d, to express the referencing importance of d to other documents in the document set. It is calculated as the total number of document references (IDREFs and Xlinks ) to d. All the tags of a document will have the same reference weight since references are usually document based and not tag based. The depth weight reflects the hierarchical structure of an XML
8918655	1445	based on weight voting. We will also present an efficient result sorting method using EIN-rings and P-trees. 4.1 Ranking by Votes Traditional methods for ranking queries results, such as Google  , are based on the use of fixed ranking formulas, which integrate different dimension weights into the ranking score and evaluate documents accordingly. This scheme is straightforward and natural;
8918657	1448	observable domains. They are also first to find an action model at the same time that they determine the agent’s knowledge about the state of the world. They draw on intuitions and results of  for known (nondeterministic) action models. If we assume that our transition model is fully known, then our results reduce to those of  for deterministic actions.sA wide range of virtual domains
8918657	1448	one of the tuples in ? sw-on causes E, sw-on keeps E ? ? sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state)  when the transition model is fully specified. Theorem 3 Let ? = ? × {R}, where ? ? S and R ? S × A × S, and let ?ai, oi? i?t be a sequence of actions and observations. If FilterR(?)
8918657	1448	section illustrates how the explicit representation of transition belief states may be doubly exponential in the number of domain features and the number 1 Filtering semantics as defined in . ??? of actions. In this section we follow the intuition that propositional logic can serve to represents ? more compactly. From here forth we assume that our actions are deterministic. In the
8918657	1449	in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables.  presents an approach that is based on inductive logic programming. Most recently,  showed how to learn stochastic actions with no conditional effects (i.e., the same stochastic change occurs at
8918657	1452	our algorithms in large domains, including over 1000 features (see  for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains.  learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to
8918657	1454	hill-climbing algorithm which is only guaranteed to reach a local optima, and there is no time guarantee for convergence on this local optima. Reinforcement learning in partially observable domains  can be Eyal Amir Computer Science Department University of Illinois, Urbana-Champaign Urbana, IL 61801, USA eyal@cs.uiuc.edu solved (approximately) by interleaving learning the POMDP with solving
8918657	1455	transition models in partially observable domains is hard. In stochastic domains, learning transition models is central to learning Hidden Markov Models (HMMs)  and to reinforcement learning , both of which afford only solutions that are not guaranteed to approximate the optimal. In HMMs the transition model is learned using the Baum-Welch algorithm, which is a special case of EM. It is
8918657	1456	problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows the underlying transition model . Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states , but
8918657	1457	one of the tuples in ? sw-on causes E, sw-on keeps E ? ? sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state)  when the transition model is fully specified. Theorem 3 Let ? = ? × {R}, where ? ? S and R ? S × A × S, and let ?ai, oi? i?t be a sequence of actions and observations. If FilterR(?)
8918657	1458	. Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states , but reinforcement learning has no similar solution known to us. In this paper we present a formal, exact, many times tractable, solution to the problem of simultaneously learning and filtering
8918657	1459	solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) . It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one
8918657	1460	solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) . It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one
8918657	1461	problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows the underlying transition model . Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states , but
8918657	1462	effects and preconditions focused on fully observable domains.  learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables.  presents an approach that is based on
8918657	1463	(in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables.  presents an approach that is based on inductive logic programming. Most recently,  showed how to learn stochastic actions with no conditional effects (i.e., the same stochastic change occurs at every state in which the action is executable). The common theme among these
8918657	1069	to flipping the switch. Learning transition models in partially observable domains is hard. In stochastic domains, learning transition models is central to learning Hidden Markov Models (HMMs)  and to reinforcement learning , both of which afford only solutions that are not guaranteed to approximate the optimal. In HMMs the transition model is learned using the Baum-Welch algorithm,
8918657	1464	change occurs at every state in which the action is executable). The common theme among these approaches is their assumption that the state of the world is fully observed at any point in time.  is the only work that considers partial observability, and it does so by assuming that the world is fully observable, giving approximate computation in relevant domains. 2 Filtering Transition
8918657	1465	our algorithms in large domains, including over 1000 features (see  for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains.  learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to
47718	1469	N can be trained with examples using, e.g., Backpropagation, and using P as background knowledge (Pazzani & Kibler, 1992). The knowledge acquired by training can then be extracted (d’Avila Garcez et al., 2001), closing the learning cycle (as in (Towell & Shavlik, 1994)). For each agent (child), a C-ILP network can be created. Each network can be seen as representing a (learnable) possible world
47718	1472	of knowledge acquisition through inductive learning. 1 Introduction Hybrid neural-symbolic systems concern the use of problem-specific symbolic knowledge within the neurocomputing paradigm (d’Avila Garcez et al., 2002a). Typically, translation algorithms from a symbolic to a connectionist representation and vice-versa are employed to provide either (i) a neural implementation of a logic, (ii) a logical
47718	1472	systems were not able to fully represent, reason and learn expressive languages other than propositional and fragments of first-order logic (Cloete & Zurada, 2000). However, in (d’Avila Garcez et al., 2002b; d’Avila Garcez et al., 2002c; d’Avila Garcez et al., 2003), a new approach to knowledge representation and reasoning in neural-symbolic systems based on neural networks ensembles has been
47718	1472	K1q1, the input neuron associated with K1¬p2, and the input neuron associated with K1¬p3 are all activated (true). The Connectionist Inductive Learning and Logic Programming (C-ILP) System (d’Avila Garcez et al., 2002a; d’Avila Garcez & Zaverucha, 1999) makes use of the above kind of translation. C-ILP is a massively parallel computational model based on an artificial neural network that integrates inductive
47718	1472	paper. By combining a number of simple C-ILP networks, we are able to model individual and common knowledge. Each network represents a possible world or an agent’s current set of beliefs (d’Avila Garcez et al., 2002b). If we allow a number of ensembles like the one of Figure 2 to be combined, we can represent the evolution in time of an agent’s set of beliefs. This is exactly what is required for a complete
47718	1472	of knowledge acquisition through inductive learning. 1 Introduction Hybrid neural-symbolic systems concern the use of problem-specific symbolic knowledge within the neurocomputing paradigm (d’Avila Garcez et al., 2002a). Typically, translation algorithms from a symbolic to a connectionist representation and vice-versa are employed to provide either (i) a neural implementation of a logic, (ii) a logical
47718	1472	systems were not able to fully represent, reason and learn expressive languages other than propositional and fragments of first-order logic (Cloete & Zurada, 2000). However, in (d’Avila Garcez et al., 2002b; d’Avila Garcez et al., 2002c; d’Avila Garcez et al., 2003), a new approach to knowledge representation and reasoning in neural-symbolic systems based on neural networks ensembles has been
47718	1472	K1q1, the input neuron associated with K1¬p2, and the input neuron associated with K1¬p3 are all activated (true). The Connectionist Inductive Learning and Logic Programming (C-ILP) System (d’Avila Garcez et al., 2002a; d’Avila Garcez & Zaverucha, 1999) makes use of the above kind of translation. C-ILP is a massively parallel computational model based on an artificial neural network that integrates inductive
47718	1472	paper. By combining a number of simple C-ILP networks, we are able to model individual and common knowledge. Each network represents a possible world or an agent’s current set of beliefs (d’Avila Garcez et al., 2002b). If we allow a number of ensembles like the one of Figure 2 to be combined, we can represent the evolution in time of an agent’s set of beliefs. This is exactly what is required for a complete
47718	1477	of a logic, (ii) a logical characterisation of a neural system, or (iii) a hybrid learning system that brings together features from connectionism and symbolic artificial intelligence (Holldobler, 1993). Until recently, neural-symbolic systems were not able to fully represent, reason and learn expressive languages other than propositional and fragments of first-order logic (Cloete & Zurada,
47718	1485	? 8 Note that ? is not required to precede every rule antecedent. In the network, neurons are labelled as ?K1L1 or K1L1 to differentiate the two concepts. 2s4 Conclusions In his seminal paper (Valiant, 1984), Valiant argues for the need of rich logic-based knowledge representation mechanisms within learning systems. In this paper, we have addressed such a need, yet complying with important principles
3893559	1487	many SRT applications. AVERAGE The task is willing to except a reservation in which constraints are not met all the time, but are met on average. Most SRT systems provide these kinds of guarantees . STRICT The task may only run if it is guaranteed to meet all timeliness constraints. Hard real-time systems are designed to provide this type of guarantee. Like the above, in many cases we expect
3893559	1489	many SRT applications. AVERAGE The task is willing to except a reservation in which constraints are not met all the time, but are met on average. Most SRT systems provide these kinds of guarantees . STRICT The task may only run if it is guaranteed to meet all timeliness constraints. Hard real-time systems are designed to provide this type of guarantee. Like the above, in many cases we expect
3893559	1490	period workloads, such as media in which the processing depends on the content of the data, the average case execution time of jobs may be known (also known as a variable processing time class ). Knowing a model of the variability, it is possible to provide some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily
3893559	1491	average case execution time of jobs may be known (also known as a variable processing time class ). Knowing a model of the variability, it is possible to provide some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent
3893559	1491	many SRT applications. AVERAGE The task is willing to except a reservation in which constraints are not met all the time, but are met on average. Most SRT systems provide these kinds of guarantees . STRICT The task may only run if it is guaranteed to meet all timeliness constraints. Hard real-time systems are designed to provide this type of guarantee. Like the above, in many cases we expect
3893559	1492	(also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines . {DEADLINE} An imprecise constraint specification states a mandatory deadline constraint for each job, with optional deadlines associated with subprocesses that may be met in best effort fashion
3893559	1493	variability, it is possible to provide some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines .
3893559	1495	some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines . {DEADLINE} An imprecise constraint
3893559	1496	period and worst-case execution of any job of a task is known, task constraints may be used for hard guarantees. This constraint type is used to check for schedulability in the periodic task model . DEADLINE+ACE For variable period workloads, such as media in which the processing depends on the content of the data, the average case execution time of jobs may be known (also known as a variable
3893559	1497	{DEADLINE} An imprecise constraint specification states a mandatory deadline constraint for each job, with optional deadlines associated with subprocesses that may be met in best effort fashion . any+u() Any of the above constraints may be paired with a utility function that associates a value with meeting the constraint. Utilities functions may consist of a fixed value or be represented
3893559	1500	fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines . {DEADLINE} An imprecise constraint specification states a mandatory deadline constraint for each job, with optional deadlines associated with
3893559	1501	be specified as of set of multiple constraints—these applications adapt to available resources by adjusting their constraints. These systems adapt allowing the system allowing dynamic QoS control .sTable 3. Types of Processing Guarantees Type Description NONE The task is able to run without any guarantee. This is provided by best-effort systems, and many SRT real-time applications are
3893559	1502	algorithms, i.e. for treating different classes equivalently for scheduling purposes (often independently of the scheduling algorithm). The following theorem is adapted from the HLS framework . Theorem 1 Any schedule that guarantees a rate x over interval y to a task of type a-RATE-b will also guarantee that a task of type a-DEADLINE+WCE-b with a worst-case execution time x and deadline
8918663	1512	frames (translation, zoom and rotation). The affine model is needed since some frames are rotated. The estimation itself is performed 28sthrough M-estimators. Part of the algorithm is described in . We used then the restoration process described in chapter 5 to compensate the frames. The estimation process takes around 3 seconds per frame on a PC and about 10 hours for the whole movie. The
11498811	1536	drastically different kinematic descriptions. The utility of viewing character motion as one or more discrete time series of joint angles and joint positions has been presented in the literature (Gleicher and Litwinowicz 1998). This view naturally suggests the application of signal processing techniques to modify motion signals so as to preserve the desired qualities of the original while modifying it to meet the goals
11498811	1539	attributes when changing from high to low resolution, and disaggregation when moving in the opposite direction. Aggregation and disaggregation have been shown to suffer from several problems (Reynolds et al. 1997), including chain disaggregation, transitional latency, and difficulties in mapping between different levels of resolution. Reynolds et al. (1997) proposed, as an alternative, the concept of
3685242	1550	images. Feature based methods  use features or tokens to get depth information and motion. Flow based methods  assume that optical flow is available. Direct methods  do not require intermediate steps such as feature extraction or flow computation and work directly with spatial and temporal image gradients. Most of the previous approaches assumes locally
3685242	1550	may require a nonsmooth local depth refinement. We show how to use the epipolar constraint and model the parallax field appropriately to deal with such cases. Parallax based approaches proposed in  assume a dominant planar region to be present in the image or the presence of a small planar region for motion estimation . Our approach does not require any such assumptions. Also, many of
3685242	1551	images. Feature based methods  use features or tokens to get depth information and motion. Flow based methods  assume that optical flow is available. Direct methods  do not require intermediate steps such as feature extraction or flow computation and work directly with spatial and temporal image gradients. Most of the previous approaches assumes locally
3685242	1551	or is a parametric function  that imposes smooth flow. The smoothness constraint on depths can be applied by assuming a smooth depth model (constant or planar) over the neighborhood (as in ) and directly using (4) and (3) to estimate Z. However, these assumptions are violated at depth boundaries. Also, the effect of noise in available depth map estimate (from a range finder or from
3685242	1552	images. Feature based methods  use features or tokens to get depth information and motion. Flow based methods  assume that optical flow is available. Direct methods  do not require intermediate steps such as feature extraction or flow computation and work directly with spatial and temporal image gradients. Most of the previous approaches assumes locally smooth
3685242	1552	require a nonsmooth local depth refinement. We show how to use the epipolar constraint and model the parallax field appropriately to deal with such cases. Parallax based approaches proposed in  assume a dominant planar region to be present in the image or the presence of a small planar region for motion estimation . Our approach does not require any such assumptions. Also, many of the
3685242	1555	with ?I. ?1 > 0, ?2 = 0 4. Intensity variation in all directions. No sufficient structure. ?1 > 0, ?2 > 0 Confidence measures based on eigen-values and/or condition number have been proposed in . We use C = ( ?1??2 ?1+?2 )2 as the confidence measure for depth estimation. Homogeneous regions (case 1) can be identified by using a threshold on the sum of eigen-values. Regions where local edge
8918674	1673	our own experience. On the other hand, other limits have also been reported in the literature—for example, Zimny et al. regard an SUV of 6.4 as dealing with a malignancy but accept a range of ?3.6 (32). In a study population of 106 patients, the mean SUV in the case of inflammatory changes is given as 3.4 ? 1.7. Berberat et al. reported a mean SUV of 3.09 for cancer and 0.87 for inflammation
8918674	1673	could explain the excellent results. For their study population of 106 patients, Zimny et al. reported a sensitivity of 89.0% and a specificity of 53.0%, but they evaluated their data only visually (32). The large number of false-negative results was explained as being due to patients with an elevated blood glucose level. After exclusion of this group of patients from the study population, PET
8918674	1673	no general agreement on the benefit of quantitative assessment by means of the SUV; for instance, Zimny et al. dispute the benefitofthe SUV, whereas other authors consider the SUV a valid parameter (32,35). Zimny et al. reported data on PET detection in lymph node and distant metastases, with a correct result of 40.0% in lymph node metastases and 52.0% in distant metastases. Bares et al. compared PET
1054991	1623	single stream when it comes to QoS guarantees, which eliminates the need for per-flow based state and processing. Such an approach is reflected in recent proposals such as the IETF Diff-Serv model . By coarsening the different levels of QoS that the network offers into a small number of service classes, the Diff-Serv model is expected to scale well. However, this gain in scalability through
1054991	1626	selected because it has the lowest mean rate among all the traces in the video library. Several important statistics of the three video sources are given in Table I. We also use voice traces from  in our investigation. In particular, the voice source 1 that we choose corresponds to Fig. 4b in  and was generated using the NeVoT Silence Detector(SD) . We refer to  for a complete
1054991	1629	I. We also use voice traces from  in our investigation. In particular, the voice source 1 that we choose corresponds to Fig. 4b in  and was generated using the NeVoT Silence Detector(SD) . We refer to  for a complete description of the codec configuration and trace collection procedure. In the following table important statistics of the voice source are given. Trace Name Mean
1054991	1631	without this being noticed at the class level. This work was supported in part through NSF grant ITR00-85930.sTECHNICAL REPORT UNIVERSITY OF PENNSYLVANIA 2 This issue was explored in an early work , which focused on the loss probability of an individual user as the relevant metric.  developed a number of explicit models to evaluate the individual loss probabilities and the overall loss
1054991	1631	cases where significant performance deviations can exist, and that user traffic parameters can have a major influence on whether this is the case or not. In this paper, we go beyond the work of  by developing a simple methodology capable of evaluating loss performance deviations in realistic settings. Specifically, we concentrate on the ability to predict performance deviations when
1054991	1631	different configurations that we consider and the performance measures that we use to evaluate loss performance deviation. For completeness, we also summarize the analytical models, developed in , that we use to evaluate the loss probability of an individual user. The voice and video traffic sources and their characteristics are thensTECHNICAL REPORT UNIVERSITY OF PENNSYLVANIA 3 described
1054991	1631	only two sources are aggregated and the other for a system where many sources are aggregated, are described. We only present the main results, while all the relating derivations can be found in . 1) Analytical model when aggregating two sources: The individual loss probability in a two source model can be evaluated simply by specializing an N-source system to the case N = 2. In an N-source
1054991	1631	loss probability. However, we believe that using merely first order statistics to predict loss deviation is a reasonable approach. This assumption is based in part on the experience obtained from , where we saw that deviations were to a large extent a function of the first order statistics of a source, e.g., the peak rate, the utilization, and the average burst duration, among which the
10166220	1633	optimization approach to estimate the overflow probability in three stages by approximating an optimal tilting parameter. The balanced likelihood ratio approach to importance sampling (see Alexopoulos and Shultes 1998, 2001) was developed for analyzing system performance in fault-tolerant repairable systems. This approach has been used to derive importance sampling estimators for limiting system unavailability
10166220	1635	particularly the time until one of these events occurs. Importance sampling is gaining popularity as an efficient method for analyzing rare events in queueing and reliability systems (see Asmussen and Rubinstein 1995, Heidelberger 1995). The application of importance sampling involves simulating the model using an auxiliary distribution designed to make the system experience rare events of Ramya Dhamodaran
8918718	1670	other text in the document conveys some of the images’ semantics. So, it is natural for Web image search systems to use text as part of the process. This approach is similar to that of Brown et al. , who used close-captioned text and speech analysis to index video data. 3 Using HTML Metadata to Find Web Images We studied the effectiveness of HTML metadata (textual content and structure) for
8918718	1671	other text in the document conveys some of the images’ semantics. So, it is natural for Web image search systems to use text as part of the process. This approach is similar to that of Brown et al. , who used close-captioned text and speech analysis to index video data. 3 Using HTML Metadata to Find Web Images We studied the effectiveness of HTML metadata (textual content and structure) for
8918718	1673	image file names. The individual images are assigned to a topic by a human judge. Users can browse or search the topics in the database and can also search usingsimage features. Research on WebSeer  investigated how to classify images into categories such as photographs, portraits and computer-generated drawings. To do this, WebSeer supplemented information from image content analysis with
1677	1683	the three-dimensional nature of anisotropy in tissues. Numerous works have already addressed the problem of the estimation and regularization of these tensor fields. References can be found in , , , , . Motivated by the potentially dramatic improvements that knowledge of anatomical connectivity would bring into the understanding of functional coupling between cortical regions ,
1677	1685	nature of anisotropy in tissues. Numerous works have already addressed the problem of the estimation and regularization of these tensor fields. References can be found in , , , , . Motivated by the potentially dramatic improvements that knowledge of anatomical connectivity would bring into the understanding of functional coupling between cortical regions , the study
1677	1685	our capacity to resolve multiple fibers orientations since local tractography becomes unstable when crossing artificially isotropic regions characterized by a planar or spherical diffusion profile . On the other side, new diffusion imaging methods have been recently introduced in an attempt to better describe the complexity of water motion but at the cost of increased acquisition times. This
1677	1689	planning or tumor growth quantification, various methods have been proposed to tackle the issue of cerebral connectivity mapping. Local approaches based on line propagation techniques ,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , ,
1677	1691	techniques ,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this
1677	1692	techniques ,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation,
1677	1693	,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation, they
1677	1696	provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation, they enable us to generate
1677	1697	fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation, they enable us to generate
1677	1698	recently introduced in an attempt to better describe the complexity of water motion but at the cost of increased acquisition times. This is a case of high angular diffusion weighted imaging ,  where the variance of the signal could give important information on the multimodal aspect of diffusion. Diffusion Spectrum Imaging ,  provides, at each voxel, an estimation of the
1677	1702	tissues. In favor of these promising modalities, parallel MRI  will reduce the acquisition time in a near future and thus permit high resolution imaging. More global algorithms such as  have been proposed to better handle the situations of false planar or spherical tensors (with fibers crossings) and to propose some sort of likelihood of connection. In , the authors make use
1677	1704	the situations of false planar or spherical tensors (with fibers crossings) and to propose some sort of likelihood of connection. In , the authors make use of the major eigenvector field and in  the full diffusion tensor provides the metric of a Riemannian manifold but this was not exploited to propose intrinsic schemes.sInferring White Matter Geometry from DT-MRI 3 We derive a novel
1677	1710	= {gij} the metric tensor, this writes ? v T Gv. Then, by setting H(x, p) = |p| ? 1, we will work on the following theorem (for details on viscosity solutions on a Riemannian manifold, we refer to ) Theorem 1. The distance function ? is the unique viscosity solution of the Hamilton-Jacobi problem ? |grad?| = 1 in M \ K (3) ?(x) = 0 when x ? K in the class of bounded uniformly continuous
1677	1713	the minimum arrival time problem. This will enable us to solve equation 3 as a dynamic problem and thus to take advantage of the great flexibility of Level Set methods. On the basis of , ,  and , we reformulate equation 3 by considering ? as the zero level set of a function ? and requiring that the evolution of ? generates ? so that ?(x, t) = 0 ? t = ?(x) (4) Osher () showed
1677	1714	arrival time problem. This will enable us to solve equation 3 as a dynamic problem and thus to take advantage of the great flexibility of Level Set methods. On the basis of , ,  and , we reformulate equation 3 by considering ? as the zero level set of a function ? and requiring that the evolution of ? generates ? so that ?(x, t) = 0 ? t = ?(x) (4) Osher () showed by using
1677	1717	has also been done by using WENO schemes in order to increase the accuracy of the method. They consist of a convex combination of nth (we take n = 5) order polynomial approximation of derivatives . A classical narrow band implementation is used to speed up the computations. 4.3 Numerical Scheme for the Geodesics Estimation We finally derive an intrinsic method for geodesics computation in
1718	1719	absorption (see ). Role absorption is important because in ontology derived KBs range and domain constraints will often have been transformed into GCIs. This is because tools such as OilEd  and Protégé  are designed to work with range of DL reasoners, some of which (e.g., FaCT) do not support range and domain axioms. Moreover, these forms of GCI are not, in general, amenable to
1718	1721	algorithm by analysing the perfomance of our FaCT++ implementation when classifying terminologies derived from realistic ontologies. 1 Introduction Many modern ontology languages (e.g., OIL , DAML+OIL  and OWL ) are based on expressive description logics, and in particular on the SHIQ family of description logics . These ontology languages typically support domain and range
1718	1722	for the Racer system to be able to classify large KBs containing many range and domain constraints, it is necessary to give a special treatment to the GCIs introduced by range and domain axioms . The approach used by Racer is to extend the lazy unfolding optimisation so that concepts equivalent to those that would besintroduced by the GCIs are introduced only as necessary. In the approach
1718	1724	is that such GCIs are not amenable to absorption, an optimisation technique that tries to rewrite GCIs so that they can be efficiently dealt with using the lazy unfolding optimisation . Absorption is one of the crucial optimisations that enable state of the art DL reasoners such as FaCT , Racer  and Pellet  to deal effectively with large knowledge bases (KBs), and these
1718	1724	on the same tableaux algorithms as the original FaCT, but has a different architecture and is written in C++ instead of Lisp. Absorption Absorption in FaCT++ uses the same basic approach as FaCT . Given a TBox T , the absorption algorithm constructs a triple of TBoxes ?Tdef, Tsub, Tg? such that: • Tdef is a set of axioms of the form A ? C (equivalent to a pair of axioms {A ? C, C ? A} ? T
1718	1725	GCIs so that they can be efficiently dealt with using the lazy unfolding optimisation . Absorption is one of the crucial optimisations that enable state of the art DL reasoners such as FaCT , Racer  and Pellet  to deal effectively with large knowledge bases (KBs), and these reasoners perform much less well with KBs containing significant numbers of unabsorbable GCIs.
1718	1725	and Empirical Evaluation We have implemented the extended tableaux algorithm and role absorption optimisation in the FaCT++ DL reasoner. FaCT++ is a next generation of the well-known FaCT reasoner , being developed as part of the EU WonderWeb project (see http://wonderweb. semanticweb.org/); it is based on the same tableaux algorithms as the original FaCT, but has a different architecture and
1718	1726	by analysing the perfomance of our FaCT++ implementation when classifying terminologies derived from realistic ontologies. 1 Introduction Many modern ontology languages (e.g., OIL , DAML+OIL  and OWL ) are based on expressive description logics, and in particular on the SHIQ family of description logics . These ontology languages typically support domain and range constraints on
1718	1727	1 Introduction Many modern ontology languages (e.g., OIL , DAML+OIL  and OWL ) are based on expressive description logics, and in particular on the SHIQ family of description logics . These ontology languages typically support domain and range constraints on roles, i.e., axioms asserting that if an individual x is related to an individual y by a role R, then x must be an
1718	1727	of the SHIQ logic, including the semantics of role boxes extended with range and domain axioms. Most details of the logic and the tableaux algorithm are little changed from those presented in . We will, therefore, focus mainly on the parts that have been added in order to deal with range and domain axioms, and refer the reader to  for complete information on the remainder. The
1718	1727	satisfying R and T . Theorem 1 Satisfiability and subsumption of SHIQ-concepts w.r.t. terminologies and role boxes is polynomially reducible to (un)satisfiability of SHIQ-concepts w.r.t. role boxes . 3 Tableaux Reasoning with Range and Domain Here we present an algorithm for deciding the satisfiability of a SHIQ-concept C w.r.t. a role box R; it is an extension of the SHIQ tableaux algorithm
1718	1727	when a clash occurs, and answers “D is satisfiable” iff the completion rules can be applied in such a way that they yield a complete and clash-free completion tree. Note that the only change w.r.t.  is addition of the domain and range-rules that add concepts to node labels as required by domain and range axioms. Lemma 2 Let D be an SHIQ-concept. 1. The tableaux algorithm terminates when
1718	1728	sense to transform GCIs into range and domain axioms. We call this new form of absorption role absorption in contrast to the usual form of absorption we will refer to as concept absorption (see ). Role absorption is important because in ontology derived KBs range and domain constraints will often have been transformed into GCIs. This is because tools such as OilEd  and Protégé  are
1718	1728	on the same tableaux algorithms as the original FaCT, but has a different architecture and is written in C++ instead of Lisp. Absorption Absorption in FaCT++ uses the same basic approach as FaCT . Given a TBox T , the absorption algorithm constructs a triple of TBoxes ?Tdef, Tsub, Tg? such that: • Tdef is a set of axioms of the form A ? C (equivalent to a pair of axioms {A ? C, C ? A} ? T
1718	1729	(see ). Role absorption is important because in ontology derived KBs range and domain constraints will often have been transformed into GCIs. This is because tools such as OilEd  and Protég??  are designed to work with range of DL reasoners, some of which (e.g., FaCT) do not support range and domain axioms. Moreover, these forms of GCI are not, in general, amenable to standard concept
8918719	1739	of ™ onto ‚@ƒ A. However, ? ? and ? need not be in the same quantization cell, and, therefore, ? ? need not be a consistent estimate of ? , as illustrated in this figure. studied in  and  for the case of periodic band-limited signals. Theoretically, these algorithms can be extended to band-limited signals in ; however, these algorithms would not be practical since they require
8299611	1747	rest of the paper describes the BT-tree in detail. 1.1 Background and Previous Work Branched-and-temporal indexing is a relatively unexplored area. However, many access methods (for example, , , ,  and ) have been proposed for temporal data. A survey and comparison of these access methods can be found in . These methods have e ectively solved the problem of providing access
8299611	1748	pages. The paper  does not consider pagination and, although the structure is \branched,&quot; only a current version can be split into branches. Old versions can not be 453 modi ed. Driscoll et al.  develop techniques for making linked data structures (e.g. binary search trees) fully persistent (allversions can be read and updated). Perhaps closest to our work is that of Lanka and Mays ,
8299611	1748	BT-tree structure problematic. We need to be able to decide whether or not one version is a descendent or ancestor of another. 2.1 Ancestor Determination Ancestor determination methods used in  and  for branched data use O(n) space. Their methods are not suitable for branched-and-temporal case where the number of timestamps is large (hence the corresponding total number of the versions
8299611	1753	describes the BT-tree in detail. 1.1 Background and Previous Work Branched-and-temporal indexing is a relatively unexplored area. However, many access methods (for example, , , ,  and ) have been proposed for temporal data. A survey and comparison of these access methods can be found in . These methods have e ectively solved the problem of providing access to versioned record
8299611	1755	that require the support of time-evolving data. Temporal database systems model explicitly the temporal behavior of data, thus providing the ability to store and query temporal data e - ciently . Conventional temporal databases assume a single line of time evolution. As an example, consider an architect's design of a new house (say Joe's house). The house design starts from scratch and
8918722	1804	code. 2.2 OCL description In  we focused on the definition of a special kind of transformations: UML refactorings, i.e. the adaptation to UML of Opdyke’s behavior-preserving transformations . We specified each transformation using pre- and post-conditions, expressed as OCL constraints at the metamodel level. This is illustrated in the following example. Example: attribute privatization
9054621	1812	future. 2 FACTORS AFFECTING SIMULATION COURSES THOUGHTS AND MUSINGS ON SIMULATION EDUCATION The topical composition of simulation courses and the allocation of time to each subject is addressed in (Nance 2000). The results of two surveys (Beckwith 1974, 1976a, 1976b) and (Nance and Overstreet 1976) are compared with a conRichard E. Nance Osman Balci Systems Research Center and Department of Computer
1817	1818	is therefore used as the continuous output of the learning-based evaluator. The Torch3 machine learning library implementation of SVMs for classification is used here with Gaussian kernels (Collobert et al. 2002). 3.2 Features In order to use the general purpose SVM for classifying translations, linguistic objects must be rerepresented in numerical form with a vector of feature values. The features used
1817	1822	final report that the BLEU metric, used for training and evaluation of the team’s MT system, seems insensitive to syntactic changes that should be noticeable to human judges at the sentence level (Och et al. 2003). Here, a metric designed to evaluate single-sentence machine translations based on machineslearning is described and tested. A novel approach leads to a flexible classification-based metric and
1817	1822	thereby improving the metric’s sensitivity to particular aspects of machine translations. For example, it has been observed that BLEU is not particularly sensitive to syntactic improvements (Och et al. 2003); a learning-based metric, on the other hand, could be modified specifically to take syntax into account, providing a much finer-grained error analysis than is currently possible. 6
1817	1823	word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics BLEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a set of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT
1817	1823	the BLEU metric, for example, has been shown to correlate highly with the judgments of bilingual human evaluators (a correlation coefficient of 0.96) when averaged over a 500-sentence corpus (Papineni et al. 2001). However, though useful for system discrimination, metrics correlating with human evaluations only over long texts (which tend to average out the “noise” of evaluation) are relatively useless for
1817	1825	of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT results. The F-Measure has been proposed as a more comprehensible alternative for MT evaluation (Turian et al. 2003), and can be defined as a simple composite of unigram precision and recall. These metrics have demonstrated some success; the BLEU metric, for example, has been shown to correlate highly with the
1817	1825	human evaluation of MT is itself inconsistent and not very reliable, automatic MT evaluation measures are even less reliable and are still very far from being able to replace human judgment.” (Turian et al. 2003) The Syntax for Statistical Machine Translation team from the 2003 Workshop further notes in its final report that the BLEU metric, used for training and evaluation of the team’s MT system, seems
1906	1908	exploration of scientific data, Reeb graphs are used to efficiently compute level sets . Reeb graphs can also function as a user-interface tool aiding the selection of meaningful level sets . A more extensive discussion of Reeb graphs and their variations in geometric modeling and visualization applications can be found in . The first algorithm for constructing the Reeb graph of a
1906	1909	running time at the cost of accuracy has been suggested in . An O(????????? ) time algorithm for loop-free Reeb graphs over manifolds of arbitrary but constant dimension has been described in . For the case of 3-manifolds, this algorithm has been extended to include information about the genus of the level surfaces in . In this paper, we focus on loops in Reeb graphs and study when
1906	1911	????? ? ? ????? ??? ????? ??? ? Figure 9 illustrates only the simple cases. Non-simple cases reduce to simple ones. For example, a ? -fold saddle can be split into ? simple saddles, as described in . Similarly, a boundary minimum/maximum with more complicated lower link than shown in Figure 9 can be split into simple (interior) saddles and a (simple) boundary minimum/maximum. Note that we just
1906	1915	graph have lead to data-base search methods for topologically similar geometric models . In the interactive exploration of scientific data, Reeb graphs are used to efficiently compute level sets . Reeb graphs can also function as a user-interface tool aiding the selection of meaningful level sets . A more extensive discussion of Reeb graphs and their variations in geometric modeling and
1258333	1926	and hybrid caching architectures , . There are three major contributions of this paper. First, unlike the previous analytic work on web caching which is based on statistic analysis, e.g., , , this paper proposes a stochastic model, which allows us to characterize the caching processes for individual documents in a two-level hierarchical caching system. In this model, the LRU
1258333	1929	in the context of hierarchical caching algorithm design. Some of the results of this study are also applicable to other caching architectures, such as distributed and hybrid caching architectures , . There are three major contributions of this paper. First, unlike the previous analytic work on web caching which is based on statistic analysis, e.g., , , this paper proposes a
1258333	1930	algorithm design when caches are arranged in a hierarchical structure. Most of the research papers on cache replacement algorithm design, to date, have focused on a single cache, e.g., , , , , . However, when caches are arranged in a hierarchical structure, running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good
1258333	1931	algorithm design when caches are arranged in a hierarchical structure. Most of the research papers on cache replacement algorithm design, to date, have focused on a single cache, e.g., , , , , . However, when caches are arranged in a hierarchical structure, running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good performance.
1258333	175	design when caches are arranged in a hierarchical structure. Most of the research papers on cache replacement algorithm design, to date, have focused on a single cache, e.g., , , , , . However, when caches are arranged in a hierarchical structure, running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good performance.
1258333	175	this assumption, cache sizes are measured in the unit of document size. Since Zipf-like distribution has been widely tested against the distributions collected from the real traces, e.g., , , , as (1) Fig. 1. Two-level hierarchical web caching structure. well as our own tests in Section IV, we model by Zipf-like distributions for and where is the normalization factor, is the popularity
1258333	1932	running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good performance. Although results on the optimal hierarchical caching exists, e.g., , they are obtained based on the assumption that global caching information is known to every cache in the cache hierarchy, which is generally unavailable in practice. Moreover, most of the research
1258333	1933	the context of hierarchical caching algorithm design. Some of the results of this study are also applicable to other caching architectures, such as distributed and hybrid caching architectures , . There are three major contributions of this paper. First, unlike the previous analytic work on web caching which is based on statistic analysis, e.g., , , this paper proposes a stochastic
1938	1939	trace, an advantage that is significant in systems that are to be operated in a number of different conditions. A. Experimental setup IV. EXPERIMENTAL RESULTS We used the SimpleScalar toolset  to evaluate the efficiency of the proposed approaches in a set of traces obtained from the execution of a number of programs in the SPEC95 benchmarks. The model used by the simulator is a modified
1938	1941	that it will be used in a large number of actual designs. Techniques that aim at reducing the power dissipated in buses by dynamically reordering and complementing the words to be transmitted  have also been proposed but their actual implementation in hardware is complex and the techniques are applicable only in very special circumstances. III. LOW OVERHEAD BUS ENCODINGS With the
1938	1942	to the problem. One approach is based on the application of transition coding, a method that has already been proposed by other researchers in conjunction with a transformation on the data words . The proposal made here uses transition coding without applying any recoding of the transmitted data, a method that, intuitively, seems to have little advantage over the transmission of the plain
1938	1942	of the Hamming distance between successive bus words requires the use of a non-trivial amount of hardware, that will, in practice, make the approach hard to apply in wide buses. Another approach  is based on the idea of changing the way bus words are encoded as to reduce the probability of occurrence of a § in the bus line (¨?© ) to a value smaller than 0.5. This is obtained by using wider
1938	1942	¨¥????????? and nothing is gained by using transition coding. This can be overcome if the words on the bus are re-encoded, with the objective of reducing the probability of the presence of a § . We must note, however, that the above analysis is not applicable to real traffic on buses. On the first place, ones and zeros are usually not equally likely to appear. On second place, consecutive
3571599	1956	have been developped for experts opinions fusion, including weighted average (see for instance , ), Bayesian fusion (see for instance , ), majority vote (see for instance , , ), models coming from incertainty reasoning: fuzzy logic, possibility theory  (see for instance ), standard multivariate statistical analysis techniques such as correpondence analysis
3571599	1957	since more than three decades (for some review references, see , , ). It has recently been successfully applied to the problem of classifiers combination or fusion (see for instance ). Many different approaches have been developped for experts opinions fusion, including weighted average (see for instance , ), Bayesian fusion (see for instance , ), majority vote (see
3571599	1962	), models coming from incertainty reasoning: fuzzy logic, possibility theory  (see for instance ), standard multivariate statistical analysis techniques such as correpondence analysis , etc. One of these approaches is based on maximum entropy modeling (see , ). Maximum entropy is a versatile modeling technique allowing to easily integrate various constraints, such as
4329218	1983	was necessary to account for the partial volume effect (Figure 1. blue). Two different methods were used to smooth and estimate the curvature function. The first method uses Taubin’s smoothing , a Gaussian filtering without shrinkage, followed by the least-squares estimation. The second method uses quintic splines to estimate the first and second derivatives to compute the curvature: min
1987	1989	Finally, a conclusion is given in Section 6.s2 Background We assume familiarity with the basic concepts of constraint programming. For a thorough explanation of constraint programming, see . A constraint satisfaction problem (CSP) consists of a finite set of variables X = {x1, . . . , xn} with finite domains D = {D1, . . . , Dn} such that xi ? Di for all i, together with a finite set
1987	1990	topological sorts because of the special structure of the graph. That computation can also be made incremental in the same way as in . Recently, that same result was independently obtained in . We however go further by considering edit distance, for which insertions and deletions are allowed as well. For deletions we need to allow “wasting” a value without changing the current state. To
1987	1991	This technique allows the resolution of over-constrained problem within traditional CP solvers. Comparatively few efforts have been invested in developing soft versions of common global constraints . Global constraints are often key elements in successfully modeling real applications and being able to easily and effectively soften such constraints would yield a significant improvement in
1987	1991	C, we introduce a function that measures the violation, and has the following form: violationC : D1 × · · · × Dn ? N. This approach has been introduced in  and was developed further in . There may be several natural ways to evaluate the degree to which a global constraint is violated and these are not equivalent usually. A standard measure is the variable-based cost: Definition 3
1987	1991	to the soft gcc. The soft gcc also inherits from the cost-gcc the time complexity of achieving domain consistency, being O(n(m + n log n)) where m = ?n i=1 |Di| and n = |X|. Note that  also consider the variable-based cost measure for a different version of the soft gcc. Their version considers the parameters l and u to be variables too. Hence, the variable-based cost evaluation
1987	1995	problems, mostly by introducing soft constraints that are allowed to be (partially) violated. The most well-known framework is the Partial Constraint Satisfaction Problem framework (PCSP ), which includes the Max-CSP framework that tries to maximize the number of satisfied constraints. Since in this framework all constraints are either violated or satisfied, this objective is
1987	1996	This technique allows the resolution of over-constrained problem within traditional CP solvers. Comparatively few efforts have been invested in developing soft versions of common global constraints . Global constraints are often key elements in successfully modeling real applications and being able to easily and effectively soften such constraints would yield a significant improvement in
1987	1996	When l = 0 in soft gcc(X, l, u, z), the arc set Aunderflow is empty. In that case, Gval has a particular structure, i.e. the only costs appear on arcs from DX to t. As pointed out in  for the soft alldifferent constraint, constraints with this structure can be checked for consistency in O(nm) time, and domain consistency can be achieved in O(m) time. The result is obtained by
1987	1997	constraints. Since in this framework all constraints are either violated or satisfied, this objective is equivalent to minimizing the number of violations. It has been extended to the Weighted-CSP , associating a degree of violation (not just a boolean value) to each constraint and minimizing the sum of all weighted violations. The Possibilistic-CSP  associates a preference to each
1987	2000	are typically encoded and solved with one of two generic paradigms: valued-CSPs  and semi-rings .sAnother approach to model and solve over-constrained problems involves Meta-Constraints . The idea behind this technique is to introduce a set of domain variables Z that capture the violation cost of each soft constraint. By correctly constraining these variables it is possible to
1987	2000	Even though there are many avenues for combining soft constraints, the objective almost always remains to minimize constraint violations. We propose here a small extension to the approach of , where meta-constraints on the cost variables of soft constraints are introduced. We illustrate this approach with the newly introduced soft gcc. Definition 12 (Soft global cardinality aggregator).
1987	2000	1} |S| ) the MaxCSP approach can be easily obtained by setting l1 = 0, u1 = 0, violation(Z) = ? d?DZ overflow(Z, d) and reading the number of violations in zagg. The sgca could also be used as in  to enforce homogeneity (in a soft manner) or to define other violation measures like restricting the number of highly violated 3 |?| refers to the number of transitions in the
1987	2001	This technique allows the resolution of over-constrained problem within traditional CP solvers. Comparatively few efforts have been invested in developing soft versions of common global constraints . Global constraints are often key elements in successfully modeling real applications and being able to easily and effectively soften such constraints would yield a significant improvement in
1987	2001	to some criteria. For each soft constraint C, we introduce a function that measures the violation, and has the following form: violationC : D1 × · · · × Dn ? N. This approach has been introduced in  and was developed further in . There may be several natural ways to evaluate the degree to which a global constraint is violated and these are not equivalent usually. A standard measure is the
1987	2002	and effectively soften such constraints would yield a significant improvement in flexibility. In this paper we study two global constraints: the widely known global cardinality constraint (gcc)  and the new regular  constraint. For each of these we propose new violation measures and provide the corresponding filtering algorithms to achieve domain consistency. All the constraint
1987	2002	and maximum number of times each value in the union of their domains should be assigned to these variables. Régin developed a domain consistency algorithm for the gcc, making use of network flows . A variant of the gcc is the costgcc, which can be seen as a weighted version of the gcc . For the cost-gcc a weight is assigned to each variable-value assignment and the goal is to satisfy
1987	2002	f is defined as cost(f) = ? w(a)f(a). a?A A minimum-cost flow is a feasible s?t flow of minimum cost. The minimum-cost flow problem is the problem of finding such a minimum-cost flow. Theorem 1 (). A solution to gcc(X, l, u) corresponds to a feasible s ? t flow of value n in the graph G = (V, A) with vertex set and edge set where V = X ? DX ? {s, t} A = As?X ? AX?DX ? ADX ?t, As?X = {(s,
1987	2002	variables. Régin developed a domain consistency algorithm for the gcc, making use of network flows . A variant of the gcc is the costgcc, which can be seen as a weighted version of the gcc . For the cost-gcc a weight is assigned to each variable-value assignment and the goal is to satisfy the gcc with minimum total cost. Throughout this section, we will use the following notation
1987	2002	all variables need to be assigned to a value and the cost function exactly measures the variable-based cost of violation. ? The graph Gvar corresponds to a particular instance of the cost-gcc . Hence, we can apply the filtering procedures developed for that constraint directly to the soft gcc. The soft gcc also inherits from the cost-gcc the time complexity of achieving domain
1987	2002	variables. Régin developed a domain consistency algorithm for the gcc, making use of network flows . A variant of the gcc is the costgcc, which can be seen as a weighted version of the gcc . For the cost-gcc a weight is assigned to each variable-value assignment and the goal is to satisfy the gcc with minimum total cost. Throughout this section, we will use the following notation
1987	2002	all variables need to be assigned to a value and the cost function exactly measures the variable-based cost of violation. ? The graph Gvar corresponds to a particular instance of the cost-gcc . Hence, we can apply the filtering procedures developed for that constraint directly to the soft gcc. The soft gcc also inherits from the cost-gcc the time complexity of achieving domain
1987	2003	been extended to the Weighted-CSP , associating a degree of violation (not just a boolean value) to each constraint and minimizing the sum of all weighted violations. The Possibilistic-CSP  associates a preference to each constraint (a real value between 0 and 1) representing its importance. The objective of the framework is the hierarchical satisfaction of the most important
1987	2004	different from the previous ones since the aggregation operator is a min/max function instead of addition. Max-CSPs are typically encoded and solved with one of two generic paradigms: valued-CSPs  and semi-rings .sAnother approach to model and solve over-constrained problems involves Meta-Constraints . The idea behind this technique is to introduce a set of domain variables Z that
2006	2007	with hierarchy-expressing rewriting rules used to zoom in and out  and to manage and display a derivation ; use of hierarchical graphs in a formal approach to plan generation ; use of hierarchically distributed graph transformations . 2' A A 1' 3' B C B C Cs15 11. Mr. Maggraphen: A lot of our C code performs graph inspections. How can we translate this into
2006	2025	The presence of such hierarchy-crossing edges greatly complicates the construction of tools for hierarchical graph rewriting. Various notations for hierarchical graph structures are described in  . Hierarchical structure assists in the display of a large graph. Zoom-in and zoom-out operations reduce the graph to manageable proportions for viewing, or delimit selected portions of the
2006	2032	processing . A chart-based parser for hierarchical graphs is discussed in . More recently, Klauck reports on a heuristically-driven chart parser and it’s application to CAD/CAM . On a related note, Henderson and Samal discuss efficient parsing of stratified shape grammars, building on the tabledriven methods used for LR(k) string grammars ; these techniques might
2006	2046	can be modularized, with some modules transforming local graphs, others changing interfaces or the global graph, and yet others changing the graph hierarchy (split or join local graphs)  . Inheritance Inheritance is a powerful tool for layering in object-oriented system design. Several forms of inheritance can be used within a graph-rewriting system; some examples are mentioned
2006	2046	out  and to manage and display a derivation ; use of hierarchical graphs in a formal approach to plan generation ; use of hierarchically distributed graph transformations . 2' A A 1' 3' B C B C Cs15 11. Mr. Maggraphen: A lot of our C code performs graph inspections. How can we translate this into graph-rewrite rules? The Maggraphens’ current software freely mixes
2047	2050	implementation and its embedded test cases. With such definition the trustability of a component will be based on the consistency between these three aspects. In a “design-by-contract” approach , the specification is systematically derived in executable contracts (class invariants, pre/post condition assertions for class methods). If contracts are complete enough, they should be violated
2047	2055	after its creation. Suppress a clone or copy instruction. Insert a clone instruction for each reference affectation. The mutation operators AOR, LOR, ROR and NOR are traditional mutation operators , the other operators having been introduced in this paper for the object-oriented domain. The data perturbation operator VCP allows to disturb state of data and to obtain a sensitivity analysis of
2047	2055	has been defined based on the quality of their associatedstests (itself based on fault injection). For measuring test quality, the presented approach differs from classical mutation analysis  as follows: - a reduced set of mutation operators is needed, - oracle functions are integrated to the component, while classical mutation analysis uses differences between original program and
2047	2056	after its creation. Suppress a clone or copy instruction. Insert a clone instruction for each reference affectation. The mutation operators AOR, LOR, ROR and NOR are traditional mutation operators , the other operators having been introduced in this paper for the object-oriented domain. The data perturbation operator VCP allows to disturb state of data and to obtain a sensitivity analysis of
9070367	2081	iPSC/2  and SGI Origin 2000  are examples of commercial systems that are based on the hypercube. Existing multicomputers  have widely used wormhole routing . This is due to its low buffering requirement, and more importantly it makes latency independent of the message distance under light traffic loads. In wormhole routing, a message is divided into
9070367	2082	on the way messages visit the virtual channels; a virtual channel has its own flit queue, but shares the bandwidth of the physical channel with other virtual channels in a timemultiplexed manner . A typical example of a deadlock-free routing widely used in practice is deterministic routing where messages visit dimensions in a predefined order. Deterministic routing has been popular in
9070367	2082	queue by a factor, V V , representing the average degree of V virtual channels multiplexing, that takes place at a physical channel. The factor V V can be estimated using the following formula  V V ? l= 0 V = V l ? l= 0 2 lP P l l (18)swhere P l ) 0 ( ? l ? V is the probability that l virtual channels at a given physical channel are busy (and is computed using equation 10). In the event
9070367	356	proposed algorithms require only one extra virtual channel per physical channel, compared to deterministic routing, allowing for an efficient router implementation. For instance, Duato’s algorithm  divides the virtual channels associated with each physical channel into two classes: a and b. At each routing step, a message visits adaptively any available virtual channel from class a. If all
9070367	356	forced to share the bandwidth of the physical channels. To reduce the effects of virtual channels multiplexing, Duato  has introduced to his original adaptive routing algorithm described in  a time-out mechanism when selecting a particular class of virtual channels. When a message is blocked upon reaching a given router, it waits for a fixed time period for one of the virtual channels
9070367	356	before they can be widely adopted in commercial multicomputers. Except from the models suggested in  for the simple version of Duato’s algorithm (i.e., with no time-out mechanism) , there has not been any model proposed in the literature for any other adaptive routing algorithms, e.g. that of . As a result, most existing studies , including
9070367	356	it suffers time-out, and as a result the message has to wait for the deterministic virtual channel corresponding to the lowest dimension still to be crossed according to deterministic routing . It is assumed that the probability of time-out at a given channel is independent of the subsequent channels. 2.3 The Communication Model Even though the proposed model can deal with different
9070367	2092	networks for practical multicomputers due to its desirable properties, including regularity, symmetry, low diameter and high connectivity. The iPSC/860 , iPSC/2  and SGI Origin 2000  are examples of commercial systems that are based on the hypercube. Existing multicomputers  have widely used wormhole routing . This is due to its low buffering
8918753	2102	With a sufficiently long prefix, inter-symbol interference (ISI) can be completely avoided, thus accommodating high data rate transmission. The performance of OFDM can be greatly enhanced by STBC  through the employment of transmit diversity. Guang-Hua Yang, Dongxu Shen, Victor O.K. Li Department of Electrical and Electronic Engineering The University of Hong Kong Pokfulam Road, Hong Kong
8918753	2102	block coded OFDM system Space-time coding (STC) achieves diversity gain through transmit diversity. An important class of STC is the space-time block code (STBC) which is proposed by Alamouti  and generalized by Tarokh . The employment of STBC requires the channel to be flat. Thus, OFDM is particularly suitable for employing STBC over broadband frequency selective fading channels. The
8918753	2102	for all the sub-channels. In our proposed scheme, sub-channel partitioning is performed at the receiver side based on the estimated channel information, which is essential to the decoding of STBC . The group membership for each sub-channel can be indicated by a number of bits and the whole partition results can be represented by the PV. When there are two groups, a single bit is sufficient
8918753	2102	computer simulations. A. System Parameters and Channel Model In Table I, we list the OFDM parameters used in the simulations. They are identical to those of IEEE 802.11a . Almouti’s STBC scheme  is employed. The delay profile of indoor wideband channel model B (see Table II), provided in ITU-R recommendation , is adopted for the simulation of uncorrelated multipath Rayleigh fading
8918753	2102	0 2 4 6 8 10 12 14 16 18 20 Eb/N0 (dB) Fig. 8. PSNR performance of different schemes. sented in Fig. 9 (a). The improvement due to STBC is dramatic. For each scheme, according to the BER trend , the performance improves as the configuration goes from 1) to 2) to 3), as expected. Further, our proposed UEP scheme has better performance than FEC-based UEP scheme for all the cases. We notice
8918753	2105	Space-time coding (STC) achieves diversity gain through transmit diversity. An important class of STC is the space-time block code (STBC) which is proposed by Alamouti  and generalized by Tarokh . The employment of STBC requires the channel to be flat. Thus, OFDM is particularly suitable for employing STBC over broadband frequency selective fading channels. The STBC-OFDM system we study is
8918753	2105	of sub-channels in an OFDM symbol. Then the pair of OFDM symbols in an STBC block are denoted as 1 Our scheme can be easily extended to multiple receive antennas, as well as other STBC schemes . h2 MB 1 MB 4 Y2 Y4 MB 2 MB 5 MB 6 Slice C B C R B4 B4 B4 B4 B4 B4 B5 B5 B5 B5 B5 VLC1 VLC2 VLC3 VLC4 VLC5 EOB VLC1 VLC2 VLC3 VLC4 EOB h1 B5 EOB Prefix Removal S->P & DFT MB 7 8 8 HP Layer LP Layer
8918753	2105	where si,k ,i =1, 2, k ?  is the modulated symbol on sub-channel k of OFDM symbol i, and T represents the transpose operation. The operation of STBC is given by the transmission matrix  ? ? s1 s2 G2 = (2) 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004 ?s ? 2 s ? 1 (1)swhere ? represents the complex conjugate operation. More specifically, at the first time slot, s1 and s2
3695299	2113	ranges approximately between 1.6 to 1.9 times the optimal solution. VIII. EXTENSIONS AND FUTURE WORK There has been some work on survivability of networks against mulitiple link failures . So far, we have presented our results, for the basic version of the problem, where we want to protect against a single link failure and the links are bidirectional and symmetric in both direction.
3695299	2117	one the over build can be significant and second it is hard to find the smallest cycle cover of a given network . An improvement to these schemes are those based on the notion of p-cycle . Here the main idea is that a cycle can be used to protect not just the links on the cycle but also the chords (spokes) of the cycle, thus showing that far fewer rings may be sufficient for
3695299	2120	extensions and future work. Section VII presents our simulation results. II. BACKGROUND AND RELATED WORK In general the protection schemes for optical and MPLS networks can be classified ( , ), based on whether the protection is local (link based) or end-to-end (path based), and whether the backup resources are dedicated or shared. Fast or local reroute mechanisms, outlined earlier, are
3695299	2121	resources, thus resulting in efficient capacity utilization. Two different techniques for local protection in MPLS networks have been proposed . The one-to-one backup technique    creates bypass LSPs for each protected service carrying LSP, at each potential point (link or node) of local repair. The facility backup technique  creates a bypass tunnel to protect a
3695299	2121	about extensions and future work. Section VII presents our simulation results. II. BACKGROUND AND RELATED WORK In general the protection schemes for optical and MPLS networks can be classified ( , ), based on whether the protection is local (link based) or end-to-end (path based), and whether the backup resources are dedicated or shared. Fast or local reroute mechanisms, outlined
3695299	2123	which ranges approximately between 1.6 to 1.9 times the optimal solution. VIII. EXTENSIONS AND FUTURE WORK There has been some work on survivability of networks against mulitiple link failures . So far, we have presented our results, for the basic version of the problem, where we want to protect against a single link failure and the links are bidirectional and symmetric in both
2133	2134	effective in practice and greatly assisted our support staff in maintaining our Ethernet. 1 Introduction The Ethernet was invented by Bob Metcalfe and others at the Xerox Palo Alto Research Center  and soon developed into an international standard . One aspect that made the Ethernet so attractive was its use of a passive communications medium: a thick coaxial cable into which taps could be
2133	2231	at will. And they were. I was a graduate student at Carnegie Mellon University (CMU) in the 1980’s as the Ethernet was deployed on campus, mostly using the early Digital-Intel-Xerox (DIX) standard . We observed that as soon as an Ethernet coax came near a group’s computers, taps would appear as if by magic. The Ethernet was such an elegantly simple and useful method of connecting computers
2133	2136	operational behavior, using the system’s own equipment is more cost effective. Also, monitoring actual packet delivery between end stations provides an end-to-end check, which is good practice . To verify that stations X and Y can communicate, the straightforward approach is to cause X to send Y a request that causes Y to send X a corresponding response. This is the strategy of Ping ,
2133	2234	request that causes Y to send X a corresponding response. This is the strategy of Ping , which is arguably the network administrator’s most often used tool. Ping uses the IP / ICMP ECHO protocol , but a similar protocol could easily be designed for the Ethernet link layer. Although theoretically a defect could discriminate between application traffic and ping traffic, causing pings to work
2133	2139	transform w? ( u, v) = w( u, v) + h( u) ? h( v) where u and v are nodes and w(u,v) is the edge weight from u to v. It is well-known that this transform leaves the weights of all cycles unchanged . Although one might try to apply alpha-scaling to produce a “cannonical” or “balanced” matrix of probability estimates, we did not do so. We just took whatever the results were that we got from the
2154	2142	This mechanism is also used in . IEEE 802.11 relies on the DCF method to coordinate the transmission of packets. The packet transmission sequence is illustrated in Figure 3. Similar to , we measure the throughput of transmitting a packet ????? ? ? ??? ??? as , ? where is the size of the packet, ??? is the time-stamp that the packet is ready at the MAC layer, and ??? is the
2154	2143	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2143	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2145	and implement our bandwidth management scheme for a wireless network consisting of heterogeneous computers and devices connected together over the IEEE 802.11 wireless MAC protocol. Unlike in , where a basestation determines the schedule of transmission for the entire network and all communication is via the base-station, in our network, transmission is distributed and peer-to-peer. The
2154	2145	reduction. 4 Related Work Past research in wireless bandwidth management has mostly focused on flow scheduling at the base-station to achieve fairness between flows competing for the channel (e.g. ). In contrast, we use a peer-to-peer MAC layer transmission model rather than the base-station model. In , the authors propose an admission control scheme for a peer-to-peer, single-hop, ad hoc
2154	2146	as perceived by different nodes in the network at the same time can also be different. The latter phenomenon is due to errors and interference that are location-dependent. Furthermore, as shown in , the overall throughput of the network can also vary dynamically as flows arrive and depart, and the number of active stations changes. The BM must therefore not just deal with admission control
2154	2148	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2149	RA will again re-negotiate to release the excess channel time proportion. This solution is equivalent to splitting up a VBR stream in the time domain into multiple CBR streams, as has been done in  in the context of ATM networks. Since this scheme only involves re-organizing the traffic rather than the network, it can be directly applied from ATM networks to wireless networks. Our solution is
2154	2150	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2150	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2151	This mechanism is also used in . IEEE 802.11 relies on the DCF method to coordinate the transmission of packets. The packet transmission sequence is illustrated in Figure 3. Similar to , we measure the throughput of transmitting a packet ????? ? ? ??? ??? as , ? where is the size of the packet, ??? is the time-stamp that the packet is ready at the MAC layer, and ??? is the
2154	2152	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2152	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2153	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2153	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2154	for this purpose. The entry for the terminating flow in the BM’s flow table is expunged. A teardown¨ 3 The computational complexity of this algorithm is ? ????? ? . A detailed pseudo-code is in .sacknowledgement message is sent to ¨ ’s RA 4 . Change in Flow’s Perception of Total Network Bandwidth: The RA of every flow periodically obtains from the TBE the flow’s current perceived total
2154	2154	the noise introduced by the measured raw throughput from packets with different sizes. We have also verified that using recent packets to estimate current channel bandwidth is feasible and robust . Throughput (bytes/sec) 250000 200000 150000 100000 50000 raw throughput normalized throughput 0 0 100 200 300 400 500 600 700 800 900 1000 Time (sec) Figure 4. Raw throughput and normalized
2154	2154	from the TBE to the RA. The other is to increase the tolerance to changes in perceived bandwidth © ? ??¨?? . The effect of these optimizations on overhead and performance are discussed in detail in . Our results show that for a small price in terms of performance (we quantify fairness and throughput jitter, the key performance metrics, in ), we can obtain large gains in overhead reduction.
2154	2155	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2155	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2157	this scheme only involves re-organizing the traffic rather than the network, it can be directly applied from ATM networks to wireless networks. Our solution is also similar to that proposed in  for CDMA networks. Frequent bursts could result in an explosion in re-negotiation overhead. We deal with the problem of frequent bursts in one of two ways: (a) adjusting © ????? ??¨?? for VBR flow
8918757	2283	pits and manure, average yields rose by 640 kg/ha compared to the control plots (Table 1). The additional gains due to the addition of inorganic fertilizer proved biggest in years of good rainfall (1994), though in other years (1996) the additional yield would not be sufficient to cover the costs of inorganic fertilizers. Similar trials over two seasons in Mali indicate that zaï pits plus manure
8918757	2283	the compost, the cost of maintenance of compost pit, the cost of emptying the pit as well as the costs of transporting the compost to the fields 18 18 These costs are derived from Sidibe et al. (1994) who measured them in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based
8918757	2292	in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based on Roose et al. (1999). Sidibe et al. (1994) measured the labor requirements for digging the compost pit and filling it. A compost of 10almost 11 m3 is needed to produce 2,5 tons of compost. Crop yields and prices vary
8918757	2292	order to generate sufficient runoff. Because the digging of zaï requires a substantial input of labor, this implies that a relatively high population density would facilitate its spreading. Freeman (1999) has tried to map the range of proven soil management practices in West Africa using digital maps and concluded that there also is a potential for expansion of zaï to, for instance, Eastern Senegal
8918757	2293	in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based on Roose et al. (1999). Sidibe et al. (1994) measured the labor requirements for digging the compost pit and filling it. A compost of 10almost 11 m3 is needed to produce 2,5 tons of compost. Crop yields and prices vary
8918757	2293	order to generate sufficient runoff. Because the digging of zaï requires a substantial input of labor, this implies that a relatively high population density would facilitate its spreading. Freeman (1999) has tried to map the range of proven soil management practices in West Africa using digital maps and concluded that there also is a potential for expansion of zaï to, for instance, Eastern Senegal
8918757	2294	in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based on Roose et al. (1999). Sidibe et al. (1994) measured the labor requirements for digging the compost pit and filling it. A compost of 10almost 11 m3 is needed to produce 2,5 tons of compost. Crop yields and prices vary
8918757	2294	order to generate sufficient runoff. Because the digging of zaï requires a substantial input of labor, this implies that a relatively high population density would facilitate its spreading. Freeman (1999) has tried to map the range of proven soil management practices in West Africa using digital maps and concluded that there also is a potential for expansion of zaï to, for instance, Eastern Senegal
8918757	2299	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2299	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918757	2300	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2300	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918757	2302	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2302	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918757	2303	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2303	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918783	2248	conventional filter methods because of its low inner-class average and its high deviation. In supervised classification problems, various wrapper methods such as Recursive Feature Elimination (RFE)  are proposed for feature selection. Since the main purpose of the wrapper method is to improve the performance of the classification algorithm, the genes chosen by the method have not been paid
8918783	2249	classification algorithm in order to extract the discriminative genes that difficult to be extracted by conventional filter methods. RFE method based on nonlinear Support Vector Machines (SVMs)  is employed to this end because it is successfully applied to classification of gene expression data. We investigate the genes extracted by the RFE method based on SVMs with gaussian kernel
8918785	2274	synsets (e.g. respectively: {local#2} (adj.), {area#1, country#4}, {network#2, communications network#1 }) in order to enrich WordNet with new domain concepts and learn domain-specific ontologies ; ??? disambiguate WordNet glosses ; – disambiguate words in a query for sense-based web query expansion . Semantic disambiguation is performed using a method we have named structural semantic
8918785	2277	algorithm, which is rather complex. A thorough description is in , but a complete reformalization is in progress. SSI is a kind of structural pattern recognition. Structural pattern recognition  has proven to be effective when the objects to be classified contain an inherent, identifiable organization, such as image data and time-series data. For these objects, a representation based on a
2278	2279	traffic. For example, as it is pointed out in , an 18 bytes of search string in a Query message may cause 90 megabytes of data to be forwarded by the P2P network peers. As another example,  states that total number of messages including the responses triggered by a single Query message can be as large as (assuming 4 connections per peer): ? C ? (C ? 1) i = 26240 (1) T T L 2 ? i=0 •
2278	173	have observed the existence of high degrees of free riding in P2P networks and they suggest that free riding may be an important threat against the existence and efficient operation of P2P networks . There may be various reasons and motivations behind free riding. For example, peers with a Network Address Translation (NAT) address may act as a free rider. Bandwidth limitation would be another
2278	173	shared earlier . Moreover, a large number of free riders and their queries will generate a great amount of P2P network traffic, which may lead 2 1% of the peers provides 37% of the content .sto degradation of P2P services. Furthermore, underlying available network capacity and resources will be decreased by free riders, which will cause extra delay and congestion to non-P2P traffic.
2278	173	the possible attacks against the proposed mechanisms by free riders. Finally, the conclusions are presented in section 5. 2 Related Work User traffic on Gnutella network is extensively analyzed in  and it is observed that 70% of peers do not share any file at all. Furthermore, 63% of the peers who share some files do not respond to any queries. Another interesting observation is that 25% of
2278	2281	in the search horizon. As the peers age in the network, they begin not to find interesting files and may leave the system for good with all the files and resources that they have shared earlier . Moreover, a large number of free riders and their queries will generate a great amount of P2P network traffic, which may lead 2 1% of the peers provides 37% of the content .sto degradation of
2278	2281	4 hours, while only 25% of the peers are alive for more than 24 hours. In another work , the median session duration of both Napster and Gnutella clients is about 60 minutes. In a similar work , 90% of average session lengths of Kazaa clients is found to be about 30 minutes. As a result, it can be assumed that peers stay connected long enough to collect statistical information about them
2278	2283	about them and take necessary actions. Another issue is whether a monitoring peer can snoop and monitor enough number of messages that are coming from or going towards the neighboring peers. In , it is reported that the average number of queries per second for three peers located at different geographic locations is about 50. Also, about 30 query responses per second are recorded. This
2278	2284	In a free riding environment, a small number of peers will serve for all other peers. Therefore, many download requests will be directed towards these peers which may lead to scalability problems . Renewal of content or presenting interesting content may decrease in time, thus the number of shared files may become limited or may grow very slowly. Faulttolerance property of P2P networks may
2278	2284	in the search horizon. As the peers age in the network, they begin not to find interesting files and may leave the system for good with all the files and resources that they have shared earlier . Moreover, a large number of free riders and their queries will generate a great amount of P2P network traffic, which may lead 2 1% of the peers provides 37% of the content .sto degradation of
2278	2284	amount of free riding in Gnutella network as well as in Napster . Another interesting observation is that 7% of the peers together provide more files than all of the other remaining peers. In , Ramaswamy and Liu concentrate on how to prevent free riding. They propose to calculate a utility function for each peer in order to estimate its usefulness to all community. According to the
2278	2284	and cheated by writing some malicious client programs. In a recent work , Vishnumurthy et.al. suggest using a single scalar value, called Karma, to evaluate a peer’s utility to a system like in . Each peer has an account consisting of Karma. When a peer uploads a file to a requesting peer, it gets some amount of Karma from the requesting peer. On the other hand, if the peer downloads a
2278	2284	peers, called the bank-set, in order to ensure Karma against loose and tampering. The transfer of Karma between peers is executed through bank-set of each peer. The main difference from the work in  is that the utility value of a peer is not stored at the peer itself but at some other peers. However, to make the scheme work, a group of peers must be known to store Karma value. Whenever a
2278	2285	network peers can join and leave the system at any time. We can find some related work in the literature about the network topology dynamics and peer characteristics of P2P applications. In , it is stated that about 40% of the peers leave the Gunetella network in less than 4 hours, while only 25% of the peers are alive for more than 24 hours. In another work , the median session
2278	188	property of P2P networks may be adversely affected due to the fact that a very small portion of the peers provides most of the content 2 . This also leads to a client-server like paradigm  and decreases P2P network advantages. Quality of search process may degrade due to increasing number of free riders in the search horizon. As the peers age in the network, they begin not to find
2278	188	25% of the peers provide 99% of the whole content in the network. In a more recent work, Saroui et. al. confirm that there is a large amount of free riding in Gnutella network as well as in Napster . Another interesting observation is that 7% of the peers together provide more files than all of the other remaining peers. In , Ramaswamy and Liu concentrate on how to prevent free riding. They
2278	188	of P2P applications. In , it is stated that about 40% of the peers leave the Gunetella network in less than 4 hours, while only 25% of the peers are alive for more than 24 hours. In another work , the median session duration of both Napster and Gnutella clients is about 60 minutes. In a similar work , 90% of average session lengths of Kazaa clients is found to be about 30 minutes. As a
2288	2289	range. Routing protocols must be able to cope efficiently with this mobility. A lot of topology-based routing protocols have been proposed that either establish a route on-demand (e.g. AODV , DSR ) or proactively maintain hop-by-hop information at each node (e.g. OLSR , TBPRF ). In case of link incidents, new routes need to be discovered and updated routing information needs
2288	2289	Protocols that avoid beaconing completely fall into the third group. Several approaches are described in the literature to mitigate the drawbacks of link incidents for topologybased protocols. AODV  implements a local route repair mechanism which aims to replace a particular broken link with an alternate path between the two nodes and to minimize the latency and induced routing overhead of
2288	2289	vmax. For example, TABLE I EXPECTED SPEED DIFFERENCE OF TWO NODES vmin  vmax  ES  1 10 5.69 1 20 9.64 1 40 16.68 10 20 18.83 10 40 29.55 we can see that that for a speed interval  m/s, two arbitrary nodes move relative to each other with almost 10 m/s. Suppose now node A moves a distance d to A’. The size of the area A(r, d), which was initially covered by node A’s
2288	2289	indicate time-out intervals t in seconds and speed intervals  in meters per second, respectively. TABLE II EXPECTED PERCENTAGE OF OUT-DATED NEIGHBORS FOR r = 250 m/r = 100 m  m/s  m/s  m/s 1 1.45/3.62 2.45/6.13 4.25/10.61 3 4.35/10.85 7.36/18.35 12.72/31.52 5 7.24/18.05 12.25/30.39 21.14/51.51 10 14.46/35.73 24.40/58.90 41.67/92.09 Even for a large r = 250 m and for
2288	2289	performance such that the impact of the proposed enhancements can be observed more easily. In Fig. 3 and Fig. 4, the delivery ratio and the end-to-end delay are depicted for a speed interval of  m/s. There are three reasons why we decided to use this high speed interval. First, even though the speed interval may seem high, the average speed of the nodes is only about 10 m/s. Second, we
2288	2292	routing protocols have been proposed that either establish a route on-demand (e.g. AODV , DSR ) or proactively maintain hop-by-hop information at each node (e.g. OLSR , TBPRF ). In case of link incidents, new routes need to be discovered and updated routing information needs to be distributed by (partially) flooding the network, which may cause long latencies and
2288	2293	current node (e.g. provided by GPS), the positions of neighboring nodes (by nodes periodically transmitting a hello message, called beacon) and the destination (e.g. obtained via a location service ). Each packet is routed independently at each node and forwarded to a neighboring node which reduces the distance to the destination. These protocols are inherently more robust to changes in the
2288	2294	All these properties make them e.g. especially suited for sensor and vehicular adhoc networks. An overview of position-based routing algorithms and location services can be found e.g. in  and . In position-based routing protocols, nodes periodically broadcast beacons to announce their presence and location to their neighbors. Each node stores all neighbors and their current
2288	2296	a particular broken link with an alternate path between the two nodes and to minimize the latency and induced routing overhead of link incidents. To avoid complete disruption of communication,  investigated the expected lifetime of routes in order to schedule the route discovery before actual link break. Unlike these protocols using hopcount as the routing metric, several other protocols
2288	2299	in the first place. In ABR  the lifetime of a link is taken into account, whereas SSA  also considers feedback from the link layer about signal strength as primary routing metric. In  and  results from analytical derivations and observations made by simulations are used to design new routing metrics which favor more stable paths. Based on link availability estimations, a
2288	2300	the first place. In ABR  the lifetime of a link is taken into account, whereas SSA  also considers feedback from the link layer about signal strength as primary routing metric. In  and  results from analytical derivations and observations made by simulations are used to design new routing metrics which favor more stable paths. Based on link availability estimations, a metric for
2288	2302	positions of nodes, information about the velocity and direction are also often known and can be utilized to estimate the expiration time of a link and to reconfigure routes timely as proposed in . Unlike , where GPS-information is only applied to maintain routes,  additionally makes use of location information in the routing decision itself to establish paths in a depth-first search
2288	2303	known and can be utilized to estimate the expiration time of a link and to reconfigure routes timely as proposed in . Unlike , where GPS-information is only applied to maintain routes,  additionally makes use of location information in the routing decision itself to establish paths in a depth-first search way. In , factors that influence the utility of hello messages were
2288	2304	, where GPS-information is only applied to maintain routes,  additionally makes use of location information in the routing decision itself to establish paths in a depth-first search way. In , factors that influence the utility of hello messages were studied for determining link connectivity in topologybased protocols. Out-dated and inaccurate neighbor tables in position-based routing
2288	2305	protocols, almost no research has been performed on link incidents and inaccurate neighbor tables in position-based routing protocols. To the best of our knowledge, the only exception is GPSR , where the authors compared the packet delivery ratio and routing overhead for different time intervals between beacons. Even though beaconing was not explicitly studied, the determination of the
2288	2305	has meanwhile left the transmission range. And thirdly, most position-based routing algorithms apply a forwarding strategy which forwards packets to nodes close to the destination, e.g. , . As a consequence, the selected neighbor is close to the border of the transmission range and thus has an even higher probability to have left the transmission range. All these factors contribute
2288	2305	strong are considered as neighbors. Unlike for beacons, data packets received at any power level are processed. VI. PROTOCOLS As a representative of a position-based routing protocol, we use GPSR . GPSR also serves as basis for all proposed enhancements, i.e. only the enhancement under consideration is modified in the original GPSR. We consider also a reactive GPSR were nodes only transmit
2288	2305	information from the global data of the simulator. The aim is to determine the performance limits of any position-based routing protocol. A. GPSR As the underlying routing protocol, we use GPSR  which is basically an extension of GFG  with MAClayer enhancements. A packet is routed in a greedy manner towards the position of the destination. Each node selects the node from its neighbor
2288	2305	it enters perimeter mode and the packet is routed according to the right-hand rule on the faces of a locally extracted planar subgraph to avoid loops and to recover from this local minimum (see  for more details). As soon as the packet arrives at a node closer to the destination than where it entered perimeter mode, the packet switches back to greedy routing. It was shown that this
2288	2307	the Home Location Register to keep track of a node’s position if it has moved to a new cell. Dissemination and replication of data in repositories for mobile ad-hoc networks was studied e.g. in . The authors propose different strategies when to trigger updates. These approaches differ from our investigation in many points, mainly that information is transmitted infrequently to some few
2288	2308	rather frequently. Furthermore, entries do not need to be periodically refreshed to remain valid. Lately, several protocols have been proposed which adopt a new paradigm for position-based routing , , , and . The next hop is not determined at the sender, but in a distributed way at the receivers. Nodes do not rely on information about neighbors anymore and allow disposing
2288	2308	vmax. For example, TABLE I EXPECTED SPEED DIFFERENCE OF TWO NODES vmin  vmax  ES  1 10 5.69 1 20 9.64 1 40 16.68 10 20 18.83 10 40 29.55 we can see that that for a speed interval  m/s, two arbitrary nodes move relative to each other with almost 10 m/s. Suppose now node A moves a distance d to A’. The size of the area A(r, d), which was initially covered by node A’s
2288	2308	indicate time-out intervals t in seconds and speed intervals  in meters per second, respectively. TABLE II EXPECTED PERCENTAGE OF OUT-DATED NEIGHBORS FOR r = 250 m/r = 100 m  m/s  m/s  m/s 1 1.45/3.62 2.45/6.13 4.25/10.61 3 4.35/10.85 7.36/18.35 12.72/31.52 5 7.24/18.05 12.25/30.39 21.14/51.51 10 14.46/35.73 24.40/58.90 41.67/92.09 Even for a large r = 250 m and for
2288	2308	definitely the differences in the results. And finally we consider speed as a proxy for any kind of topology changes as mentioned in section III. We ran the simulation also with a speed interval of  m/s. The results are similar, except that the delay and the packet loss rate are about 30% and 50% lower respectively. As expected the performance suffers in case of low pause times for all
2288	2310	Furthermore, entries do not need to be periodically refreshed to remain valid. Lately, several protocols have been proposed which adopt a new paradigm for position-based routing , , , and . The next hop is not determined at the sender, but in a distributed way at the receivers. Nodes do not rely on information about neighbors anymore and allow disposing beaconing
2288	2311	entries do not need to be periodically refreshed to remain valid. Lately, several protocols have been proposed which adopt a new paradigm for position-based routing , , , and . The next hop is not determined at the sender, but in a distributed way at the receivers. Nodes do not rely on information about neighbors anymore and allow disposing beaconing completely. These
2288	2312	a neighbor has meanwhile left the transmission range. And thirdly, most position-based routing algorithms apply a forwarding strategy which forwards packets to nodes close to the destination, e.g. , . As a consequence, the selected neighbor is close to the border of the transmission range and thus has an even higher probability to have left the transmission range. All these factors
2288	2312	simulator. The aim is to determine the performance limits of any position-based routing protocol. A. GPSR As the underlying routing protocol, we use GPSR  which is basically an extension of GFG  with MAClayer enhancements. A packet is routed in a greedy manner towards the position of the destination. Each node selects the node from its neighbor table which is geographically closest to the
2288	2313	This assumption is valid as nodes move independently of each other and have symmetric transmission ranges. The probability density function fS of the nodal speed s is derived e.g. already in  and is given by fS(s) = s ln 1 ? vmax vmin We first derive the expected value of the relative speed vector, i.e. the difference of two arbitrary speed vectors, in the unbounded random waypoint
2288	2313	placed in a rectangular area of 600m x 3000m. The nodes move according to the random waypoint mobility model. We implemented the stationary distribution of the random waypoint model as described in . Thereby the simulations do not need an initial warm-up phase, whose duration is difficult to predict, to reach a stable state. The simulations last for 900s and data transmission starts at 180s
2288	2315	interval and the time-out interval is set to 1.5 s and 6.75 s respectively. The minimum speed is set unequalsIEEE INFOCOM 2005 8 0 m/s as otherwise the stationary distribution would be static . The following subsection VII-B provides further explanation for the chosen values. The possible combinations and variations for all the parameters of the beaconing strategies, the prediction, and
2350	1623	followings. For example, some routes in the Internet differentiate the service among different classes of data flows by giving priority to specific classes of data flows; see, e.g., Blake, et al. . ??? In general, one would expect that the network be stable under the normal offered load condition. From the above counter-example, we have seen that the bandwidth capacity realization 4 ?sproblem
2350	2351	for more details. A stability result for the network with this class of utility functions is also presented in their paper. Their stability result, as well the earlier one in de Veciana et al. , is a significant step toward a better understanding of the stability issue of the Internet, noting that any utility function is only an approximation to the precise behavior of TCP. Moreover, it
2350	2243	academics and the telecommunication industry. Currently, the majority of the Internet traffic is dominated by various versions of TCP (the Transmission Control Protocol; see for example Jacobson ), and a lot of efforts have been placed on the study of the TCP congestion control. However, due to its complex nature, the behavior of the Internet traffic is not fully understood yet and remains
2350	2354	proportionally fairness criterion are stable under the normal offered load condition. Readers are referred to Bertsekas and Gallager  for detailed descriptions on the max-min fairness, and Kelly  and Kelly, et al.  on the proportionally fairness. In Bonald and Massoulie , a stability result is also established for networks employing a class of (p, ?)-proportionally fair bandwidth
2350	2355	offered load condition, a data network is stable when the bandwidth of the network is allocated so as to maximize a class of general utility functions. Using the microscopic model proposed by Kelly  for a TCP congestion control algorithm, we argue that the bandwidth allocation in the network dominated by this algorithm can be modelled as our bandwidth allocation model, and hence that the
2350	2355	that the network under a bandwidth allocation that maximize a class of more general utility functions is stable under the normal offered load condition; see Section 2. By using a result in Kelly , which models the microscopic behavior of a TCP congestion control algorithm, we show that our bandwidth allocation model does capture some important characteristics of the macroscopic behavior of
2350	2355	light on the connection level stability issue of the Internet, though we should realize that it is impossible to provide a complete and satisfactory answer to such an issue in this study. In Kelly , the bandwidth allocation for TCP is modelled at the microscopic level as follows. , this model approximates the bandwidth allocation of a network
2350	2355	being within the link capacity should be admissible. Finally, this stability result, combined with a recent result in the microscopic modelling of a TCP congestion control algorithm in Kelly , is used to heuristically show the stability of the network dominated by the TCP congestion control algorithm under the normal offered load condition. An assumption of the bandwidth allocation in
2350	2358	and satisfactory answer to such an issue in this study. In Kelly , the bandwidth allocation for TCP is modelled at the microscopic level as follows. , this model approximates the bandwidth allocation of a network dominated by (various versions of) TCP Reno.] For any given number xr of ongoing connections at each route, the bandwidth allocation
2350	2360	result is also established for networks employing a class of (p, ?)-proportionally fair bandwidth allocation policies. This class of bandwidth allocation policies, first proposed by Mo and Walrand , include the bandwidth allocation policies satisfying the proportional fairness criterion and the minimal potential delay criterion as special cases. (We should brief these bandwidth allocation
2361	2363	diversity of classifiers can be achieved on the basis of two approaches: a DT ensemble technique  and an averaging technique based on Bayesian Markov Chain Monte Carlo (MCMC) methodology . Both DT techniques match the above requirements well and have revealed promising results when applied to some real-world problems . By definition, DTs consist of splitting nodes and
2361	2363	some data is calculated for each terminal node . The Bayesian generalization of tree models required to evaluate the posterior distribution of the trees has been given by Chipman et al. . Denison et al.  have suggested MCMC techniques for evaluating the posterior distribution of decision trees. This technique performs a stochastic sampling of the posterior distribution. In this
2361	2363	( | D) , then we can write N N ( ¡ i ¡ ) ¡ ( i) 1 ( i) p( y | x , D) ? p( y | x, , D) p( | D) = p( y | x, , D) . (2) i= 1 N i= 1 This is the basis of the MCMC technique for approximating integrals . To ¡ perform the approximation, we need to generate random samples from p ( | D) by running a Markov Chain until it has converged to the stationary distribution. After this we can draw samples
2361	2363	accepted. As a result, RJMCMC algorithms cannot explore a full posterior distribution. The space which is explored can be extended by using a restarting strategy as Chipman et al. have suggested in . The idea behind the restarting strategy is based on multiple runs of the RJMCMC algorithm with short intervals of burn-in and post burn-in. For each run, the algorithm creates an initial DT with
2385	2388	Schultz, and Adams 1998). As interface development progressed, we saw a need to integrate the language and gestural capabilities of the interface by tracking goals in human/robot interactions (Perzanowski et al 1999). We argued that tracking goals provided us with a means of achieving varying levels of autonomy. Recently, we included a mechanical means of communication with the robots via palm devices
2385	2392	digital assistant (in this case, any of the Palm devices). Speech is initially processed by a speech-to-text system (IBM's ViaVoice), and our natural language understanding system, Nautilus (Wauchope 1994), robustly parses the language input and translates it into a semantic representation which is then mapped to a command after gestural information is incorporated. Gestures can be either distances,
2417	2552	BMS98]. Some of these approaches use the term ‘information architecture’, or ‘architecture of information systems’, while yet others refer to the same concept as ‘enterprise (IT) architecture’. In , the concept of architecture is defined as: “The fundamental organization of a system embodied in its components, their relationships to each other, and to the environment, and the principles
2417	2552	has been decided upon, while responsibilities (such as functionality) have been assigned to the (overall) components of the system. In the conceptual framework for architecture, as defined in , an architectural description can be organised into one or more constituents called architectural views. Each view addresses one or more of the concerns (interests) of the stakeholders of a system.
2417	2422	and architects with the burden of selecting the viewpoints to be used in a specific situation. Some of these frameworks of viewpoints are: The Zachman framework , Kruchten’s 4+1 framework , RM-ODP , ArchiMate  and TOGAF . The aim of this paper is not to provide ‘yet another framework of viewpoints’, but rather to lay a foundation to be able to reason about
2417	2422	of the above three angles in separate sections (sections 2 to 4). To make our results more concrete, section 5 briefly discusses two example frameworks of viewpoints (Kruchten’s 4+1 framework  and RMODP ), from the perspective of our meta-framework. This is followed by a brief discussion on directions of further research and elaboration in section 7. 2 Modelling The aim of this
2417	2422	have been left implicit. Furthermore, the case studies are part of ongoing research efforts. A more detailed elaboration of the cases presented is part of these efforts. 5.1 The ‘4+1’ view model In , Kruchten introduces a framework of viewpoints (a view model) comprising five viewpoints. The use of multiple viewpoints is motivated by the observation that it “allows to address separately the
50485	2433	. This method was generalized to apply more generally to fitting non-convex cost-functions arising in a variety of problems, e.g., finding the optimal wiring for a densely wired computer chip . The method of simulated annealing consists of three functional relationships. 1. g(x): Probability density of state-space of D parameters x ={x i ; i = 1, D}. 2. h(x): Probability density for
50485	2433	examples requiring bona fide ‘‘temperatures’’ and ‘‘energies.’’ Rather, this methodology can be readily extended to any problem for which a reasonable probability density h(x) can be formulated . III. Fast Annealing It was also noted that this methodology can be readily extended to use any reasonable generating function g(x), without relying on the principles underlying the ergodic nature
50485	2436	G + ˆg G i ?i , < ? i? t? ?i t > ?= ? (t ? t?)? ii? , i = 1, ..., ? , G = 1, ..., ? . (25) Expanded sets of equations can represent a field M G (r, t), and the discussion below generalizes as well . Another mathematically equivalent representation is given by the Fokker-Planck equation, in terms of the ‘‘drifts’’ g G and ‘‘diffusions’’ g GG? , ?P ?t = VP + ?(?gG P) ?M G + 1 2 g G = f G + 1 2
50485	2437	diffusions are also parametrized. For example, these parameters can enter as expansion coefficients of polynomials describing accepted models of particular systems, e.g., modeling economic markets , or combat scenarios . In combat systems, such equations appear as ?r = x r bb + y r br br + z r ?r , ?b = x b r r + y b rbrb + z b ? b , (27) where the M G are Red (r) and Blue (b) force
50485	2441	is desired. The above problems provide motivation for the development of a new algorithm. Consider a parameter ? i k in dimension i generated at annealing-time k with the range ? i k ? , (12) calculated with the random variable y i , ? i k+1 = ? i k + y i (B i ? A i), y i ? . (13) Define the generating function gT (y) = D 1 ? i=1 2(|yi | + Ti) ln(1 + 1/Ti) Its cumulative
50485	2443	+ y i (B i ? A i), y i ? . (13) Define the generating function gT (y) = D 1 ? i=1 2(|yi | + Ti) ln(1 + 1/Ti) Its cumulative probability distribution is G T (y) = y 1 ?1 y D ? g i T (y i ) . (14) D ? i=1 ? ... ? dy?1 ...dy? D gT (y?) ? D ? G i T (y i ), ?1 G i T (y i ) = 1 2 + sgn(yi ) 2 ln(1 + |y i |/T i) ln(1 + 1/T i) y i is generated from a u i from the uniform distribution u i ?U
50485	2449	depending on the numerical algorithm used, to find the long-time evolution of the system. Details of the above parameter-fits of Lagrangians to empirical data will be published at a later date . VII. Discussion An algorithm of very fast simulated re-annealing has been developed to fit a empirical data to a theoretical cost-function over a D-dimensional parameter-space. The annealing
423617	2451	supervised methods and bootstrap learning techniques. KNOWITALL uses a novel form of bootstrapping that does not require any manually tagged training sentences. Other bootstrap IE systems such as  still require a small set of domain-specific seed instances as input, then alternately learn rules from seeds, and further seeds from rules. Instead, KNOWITALL begins with a domainindependent set
423617	2452	summary of our contributions. 1.1 Previous Work Whereas search engines locate relevant documents in response to a query, web-based Question Answering (QA) systems such as Mulder , AskMSR , Radev’s work , and others locate potentially relevant answers to individual questions but are not designed to compile large bodies of knowledge. Much of the previous work on Information
423617	2452	of the web for an ample supply of simple sentences that are relatively easy to process. This notion of “redundancy-based extraction” was introduced in Mulder  and further articulated in AskMSR . Several previous projects have attempted to automate the collection of information from the web with some success. Information extraction systems such as Google’s Froogle 1 , Whizbang’s flipdog 2
423617	2455	supervised methods and bootstrap learning techniques. KNOWITALL uses a novel form of bootstrapping that does not require any manually tagged training sentences. Other bootstrap IE systems such as  still require a small set of domain-specific seed instances as input, then alternately learn rules from seeds, and further seeds from rules. Instead, KNOWITALL begins with a domainindependent set
423617	2456	computes semantic tags for a large number of Web pages. KNOWITALL’s task is to automatically extract the knowledge that SemTag takes as input. KNOWITALL was inspired, in part, by the WebKB project  and its motivation. However, the two projects rely on a different architecture and very different learning techniques. Most important, WebKB relies on supervised learning methods that take as input
423617	2457	computes semantic tags for a large number of Web pages. KNOWITALL’s task is to automatically extract the knowledge that SemTag takes as input. KNOWITALL was inspired, in part, by the WebKB project  and its motivation. However, the two projects rely on a different architecture and very different learning techniques. Most important, WebKB relies on supervised learning methods that take as input
423617	2459	bound to predicate arguments. Keywords are formed from literals in the rule, and are sent as queries to search engines. The features chosen are combined using a “naive Bayesian” probability update . Given n observed features f1 . . . fn, which are assumed conditionally independent, the Assessor uses the following equation to calculate the expected truth of an atomic formula ?: P (?|f1, f2, .
423617	2459	effect. Since the naive Bayes formula is notorious for producing polarized probability estimates that are close to zero or to one, the estimated probabilities are often inaccurate. However, as  points out, the classifier is surprisingly effective because it only needs to make an ordinal judgment (which class is more likely) to classify instances correctly. Similarly, our formula produces
423617	2462	are not designed to compile large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements
423617	2463	{“,”}“and other” NP2 NP1 {“,”}“including” NPList2 NP1 “is a” NP2 NP1 “is the” NP2 “of” NP3 “the” NP1 “of” NP2 “is” NP3 Some of our rule templates are adapted from Marti Hearst’s hyponym patterns  and others were developed independently. To see how these patterns can be used as extraction rules, suppose that NP1 in the first pattern is bound to the name of a class in the ontology. Then each
423617	2464	avoids repeated passes over a string by compiling the pattern into a finite-state machine. XScan, our incremental XML query processing algorithm, uses a similar compilation scheme onsstreaming data . Finally, RETE matching has been successfully applied to the problem of matching large sets of production rules against a working memory database . In our case, we could compile rules into a
423617	2465	scaling to the Web due to the diversity of text styles and genres on the web and the prohibitive cost of creating an equally diverse set of handtagged documents. Wrapper induction systems  are able toslearn extraction patterns with a small amount of training, but operate only on highly structured documents and cannot handle the unstructured text that KNOWITALL exploits. The TREC
423617	21199	scaling to the Web due to the diversity of text styles and genres on the web and the prohibitive cost of creating an equally diverse set of handtagged documents. Wrapper induction systems  are able toslearn extraction patterns with a small amount of training, but operate only on highly structured documents and cannot handle the unstructured text that KNOWITALL exploits. The TREC
423617	2466	work and a concise summary of our contributions. 1.1 Previous Work Whereas search engines locate relevant documents in response to a query, web-based Question Answering (QA) systems such as Mulder , AskMSR , Radev’s work , and others locate potentially relevant answers to individual questions but are not designed to compile large bodies of knowledge. Much of the previous work on
423617	2466	relies on the scale and redundancy of the web for an ample supply of simple sentences that are relatively easy to process. This notion of “redundancy-based extraction” was introduced in Mulder  and further articulated in AskMSR . Several previous projects have attempted to automate the collection of information from the web with some success. Information extraction systems such as
423617	2469	the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements corpus, and have difficulty scaling to the Web. These
423617	2470	systems extract information from relatively small corpora of newswire and newspaper articles, while KNOWITALL extracts information from the Web. As a result, top performing systems in TREC (e.g., ) focus on “deep” parsing of sentences and the production of logical representations of text in contrast with the lighter weight techniques used by KNOWITALL. Recent IE systems have addressed
423617	2471	our contributions. 1.1 Previous Work Whereas search engines locate relevant documents in response to a query, web-based Question Answering (QA) systems such as Mulder , AskMSR , Radev’s work , and others locate potentially relevant answers to individual questions but are not designed to compile large bodies of knowledge. Much of the previous work on Information Extraction (IE) has
423617	2473	supervised methods and bootstrap learning techniques. KNOWITALL uses a novel form of bootstrapping that does not require any manually tagged training sentences. Other bootstrap IE systems such as  still require a small set of domain-specific seed instances as input, then alternately learn rules from seeds, and further seeds from rules. Instead, KNOWITALL begins with a domainindependent set
423617	2474	are not designed to compile large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements
423617	2475	large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements corpus, and have difficulty
423617	403	large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements corpus, and have difficulty
423617	2476	begins with a domainindependent set of generic extraction patterns from which it induces a set of seed instances. Another distinctive feature of KNOWITALL is its use of Turney’s PMI-IR methods  to assess the probability of extractions using “web-scale statistics”. This overcomes the problem of maintaining high precision, which has plagued bootstrap IE systems. Another system that uses hit
423617	2476	the Assessor uses a form of pointwise mutual information (PMI) between words and phrases that is estimated from web search engine hit counts in a manner similar to Turney’s PMI-IR algorithm . For example, suppose that the Extractor has proposed “Liege” as the name of a city. If the PMI between “Liege” and a phrase like “city of Liege” is high, this gives evidence that “Liege” is indeed
423617	2476	for “city of California” than there are for many obscure, but legitimate, cities. In order to compensate for this bias, we considered dividing by the frequency of the instance I. Following Turney , we compute the pointwise mutual information (PMI) between the candidate instance and a discriminator phrases as |Hits(D + I)| PMI(I, D) = (2) |Hits(I)| One potential problem with the PMI approach
423617	2477	that KNOWITALL exploits. The TREC conference has introduced a “list” track where answering a question requires finding all instances of a specific subclass such as “10 movies starring Tom Cruise” . One key difference between the TREC systems and KNOWITALL is that the TREC systems extract information from relatively small corpora of newswire and newspaper articles, while KNOWITALL extracts
2482	2483	However, they do not suggest basis functions and do not implement their method. Primal-Dual representations of the American option problem allow both an upper and lower bound to be calculated . However, the upper bound involves calculating an expectation, which has been done using another Monte Carlo simulation. This simulation on simulation is computationally expensive. Glasserman and
2482	2483	asymptotically to the true value. This method is computationally demanding, but the speed has been improved by using low–discrepancy sequences in . Duality approaches have been suggested in , however these methods tend to converge even more computationally expensive than the low biased estimators. We view and as deterministic functions. Define both M0 = 0 and Each summand is ? + i (.)
2482	2483	basis functions that are martingales for assets driven by stochastic processes other than geometric Brownian motion. A systematic comparison of the duality methods of Andersen and Broadie , Glasserman and Yu , Kogan and Haugh , Rogers , using QuantLib for the implementation, would allow an informed evaluation of their relative merits to be made. Andersen and Broadie
2482	2484	options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include parameterization of the optimal exercise boundary , a quantization tree algorithm , wavelets [21,
2482	2486	European style financial derivatives was first suggested in . Progress in using simulation methods to price American style options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include
2482	2487	sequences . Other methods include parameterization of the optimal exercise boundary , a quantization tree algorithm , wavelets , irregular grid approximations , and sparse grid methods . Regression methods include . The regression method of Longstaff and Schwartz  has proved particularly popular, due to its accuracy and simplicity. The
2482	2497	European style financial derivatives was first suggested in . Progress in using simulation methods to price American style options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include
2482	2497	the American option value. The simplest is simulating each path and choosing the optimal exercise time. This simple estimate uses future information and is biased high. The stochastic mesh method  gives lower and upper bounds which both converge asymptotically to the true value. This method is computationally demanding, but the speed has been improved by using low–discrepancy sequences in
2482	2498	options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include parameterization of the optimal exercise boundary , a quantization tree algorithm , wavelets [21,
2482	2498	step and initial continuation value ˆV0 ? max(h0(X0), ?0) the estimated option value ? ? ?1 .sN. P. Firth 3.2. Regression Later. Glasserman and Yu  interpret the method of Broadie et al.  as a regression method, but with the regression taking place one time step ahead. They refer to this as ‘regression later’. To do this Glasserman and Yu  introduce the idea of using basis
2482	2499	International plc. for funding this research. The author would also like to thank Dr. W. T. Shaw, Dr. B. Hambly, A. Dickinson, and particularly Dr. R. A. Stalker Firth. 1 analytical work includes sUpper Bounds for American Option Prices The rest of this paper is structured as follows: We formulate the American option pricing problem, following , as an optimal stopping problem whose
2482	2502	have been found for American option prices 1 . Therefore much work has been done pricing American options numerically. Early examples include finite differences  and the binomial lattice . Grid based methods work well for single asset options, and have been extended to higher dimensions . However, these methods suffer from the ‘curse of dimensionality’, as they become
2482	2507	in . The relationship between the number of basis functions and number of paths required is investigated by Glasserman and Yu . For a comparison of simulation approaches see Fu et al.  or the recent book by Glasserman . Glasserman and Yu  investigate the relative merits of ‘regression now’ versus ‘regression later’. ‘Regression now’ involves using basis functions defined
2482	2514	However, they do not suggest basis functions and do not implement their method. Primal-Dual representations of the American option problem allow both an upper and lower bound to be calculated . However, the upper bound involves calculating an expectation, which has been done using another Monte Carlo simulation. This simulation on simulation is computationally expensive. Glasserman and
2482	2514	asymptotically to the true value. This method is computationally demanding, but the speed has been improved by using low–discrepancy sequences in . Duality approaches have been suggested in , however these methods tend to converge even more computationally expensive than the low biased estimators. We view and as deterministic functions. Define both M0 = 0 and Each summand is ? + i (.)
2482	2514	for assets driven by stochastic processes other than geometric Brownian motion. A systematic comparison of the duality methods of Andersen and Broadie , Glasserman and Yu , Kogan and Haugh , Rogers , using QuantLib for the implementation, would allow an informed evaluation of their relative merits to be made. Andersen and Broadie  produces an upper bound by generating an
2482	2515	of the optimal exercise boundary , a quantization tree algorithm , wavelets , irregular grid approximations , and sparse grid methods . Regression methods include . The regression method of Longstaff and Schwartz  has proved particularly popular, due to its accuracy and simplicity. The paper explains the method, but an introduction to the method is also
2482	2515	Convergence results are given in . Algorithm 1. We make the method concrete by describing the computation in algorithm 1. This describes the Least Squares Monte Carlo (LSMC) method of  in this setting, as well as the method developed in . In Glasserman and Yu  discounting is not explicitly considered. We explicitly denote the discount factor between times i and i+1 by
2482	2515	8.508 7.700 8.488 (.024) 8.495 (.023) 8.495 (.023) Table 1: Comparison of ‘regression now’ and ‘regression later’ with the results for American style put option in Table 1 in Longstaff and Schwartz . X0 is the initial asset price, ? is the volatility of returns, and T is the number of years until the option expiry date. The continuously compounded short–term interest rate is 0.06, and the
2482	2515	‘regression later’ provides good estimates for the option value. The results in table 1 compare ‘regression now’ with ‘regression later’, following the results in table 1 in Longstaff and Schwartz . Only in-the-money paths are used in the regression . Both regressions use the first three martingale basis functions described above. We find that ‘regression later’ does indeed provide a good
2482	2515	and Future Directions. The numerical results show that regression estimates using ‘regression later’ are more accurate than existing methods using ‘regression now’, such as Longstaff and Schwartz . However, the ‘regression later’ method depends, through condition (C3), on the availability of basis functions that are martingales. We have obtained results for single asset options on underlying
2482	2524	of the optimal exercise boundary , a quantization tree algorithm , wavelets , irregular grid approximations , and sparse grid methods . Regression methods include . The regression method of Longstaff and Schwartz  has proved particularly popular, due to its accuracy and simplicity. The paper explains the method, but an introduction to the method is also
2482	2524	optimal exercise strategy the estimate has mixed bias. Estimators with definite low bias and definite high bias will be given in sections 6 and 7, respectively. Convergence results are given in . Algorithm 1. We make the method concrete by describing the computation in algorithm 1. This describes the Least Squares Monte Carlo (LSMC) method of  in this setting, as well as the method
2550	2551	of the searches outlined above, but have significant limitations that prevent their extension to all domains. Recently, Nene and Nayar proposed a method for effective NN search in high dimensions . Their method, while providing a good search time, generates a static structure that prevents the insertion of additional elements without an expensive tree rebuilding. Such a solution would not be
2550	2551	reason to use a tree structure is the ability to add elements. If an a priori data analysis is always possible, and insertions and deletions are infrequent, a structure such as that presented in  is likely a better choice than a k-d tree. Nearest neighbor searches generally begin with the construction of a “clipping window,” which defines a region of the tree space to search. For example, a
2550	2551	an initial value for ? based on the probability that at least one element will lie within the hyper-sphere around ? bounded by ?. The method for choosing an appropriate initial ? is outlined in , and has proven successful in their fixed technique. While the results of those experiments are beyond the scope of this paper, we present two critical findings from our additional work on the
2550	2552	hashing and indexing, various types of trees, and many hybrid and novel approaches. Proposed tree solutions alone include k-d, B+, R+, BBD, VAMSplit k-d, red-black, Patricia and other variants . Tree-based search strategies are popular for many reasons, including, for n cases, O(log n) search and insertion time, O(n log n) construction time, and reasonable space requirements. Tree
2550	2552	but not exhaustively, since its introduction in 1975 . Friedman, Bentley, and Finkel presented good search and construction algorithms for NN searching with kd trees as early as 1977 . More recently, Arya and Mount have presented refined search tactics that have been especially effective for approximate searches . The refinements present here are effective for all thessearch
2550	2552	of the tree. It is this absence of information that the tracking node solves for. Other complicated methods have been proposed for tracking, primarily variants of the Bounds-Within-Ball (BWB) test . Our tracking nodes achieve the same effect as the BWB test with just one simple structure and a single comparison at each branch. Also, it is our claim that the code for the tracking node solution
2550	2553	hashing and indexing, various types of trees, and many hybrid and novel approaches. Proposed tree solutions alone include k-d, B+, R+, BBD, VAMSplit k-d, red-black, Patricia and other variants . Tree-based search strategies are popular for many reasons, including, for n cases, O(log n) search and insertion time, O(n log n) construction time, and reasonable space requirements. Tree
2550	2553	algorithms for NN searching with kd trees as early as 1977 . More recently, Arya and Mount have presented refined search tactics that have been especially effective for approximate searches . The refinements present here are effective for all thessearch types outlined and provide the maximum speed-up for exact NN queries. 2 Traditional Search Many papers have examined the best ways to
2550	2559	types of search are of interest to different disciplines. Nearest neighbor (NN) search is important to many case-based reasoning (CBR) as well as various classification and matching problems . Approximate nearest neighbor search is important to many AI systems, and in systems where there is an acceptable trade-off between exact answers and performance. K-NN and other multivariate range
2550	2559	hashing and indexing, various types of trees, and many hybrid and novel approaches. Proposed tree solutions alone include k-d, B+, R+, BBD, VAMSplit k-d, red-black, Patricia and other variants . Tree-based search strategies are popular for many reasons, including, for n cases, O(log n) search and insertion time, O(n log n) construction time, and reasonable space requirements. Tree
2550	2559	method, but present our method only for future comparisons. In fact, our search and pruning techniques work equally well with a tree that has been built using one of the bulk-loading techniques  as with our incremental construction technique, outlined below. We start with a randomized set of elements of dimensionality d, which are added to the tree one at a time. The single parameter
2550	2559	of the tree. It is this absence of information that the tracking node solves for. Other complicated methods have been proposed for tracking, primarily variants of the Bounds-Within-Ball (BWB) test . Our tracking nodes achieve the same effect as the BWB test with just one simple structure and a single comparison at each branch. Also, it is our claim that the code for the tracking node solution
2550	2559	has dramatic results, as has been shown previously. But a critical aid to DFBB search, appropriate path ordering, has been neglected in the same studies. The critical code fragment presented in  demonstrates an algorithm for ordering that examines tree leaves in a static order. The algorithm operates independently of freely available information from tracking nodes. Such information can be
2562	2566	On the other hand we claim that agent communication languages can be successfully used for this task, especially if they provide support for fault tolerant communication primitives as suggested in . In the following we outline the main features of our Knowledge Level OSA. We assume knowledge level agents , that is, they should concern with the use, request and supply of knowledge without
2562	2566	as in IRS-II. Agent Communication. Agents access services and communicate each other using a fault tolerant Agent Communication Language (ACL) which provides one-to-one and one-to-many primitives . We assume an asynchronous communication and a reliable message passing, i.e., whenever a message is sent it must be eventually received by the target agent (thus we do not handle communication
2562	2566	Knowledge Base (KB) with a new capability (for example adding a task Tk), it forwards this information to all the other agents, for example by means of an agent primitive register(myself, Tk) as in . We also assume that agents communicate only their capabilities, not their PSMs. Therefore each agent knows all the problems which can be solved by the OSA, but it knows how to solve a task only if
2562	2566	language or knowledge representation formalisms it adopts, but it reacts to a well defined protocol based on the standard primitives of an agent communication language. The primitives of this ACL  can be divided into four categories as shown in Table 1. Contents based services requests are realized as one-to-many primitives: whenever an agent needs a given service which solve a task T it can
2562	2566	from another agent in the network. At the facilitator level some primitives, like ask-one and tell, have an extra parameter used to identify the right answer of the message, as specified in . To deeply understand how the communication is implemented, suppose that agent A would communicate with agent B. Suppose that the selected communication primitive is the ask-one(B, A, p). Agent B
2562	2568	and any IRS-II service automatically appears as a standard web service to other web service infrastructures. 2.1 IRS-II Approach Semantic Web Services are described by means of the UPML framework  developed within the IBROW project . Figure 1 shows how a single knowledge based application would be UBLCS-2004-14 2sdescribed in UPML terms. The UPML framework partitions knowledge into four
2562	2570	A significant challenge to address is the design of distributed reasoning infrastructures tightly integrated with current Internet components and technologies that would allow semantic Web Services  to be exploited in the large. While new standards are emerging such as the Ontology Web Language (OWL) , and the community is currently addressing many central issues as the design of a Web
2562	2570	facilities . We describe here a Knowledge-Level extension of this architecture which extends Web servers with the IRS functionalities and agents’ capabilities providing semantic Web Services . Agents becomes the main building block of this architecture. They are geographically distributed (as Web servers are) and can provide a set of semantic Web Services to the outside world which
2562	2570	A significant challenge to address is the design of distributed reasoning infrastructures tightly integrated with current Internet components and technologies that would allow semantic Web Services  to be exploited in the large. While new standards are emerging such as the Ontology Web Language (OWL) , and the community is currently addressing many central issues as the design of a Web
2562	2570	facilities . We describe here a Knowledge-Level extension of this architecture which extends Web servers with the IRS functionalities and agents’ capabilities providing semantic Web Services . Agents becomes the main building block of this architecture. They are geographically distributed (as Web servers are) and can provide a set of semantic Web Services to the outside world which
2562	2572	possible role for agents is to enhance the capabilities of servers using their intelligence to provide more complex services and behaviors. For example the Internet Reasoning Service (IRS-II)  is a knowledge based server which supports the publication, location, composition and execution of semantic Web Services. The IRS provides facilities to achieve complex tasks, if all the needed
2562	2572	of the proposed OSA. Then in Section 5 we present a Tomcat based implementation of the OSA. The final Section of the paper contains our conclusions and highlights our future work. 2 IRS-II IRS-II  is a framework and implemented infrastructure which is aimed at supporting the publication, location, composition and execution of semantic Web Services. IRS-II has three main classes of features
2562	2572	1. The UPML framework. The application of the UPML framework to semantic Web Services provides a number of advantages. In particular, the explicit separation between tasks and methods provides : ??? A task-based mechanism for aggregating services. It is possible to specify service types (i.e. tasks) independently from specific service providers. • A basic model for dealing with ontology
2562	2712	current Internet components and technologies that would allow semantic Web Services  to be exploited in the large. While new standards are emerging such as the Ontology Web Language (OWL) , and the community is currently addressing many central issues as the design of a Web Service Modelling Ontology (WSMO) , there is still not a widely accepted architecture for the underlying
2576	2728	performance, then it is difficult to know whether to attribute differences in performance to project activities or to the effects of pre-existing village characteristics. For example, Pitt et al. (1993) describe a case in Indonesia that showed that villages covered for several years under a major family planning program actually had higher fertility rates than those outside of the program. One
2576	2728	there was insufficient data to match them in a more rigorous manner. A nonexperimental design was used instead. Several nonexperimental approaches are possible. One way comes from the Pitt et al. (1993) study of Indonesian poverty programs referred to in Section 1. Because those programs were intentionally located in the poorest areas, purely cross-sectional analysis would have suggested
2576	2728	and if program placement is independent of treatment effect. Since this is unlikely to be the case, the estimated effect of the treatment is likely to be biased. In the case analyzed by Pitt et al. (1993), the project coefficient b in equation 1 had a negative coefficient; i.e., the analysis suggested incorrectly that the project contributed to poverty. Pitt et al. approached this problem by
8918831	2634	provided by rating agencies to be useful and have incorporated it into their investment decisions as indicated by movements in yields on sovereign bonds in response to changes in credit ratings (Cantor and Packer 1996, Clark and Lakshmi 2003, Larrain, Reisen, and von Maltzan 1997). The Basel II capital accords may further increase the importance that markets attach to the assessments of credit rating agencies.
8918831	2634	ratings are influenced by the same fundamentals as investors’ private information, ratings will still affect investors’ decisions. This is consistent with the findings of several empirical studies (Cantor and Packer 1996, Larrain, Reisen, and von Maltzan 1997). Credit ratings have this effect in part because the revision of the rating agency’s signal in response to changes in fundamentals causes investors to revise
12120132	2663	increments process and then generate a realization of fBM by calculating the cumulative sums process of fractional Gaussian noise, such as those of Levinson (Coeurjolly 2000) and of Wood and Chan (Dietrich and Newsam 1997; Wood and Chan 1994). These methods take advantage of the fact that the increment process is a stationary one and its covariance matrix is a Toeplitz matrix. Finally, there are methods that rely on
12120132	2663	and Bottom Panels: Realizations of Fractional Stable Motions for ? = 1.2 and ? = 0.6. In all cases, Left Panel H = .1, Middle Panel: H = .5 and Right Panel H = .9 is a m × m circulant matrix (Dietrich and Newsam 1997). Let SY n (t), 0 ? t ? 1 be a stepwise constant function such that SY n (k/n) = ?k i=1 Ym,i, 0? k ? n. In the sequel S ? n(t), SX n (t) and SU n (t) etc are similarly defined. Let also ? =[(?? +
12120132	2663	Let a = (a1,...am) and e = (?0,?1?m,?2?m,...,??1) and let ? =FFT(a) and ? =FFT(e) be their corresponding discrete Fourier transforms. Then due to the fact that the m × m matrix A is circulant (Dietrich and Newsam 1997) we have Y =IFFT(v), where v = (v1,...,vm) and vj = ?j ?j . (7) The proposed algorithm can be summarized as follows: Step 1: Compute FFTA; i.e. the FFT of the vector  Step 2: Compute
12120132	2673	computing the wavelet coefficients corresponding to wavelet transform of fBM and then synthesize fBM through the inverse wavelet transformation, while the random midpoint displacement method (Leland et al. 1994) progressively subdivides the interval over which a sample path is generated and at each subinterval a Gaussian displacement is used to determine the value of the process at the midpoint. Other
12120132	2675	the behavior of such networks (Park and Willinger 1995; Yuksel et al. 2000). One then must be able to generate traffic that exhibits the necessary temporal behavior over large time scales (Norros 1995; Norros, Mannersalo and Wang 1999). One of the simplest models exhibiting long-range dependence is fractional Brownian motion (fBM) introduced by Kolmogorov (1940) and further developed by
12120132	2678	the number of points being generated, see Coeurjolly (2000), or the fact that due to the approximate nature of the method, the quality of the generated process can not be accurately assessed (e.g. Paxson 1997). However, the simulation study of Coeurjolly (2000) suggests that the method of Wood and Chan (1994) with a time complexity of O(n log n) and memory complexity of O(n) performs satisfactorily. Our
12120132	2679	of EECS University of Michigan Ann Arbor, MI 48105, U.S.A. George Michailidis Department of Statistics University of Michigan Ann Arbor, MI 48105, U.S.A. porting a heavy tailed assumption (Paxson and Floyd 1995) backed by theoretical work that explains how the former assumption induces through an appropriate mechanism long range dependence in the aggregate traffic (Konstantopoulos and Lin 1998).
8918845	2691	the challenge of creating a better model. Bounded Optimality Russell et al. define a model of rationality they believe is appropriate for determining if an AI system exhibits rational behaviour(Russell & Subramanian 1993). To motivate the approach taken four possible definitions of rationality are given. The first three are currently used in AI and the fourth is the proposed definition upon which the paper is
8918845	2691	successful in many applications. An agent function is a mapping from an agent’s perceptions to its actions. An agent’s program is what generates its actions. The agent function is defined in (Russell & Subramanian 1993) such that the action may be a null action if the agent is still calculating. Russell et al. construct asframework for designing bounded optimal agents in specific environments. A bounded optimal
2702	2703	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	2704	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	1939	and on caches employing the drowsy cache and cache decay technique for energy savings. In order to perform the experiments, weshave built a simulation framework on top of the SimpleScalar simulator  that permits the injection of soft errors into the cache and also allows the modeling of different cache interleaving schemes and error detection/correction schemes. Our experimental results show
2702	1939	the different soft error cases to provide a realistic evaluation. Qss2.2 Soft Error Injection In this work, we implement a random soft error injection in the cache memories using the SimpleScalar  simulator. A random variable generated by the simulator along with the SERs provided in Table 1 is used to determine whether a soft error happens and the number of bits influenced by the error. In
2702	1939	employing the commonly used write-back approach. 3.3 Error Behavior of Different Approaches We implement the soft error injection, drowsy cache, and cache decay in the simulator SimpleScalar 3.0 . Our benchmarks are from SPEC CINT2000 . Each benchmark is first fast forwarded 300 million instructions and then simulated 1 billion instructions. Table 2 gives the total number of cycles taken
2702	2706	voltages of transistors have reduced. Though lowering supply voltage helps to reduce the dynamic power consumption, the consequent decrease in threshold voltage has increased the leakage energy . As cache memories constitute a significant portion of the transistor budget of current microprocessors, leakage reduction for cache memories has been of particular importance. It has been
2702	2707	are not accessed for a long period of time. Different designs of low power cache memories can potentially have different immunity to soft errors due to supply voltage changes and circuit structure . These differences in soft error vulnerability are important as they influence the complexity of error detection and correction circuitry employed in on-chip memories. We believe that the issues of
2702	2707	by Equation 1, error rates for low power mode was fixed as one order of magnitude higher than the corresponding high power mode, as the error rate is exponential dependent on the supply voltage . During read and write operations, the error can manifest in either the SRAM cell or the peripheral circuits like sense amps. Also, when the wordline in SRAM is asserted, charge sharing occurs and
2702	2708	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	2708	optimizations: drowsy cache and cache decay. 3.1 Drowsy Cache Drowsy cache is based on controlling the leakage by using dynamic voltage scaling (DVS) while using the standard 6T SRAM cell structure . This method makes use of the fact that to retain a value in the SRAM cell, the source voltage of the cell can be just about 1.5 times of Vt. Thus, for a 70nanometer technology based SRAM cell that
2702	2708	be reduced to up to 0.3V when accesses are not required while still retaining the data. Substantial leakage energy saving can be gained when placing the cache line in this low voltage drowsy state . However, one cycle penalty is incurred when accessing a drowsy cache line, as the supply rails have to be restored to 1.0V before a read or write operation. For voltages less than 0.3V it was
2702	2710	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	2712	when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the values of cache blocks. In contrast, the cache decay scheme  completely turns off the cache blocks, losing the stored data, when they are not accessed for a long period of time. Different designs of low power cache memories can potentially have different
2702	2712	capacitance is also a function of the supply voltage). Thus, the use of DVS provides an interesting opportunity for trade off between leakage reduction and soft error immunity. 3.2 Cache Decay In , Kaxiras et al. present a leakage energy reduction technique, called Cache Decay, for cache memories. Cache Decay exploits the temporal behavior of cache blocks to reduce leakage consumption. This
2702	2714	reduction for cache memories has been of particular importance. It has been estimated that leakage energy accounts for 30% of L1 cache energy and 70% of L2 cache energy for a 0.13 micron process . Another consequence of the technology scaling is the smaller supply voltages and reduced capacitive values of the circuit nodes. This has raised reliability concerns due to the increased
2702	2716	Scheme 1, 2, and 3). in L1 data cache. This method keeps the blocks in L1 cache in clean mode, but it significantly increases the number of L2 cache accesses. Therefore, a coalescing write-buffer  is always employed with write-through cache to merge the writebacks to L2 cache. But the total number of L2 cache accesses of write-through cache is still significantly larger than that of
2702	2717	scheme for all data. This is also different from other schemes provided for energy-efficient data protection through duplication of data , or protecting only the frequently used cache lines . The approach involving cache line duplication is orthogonal to our approach and has been investigated using a cache using both ECC and parity. In contrast, our work focuses on providing different
2702	2719	commercial caches that use a uniform error protection scheme for all data. This is also different from other schemes provided for energy-efficient data protection through duplication of data , or protecting only the frequently used cache lines . The approach involving cache line duplication is orthogonal to our approach and has been investigated using a cache using both ECC and
2721	2722	Communications Systems Laboratory Nokia Research Center Mountain View, CA 94303 Email: charliep@iprg.nokia.com destinations for which traffic exists. A couple of examples of such protocols are DSR  and AODV  . For the purposes of this paper, the main feature of these protocols is that a node uses a series of network-wide all-node broadcasts to disseminate its route request to discover a
2721	2722	Subsequent work  provides more advanced algorithms and more sophisticated methods to handle node movement, shutdown and power-on.  also suggests a way to run Dynamic Source Routing (DSR)  over a connected dominating set of the network. The dominating set of a network is a subset of nodes such that each node is either in the dominating set, or is adjacent to a node in the dominating
2721	2289	Systems Laboratory Nokia Research Center Mountain View, CA 94303 Email: charliep@iprg.nokia.com destinations for which traffic exists. A couple of examples of such protocols are DSR  and AODV  . For the purposes of this paper, the main feature of these protocols is that a node uses a series of network-wide all-node broadcasts to disseminate its route request to discover a route to an
2721	2724	algorithm suggested in  uses a greedy approach to minimize the number of repeater nodes while trying to achieve the desired MPR COVERAGE. The Topology Broadcast Reverse Path Forwarding (TBRPF)  routing mechanism uses broadcasts and limits flooding through a packet cache similar to AODV. It is based on , which has the potential to limit the default blind flooding but does not have any
2721	2727	not originate any. Thus, no other node will create a route through the passive member. There have been a few proposals for establishing a virtual backbone over which routing takes place (e.g., , ). In  a spine is used for all communications, while in  the backbone is used as a secondary route in case shortest-pathsroutes fail. These approaches assume a perfectly scheduled MAC layer.
2721	2728	is used for all communications, while in  the backbone is used as a secondary route in case shortest-pathsroutes fail. These approaches assume a perfectly scheduled MAC layer. Subsequent work  provides more advanced algorithms and more sophisticated methods to handle node movement, shutdown and power-on.  also suggests a way to run Dynamic Source Routing (DSR)  over a connected
2721	2728	in the dominating set. Obtaining the minimum connected dominating set of a graph is known to be NP-hard   even when the complete network topology is available. Our work is distinct from  and OLSR’s MPR scheme in that we only use dominating sets for flooding control, not packet routing, and we construct a more robust connected dominating set through several heuristics. In
2721	2729	subset of nodes such that each node is either in the dominating set, or is adjacent to a node in the dominating set. Obtaining the minimum connected dominating set of a graph is known to be NP-hard   even when the complete network topology is available. Our work is distinct from  and OLSR’s MPR scheme in that we only use dominating sets for flooding control, not packet routing, and we
2721	2731	describes how DP is applied to the forwarding of route request (RREQ) packets in AODV. To facilitate DP, each node needs two-hop neighbor information. We use a neighbor-exchange protocol called NXP . Applying DP directly to AODV does not result in better performance from that obtained by the conventional AODV in many situations, due to the loss of RREQ packets. We present several heuristics to
2721	2732	Section III presents the results of our simulation performance analysis between conventional AODV, AODV with DP, and AODV-DS. The results from our analysis are consistent with the findings for OLSR , which show that multi-point relays (MPR) significantly reduce the protocol overhead, but also results in a lower route availability and packet delivery rates, except for one topology. Our results
2721	2733	the destination from either an active route or broken route, we add the listed next-hop to the forwarder list. III. SIMULATION RESULTS We implemented AODV draft 10 and Dominant Pruning in GlomoSim . We did not use the version of AODV distributed with Glomosim, but rather made a new version to conform with recent AODV specification. This section first reviews our implementation of AODV, then
2721	2734	packets – broadcast and unicast – are jittered by an exponential delay with mean value of 10 milliseconds, with a minimum of 1 ms and a maximum of 100 ms. Our simulations generally replicate  for a 50-node network. We have scenarios with 10 source nodes and 30 source nodes, transmitting 4 packets/sec CBR traffic of 512 byte UDP packets. Nodes begin transmitting at 50 seconds plus an
2721	2734	number seeds. Each data point represents the mean over the 10 trials. We show 95% confidence intervals all graphs except some cumulative distribution plots. Our performance metrics are similar to . We measure the delivery ratio of CBR packets received to packets transmitted, the latency of received CBR data packets, and the control overhead. The control overhead is the ratio of the total
2721	2734	200 300 500 700 900 pause time aodv-1s aodv-2s dp-1s dp-2s aodv-ds-1s aodv-ds-2s Fig. 4. Average # RREQs transmitted, 30 sources AODV-DS is close to AODV. When we compare our results with those in , there are three significant differences between AODV in  and our implementation of AODV.  uses link-layer feedback for failed links. When the MAC layer fails an RTS/CTS/ACK handshake and
2721	2734	retries, it notifies AODV that a specific packet failed. AODV immediately breaks the link.  does not use Hello packets, which we rely on to detect link failures and exchange two-hop data.  broadcasts all RERR packets while our implementation sometimes unicasts them. In terms of delivery ratio, our simulations of AODV showsfraction of RREQ transmissions 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3
2721	2734	higher delivery ratio. For 30 source nodes, our work shows between 68% and 89% delivery for conventional AODV and between 75% and 92% delivery for AODV-DS. For the control load (“routing load” in ), one would expect a large difference because we used Hello packets. For 10 source nodes,  reports a routing load of 1.0 or less, dropping towards zero as the pause time increases. For 30
2796	2961	Energy curve of a signal in decibel. 4.1.6 Pitch Inherits from Sequence, template parameter set to double Subclasses: StPitch Pitch of a signal. Computation of pitch is performed by ESPS library . 4.1.7 StPitch Inherits from Pitch Subclasses: none 61sPitch of a signal in semitones. 4.1.8 LPCraw Inherits from Sequence, template parameter set to double Subclasses: none This class provides LPC
2796	2806	tune For simulated annealing, for example, parallelization can be performed in various ways: simultaneous independent annealing , speculative decisions , periodically interacting searches . As the training set is composed of a set of separate signal files that can be processed separately without any contradiction the latter has been chosen. Every tuning strategy (annealing, genetic,
2796	2814	Fujisaki’s parameter from natural pitch contour. Other systems were developed to identify syllable boundaries in speech signals. This segmentation can be interesting both for speech recognition  and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also
2796	2815	average over nine critical frequency bands every 10ms is also cosidered. The resulting vector was concatenated with log-RASTA features and used as input to a multilayer perceptron. Greenberg et al.  introduced the speech modulation spectrogram, a system for the research of invariant features related to frequency portions of the speech spectrum distributed across critical band-like channels.
2796	2816	in phones, but they will be computationally more expensive than syllabification and furthermore syllables are, according to many author, pointed as an important unit for speech processing , because many linguistic phenomena (such as co-articulation) are confined into syllables. Pitch stylizations of are provided in a number of way to take in account or not breaks at pauses and to
2796	2816	as source of useful information. Recent ASR systems try to use some prosodic feature to improve their analysis. Most of their effor only focused on syllable segmentation and recognition. Greenberg  showed how syllables can be important in speech recognition according to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system.
2796	2818	and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also developed . A possible use of such a system is to provide a new feature useful to disambiguate some utterances. The following paragraphs show more in detail some of these systems. 1.3 Syllables
2796	2818	can be used to distinguish among them. 26sStress is tied both to duration of syllable, energy and pitch. Various efforts were devoted to automatic stress detection. Hieronymus and Williams  used a combination of these quantities to locate stressed syllables, while Silipo and Greenberg  showed a comparison between a wide range of combinations of these quantities. They found better
2796	2825	parameter from natural pitch contour. Other systems were developed to identify syllable boundaries in speech signals. This segmentation can be interesting both for speech recognition  and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also
2796	2825	27sChapter 2. ALGORITHMS 2.1 Syllabification 2.1.1 Why syllables? Syllable segmentation is important in APA for tempo determination, but it can also be used as basic unit in recognition . The definition of what one call “syllable” is controversial. Some author bound syllables to jaw movement , to chest burst  or they consider syllables as the basic unit of speech
2796	2825	in phones, but they will be computationally more expensive than syllabification and furthermore syllables are, according to many author, pointed as an important unit for speech processing , because many linguistic phenomena (such as co-articulation) are confined into syllables. Pitch stylizations of are provided in a number of way to take in account or not breaks at pauses and to
2796	2834	it is also necessary to develop systems able to detect accent and phrase command from natural speech to mimic them in synthetic utterances. It was the goal of many efforts such those from Mixdorff  and Salvo Rossi et alii . The former work starts its analysis from the quadratic spline stylization of pitch contour. It was done in order to ignore short time disturbing variations and to
2796	2834	made via algorithmic methods, the problem of determining the right value of some parameters arise. In many previous work, authors simply refer to “empirically determined parameter” (e.g., see ), while when the number of parameters is quite small, say less than four, some author  tried to perform an exhaustive search among every plausible combination of parameter values.
2796	2836	by assigning a weight to each element within a set of spectral bands.An algorithm evaluating the shape of the loudness pattern (convex-hull) is used to find syllable boundaries. Pfitzinger et al.  process the speech signal in three steps: firs. They used a bandpass filter, then they compute the energy pattern using a short term window and finally they low-pass filtered this energy function.
2796	2836	many previous work, authors simply refer to “empirically determined parameter” (e.g., see ), while when the number of parameters is quite small, say less than four, some author  tried to perform an exhaustive search among every plausible combination of parameter values. Tools are shown in this work that help to fix these parameters with a more motivated approach.
2796	2836	function of the number of values tried in most cases the considered number of values is too small to find a reasonable solution. The exhaustive search was formerly used by Pfitzinger et alii , as they only had three parameters with an average of 4 values each: their search was then in a space of only 64 configurations. In the syllabification case, exhaustive search can not be used. 3.1
2796	2836	found marker is 13.4ms on both training and test corpora. Results in Italian seem to be close to those from other authors (cfr.  for boundary accuracy on spontaneous speech, and  on nuclei detection), while the training strategies seem to be roughly equivalent even with slightly better performances from simulated annealing, especially in the modified version. The comparison
2796	2836	be distributed differentially. The error rate obtained after training is quite equivalent to those obtainable with more complex techniques , or better than other algorithm based techniques . Further modifications to syllabification algorithm are required for English. Results about tone unit seem poor, but the agreement among human experts is not so far from the error rate
2796	2839	model Fujisaki and his co-workers proposed, between the 70s and the 80s, an analytical model describing the fundamental frequency (F0) variations . The model, tested on many languages , assumes that (in a logarithmic scale) the contour is the superposition of two contributions, namely a phrase component yp and an accent component ya, obtained by filtering two signals. The first
2796	2842	considering two kinds of factors: speech rhythm parameters and the auditory temporal integration of the slowest spectral components. Starting from the modulation spectrogram Shastri et al.  used a different kind of artificial neural network, the temporal flow network introduced by Watrous . With this tool they compute a function having local peaks at syllabic nuclei. The main
2796	2842	deviation of distances between reference markers and automatically found marker is 13.4ms on both training and test corpora. Results in Italian seem to be close to those from other authors (cfr.  for boundary accuracy on spontaneous speech, and  on nuclei detection), while the training strategies seem to be roughly equivalent even with slightly better performances from simulated
2796	2842	number of deletion and insertion, basically errors can be distributed differentially. The error rate obtained after training is quite equivalent to those obtainable with more complex techniques , or better than other algorithm based techniques . Further modifications to syllabification algorithm are required for English. Results about tone unit seem poor, but the agreement
2796	2844	The main reason to provide a TCL/TK interface to APA is the need of a Graphical User Interface (GUI). In place of writing a new one, it was chosen to use Wavesurfer as GUI for APA. WaveSurfer  is an Open Source tool for sound visualization and manipulation. It has been designed to suit both novice and advanced users. WaveSurfer has a simple and logical user interface that provides
2796	2846	to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for automatic prosodic analysis Many systems have already been developed to analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main
2796	2847	analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main efforts in prosodic analysis were spent on intonation modelization. Examples are the Tilt Model , that directly detect those pitch variations related to linguistic events and give their description; while the MOMEL algorithm performs a stylization of the pitch contour where non-significant
2796	2847	which affect the pitch contour but are not important in this sense (e.g. segmental perturbations). Examples of an Intonation Model for which some automatic system is available are: the Tilt model  from Taylor, Fujisaki model , the Hirst model . The main desired properties of such representations are: • The representation should be as compact as possible, with few degrees of freedom.
2796	2847	amenable to automatic analysis and synthesis, if no care is devoted linguistic relevance of the representation. 1.4.1 Tilt model The basic unit in the Tilt model is the intonational event. Taylor  found two types of intonational events: pitch accents and boundary tones. Pitch 19saccents are pitch excursions associated with syllables which are used to give some degree of emphasis to a
2796	2848	known as tilt parameters, are determined from examination of the local shape of the event's F0 contour. The tilt model is built on a simpler model, the Rise/Fall/Connection (RFC) model. In the RFC  model, each event is modeled by a rise part followed by a fall part. Each part has an amplitude and duration, and two parameters are used to give the time position of the event in the utterance and
2796	2850	to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for automatic prosodic analysis Many systems have already been developed to analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main
2796	2855	this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for automatic prosodic analysis Many systems have already been developed to analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main efforts
2796	2856	analysis. Most of their effor only focused on syllable segmentation and recognition. Greenberg  showed how syllables can be important in speech recognition according to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for
2796	2856	Fujisaki’s parameter from natural pitch contour. Other systems were developed to identify syllable boundaries in speech signals. This segmentation can be interesting both for speech recognition  and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also
2796	2856	approach. 27sChapter 2. ALGORITHMS 2.1 Syllabification 2.1.1 Why syllables? Syllable segmentation is important in APA for tempo determination, but it can also be used as basic unit in recognition . The definition of what one call “syllable” is controversial. Some author bound syllables to jaw movement , to chest burst  or they consider syllables as the basic unit of speech
2796	2856	of Bark17sscaled loudness spectra calculated every 10 ms. Two kinds of artificial neural networks were compared, a multilayer perceptron and a radial basis function neural network. Wu et al.  based their analysis on smoothed speech spectra computed by two dimensional filtering techniques. In this way energy changes of the order of 150ms are enhanced, they apply also other techniques
2796	2856	but not least, the main aim of this work is to create the premises for the integration of data computed by APA in an Automatic Speech Recognizer. Syllables were already used in such an application , but stresses and intonational patterns are relatively not used, APA could provide interesting news in this field. 88sAppendix A. SIMULATED ANNEALING PSEUDOCODE T := sequence of temperatures
3847999	2859	the CP, so that a time-domain equalizer (TEQ) is placed in the receiver with the purpose of appropriately shortening the overall impulse response (OIR). Several TEQ designs have been proposed , , , , . Melsa et al.  introduced the Shortening SNR (SSNR) as the ratio of the OIR energy inside a window of size to that outside the window and suggested to choose the TEQ in order to
3847999	2859	in the minimization of alone. Fig. 1 shows the delay spread obtained with the MDS TEQ (8) of various lengths and for different values of . The channel was the standard DSL test loop CSA 1  combined with a POTS splitter and a twelfth-order Chebyshev bandpass filter for the 30–1000 kHz frequency band, and truncated to 512 samples (see Fig. 2). The sampling frequency was 2.208 MHz. The
2867	2868	artificial intelligence, and databases. Initially, mining problems have been grouped in three categories: identifying classifications, finding sequential patterns, and discovering associations . Intelligent solutions for such problems are application-dependent and different applications usually require different mining techniques. A field where artificial intelligence (AI) has the
2867	2878	a statistic for many pairwise tests) is usually handled by estimating corrected p-values for clusters. Although approaches have been proposed that seek to overcome the multiple comparison problem , they are based on a linearization of the 3D domain that might fail to preserve 100% the spatial locality of the ROIs. Another approach to detect functional associations in the human brain is to
2867	2878	domain to discover discriminative areas. This technique reduces the multiple comparison problem encountered in voxel-based analysis by applying statistical tests to groups of voxels. Compared with  this step of analysis is performed directly on the 3D domain (hyper-rectangles) without any loss of spatial locality. For classification, to avoid problems with distribution estimation techniques
2867	2879	of the ROIs. Another approach to detect functional associations in the human brain is to model (estimate) their underlying distributions when distinct classes are present (controls vs. patients) , , utilizing parametric, non-parametric or semi-parametric techniques. EM and k-means algorithms  have been employed for this purpose, and statistical distance based methods have been used
2867	2884	the first step of the analysis we employ Adaptive Recursive Partitioning (ARP) that has been so far applied mainly to realistic and synthetic 3D region datasets of discrete (binary) voxel values . Some initial results from attempts to apply the technique on real fMRI datasets have been presented in . The main idea of this technique is to treat the initial 3D volume as a hyper rectangle
2867	2885	mainly to realistic and synthetic 3D region datasets of discrete (binary) voxel values . Some initial results from attempts to apply the technique on real fMRI datasets have been presented in . The main idea of this technique is to treat the initial 3D volume as a hyper rectangle and search for informative regions by partitioning the space into sub-regions. The intelligence of the tool
2867	2888	We apply a method that efficiently extracts a k-dimensional feature vector using concentric spheres in 3D (or circles in 2D) radiating out of the ROI’s center of mass, initially presented in  and applied on artificially generated data. Here we demonstrate the potential of the technique to be utilized for characterizing real ROIs. The proposed technique extends the original idea of
2867	2888	technique has been shown to be two orders of magnitude faster than mathematical morphology (namely the “pattern spectrum”) although it achieves comparable to or even better characterization results . The purpose of extending these two approaches to be applicable on real data and combining them in the context of a unified approach is to create an intelligent brain informatics tool. This can be
2867	2888	following similar behavior and the two classes barely overlap. The curvature of the signatures conveys information about the activation patterns of the original data. As demonstrated initially in  with synthetic data, using morphological operators for such an analysis is two orders of magnitude slower than the approach employed here. As illustrated, patient samples exhibit positive
1884	2905	tails of the function in the range is very small, the effect of aliasing is essentially negligible. Note that in every case the energy terms are proportional to the sampling rate. It can be shown  that the energy of any uniformly (super-critically) sampled version of a band-limited signal is proportional to the sampling rate. APPENDIX C IS A REASONABLE ASSUMPTION? Suppose that we first wish
1884	2906	we can easily compute the following terms: (71) (72) (73) (74) (75) (76) (77) (78) 12 To recover exactly ?@?Y ?A would mathematically require an infinite number of measurements (or samples) ???@?Y ?A . But since we have considered a fairly large range (0IH to 10) for sampling, and since the energy in the tails of the function in the range is very small, the effect of aliasing is essentially
1884	2907	we discuss a more general case where neither the intensities and , nor the distance , are known. 8 Equation 7 Similar analysis for the two-dimensional extension of this problem is presented in . 8 But we assume that C aPis known to the detector.s682 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 5, MAY 2004 (22) leads to a detection problem defined in terms of a linear model over the
1884	2908	for indirect imaging such as in computed tomography. As for other extensions and applications in optical imaging, an appealing direction is to study the limits to super-resolution from video ???. The analysis presented here can help answer questions regarding the ability of image super-resolution methods to integrate multiple low resolution frames to produce a high resolution image
1884	2910	for indirect imaging such as in computed tomography. As for other extensions and applications in optical imaging, an appealing direction is to study the limits to super-resolution from video ???. The analysis presented here can help answer questions regarding the ability of image super-resolution methods to integrate multiple low resolution frames to produce a high resolution image from
2953	2957	methods. Feature-image methods are the focus of Petra van den Elsen’s work and our own work . For ultrasound to MR/CT registration, a featurefeature image registration method is proposed in . Vessel voxels are identified in both images and the distance between those sets of voxels is minimized. In  a deformable surface method is used to align ultrasound images and quantify liver
1194777	2981	connections. In contrast to the first approach, this can be done at the data middleware or application level. This approach has been implemented several times, including PSockets  and GridFTP . It has been observed that effectively utilizing high performance links can require dozens to hundreds of sockets. This can create an overhead, limiting the usefulness of this approach. In
1194777	2982	transmission mechanisms. SABUL implements these control functions in a separate TCP control channel. This approach is in contrast to the approach of other high performance protocols such as NETBLT  which combine the data and control channels. In SABUL, the packets on the UDP channel consist of the usual UDP header plus a 32 bit field for a sequence number. On the TCP channel, each packet
1194777	2990	distributed mining of geoscience data. Section 10 is the summary and conclusion. We are currently continuing the experiments described in this paper and preparing an expanded version of this paper . 2 Background and Related Work In this paper, we are concerned with supporting remote data analysis and distributed data mining applications with high performance data transport services. In
1194777	2993	TCP network connections. In contrast to the first approach, this can be done at the data middleware or application level. This approach has been implemented several times, including PSockets  and GridFTP . It has been observed that effectively utilizing high performance links can require dozens to hundreds of sockets. This can create an overhead, limiting the usefulness of this
1194777	2993	NSF’s vBNS network or the Internet 2’s Abilene network. In practice, very large bandwidth applications have to be scheduled on these networks and require the use of specialized transport protocols . As optical networking architectures become more common, a new possibility is emerging. A bandwidth demanding application can request an optical connection between the data sources and the data
1194777	2994	with this problem. One approach to improving TCP performance for data intensive applications is to adjust the TCP window size to be the product of the bandwidth and the RT T delay of the network . This approach requires modifying and tuning the kernel of each of the operating systems transporting the packets and ensuring that the networking hardware can support these large or jumbo packets.
1194777	2995	as an application library without making any changes to the existing network infrastructure. Another approach is to improve TCP in various ways. High Speed TCP , Scalable TCP , and FAST  are examples of this approach. Although these approaches all appear to be promising, work is still required to understand their friendliness, performance, and scalability. In addition, new TCP
1194777	2996	sites, and to collect the intermediate results and models. Systems with this architecture include the JAM system developed by Stolfo et al. , the BODHI system developed by Kargupta et al. , the Kensington system developed by Guo et al. , and the Papyrus system developed by Grossman et al. . The second approach is to use cluster middleware. Systems with this architecture
1194777	2997	In addition, new TCP variants require significant changes to the current network infrastructure. Another approach is to create entirely new protocols, such as Explicit Control Protocol (XCP)  and the Datagram Congestion Control Protocol (DCCP) . Again, deploying these new protocols will take some time due to the significant changes required to the current network infrastructure. 4s3
1194777	2998	it can be deployed as an application library without making any changes to the existing network infrastructure. Another approach is to improve TCP in various ways. High Speed TCP , Scalable TCP , and FAST  are examples of this approach. Although these approaches all appear to be promising, work is still required to understand their friendliness, performance, and scalability. In
1194777	3006	control the data mining algorithms at the different sites, and to collect the intermediate results and models. Systems with this architecture include the JAM system developed by Stolfo et al. , the BODHI system developed by Kargupta et al. , the Kensington system developed by Guo et al. , and the Papyrus system developed by Grossman et al. . The second approach is to use
3008	3188	each node is also annotated with a location field to describe the 2 We assume that the surrogate can be discovered in the local environment by some discovery service such as the Jini lookup service.sClass: A; Memory: 5KB; AccessFreq: 10; Location: surrogate; isNative: false; InteractionFreq: 12 BandwidthRequirement: 1 KB Figure 2. Illustration of our application program execution graph model.
3008	3009	computing environment. To support application-specific adaptation, application developers can use meta-level programming tools for deploying their applications in pervasive computing environments . One of the key differences between our dynamic offloading system and the above work is that pervasive application delivery can be realized without modifying the application or assuming that the
3008	3010	Unfortunately, these solutions are often associated with high levels of resource. Different approaches have been proposed to solve the problem by using application-based or system-based adaptations . However, these approaches often require degrading an application’s fidelity to adapt it to resource-constrained mobile devices. Moreover, adaptation efficiency is often limited by coarse-grained
3008	3011	without modifying applications. However, they assume that the application is already written in a component-based fashion and has exported component interfaces to the system. In the Gaia project , we proposed a dynamic service composition and distribution framework for delivering component-based applications in a pervasive computing environment. To support application-specific adaptation,
3008	3012	computing environment. To support application-specific adaptation, application developers can use meta-level programming tools for deploying their applications in pervasive computing environments . One of the key differences between our dynamic offloading system and the above work is that pervasive application delivery can be realized without modifying the application or assuming that the
3008	3013	partitioned at runtime based on its execution history and system/network resource information. Other closely related work includes application partitioning under different contexts. The Coign  project proposed a system to statically partition binary applications built from COM components. Unlike Coign, our approach performs dynamic runtime partitioning without any offline profiling.
3008	3015	to the surrogate and which program objects should be pulled back to the mobile device during an offloading action. To achieve both flexibility and stability, OLIE employs the Fuzzy Control model  for making offloading decisions. The Fuzzy Control model has previously been applied to coarse-grained application adaptations. The novelty of our approach is to apply the model to fine-grained
3008	3015	offloading, and (2) intelligent selection of an application partitioning policy. 3.1 Triggering of Adaptive Offloading OLIE makes the offloading triggering decision based on the Fuzzy Control model . The Fuzzy Control model includes: (1) a generic fuzzy inference engine based on fuzzy logic theory, and (2) decision-making rule specifications provided by system or application developers. For
3008	3017	expensive to rewrite an application according to the capacity of each mobile device. Hence, a fine-grained runtime offloading system, called adaptive infrastructure for distributed execution (AIDE) , has been proposed to solve the problem without modifying the application or degrading its fidelity. The key idea is to dynamically partition the application during runtime, and migrate part of the
3008	3017	dynamic offloading system architecture. device 2 . AIDE is responsible for properly transforming method invocations to objects that were offloaded to the surrogate into remote invocations . OLIE does not require any prior knowledge about an application’s execution or the resources of the system and the network to make offloading decisions. OLIE collects and analyzes all of the
3008	3018	Unfortunately, these solutions are often associated with high levels of resource. Different approaches have been proposed to solve the problem by using application-based or system-based adaptations . However, these approaches often require degrading an application’s fidelity to adapt it to resource-constrained mobile devices. Moreover, adaptation efficiency is often limited by coarse-grained
3008	3018	both application-based and system-based adaptations have been proposed to overcome resource constraints and environmental changes (e.g., wireless network fluctuations). The Odyssey project  introduced an application-aware adaptation service within the end host to accommodate resource changes, such as wireless network bandwidth fluctuations. Fox et. al. proposed an applicationbased
3008	3019	Unfortunately, these solutions are often associated with high levels of resource. Different approaches have been proposed to solve the problem by using application-based or system-based adaptations . However, these approaches often require degrading an application’s fidelity to adapt it to resource-constrained mobile devices. Moreover, adaptation efficiency is often limited by coarse-grained
3008	3019	both application-based and system-based adaptations have been proposed to overcome resource constraints and environmental changes (e.g., wireless network fluctuations). The Odyssey project  introduced an application-aware adaptation service within the end host to accommodate resource changes, such as wireless network bandwidth fluctuations. Fox et. al. proposed an applicationbased
3008	3021	profiling. Furthermore, we do not assume a componentbased application and enable mobile delivery for any application, even a complex monolithic application. More recently, Teodorescu et. al.  presented a system to support mobile Java program deployment by partitioningsJava program execution between system nodes and mobile devices. The system nodes prepare a Java application for
3008	3022	computing environment. To support application-specific adaptation, application developers can use meta-level programming tools for deploying their applications in pervasive computing environments . One of the key differences between our dynamic offloading system and the above work is that pervasive application delivery can be realized without modifying the application or assuming that the
3025	3026	on Internet-like topologies. We first devise a scenario where requests are homogeneously distributed across a chosen topology. The two topologies used in this section were generated by BRITE , a topology generator with the ability to create AS level topologies. Homogeneous Traffic (HT) This experiment exemplifies BGRP, BPS, and WDS behavior in the topology illustrated in Fig. 6 (b),
3025	3027	count, e.g., based on a given AS path size distribution. To obtain realistic values for ¤ , we use the values collected by Telstra  and presented in  1 . This model, an AS-level dumbbell  topology, is simple and sufficient to explain the state accounting methodology we use, since state along any path differs as a function of an AS location: sources, destinations, and intermediate
3025	3028	Data path Quality of Service (QoS) issues are by now reasonably well understood, and a number of different alternatives have been proposed and investigated, e.g., Integrated Services (IntServ)  and Differentiated Services (DiffServ) , each representing a different trade-off in terms of capability and scalability. However, the same understanding is not really available for control path
3025	3029	of mechanisms for reserving and maintaining the necessary data path resources, as embodied in proposals such as Internet Streaming Protocol (ST-II)  and Resource Reservation Protocol (RSVP) , with the latter being the current solution of ¡ This work was supported by the program Programa Operacional Sociedade de Informação (POSI), of the Portuguese Fundação para a Ciência e Tecnologia
3025	3032	by means of simulations, the performance of the algorithms for different network topologies. Finally, Section 5 summarizes findings and outlines future work. 2. Related Work Guérin et al.  present a survey of possible approaches to aggregate RSVP requests assuming unicast scenarios and covering issues such as RSVP state management and path characterization. The authors propose the
3025	3025	changes. Also, to achieve statistically meaningful results, each experiment has been repeated several times using different random number seeds. Further and more detailed results can be found in . 9 10 4 3 2 11 1 12 0 5 (a) 4.1. Tree Topology 6 5 18 31 44 6 41 24 46 22 34 14 29 9 7 1 0 20 25 4 38 21 (b) 11 2 13 27 3 33 23 3 42 16 16 7 40 48 39 8 49 0 10 30 8 10 43 26 35 13 36 17 14 28 19 19
3025	3034	tunnels, i.e., pipes between entry and exit points of a defined aggregation region, i.e., a cloud of routers where regular RSVP messages are ignored. A similar approach is followed by Berson et al. . They consider unicast and multicast scenarios, focusing also on RSVP aggregation within an aggregation region. These two approaches are concerned with RSVP scalability: RSVP requires all the
3025	3039	accounting is done both on the AS and edge router level by 2 We chose a Poisson process to model the arrival of requests since it is known to describe well user session arrivals, as mentioned in .scollecting statistics dynamically for incoming and outgoing reservations: minimum, average and maximum values are updated each time the corresponding variable changes. Also, to achieve
4470241	3045	who would consistently act contrary to his or her assignment. Under this assumption, which Imbens and Angrist (1994)call monotonicity, the inequalities in Equation (3.26) can be tightened (Balke and Pearl, 1997) to give P (y, X = 1|Z = 1) ? P (y, X = 1|Z = 0) P (y, X = 0|Z = 0) ? P (y, X = 0|Z = 1) (3.27) for all y ? {0, 1}. Violation of these inequalities now means either selection bias or a direct
4470241	3051	bounds as a function of sample size. Testable implications The two assumptions embodied in the model of Figure 5(a), that Z is randomized and has no direct effect on Y , are untestable in general (Bonet, 2001). However, if the treatment variable may take only a finite number of values, the combination of these two assumptions yields testable implications, and these can be used to alert investigators to
4470241	3057	maintained if the errors associated to X and to Y given X are not independent? This causal interpretation is more evident in procedures using observational and experimental data at the same time (Cooper and Yoo, 1999), in which the truncated factorization is used in the score derivation. The discovering of causal relationships from observational data has been discussed in learning literature. But I think that
4470241	3077	based on decision theory—in particular, the work of Savage (1954). That is, we defined Pearl’s causal model as well as the do operator and cause and effect in terms of Savage’s primitives (Heckerman and Shachter, 1995). As an additional benefit, we showed how Pearl’s causal model can be equivalently represented as an influence diagram—a graphical representation of decision making under uncertainty used now for
4470241	3101	Angrist et al., 1996; Greenland et al., 1999b), structural equation models Heckman and Smith (1998), and a more recent formulation, which unifies these approaches under a single interpretation (Pearl, 1995a, 2000). This paper aims at making these advances more accessible to the general research community 1 . To this end, Section 2 begins by illuminating two conceptual barriers that impede the
4470241	3101	that the corresponding background variables, UY and {UZ1 , . . . , UZk }, are independent in P (u). These assumptions can be translated into the potential-outcome notation using two simple rules (Pearl, 1995a, p. 704); the first interprets the missing arrows in the graph, the second, the missing dashed arcs. 1. Exclusion restrictions: For every variable Y having parents P A Y and for every set of
4470241	3122	formalized, can be extremely powerful in refining assumptions (Angrist et al., 1996), deriving consistent estimands (Robins, 1986), bounding probabilities of necessary and sufficient causation (Tian and Pearl, 2000), and combining data from experimental and nonexperimental studies (Pearl, 2000). The Section (4.3) presents a way of combining the best features of the two approaches. It is based ons316 J. Pearl
3130	3132	consider variations of Problem 1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the
3130	3133	and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer
3130	3134	variations of Problem 1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid
3130	3137	and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic
3130	3139	of Problem 1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state
3130	3140	switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming
3130	3142	mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or
3130	3143	mixed-integer quadratic programming  or iteratively by alternating between assigning data points to models and computing the model parameters starting from a random or ad-hoc initialization , . The first algebraic approach to the identification of PWARX models appeared in , where it was shown that one can identify the model parameters in closed form when the ARX models are of known
3130	3144	are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or
3130	3145	n) ?Mn?j?1 k=1 ?2 k (Ljn) + µ(Mn ? j), (14) where ?k(L j n) is the k th singular value of L j n and µ is a parameter. The above formula for estimating m is motivated by model selection techniques  in which one minimizes a cost function that consists of a data fitting term and a model complexity term. The data fitting term measures how well the data is approximated by the model – in this case
3130	3146	are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or iteratively
3130	3147	known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or iteratively by
3130	3151	1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has
3130	3152	in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been
3130	3154	. , zK] T ??  T , (6) with I chosen in the degree-lexicographic order; and ? ? ? ? n + K ? 1 n + K ? 1 Mn = = K ? 1 n is the total number of independent monomials. One can show  that the vector of coefficients hn ? RMn is simply a vector representation of the symmetric tensor product of the individual model parameters {bi} n i=1 , i.e. ? b?(1) ? b?(2) ? · · · ? b?(n), (8)
3130	3155	data points to models and computing the model parameters starting from a random or ad-hoc initialization , . The first algebraic approach to the identification of PWARX models appeared in , where it was shown that one can identify the model parameters in closed form when the ARX models are of known and equal order and the number of discrete states is less than or equal to four. In
3130	3155	II-A and II-B show how to decouple the identification of the model parameters from the estimation of the discrete state via a suitable embedding into a higherdimensional space, as proposed in . Sections II-C and IID show how to identify the orders and the model parameters from the derivatives of a polynomial whose coefficients are obtained from a rank constraint on the embedded data.
3130	3155	i = 1, . . . , n, then we have that for all t ? na there exists a discrete state ?t?1 = i ? {1, . . . , n} such that b T i xt = 0. (2) Therefore, the following hybrid decoupling constraint (HDC)  must be satisfied by the model parameters and the input/output data regardless of the value of the discrete state and regardless of the switching mechanism generating the evolution of the discrete
3130	3155	?n(xna+1) T . ?n(xT ) T ? ? ? hn = 0 ? R T ?na+1 , (9) where Ln ? R (T ?na+1)×Mn is the matrix of embedded input/output data. We are now interested in solving for hn from (9). In our previous work  we considered ARX models of equal and known orders na(1) = na(2) = · · · = na(n) = na and showed that if the number of measurements is such that T ? Mn+na?2 and at least 2na measurements correspond
3130	3155	hn from input/output data {ut, yt} T t=0. The rest of the problem is to recover the model parameters {bi} n i=1 and the orders of the ARX models {na(i)} n i=1 from hn. In our previous work , which deals with the case of ARX models of equal and known orders, we showed that one can identify the model parameters directly from the derivatives of pn(z) at a collection of n regressors {zi}
8918942	3165	type of learning has found many useful applications in domains with large amount of data where labeling of a training set for supervised learning is cost prohibitive or where autonomy is essential . However, clustering algorithms generally rely on some prior knowledge of the structure present in a data set. For instance, one needs to know whether or not clusters actually exist in data prior
8918942	3166	type of learning has found many useful applications in domains with large amount of data where labeling of a training set for supervised learning is cost prohibitive or where autonomy is essential . However, clustering algorithms generally rely on some prior knowledge of the structure present in a data set. For instance, one needs to know whether or not clusters actually exist in data prior
8918942	3166	and autonomously creates a new category. Another ad2 Louis Massey vantageous and distinguishing feature of ART is its ability to discover concepts in data at various levels of abstraction . This is achieved with the vigilance parameter ? ? (0,1]. First, a similarity measure S (Eq. 1) is computed to determine if an existing cluster prototype T j appropriately represents the critical
8918942	3166	obtained from HotBits (http://www.fourmilab.ch/hotbits/) ). Other similar experiments have been conducted with several real-life or benchmark data sets. One of these experiments is documented in . 6. Conclusion and Future Work 0.5 Vigilance 0.7 Non-Random random 0.9 Pseudo-Random Vs Random We have shown how the vigilance parameter of a binary ART neural network can be used to determine the
8918942	3167	We have described a method to establish if natural groups occur in the data. The residual question is whether those groups are the result of mere coincidence. Indeed, it can easily be demonstrated  both analytically and empirically that clusters do occur in a random data set. Evidently, such clusters are meaningless and clustering tendency of such origin must be appropriately detected. We now
8918942	3167	qualitative approach may not be ideal, but it suits our current objective by giving an idea of whether or not clusters may be due to mere chance. Elements of a quantitative approach are given in . 5. Empirical Validation Patterns are bit strings of length N. In the first experiment, we consider the case where tendency is determined by failing to reach M=1 at ?< ? min . The data set with
634091	3175	may replace its set toBeMoved by the intersection of that set with the set toBeMoved of another one. We do not give a preferred strategy here, one can refer to algorithms for the write-all problem . 3.5.8 MoveElement The procedure moveElement moves a value to the new hashtable. Note that the value is tagged as outdated in moveContents before moveElement is called. proc moveElement(v : Value \
634091	3176	store huge but sparsely filled tables. As far as we know, no wait- or lock-free algorithm for hashtables has ever been proposed. There are very general solutions for wait-free addresses in general , but these are not efficient. Furthermore, there exist wait-free algorithms for different domains, such as linked lists , queues  and memory management . In this paper we present an
634091	3176	H?index > 0 Ha2: H(i) < H?index Ha3: i ?= j ? Heap(H(i)) ?= ? ? H(i) ?= H(j) 23sHa4: index ?= currInd ? H(index) ?= H(currInd) Invariants about counters for calling the specification. Cn5: pc ?  ? cntfi = 0 Cn6: pc ?  ? pc ?  ? returngA = 10 ? pc ?  ? (returnrA = 59 ? returngA = 10 ? returnrA = 90 ? returnref = 10 ? pc ? 90 ? returnref = 10 ? cntfi = ?(rfi = null ?
634091	3176	?= null}]) f is injective, where h = H(next(currInd)) Invariants concerning procedure find (5. . . 14) fi1: afi ?= 0 fi2: pc ? {6, 11} ? nfi = 0 fi3: pc ? {7, 8, 13} ? lfi = hfi.size fi4: pc ?  ? pc ?= 10 ? hfi = H(index) fi5: pc = 7 ? hfi = H(currInd) ? nfi < curSize fi6: pc = 8 ? hfi = H(currInd) ? ¬Find(rfi, afi) ? rfi ?= done ? ¬ Find(Y, afi) fi7: pc = 13 ? hfi
634091	3177	a few extra pointer indirections suffices. Sometimes, however, the time and space complexities of a lock-free algorithm is substantially higher than its sequential, or ‘synchronized’ counterpart . Furthermore, some machine architectures are not very capable of handling shared variables, and do not offer compare-and-swap or test-and-set instructions necessary to implement lock-free
634091	3177	H?index > 0 Ha2: H(i) < H?index Ha3: i ?= j ? Heap(H(i)) ?= ? ? H(i) ?= H(j) 23sHa4: index ?= currInd ? H(index) ?= H(currInd) Invariants about counters for calling the specification. Cn5: pc ?  ? cntfi = 0 Cn6: pc ?  ? pc ?  ? returngA = 10 ? pc ?  ? (returnrA = 59 ? returngA = 10 ? returnrA = 90 ? returnref = 10 ? pc ? 90 ? returnref = 10 ? cntfi = ?(rfi = null ?
634091	3187	out a performance bottleneck, and failure of a single process can force all other processes to come to a halt. Therefore, wait-free, lock-free, or synchronization-free algorithms are of interest . An algorithm is wait-free when each process can accomplish its task in a finite number of steps, independently of the activity and speed of other processes. An algorithm is lock-free when it
634091	3187	very general solutions for wait-free addresses in general , but these are not efficient. Furthermore, there exist wait-free algorithms for different domains, such as linked lists , queues  and memory management . In this paper we present an almost wait-free algorithm for hashtables. Strictly speaking, the algorithm is only lock-free, but wait-freedom is only
634091	3187	hashing with open addressing, cf. . We do not use direct chaining, where colliding entries are stored in a secondary list, because maintaining these lists in a lock-free manner is tedious , and expensive when done wait-free. A disadvantage of open addressing with deletion of elements is that the contents of the hashtable must regularly be refreshed by copying the non-deleted elements
634091	3188	solutions for wait-free addresses in general , but these are not efficient. Furthermore, there exist wait-free algorithms for different domains, such as linked lists , queues  and memory management . In this paper we present an almost wait-free algorithm for hashtables. Strictly speaking, the algorithm is only lock-free, but wait-freedom is only violated when a
3190	3372	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3372	in . 3 Related work There are currently several peer-to-peer systems in use, and many more are under development. Among the most prominent are file sharing facilities, such as Gnutella  and Freenet . The Napster  music exchange service provided much of the original motivation for peer-to-peer systems, but it is not a pure peer-to-peer system because itssdatabase is
3190	3191	the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional, persistent storage service that supports
3190	3192	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3192	of systems like FreeNet with the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional, persistent storage
3190	3193	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3193	3 Related work There are currently several peer-to-peer systems in use, and many more are under development. Among the most prominent are file sharing facilities, such as Gnutella  and Freenet . The Napster  music exchange service provided much of the original motivation for peer-to-peer systems, but it is not a pure peer-to-peer system because itssdatabase is centralized. All three
3190	3194	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3194	like FreeNet with the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional, persistent storage service that
3190	3195	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3195	of systems like FreeNet with the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional,
3190	3197	of network hops, while retaining the scalability of FreeNet and the self-organizing properties of both FreeNet and Gnutella. Pastry and Tapestry bear some similarity to the work by Plaxton et al . The approach of routing based on address prefixes, which can be viewed as a generalization of hypercube routing, is common to all three schemes. However, the Plaxton scheme is not self-organizing
3190	939	a distributed directory service to locate content; this is different from PAST’s Pastry scheme, which integrates content location and routing. Pastry, along with Tapestry , Chord  and CAN , represent a second generation of peer-to-peer routing and location schemes that were inspired by the pioneering work of systems like FreeNet and Gnutella. Unlike that earlier work, they guarantee
3190	940	a file, which does not guarantee that the file is no longer available. These weaker semantics avoid agreement protocols among the nodes storing the file. An efficient routing scheme called Pastry  ensures that client requests are reliably routed to the appropriate nodes. Client requests to retrieve a file are routed to a node that is “close in the network” 1 to the client that issued the
3190	940	shown that the average distance traveled by a message, in terms of the proximity metric, is only 50% higher than the corresponding “distance” of the source and destination in the underlying network . Moreover, since Pastry repeatedly takes a locally “short” routing step, messages have a tendency to first reach a node, among the k nodes that store the requested file, that is near the client,
3190	940	One experiment shows that among 5 replicated copies of a file, Pastry is able to find the “nearest” copy in 76% of all lookups and it finds one of the two “nearest” copies in 92% of all lookups . Node addition and failure A key design issue in Pastry is how to efficiently and dynamically maintain the node state, i.e., the routing table, leaf set and neighborhood sets, in the presence of
3190	940	their current leafs sets, updates its own leaf set and then notifies the members of its presence. Routing table entries that refer to failed nodes are repaired lazily; the details are described in . Fault-tolerance The routing scheme as described so far is deterministic, and thus vulnerable to malicious or failed nodes along the route that accept messages but do not correctly forward them.
3190	3198	scheme in PAST ensures that the global storage utilization in the system can approach 100%, despite the lack of centralized control and widely differing file sizes and storage node capacities . In a decentralized storage system where nodes are not trusted, an additional mechanism is required that ensures a balance of storage supply and demand. Towards this end, PAST includes a secure
3190	3198	sufficient availability. In the event of storage node failures that involve loss of the stored files, the system automatically restores k copies of a file as part of a failure recovery procedure . Data privacy and integrity Users may use encryption to protect the privacy of their data, using a cryptosystem of their choice. Data encryption does not involve the smartcards. Data integrity is
3190	3198	show that PAST can achieve global storage utilization in excess of 95%, while the rate of rejected file insertions remains below 5% and failed insertions are heavily biased towards large files . Any PAST node can cache additional copies of a file, which achieves query load balancing, high throughput for popular files, and reduces fetch distance and network traffic. Storage management and
3190	943	Farsite uses a distributed directory service to locate content; this is different from PAST’s Pastry scheme, which integrates content location and routing. Pastry, along with Tapestry , Chord  and CAN , represent a second generation of peer-to-peer routing and location schemes that were inspired by the pioneering work of systems like FreeNet and Gnutella. Unlike that earlier work,
3190	3199	that certain operations were initiated by the same user. To provide stronger levels of anonymity and other properties such as anti-censorship, additional mechanisms may be layered on top of PAST . 2 PAST design Some of the key aspects of PAST’s architecture are (1) the Pastry routing scheme, which routes client requests in less than ?log16N? steps on average within a selforganizing, fault
3190	3200	that certain operations were initiated by the same user. To provide stronger levels of anonymity and other properties such as anti-censorship, additional mechanisms may be layered on top of PAST . 2 PAST design Some of the key aspects of PAST’s architecture are (1) the Pastry routing scheme, which routes client requests in less than ?log16N? steps on average within a selforganizing, fault
3190	3201	that certain operations were initiated by the same user. To provide stronger levels of anonymity and other properties such as anti-censorship, additional mechanisms may be layered on top of PAST . 2 PAST design Some of the key aspects of PAST’s architecture are (1) the Pastry routing scheme, which routes client requests in less than ?log16N? steps on average within a selforganizing, fault
3203	3219	diffusion path of ions. The diffusion field in these restricted environments changes by a tortuosity factor ? that characterizes the diffusivity in porous media. In the recent summary of this topic  the evolution of the diffusion actions of Liesegang rings formation, BelousovZhabotinsky waves and the cAMP (cyclic adenosine 3´,5´monophosphate) waves were analyzed. The main trend for all three
3229	3230	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3232	this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author, e-mail: uhl@cosy.sbg.ac.at School of Telematics & Network Engineering Carinthia
3229	3233	this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author, e-mail: uhl@cosy.sbg.ac.at School of Telematics & Network Engineering Carinthia
3229	3235	environment is mandatory for multimedia security applications. An overview about current requirements and implementations of contents protection systems for digital multimedia data is given in . Selective or partial encryption (SE) of visual data is an example for such an approach. Here, application specific data structures are exploited to create more efficient encryption systems (see
3229	3242	of visual data is an example for such an approach. Here, application specific data structures are exploited to create more efficient encryption systems (see e.g. encryption of MPEG video streams ). Consequently, selective encryption only protects the visually most important parts of an image or video representation relying on a secure but slow “classical” cipher. See  for a discussion
3229	3243	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3244	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3246	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3247	selective video encryption schemes resistant to bit errors Corresponding author, e-mail: uhl@cosy.sbg.ac.at School of Telematics & Network Engineering Carinthia Tech Institute, Klagenfurt, AUSTRIA  and compliant to video formats  have been proposed for wireless environments. In this work we propose and evaluate selective bitplane encryption for confidential transmission of image data in
3229	3251	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
715250	3255	(IntServ) model or in RSVP. It refers to all packets moving between each pairs? of source and destination. For instance, Flow ?? indicates all packets being transmitted from node ? to node ?.s, . In DiffServ networks, traffic is classified into three service classes: premium, assured and best-effort. The premium class traffic has the highest priority in comparison to other classes of
715250	3255	at each node along a path account for the most significant part of the whole end-to-end delay for that path. With the highest priority, premium class traffic experiences almost no queueing delays , . Therefore, choosing a fairly longer (in terms of hop-count) path from a source to a destination does not compromise its delay requirements. B. Optimal Premium-class Routing (OPR) Problem In
715250	3256	into consideration, the premium class traffic imposes very negative influences on other classes of traffic, especially when the network is highly loaded. We call this the inter-class effects . In , the authors presented simple performance models and analysis for DiffServ schemes. However, to make a strong case for the negative impacts the premium class traffic may impose on all other
715250	3256	and expediency of problem definition, we assume each node in the topology tries to reserve More simulation details and extensive results and measurements are presented in our technical report . Loss Probability 1 0.8 0.6 0.4 0.2 0 0.4 0.6 0.8 1 1.2 Offered load (Lambda) 1.4 1.6 1.8 Packet Delay (in packet) 100 80 60 40 20 2 0.1 (a) Packet Loss Probability 0 0.3 0.4 0.5 0.6 0.7 Offered
715250	3256	each node along a path account for the most significant part of the whole end-to-end delay for that path. With the highest priority, premium class traffic experiences almost no queueing delays , . Therefore, choosing a fairly longer (in terms of hop-count) path from a source to a destination does not compromise its delay requirements. B. Optimal Premium-class Routing (OPR) Problem In
715250	3257	consideration, the premium class traffic imposes very negative influences on other classes of traffic, especially when the network is highly loaded. We call this the inter-class effects . In , the authors presented simple performance models and analysis for DiffServ schemes. However, to make a strong case for the negative impacts the premium class traffic may impose on all other service
715250	3258	may be reduced). Actually, if we look at the problem from the second perspective, ?× becomes a load-balancing or fairness index of a network, which is similar to the max-min fairness in , , . Together with the assumption of hop-by-hop routing, an interesting problem is what is the optimal value of ? × (denoted as ?Ñ?Ü) we can get. This problem is closely related to the routing
715250	3258	An optimal local solution for a single source may not be the solution to the whole OPR problem. It is not difficult to show that this problem is a NP-Complete problem by following the proof in . Therefore, the SP routing algorithm may not always be the optimal one. By using the same topology in Figure 1, but applying a different routing algorithm, Figure 4 illustrates a simple case where
715250	3258	the Optimal Premium-class Routing (OPR) problem - to find an optimal routing algorithm to efficiently service premium class traffic and reduce negative inter-class effects simultaneously. In , J. Kleinberg addressed an NP-Complete problem which combined the selecting paths for routing and allocating bandwidth fairly among connections in the max-min sense. Following their proof of
715250	3259	may be reduced). Actually, if we look at the problem from the second perspective, ?× becomes a load-balancing or fairness index of a network, which is similar to the max-min fairness in , , . Together with the assumption of hop-by-hop routing, an interesting problem is what is the optimal value of ? × (denoted as ?Ñ?Ü) we can get. This problem is closely related to the routing
715250	3260	IV. Simulations and results are illustrated in Section V. Finally, Section VI concludes this paper. II. RELATED WORK Extensive researches have been conducted on QoS routing issues recently. In , S. Chen and K. Nahrstedt did a thorough survey on QoS routing algorithms. But they focused on network models in virtual circuit mode. Our work in this paper is based on the hop-by-hop routing
715250	3261	in virtual circuit mode. Our work in this paper is based on the hop-by-hop routing scheme. Issues on hop-by-hop routing algorithms, such as isotonicity, search of optimal paths, were studied in . The author provided an elegant algebra basis to study the QoS routing issues in the Internet. In our paper, although we will borrow some definitions and theorems from the work in , the problem
715250	3261	? to , then there will be a forwarding loop between ? and ? in the network, although the two paths are loop-free individually. Formal definitions of isotonicity and strict isotonicity were given in , and they state that the order relation between the weights of any two simple paths is preserved if both of them are either prefixed or appended by a common, third, simple path. Since both of them
715250	3261	smallest identifier to break the tie. However, the WSP always chooses the widest path among the set of shortest paths between any pair of source and destination. The WSP has been well-studied in , . Although it does not have a strict isotonicity, the isotonicity still holds. Therefore, by using the Dijkstra-oldtouch-first (Dijkstra-OTF) algorithm proposed in , we cansguarantee that
715250	3261	isotonicity property holds, the BSP algorithm with this weight function can guarantee that packets are transmitted through the lightest path between any pair of source and destination without loop . Therefore, it can be used as a hop-by-hop routing algorithm as well. If there are more than one lightest paths between the source and destination, the path with the least hop count is selected.
715250	3262	whole network topology and tries to address the OPR problem in a global view. In our work we show that an optimal local solution for a single source may not be the solution to the OPR problem. In , the authors addressed the QoS routing from the precomputation perspective and proposed a hierarchical algorithm to solve the All-Hops Problem. In , the authors discussed path selection
715250	3263	identifier to break the tie. However, the WSP always chooses the widest path among the set of shortest paths between any pair of source and destination. The WSP has been well-studied in , . Although it does not have a strict isotonicity, the isotonicity still holds. Therefore, by using the Dijkstra-oldtouch-first (Dijkstra-OTF) algorithm proposed in , we cansguarantee that WSP
715250	3263	gain, we need to choose a “best” path from a broader range. Therefore, the BSP algorithm is introduced. The BSP algorithm was studied and called the “Shortest-distance path algorithm” in . It was used in a call-based or connection-oriented network (such as the ATM network). The BSP is basically a shortest-path algorithm with the distance or weight function defined as and Û Ú??Ú? ?
3294	3296	problem is, in general, undecidable.  defines a scheme based on abstract interpretation for the (static) analysis of the unsatisfiability of equation sets, and show how various analyses such as  can be seen as instances of the scheme. In this work, we are concerned with equation solving or E-unification with respect to a given set E of equations. We define an optimization of (basic)
3294	3296	been dropped using either of the standard simplifying strategies of narrowing which rest on the use of loop-checking , unification rules , operator joinability  or eager normalization  techniques. The following theorem proves that our approach is sound and complete. Theorem 4.4. correctness and completeness of refined Conditional Narrowing Let E be a level-canonical Horn
3294	3297	has been implemented. In spite of the fact that it does not have the simplicity of simple basic narrowing, we have chosen to implement an innermost selection basic conditional narrowing procedure  since it further reduces the size of the search space. It is difficult to state a general result for the efficiency improvement of the optimization. Table 1 gives an impression of some achievable
3294	3304	occurring in the syntactic object s. A free variable is a variable that appears nowhere else. The symbol ? denotes a finite sequence of symbols. We describe the lattice of equation sets following . We let Eqn denote the set of possibly existentially quantified finite sets of equations over terms. We let fail denote the unsatisfiable equation set, which (logically) implies all other equation
3294	3304	bCN we introduced above. For this purpose, we follow the top-down approach to abstract interpretation which is based on constructing and examining abstract transition systems as defined in . Some of the following definitions are already in  and are reported for completeness. A difference with respect to  is in the definition of abstract most general unifier (Definition 3.5)
3294	3304	states which are equivalent modulo variable renaming. To ensure finiteness of the analysis, we collapse states which are variable renamings of each other into a single ‘equivalent’ state, as in . In the following, we formalize the idea that abstract narrowing reduction approximates narrowing reduction with abstract states, abstract unification and abstract term rewriting systems replacing
3294	3305	missing proofs can be found in . 2 Preliminaries Let us first summarize some known results about equations, conditional rewrite systems and equational unification. For full definitions refer to . Throughout this paper, V will denote a countably infinite set of variables and ? denotes a set of function symbols, each with a fixed associated arity. ?(? ?V) and ?(?) denote the sets of terms
3294	3306	execution. Keywords: Abstract interpretation, equational logic programming, term rewriting systems, universal unification. 1 Introduction The recent interest in logic programming with equations  has promoted much work on equational unification  and narrowing . Equational unification (E-unification) characterizes the problem of solving equations modulo an equational theory
3294	3306	problem is, in general, undecidable.  defines a scheme based on abstract interpretation for the (static) analysis of the unsatisfiability of equation sets, and show how various analyses such as  can be seen as instances of the scheme. In this work, we are concerned with equation solving or E-unification with respect to a given set E of equations. We define an optimization of (basic)
3294	3306	removed subtree could not have been dropped using either of the standard simplifying strategies of narrowing which rest on the use of loop-checking , unification rules , operator joinability  or eager normalization  techniques. The following theorem proves that our approach is sound and complete. Theorem 4.4. correctness and completeness of refined Conditional Narrowing Let E be
3294	3312	missing proofs can be found in . 2 Preliminaries Let us first summarize some known results about equations, conditional rewrite systems and equational unification. For full definitions refer to . Throughout this paper, V will denote a countably infinite set of variables and ? denotes a set of function symbols, each with a fixed associated arity. ?(? ?V) and ?(?) denote the sets of terms
3294	3312	assumptions hold for all theories we consider in this paper. The equational theory E is said to be canonical if the binary one-step rewriting relation ?R defined by R is noetherian and confluent . A function symbol f ? ? is irreducible iff there is no rule (? ? ? ? e1,e2,...,en) ? R such that f occurs as the outermost function symbol in ?, otherwise it is a defined function symbol. In
3294	3315	rewriting systems, universal unification. 1 Introduction The recent interest in logic programming with equations  has promoted much work on equational unification  and narrowing . Equational unification (E-unification) characterizes the problem of solving equations modulo an equational theory E. The narrowing mechanism is a powerful tool for constructing complete
3294	3315	narrowing has quite a large search space, several strategies to control the selection of redexes have been devised to improve the efficiency of narrowing by getting rid of some useless derivations . Narrowing at only basic positions has been proven to be a complete method for solving equations in the theory defined by a level-canonical conditional term ? This work has been partially supported
3294	3315	de Valencia, Camino de Vera s/n, Apdo. 22012, 46020 Valencia, Spain. ‡ Dipartimento di Elettronica e Informatica, Università di Padova, Via Gradenigo 6/A, 35131 Padova, Italy.srewriting system . In , a further refinement is considered which derives from simulating SLD-resolution on flattened equations and in which the search reduces to an innermost selection strategy. The ability
3294	3315	of E. The set of all E-unifiers of E is recursively enumerable . Conditional narrowing has been shown to be a complete E-unification algorithm for theories satisfying different restrictions . 3 Abstract Basic Conditional Narrowing Basic (conditional) narrowing is a restricted form of (conditional) narrowing where only terms at basic occurrences are considered to be narrowed .
3294	3315	of basic is to avoid narrowing steps on subterms that are introduced by instantiation. Basic Conditional Narrowing is a complete E-unification algorithm for level-canonical Horn equational theories . Let R be a level-canonical TRS. We formulate a Basic Conditional Narrowing calculus according to the partition of equational goals into a skeleton and an environment part, as in . The skeleton
3294	3317	of ? is a conjunction of equations and the second occurrence is the application of a substitution. We consider the usual preorder on substitutions ?: ? ? ? iff ??. ? ? ??. Note that ?? ? ? iff ? ? ? . A substitution {x1/t1,...,xn/tn} is a unifier of an equation set E iff {x1 = t1,...,xn = tn} ? E. We denote the set of unifiers of E by unif(E) and mgu(E) denotes the most general unifier of the
3294	3317	we show how the test ? of ‘compatibility’ on abstract substitutions can be effectively checked by providing an algorithm to decide it. First we need to extend the notion of parallel composition  from substitutions to abstract substitutions. Parallel composition was proposed in  in order to provide a compositional characterization of the semantics of Horn Clause Logic. The formalization
3294	3317	combined afterwards in order to get the final result. Roughly speaking, parallel composition is the operation of unification generalized to substitutions. It can be performed in the following way . Given two idempotent substitutions ?1 and ?2, consider the set of all pairs corresponding to the bindings of both ?1 and ?2. Then, compute the most general unifier of such a set. Note that the
3320	3325	retrieval purposes. Furthermore a lot of work on this subject is done in the Semantic Web community. The development of annotation formalisms and reasoning in Web environments (e.g. Motta et al., ), language technology for the Semantic web (most notable the OntoWeb SIG-5 4 ), the development of tools to support manual 4 For more information on the OntoWeb cf
3320	3326	retrieval purposes. Furthermore a lot of work on this subject is done in the Semantic Web community. The development of annotation formalisms and reasoning in Web environments (e.g. Motta et al., ), language technology for the Semantic web (most notable the OntoWeb SIG-5 4 ), the development of tools to support manual 4 For more information on the OntoWeb cf
3320	3331	to describe words that belong to distinct categories as synonyms for one concept. In this sense the lexicon reflects the EuroWordNet strategy to allow for cross-category listing of synonyms . – Regular expression-like syntax can be used to combine entries with the same structure that differ for example only in the choice of a word (such as alternative prepositions expressing the same
3320	3335	further in this section. 6.1 Annotation of Video Recordings The only textual sources that can be directly matched to the video recordings are the transcripts of the speech in those recordings . Even though Information Extraction results for these transcripts is possible, the transcripts themselves contain more errors than the other sources. Furthermore they contain relatively few actual
3320	3337	that aim at semantic access of Web content. Nicolas et al.  describe an Information Retrieval system in which both text documents and queries are translated to a CG representation, Zhong et al.  use CG’s to retrieve online descriptions of garments, Ounis and Pasca , Myaeng  and Montes-y-Gomez  also use CG’s for information retrieval purposes. Furthermore a lot of work on this
