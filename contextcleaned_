9074080	22	and type II diabetes. Finally, experimental findings from animal studies show that the refractive state of young chicks will adapt to compensate for refractive errors induced by spectacle lenses. The combination of genetic and environmental influences on the development of myopia suggests that myopia is a complex disorder and should not be classified as a simple Mendelian trait. Further
9074107	49	matching procedure gives rise to the selection of the appropriate winning unit. In the present paper, we investigate Wittmeyer’s iteractive scheme  for solving the input reconstruction problem  and combine it with a winner-takeall method, where the winner is selected upon a least squares Goodness-Of-Fit (GOF) of the input reconstruction. At this point, the method resembles the
9074107	49	between the input and the memories), and only one neuron stays active. It is also true for the normalised memories that the neuron with the largest initial activity will be ‘selected’ as the winner . The DCR scheme that utilises overlapping memories can thus be viewed as a linear network that promotes contrast enhancement sometimes up to the extreme of WTA behaviour at the level of the
9074107	58	at the level of the internal representation via indirect inhibitory action between neurons (expressed by the term ?a e), and the inhibitory action is mediated by the reconstruction procedure . 3. Extension using the Winner-TakeAll Procedure It is straightforward to extend the DCR scheme to a categorising architecture by designing DCR subnets for each category and utilising a WTA layer
9074107	65	to the DCR net is strongly recommended. Independent component analysis is thus attractive, since it is closely related to sparse representation and prefers concentrated neural activities . In this paper, no effort was made to develop such a preprocessing stage, e.g. in the form of Gabor filters: it means that the results could be improved. If one considers how simple and fast the
9074120	80	the MPS from the initial probability vector and Vi, j t ( ) , according to the standard computation algorithms. 4 DEEM at work As already described in Section 2, DEEM possesses a GUI inspired by  and realized using an X11 installation with Motif runtime Libraries which the user employs to define his model of a MPS. We remark that while building the models, the attributes of the model
9074120	82	models a phase change.sFigure 2: Property windows associated to Transition t1 Figure 3: Property windows associated to TransitionSO1-yes Figure 4: DEEM Interface and the DSPN model of the MPS in  Each net is made dependent on the other one by marking-dependent predicates which modify transition rates, enabling conditions, reward rates, etc., to model the specific MPS features. Marking
9074121	86	to n-dependent and n-independent functions. Probably this gap can be reduced by considering the fixed points of f1,f2,..., successively, and by using a proper renormalization procedure . However, this has not yet been successfully carried out.sN. Kataoka, K. Kaneko / Physica D 181 (2003) 235–251 241 Fig. 4. Phase diagram (·-2) regarding the behavior of the attractor of fn,
7448224	90	important one. When the term user implies a team of persons, different criteria of what constitutes credibility can apply. Various formal methods of credibility assessment are then often suitable (Balci 1998). For implementation we hence need a simulation model in which the user can have faith in the sense that it is comStåhl patible with her own mental model of the studied system. In order to
7448224	94	slightly modified. Some examples are Simula (based on Algol 60), SIMSCRIPT (based on FORTRAN), MODSIM (based on Modula), CSIM19 (based on C++; see Schwetman 2001) and Silk (based on Java; see Kilgore 2000). Since they have a GPL, often object oriented, in the core, they can be used for writing in principle any kind of program, doing this in a very structured fashion. 3. Block Based Simulation
8918452	125	and structures as the works of Lefebvre, de Certeau and the Situationists might imply. Everyday life might first and foremost be constituted by the search for simplicity, comfort and order . We will argue here, that there is no need to consider the two interpretations mutually exclusive. Actually the very articulation of social friction might prove a manoeuvre that can bind them
172	173	However, individual rationality results in free-riding behavior among peers, at the expense of collective welfare. Empirical studies have shown prevalent free-riding in P2P file sharing systems . While it is possible that free-riding can be sustainable in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of
172	174	???¤£ and ????? are positive constants. Hence user benefit is an increasing function of the number of contributors, but with diminishing returns—a form widely accepted in this context (see, e.g., , , ). Thus, the performance of the system, denoted by ????????????? , is defined as the difference between the average benefit received by all users (including both contributors and
172	175	and ????? are positive constants. Hence user benefit is an increasing function of the number of contributors, but with diminishing returns—a form widely accepted in this context (see, e.g., , , ). Thus, the performance of the system, denoted by ????????????? , is defined as the difference between the average benefit received by all users (including both contributors and free-riders)
172	176	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	176	the free-riders. We can interpret the value p as the probability that a free-rider can be caught and excluded from the system. Alternatively, we can adopt a service differentiation interpretation , where the free-riders receive a reduced benefit of (1 - p)Q. Downgrading the benefit of the free-riders increases user contribution in two ways. First, the reduction in system load reduces the
172	177	are two ways to counter whitewashing attacks. The first is to require the use of free but irreplaceable pseudonyms, e.g., through the assignment of strong identities by a central trusted authority . In the absence of such mechanisms, it may be necessary to impose a penalty on all newcomers, including both legitimate newcomers and whitewashers. This results in a social cost due to cheap
172	178	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	180	the free-riders. We can interpret the value p as the probability that a free-rider can be caught and excluded from the system. Alternatively, we can adopt a service differentiation interpretation , where the free-riders receive a reduced benefit of (1 - p)Q. Downgrading the benefit of the free-riders increases user contribution in two ways. First, the reduction in system load reduces the
172	181	re-joins the network under new identities to avoid the penalty imposed on free-riders. The whitewashing attack is made feasible by the availability of low cost identities or cheap pseudonyms . There are two ways to counter whitewashing attacks. The first is to require the use of free but irreplaceable pseudonyms, e.g., through the assignment of strong identities by a central trusted
172	181	necessary to impose a penalty on all newcomers, including both legitimate newcomers and whitewashers. This results in a social cost due to cheap pseudonyms, as demonstrated by Friedman and Resnick . We develop a simple modeling framework that helps to predict the level of free-riding in P2P systems. We use this model to quantify the effect of a penalty mechanism, which gives free-riders
172	181	same time, a fraction §? ¢¡£s? §? ¤¡ ?¦¥ ? of users whitewash under FI. strategy by imposing the penalty on all newcomers. However, this results in a social cost, as shown by Friedman and Resnick . In this section, we are interested in quantifying the social cost of cheap pseudonyms in terms of system performance loss. We do so by extending our model from section 4 into a dynamic model with
172	182	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	183	peers according to their contribution level. However, imposing penalties on free-riders require some means of identifying free-riders and distinguishing them from contributors. Reputation systems  may help, but these systems may be vulnerable to the whitewashing attack, where a free-rider repeatedly re-joins the network under new identities to avoid the penalty imposed on free-riders. The
172	183	the free-riders. We can interpret the value p as the probability that a free-rider can be caught and excluded from the system. Alternatively, we can adopt a service differentiation interpretation , where the free-riders receive a reduced benefit of (1 - p)Q. Downgrading the benefit of the free-riders increases user contribution in two ways. First, the reduction in system load reduces the
172	185	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
172	186	peers according to their contribution level. However, imposing penalties on free-riders require some means of identifying free-riders and distinguishing them from contributors. Reputation systems  may help, but these systems may be vulnerable to the whitewashing attack, where a free-rider repeatedly re-joins the network under new identities to avoid the penalty imposed on free-riders. The
172	187	????? are positive constants. Hence user benefit is an increasing function of the number of contributors, but with diminishing returns—a form widely accepted in this context (see, e.g., , , ). Thus, the performance of the system, denoted by ????????????? , is defined as the difference between the average benefit received by all users (including both contributors and free-riders) and
172	188	However, individual rationality results in free-riding behavior among peers, at the expense of collective welfare. Empirical studies have shown prevalent free-riding in P2P file sharing systems . While it is possible that free-riding can be sustainable in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of
172	189	in equilibrium and may even occur as part of the socially optimal outcome , there has been significant interest in the design of incentive mechanisms to encourage cooperation in P2P systems . In many of the proposed incentive schemes, rewards and/or punishments are handed out to peers according to their contribution level. However, imposing penalties on free-riders require some means
190	192	the union of pairwise Minkowski sums. After the second step, there can be O(n 2 ) pairwise Minkowski sums. The pairwise convex Minkowski sums are convex. Their union can have O(n 6 ) complexity . 3.3 Our Approach Our algorithm for computing the Minkowski sum is based on the decomposition property. We have a set of convex primitives consisting of the pairwise convex Minkowski sums whose
190	195	Motion Planning with Translation Motion 5 sums . In the worst case, k = O(n 2 ). However, for non-convex polyhedra in 3D, the Minkowski sum can have O(n 6 ) worst-case complexity . One common approach for computing Minkowski sum of general polyhedra is based on convex decomposition. It uses the following property of Minkowski sum. If P = P1 ? P2, then P ? Q = (P1 ? Q) ? (P2
190	196	the two polyhedra and compute pairwise Minkowski sums between the convex pieces. We used a modification of the convex decomposition scheme available in a public collision detection library, SWIFT++ . We used a convex hull algorithm to compute the pairwise Minkowski sums. This algorithm adds the vectors of each vertex of one polyhedron with that of every vertex of the other polyhedron to get a
190	200	for polygonal objects was also developed by Avnaim and Boissonant . Sacks  presented a practical configuration space computation algorithm for pairs of curved planar parts. Halperin  presented efficient and robust algorithms to compute the Minkowski sum of 2D polygonal objects and used them for exact motion of planning of 2D objects undergoing translation motion. Aronov and
190	201	potential field . These approaches can be resolution complete if the resolution parameters are selected properly, but not exact or complete. Other algorithms are based on probabilistic roadmaps , which have been successfully applied to many high-dof robots. However, they may not terminate in a deterministic manner when there is no collision-free path.s2 Varadhan et. al. In this paper, we
190	201	connectivity roadmap. 5.1 Sampling We construct a roadmap by performing a sampling of the free space. Unlike previous approaches such as probabilistic roadmaps (PRMs) that generate samples randomly , we construct a roadmap in a deterministic fashion. Our goal is to sample the free space sufficiently to capture its connectivity. If we do not sample the free space adequately, we may not detect
190	206	efficient algorithm for a polygonal robot among polygonal obstacles with 3-dof configuration space. A similar algorithm for polygonal objects was also developed by Avnaim and Boissonant . Sacks  presented a practical configuration space computation algorithm for pairs of curved planar parts. Halperin  presented efficient and robust algorithms to compute the Minkowski sum of 2D
190	206	the configuration space obstacle corresponds to performing a sliced Minkowski sum operation on RSV and OSV . This is related to the concept of computing critical slices in the configuration space . By using the above formulation, we can use our path planning algorithm to compute a collision-free path. 7.3 Tangential Contact The sampling algorithm presented earlier uses an octree to perform
12008184	228	with humans and while performing tasks. Parts of this architecture have already been extended to several robots designed specifically for enhanced human interaction, such as MIT’s robot Leonardo (Breazeal, 2003) (Figure 3). While Leonardo is not a humanoid, it is being developed with human-like characteristics and functionalities. We are also extending the architecture and methodology to include and study
12008184	230	high-level control mechanisms aboard Robonaut. These cognitive architectures are ACT-R/S (Harrison and Schunn, 2003) based upon the ACT-R architecture (Anderson and Lebiere, 1998) and Polyscheme (Cassimatis, 2002). ACT-R is one of the most prominent cognitive architectures to have emerged in the past two decades as a result of the information processing revolution in the cognitive sciences. Also called a
12008184	230	can be performed. We use ACT-R/S to create cognitively plausible models of human performance of tasks to be performed by the robots. Furthermore, we are using Cassimatis’ Polyscheme architecture (Cassimatis, 2002) for spatial, temporal and physical reasoning. The Polyscheme cognitive architecture enables multiple representations and algorithms (including ACT-R models), encapsulated in “specialists” to be
12008184	237	individuals who do not know the spatial reasoning capabilities and limitations of the robot provide instructions to the robot for performing various tasks where spatial referencing is required (Perzanowski, et al., 2003). The results of this study will be used to enhance the multimodal interface by establishing a common language for spatial referencing which incorporates those constructs and utterances most
12008184	241	we might want to build into an intelligent, collaborative robot. The capabilities described above have been successfully implemented and demonstrated on several mobile robotic platforms (Sofge, et al., 2004), and we are now porting them to Robonaut. We are also extending the capabilities of the cognitive architectures (both ACT-R/S and Polyscheme) and their perspective-taking cognitive models. Future
12008184	242	“north.” Interaction with a robot capable of manipulating the same representation instead of traditional real number matrices would be more natural and efficient. In (Bugajska, et al., 2002) and (Trafton, et al., 2003) we used cognitive models of human performance of the task to augment the capabilities of robotic systems. We are investigating the use of two cognitive architectures based on human cognition for
12008184	243	that communication among them will be much easier. For example, in tasks requiring direction generation, humans naturally use qualitative spatial relationships (Miller and JohnsonLaird, 1976; Tversky, 1993) such as “left,” “up”, “east,” or “north.” Interaction with a robot capable of manipulating the same representation instead of traditional real number matrices would be more natural and efficient.
8918473	288	the user to focus on the customer, rather than worrying about how to access and update the information. Empowering users with access to their sales data via a Natural Language Interface (NLI)  does just that. Rather than taking the time to learn how to navigate through extensive menus only to be forced to sit through multiple screen refreshes, users simply ask for what they want in one
8918473	289	This is all the more important in a mobile setting, where the NLI does not have the luxury of a complementing GUI. An Agent-Oriented NLI The Adaptive Agent Oriented Software Architecture (AAOSA) is an Agent Oriented Software Engineering (AOSE)  system used for Natural Language Interfaces (NLI) . To create an NLI using AAOSA, an engineer will model the application, not the language,
303	307	matrix, further denoted by ?, we have For the resulting within-class means, further denoted by ?i, we have It can be shown, but this falls outside the scope of this paper, that 1 ? = Nex ? 1 S2W. (8) ?i = U ? Wµi, i = 1, . . . , Nuser. (9) ?j,j = 1, j = 1, . . . , NPCA ? Nuser + 1, (10) (?i)j = 0, j = 1, . . . , NPCA ? Nuser + 1. (11) This means that only the last Nuser ? 1 dimensions of U ? WW
303	307	and a test set of, nearly, equal sizes. This was done in such a way that the three measurements for the same grip were kept together. The total mean, the diagonal within-class covariance matrix ? (8), the within-class means ?i (9) and the total transform matrix T (12) were estimated from the training set. Similarity scores (13) were computed for all the data in the training and in the test set.
303	309	by ?i, we have It can be shown, but this falls outside the scope of this paper, that 1 ? = Nex ? 1 S2W. (8) ?i = U ? Wµi, i = 1, . . . , Nuser. (9) ?j,j = 1, j = 1, . . . , NPCA ? Nuser + 1, (10) (?i)j = 0, j = 1, . . . , NPCA ? Nuser + 1. (11) This means that only the last Nuser ? 1 dimensions of U ? WW can contribute to the verification. Therefore, a further dimension reduction is
316	317	protein interactions, relational data mining, graph-based data mining 1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics
316	317	YMR207C Definition 1. A single-node basis set is identical to a set of descriptors Di ?D. This definition is equivalent to the basic definition of an item set used in association rule mining . Our goal is to mine relational basis sets that will be constructed from multiple descriptor sets that belong to the same tuple of a joined relation. An edge relation has two attributes RE(Tl,Tr),
316	318	protein interactions, relational data mining, graph-based data mining 1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics
316	318	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
316	8918479	the known correlations dominate all other observations either directly or indirectly. This problem has been observed when relational association rule mining is directly applied to protein networks . Excluding matching items of interacting proteins is therefore commonly advisable in the interest of getting meaningful results alone . Matching annotations can be studied by simple correlation
316	321	of annotations on an individual network is discussed in . These approaches fall short of contrasting annotations in different networks. A further related research area is graphbased ARM . Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find frequent substructures in that setting. Removal of a class of redundant
316	322	principle, have the alternate form RE(Tl,Tr,D (E) )withD (E) being a set of edge descriptors. We could split such a relation into a separate node relation as well as a standard edge relation as in . Joined-relation basis sets are formed in multiple steps. Edge and node relations are joined through a natural join operation (?). Attribute names are changed  such that they are unique. We use
316	322	join-relation and any higher order relation. The support and confidence will however vary depending on that context, and a rule that is strong in one context may not be so in another. We follow  in always using the lowest order possible. For network comparison purposes we need three entities to derive 2-node rules. See definition (6). The problems associated with multiple contexts leads us
316	323	ARM is to find frequent substructures in that setting. Removal of a class of redundant rules is an important part of differential rule mining. Redundant rules have been studied, and closed sets  have proven a successful approach to their elimination. Closed sets alone do not, however, address the problem of contrasting different nodes or networks. Since we know what kinds of rules we want
316	324	1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics problems, biologists are interested in comparing different sets of items.
316	324	notes the problem of what we term repetitious item sets but does not resolve it. Relational association rule mining has more generally been addressed in the context of inductive logic programming . These approaches are very flexible and leave most choices up to the user. This paper, on the other hand, addresses the question of what specifications allow extracting meaningful rules. It is
316	327	protein interactions, relational data mining, graph-based data mining 1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics
316	327	rule mining is commonly defined and implemented over sets of items. We combine the concept of sets with the relational algebra framework by choosing an extended relational model similar to  . Attributes within this model are allowed to be set-valued, thereby violating first normal form. We go one step further by allowing sets of tuples, i.e. relations themselves, as attribute values.
316	329	of annotations on an individual network is discussed in . These approaches fall short of contrasting annotations in different networks. A further related research area is graphbased ARM . Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find frequent substructures in that setting. Removal of a class of redundant
316	332	1. INTRODUCTION Association Rule Mining (ARM) is a popular technique for the discovery of frequent patterns within item sets . The technique has been generalized to the relational setting  including the study of annotations of proteins within a protein-protein interaction network . In many bioinformatics problems, biologists are interested in comparing different sets of items.
316	332	notes the problem of what we term repetitious item sets but does not resolve it. Relational association rule mining has more generally been addressed in the context of inductive logic programming . These approaches are very flexible and leave most choices up to the user. This paper, on the other hand, addresses the question of what specifications allow extracting meaningful rules. It is
316	333	North Dakota 58105 anne.denton Ajay Yekkirala Biology Dept North Dakota State University Fargo, North Dakota 58105 ajay.yekkirala actions are detected in silico by comparing different species . Two genes in one species are labeled as interacting if they have homologs in another species and those homologs are exons of the same gene. Previous approaches to network comparison have studied
316	334	following equation defines this step for a given rule A?C: ?G(A) ? ?G(C) ==? (8) 4.1 Data sets Our data consist of one node relation gathered from the Comprehensive Yeast Genome Database at MIPS , gene orf. The gene orf node relation represents gene annotation data. Annotations are hierarchically structured, with hierarchies for function, localization, protein class, complex, enzyme
316	338	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
316	343	sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A tuple in a joined-relation can
316	345	of annotations on an individual network is discussed in . These approaches fall short of contrasting annotations in different networks. A further related research area is graphbased ARM . Graph-based ARM does not typically consider more than one label on each node or edge. The goal of graph-based ARM is to find frequent substructures in that setting. Removal of a class of redundant
316	346	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
316	347	ARM is to find frequent substructures in that setting. Removal of a class of redundant rules is an important part of differential rule mining. Redundant rules have been studied, and closed sets  have proven a successful approach to their elimination. Closed sets alone do not, however, address the problem of contrasting different nodes or networks. Since we know what kinds of rules we want
316	348	of repetitious and out-of-scope item sets. There are other areas of research on ARM in which related transactions are mined in some combined fashion. Sequential pattern or episode mining  and intertransaction mining  are two main categories. Some similarities in the formalism can be observed since we are also interested in mining across what can be considered transactions. A
349	350	delay by providing multiple path options. However, the router delay for deterministic routers, and consequently their corresponding clock cycles, can be significantly lower than adaptive routers . This di erence in router delays is due to two main reasons: number of VCs and output (OP) channel selection. Two VCs are su cient to avoid deadlock in dimension ordered routing ; while adaptive
349	350	the OP channel selection policy depends also on the state of the router (i.e the occupancy of various VCs) causing increased router complexity and higher router delays. The results reported in  show that the router delays for adaptive routers are about one and a half to more than twice as long as the dimension-order router for wormhole routing. The advantage of adaptive routing in
349	350	blocking. 2.3 Modeling Router Delay In this section we describe a router delay model for the virtual cut-though deterministic and adaptive routers. The model is based on the ones described in . These models account for both the logic complexity of the routers as well as the size of the crossbar as determined by the number of VCs that are multiplexed on one PC. These models were modi ed
349	350	and dominated by Ts for large bu er sizes. All of these added delays result in adaptive routers that are 15 to 16 % slower than deterministic routers. These results are similar to the results in  where 15% to 60% improvement is required for f- at routers with similar number of VCs and under worm-hole routing. 3 Hybrid Routing This section describes the mechanism of the hybrid routing scheme
349	350	in understanding the e ects of router complexity on cycle time involved deterministic routers . Adaptive and deterministic router implementations were then compared for worm-hole routing . However, the comparison in  does not account for the reduced queuing delay in adaptive routing. In  the reduction in queuing delay for worm-hole routing is taken into account and the
349	352	delay by providing multiple path options. However, the router delay for deterministic routers, and consequently their corresponding clock cycles, can be significantly lower than adaptive routers . This di erence in router delays is due to two main reasons: number of VCs and output (OP) channel selection. Two VCs are su cient to avoid deadlock in dimension ordered routing ; while adaptive
349	352	the OP channel selection policy depends also on the state of the router (i.e the occupancy of various VCs) causing increased router complexity and higher router delays. The results reported in  show that the router delays for adaptive routers are about one and a half to more than twice as long as the dimension-order router for wormhole routing. The advantage of adaptive routing in
349	352	blocking. 2.3 Modeling Router Delay In this section we describe a router delay model for the virtual cut-though deterministic and adaptive routers. The model is based on the ones described in . These models account for both the logic complexity of the routers as well as the size of the crossbar as determined by the number of VCs that are multiplexed on one PC. These models were modi ed
349	352	channels. Note that this relationship includes the delivery port. Delay equations for the routers are derived, using the above parameters. The constants in these equations were obtained in  using router designs along with gate-level timing estimates based on a 0.8 micron CMOS gate array process. The three main operations (delays) prevalent in all of the routers simulated here are as
349	352	in understanding the e ects of router complexity on cycle time involved deterministic routers . Adaptive and deterministic router implementations were then compared for worm-hole routing . However, the comparison in  does not account for the reduced queuing delay in adaptive routing. In  the reduction in queuing delay for worm-hole routing is taken into account and the
349	355	path to be utilized more often and o sets any adaptivity loss. 5 Related Work Some of the earliest work in understanding the e ects of router complexity on cycle time involved deterministic routers . Adaptive and deterministic router implementations were then compared for worm-hole routing . However, the comparison in  does not account for the reduced queuing delay in adaptive
349	356	delays is due to two main reasons: number of VCs and output (OP) channel selection. Two VCs are su cient to avoid deadlock in dimension ordered routing ; while adaptive routing (as described in ) requires a minimum of three VCs in k-ary n-cube networks. In dimension-ordered routing, the OP channel selection policy only depends on information contained in the message header itself. In
349	356	dimensions, no cycle exists in the channeldependency graph and the algorithm is deadlock-free. The adaptive routing scheme considered in this work (Duato's or *-channels algorithm) is described in . In this algorithm, adaptive routing is obtained by using adaptive VCs along with dimension-order routing. A message is routed on any adaptive channel until it is blocked. Once blocked, a message
349	356	through a given stage. This routing scheme is deadlock free: for any given message, the selection of paths is always a true subset of those that could be selected by the adaptive algorithm in . Since the adaptive algorithm has been proven deadlock free, the hybrid is also deadlock free. 3.2 Pipelined Implementations The Pipelined Hybrid Router (PHR) implementation is shown in Figure 4.
349	360	Several studies have demonstrated that adaptive routing can achieve alower latency, for the same load, than deterministic routing when measured by a constant clock cycle for both routers . The delay experienced by a message, at each node, can be broken down into: router delay and queuing (or waiting) delay. The former is determined primarily by the complexity of the router. The
349	361	or multi-machine level implementations (NOWs). 2 Deterministic and Adaptive Routing The interconnection network model considered in this study is a k-ary n-cube using virtual cut-through switching : message advancement is similar to worm-hole routing , except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu ered
349	364	and Adaptive Routing The interconnection network model considered in this study is a k-ary n-cube using virtual cut-through switching : message advancement is similar to worm-hole routing , except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu ered at a single node. Note that a header it can progress to a
349	365	and non-minimal fully adaptive routing is possible . The Cray T3E router is also a hybrid router. Messages can be routed deterministically or adaptively by simply setting a bit in the header . The router supports a shortcut for messages that continue traveling in the same dimension and uses directionorder routing for its deterministic routing algorithm. It also implements a routing
366	369	the way RBAC can be extended practically. 3 Role Attributes The attributes associated with a role has variously been discussed in . A more sophisticated approach than that taken in  is needed as soon as the elemen3stary model is to be extended. The attributes highlighted below, lack of explicit mention so far in the literature, are mainly for their usefulness.. Why do we need
366	373	environment. ??? ???????????????????? When the responsibility and capability of a subject interacts with that of another, there is the concept of role interaction. This has been amply discussed in , under both the concept of role relationships and relationship classes. Apart from this significant concept, there is a need to recognise the relative nature of roles with respect to role owner.
366	373	matters are reported. In this case, role relativity provides distinct views according to the role owner, and adds clarity to the notion of role, something that eludes the definitions given in . The single most important application of this concept is in enforcing RBAC related to personal data privacy laws, where role owner’s identity must be taken into account. ??? ???????????????? Often
366	375	a role becomes active. This is very similar to role scope establishment which is orthogonal to the activation requirements. In the literature, role has been regarded as a collection of policies in . This treatment will subsume the requirements mentioned in this section, provided that the way policies are described is sufficiently generic, with descriptive power at least as encompassing as
366	376	another, the transferor will no longer retain the capabilities. How delegable and transferable a role is could be an attribute of the role itself. Previous discussion on this area can be found in  which looks at relatively simple scenarios. For convenience, we coin the phrase “role empowerment” to mean either role delegation or role transfer. A more complete approach to role empowerment
1500758	389	given point in time. Such an assumption is appropriate for active sensors such as pan-tilt-zoom (PTZ) cameras, which are finding significantly increased usage in the current geopolitical climate , or when the computational requirements of tracking algorithms support only a single target assignment. With this in mind, our problem can be viewed as an optimal allocation of resources for target
1500758	381	This yielded significant improvements to methods used in mobile robot navigation, localization and mapping . Thrun et al. have also contributed significant research to these areas . However, our work distinguishes itself from traditional data fusion techniques in that the sensors themselves are actively managed to improve the quality of the measurements obtained prior to the
1500758	382	This yielded significant improvements to methods used in mobile robot navigation, localization and mapping . Thrun et al. have also contributed significant research to these areas . However, our work distinguishes itself from traditional data fusion techniques in that the sensors themselves are actively managed to improve the quality of the measurements obtained prior to the
1500758	383	resulting in corresponding improvements in state estimation. There has been other related research under the heading of sensor networks. Cortes et al. investigated the issue of sensor coverage . In their model a single sensor is sufficient to cover a point, however the quality of coverage decreases with distance. This research focused on the movement of sensor networks while ensuring
1500758	389	a subset of X ×Y ×W such that every element of X ? Y ? W belongs to exactly one element of A) such that ? (i,j,k)?A c(i, j, k) is minimized. 3D-Assignment (3DA) is NP-hard  and inapproximable . It is easy to see that any instance of 3DA can be reduced to an instance of FOA just by setting cFOA(i, j, k) =c3DA(i, j, k) whenever c3DA(i, j, k) is defined and infinite otherwise. Moreover,
1500758	393	followed random trajectories, and were tracked in simulation using particle filters. The respective particle sets were employed to generate a numerical error metric for the targets as discussed in . Two algorithms were investigated for this maximization approach. The first employed a greedy assignment strategy, and the second a 2-locally optimal approach as discussed in Section 3.5. The
1500758	394	can serve as inputs to traditional Bayesian filters (i.e. Kalman or particle) which yield estimates of the target pose. The latter has shown robustness for tracking features in a cluttered image . For the arbitrary sensor placement problem, general visibility constraints (range, occlusions, etc.) can also be accommodated by 32smerely eliminating the corresponding triples from the feasible
8918487	403	is the set of text extraction rules that identify the relevant information to be extracted. IE systems have been built with different levels of success on several kinds of text domains. CRYSTAL  was a learning-based IE system that took parsed annotated sentences and found patterns for extraction in novel sentences. Webfoot  was an attempt at general IE that processed fragments by
8918487	404	levels of success on several kinds of text domains. CRYSTAL  was a learning-based IE system that took parsed annotated sentences and found patterns for extraction in novel sentences. Webfoot  was an attempt at general IE that processed fragments by looking at Hyperlink Markup Language (HTML) tags. SRV  was another learning architecture for IE. It took a user-defined feature set
8918487	405	annotated sentences and found patterns for extraction in novel sentences. Webfoot  was an attempt at general IE that processed fragments by looking at Hyperlink Markup Language (HTML) tags. SRV  was another learning architecture for IE. It took a user-defined feature set together with a set of hand tagged training documents and learned rules for extraction. Craven et al.  reported that
8918487	406	tags. SRV  was another learning architecture for IE. It took a user-defined feature set together with a set of hand tagged training documents and learned rules for extraction. Craven et al.  reported that greater accuracy could be achieved by representing each web page as a node in graph and each hyperlink an edge. Cardie  provided a list of learning-based IE problems, including the
8918487	407	documents and learned rules for extraction. Craven et al.  reported that greater accuracy could be achieved by representing each web page as a node in graph and each hyperlink an edge. Cardie  provided a list of learning-based IE problems, including the difficulty of obtaining enough training data and the lack of corpora annotated with the appropriate semantic and domainspecific
8918487	410	combinators and a markup algebra. The markup algebra extracts structured and unstructured values from pages for computation, and is based on algebraic operations on sets of markup elements. Un Yong  describes a system called DiscoTEX that combines IE and KDD methods to perform a text-mining task, discovering prediction rules from natural-language corpora. Hence by parsing the HTML formatting,
8918487	412	predefined HTML templates. The systems generate delimiter-based rules that use linguistic constraints. Wien  uses only delimiters that immediately precede and follow the actual data. SoftMealy  is a wrapper induction algorithm that generates extraction rules expressed as finite-state transducers. World Wide Web Wrapper Factory  does extraction by using an HTML parser to construct a
414	415	In the case that there are missing data in the kernel matrix, Tsuda et al. (2003) developed a parametric approach to kernel matrix completion using the em algorithm based on information geometry (Amari, 1995). In their approach, the Kullback-Leibler (KL) divergence is used for measuring the similarity between kernel matrices. Assuming that the kernel matrix is a random positive definite matrix
414	416	a spectral method (Cristianini et al., 2002), semi-definite programming (SDP) (Lanckriet et al., 2002), the Gram-Schmidt method (Kandola et al., 2002), and a gradient-based method (Bousquet & Herrmann, 2003). Crammer et al. (2003) cast the kernel matrix learning problem under the boosting paradigm for constructing an accurate kernel from simple base kernels. In the case that there are missing data in
414	418	matrix. More specifically, we need a criterion for optimizing the kernel matrix. The alignment was proposed as such a criterion defined in the form of a similarity measure between kernel matrices (Cristianini et al., 2002). Based on this criterion, several methods have been proposed for optimizing the kernel matrix, including a spectral method (Cristianini et al., 2002), semi-definite programming (SDP) (Lanckriet et
414	418	algorithm can work on distributions over random variables or random vectors (instead of random matrices). This is motivated by some existing kernel matrix learning methods (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), which constrain the target kernel matrix to a weighted combination of some fixed base kernel matrices so that the kernel matrix learning problem can be
414	418	later, we define the kernel matrix K = (A + B)/2 as the discriminant kernel (Zhang, 2003), where A = ? exp(??xi ? xj? 2 /?) ? n×n is the standard Gaussian kernel and B is the ideal kernel (Cristianini et al., 2002) based on the training set, i.e., ij = ? 1 yi = yj 0 yi ?= yj. Following the generative model formulation of the kernel matrix in (Zhang et al., 2003a), we now assume that the kernel matrix K is
414	418	algorithm. In this section, we propose a simplified Bayesian model that makes it possible to develop an efficient implementation. In the kernel matrix learning literature (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), it is common to constrain the target kernel to a weighted combination of some available base kernels so that the learning problem is simplified to the
414	418	U and µ iµ ? i ’s as the base matrix and the base kernel matrices, respectively. Now, given U, we want to estimate ?i’s and hence G to approximate some desired kernel H, such as the ideal kernel (Cristianini et al., 2002), based on some criterion like the kernel alignment or the KL divergence between G and H. This motivates us to devise a simplified Bayesian hierarchical model and then an efficient implementation
414	424	et al., 2002). Based on this criterion, several methods have been proposed for optimizing the kernel matrix, including a spectral method (Cristianini et al., 2002), semi-definite programming (SDP) (Lanckriet et al., 2002), the Gram-Schmidt method (Kandola et al., 2002), and a gradient-based method (Bousquet & Herrmann, 2003). Crammer et al. (2003) cast the kernel matrix learning problem under the boosting paradigm
414	424	over random variables or random vectors (instead of random matrices). This is motivated by some existing kernel matrix learning methods (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), which constrain the target kernel matrix to a weighted combination of some fixed base kernel matrices so that the kernel matrix learning problem can be simplified to the
414	424	we propose a simplified Bayesian model that makes it possible to develop an efficient implementation. In the kernel matrix learning literature (Crammer et al., 2003; Cristianini et al., 2002; Lanckriet et al., 2002; Tsuda et al., 2003), it is common to constrain the target kernel to a weighted combination of some available base kernels so that the learning problem is simplified to the estimation of the
414	428	(x, y), we define a kernel matrix K on (X × Y) × (X × Y) in this paper. Specifically, in our experiments to be presented later, we define the kernel matrix K = (A + B)/2 as the discriminant kernel (Zhang, 2003), where A = ? exp(??xi ? xj? 2 /?) ? n×n is the standard Gaussian kernel and B is the ideal kernel (Cristianini et al., 2002) based on the training set, i.e., ij = ? 1 yi = yj 0 yi ?= yj.
10549099	473	growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However, it has been noted that square or 90-degree angles have serious issues and drawbacks , including the particularly serious parity constraint, i.e. any pair of amino acids which are an odd-distance apart from each other can never lie on adjacent square lattice points. 4sDue to this,
10549099	474	simplified models suffer from computational intractability in the worst case. For example, optimizing the simple 2-D square lattice hydrophobic-hydrophilic (HP)  has been shown to be NPComplete . In the past several years, numerous algorithms and techniques have been proposed and explored for quickly determining native conformations based on models such as the HP models. Methods such as
10549099	474	2D-lattice. The standard assumption has been that the lattice is square in structure. Under this assumption, it has been proven that protein folding on the two- dimensional HP model is NP-complete . Several methods have been presented to try to solve this problem, such as the chain growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However,
10549099	476	it has been proven that protein folding on the two- dimensional HP model is NP-complete . Several methods have been presented to try to solve this problem, such as the chain growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However, it has been noted that square or 90-degree angles have serious issues and drawbacks , including the
10549099	477	2D-lattice. The standard assumption has been that the lattice is square in structure. Under this assumption, it has been proven that protein folding on the two- dimensional HP model is NP-complete . Several methods have been presented to try to solve this problem, such as the chain growth algorithm, fast protein folding approximating algorithms , and genetic algorithm(s) . However,
10549099	480	numerous algorithms and techniques have been proposed and explored for quickly determining native conformations based on models such as the HP models. Methods such as genetic and memetic algorithms , tabu search , and ant colony optimization  use approximation and randomized search in an effort to find good solutions in a reasonable amount of time. The fundamental nature of such
10549099	480	of the extra difficulty, simple genetic algorithms (GA) and evolutionary approaches have a difficult time solving 2D triangular lattice HP problems. As researchers have empirically demonstrated , more sophisticated optimization techniques such as hybrid local/global search, multi- meme GAs, scatter/gather searching, etc. must be employed in a variety of ways to improve diversity of the
10549099	480	to compute the computational effort saved through caching. We implemented both versions in C++ on a Pentium IV 2GHz machine with 1G RAM running Linux. The testbed proteins we employ are drawn from  for triangular 2D-HP and from  which have yet to be computed in triangular 2D-HP. Table 5.1 contains the HPs used in our experiments. We note again that our goal in this experiment is to
10549099	480	-17 30 PHHHPHHHPPPHPHHPHHPPHPHHHHPHPPHHHHHPHP HHPPHHP 45 n/a -36 31 HPHHHPHHHPPHHPHPHHPHHHPHPHPHHPPHHHPPHP HPPPPHPPHPPHHPPHPPH 57 n/a -38 Table 5.1. HP Testbed Proteins. Protein ID 1-20 are from . Protein ID21-31 are from . 16sAbsolute Energy Improvement 1.80 1.60 1.40 1.20 1.00 0.80 0.60 0.40 0.20 0.00 1 4 7 10 13 16 19 Protein Id # 22 25 28 31 Fig. 5.2. Average absolute energy value
481	483	(uncontrolled) sensor networks grew out of the field of distributed sensing, estimation, and data fusion. Recent work focuses on large dynamic sensor networks in which attribute based naming  must be used instead of traditional address naming. Simulations of information-theoretic sensor selection algorithms in networks of up to a 100 nodes are reported in . This work builds on a
481	483	or • what additional information about the world to use (an observation ? or a belief b) Technically, all of these options can implemented in scalable fashion provided that attribute based naming  is used and proper communication infrastructure is available. A tree network created and maintained by the DDF algorithm is suitable for transmitting such command messages as well. The four options
481	484	(II): The research in this field to date has been limited to communication between one or several humans and a small number of robots. The field of adjustable autonomy  or collaborative control  is an active research area which aims to span the gap between teleoperation and full autonomy. Current solutions call for bidirectional communication in the form of a human-robot dialog. With
481	485	and environment as a set of continuous states, together with the use of information as payoff, allows the information acquisition problem to be formulated as a standard optimal control problem . Several classification systems for human-robot interactions have been suggested. For example, Scholtz  identifies mechanic, supervisory, and peer-to-peer levels of human-robot interaction.
481	486	three indoor robots cooperatively build an map of a building in . Both data fusion and control part of the algorithm are centralized. Decentralized control techniques are combined with DDF in . In all of these works, the issues of human-robot interface are not addressed. This research area also includes a large body of work on reactive architectures, most of which are fully decentralized
481	486	platform, the sensor and actuator models, and the current state of the world supplied by the node, the controller will issue commands to the actuator which will maximize a certain reward function . Any number of nodes, sensors, and actuators may be attached to a single platform but from the point of view of a node, a sensor, or a controller platform assignment is unique. Likewise, an
481	486	as point features by the laser range finders. The objective of the network is to find and localize the point features. An information surfing controller is implemented on the Pioneer robots . It is a zero look-ahead control law which moves the platform in the direction of the steepest descent in information space. All software components run as separate applications and communicate
481	490	interface are not addressed. This research area also includes a large body of work on reactive architectures, most of which are fully decentralized and some were applied to sensing applications . They fall outside of the scope of this work because they do not perform network-level data fusion and typically do not interact with humans. Human-robot interaction in multi-robot systems (II):
481	491	interface are not addressed. This research area also includes a large body of work on reactive architectures, most of which are fully decentralized and some were applied to sensing applications . They fall outside of the scope of this work because they do not perform network-level data fusion and typically do not interact with humans. Human-robot interaction in multi-robot systems (II):
481	492	constructed and programmed in a modular fashion. 3.1 Multi-Agent Framework To make discussion of various aspects of communication within an ASN more concrete, a formal team framework will be used . The problem is stated as two tuples. ?S, A?, ??, P, ??, O?, B?, R, T ? , ???, ?A? (1) where for each agent i in team ? S world states (terrain, features, agents, etc.) agent’s domain-level actions
481	494	the information acquisition problem to be formulated as a standard optimal control problem . Several classification systems for human-robot interactions have been suggested. For example, Scholtz  identifies mechanic, supervisory, and peer-to-peer levels of human-robot interaction. Alternatively, the potential control methods can be identified by examining Equation 1. An operator can tell a
481	495	issues are not addressed. An architecture for DDF and control with an emphasis on scalability is described in . A team of three indoor robots cooperatively build an map of a building in . Both data fusion and control part of the algorithm are centralized. Decentralized control techniques are combined with DDF in . In all of these works, the issues of human-robot interface are
481	497	attribute based naming  must be used instead of traditional address naming. Simulations of information-theoretic sensor selection algorithms in networks of up to a 100 nodes are reported in . This work builds on a decentralized data fusion (DDF) architecture , demonstrated in a network of up to 8 nodes. Active sensor networks (I): A hierarchical architecture applied to real time
498	499	are labeled with 0 or 1 and correspond to the constant Boolean functions. The root node root(Gf ) corresponds to the function f. In the following, BDD refers to a reduced ordered BDD (as defined in ) and the size of a BDD is given by the number of nodes. Definition 1 A one-path in a BDD Gf = (V, E) is a path p = (v0, ..., vl?1, vl), vi ? V, (vi, vi+1) ? E with v0 = root(Gf ) and label(vl) = 1.
498	500	BDDs in general are an efficient data structure for representation and manipulation of Boolean functions. They are well-known and widely used in logic synthesis  and formal verification  of integrated circuits. BDDs are well-suited for applications in the area of logic synthesis, because the cubes in the ON-set of a Boolean function are implicitly represented in this data
498	509	function are implicitly represented in this data structure. A hybrid approach for the minimization of DSOPs relying on BDDs in combination with structural methods has recently been introduced in . It hassbeen shown that BDDs are applicable to the problem of DSOP minimization. Given a BDD of a Boolean function, the DSOP can easily be constructed: each one-path, i.e. a path from the root to
498	509	variable is chosen. No variable is chosen twice during this process. For the evaluation of our fitness function, in the following the improved pathminimization algorithm based upon sifting from  is used. This algorithm employs structural techniques to reduce the number of cubes in the DSOP. mp, 3 Evolutionary Algorithm In this section we describe the Evolutionary Algorithm (EA) that is
498	509	a fitness that measures the quality of the variable ordering. First the BDD is constructed using the variable ordering given by the individual, then the number of reduced one-paths are counted . The selection is performed by roulette wheel selection and we also make use of steady-state reproduction : The best individuals of the old population are included in the new one of equal size.
498	509	times to the 37 benchmarks in the test suite. Each time a randomly chosen seed for the random number generator was used. In column hybrid the number of cubes resulting from the method proposed in  for each function is given. Columns EA summarize the results from the EA proposed in this paper. min. and max. show the minimal and maximal DSOP of all test runs, respectively. In column median the
498	515	used in several applications in the area of CAD, e.g. the calculation of spectra of Boolean functions  or as a starting point for the minimization of Exclusive-Or-Sum-Of-Products (ESOPs) . In  some techniques for minimization of DSOPs have been introduced. They are working on explicit representations of the cubes and therefore are only applicable to small instances of the
498	519	applications in the area of CAD, e.g. the calculation of spectra of Boolean functions  or as a starting point for the minimization of Exclusive-Or-Sum-Of-Products (ESOPs) . In  some techniques for minimization of DSOPs have been introduced. They are working on explicit representations of the cubes and therefore are only applicable to small instances of the problem. BDDs
528	534	the LBM a number of physically accurate effects, such as the buoyancy effect of hot gases, additional realism can be provided. Our framework makes efficient use of the concept of textured splats , which are associated with the macroscopic particles to represent the gaseous phenomena. The textured splats form the observable “display particles,” such as the smoke particles or dust particles,
528	534	to give periodic and chaotic vector fields that can be combined with the global motions. Another approach is to take advantage of commodity texture mapping hardware, using textured splats  as the rendering primitive. King et al.  first used this technique to achieve fluid animation based on simple and local dynamics. A drawback of their model is, however, that it lacks the
528	535	section, we present a complete framework for the LBM-based fluid simulation and how its calculation is implemented based on the fast growing GPU technology. Physically-based particle models , , , , , ,  have also been used to describe fluid behaviors. Particle systems were first introduced by Reeves  as a technique for modeling fuzzy objects, such as fire, clouds,
528	535	the interaction of particles due to the thermal energy. Terzopoulos et al.  implemented a similar approach. Particles and springs are utilized to render a series of blobbies. Desbrun and Gascue ,  developed a paradigm extended from the Smoothed Particle Hydrodynamics approach used by physicists for cosmological fluid simulation. This technique defines a type of particle system which
528	536	of particles due to the thermal energy. Terzopoulos et al.  implemented a similar approach. Particles and springs are utilized to render a series of blobbies. Desbrun and Gascue ,  developed a paradigm extended from the Smoothed Particle Hydrodynamics approach used by physicists for cosmological fluid simulation. This technique defines a type of particle system which uses
528	539	rendering speed. Finally, we outline our implementation and describe several examples in Section 8. 2 PREVIOUS WORK A common approach to simulating gaseous phenomena is procedural modeling , , , where fluid behaviors are described by procedural functions. This method is fast and easy to program, but it is difficult to find the proper parameter settings that achieve realistic
528	541	gave an analytic solution to the NS equations by using simple flow primitives. Chen and Lobo  solved a simplified NS equations in 2D using a finite difference approach. Later, Foster and Metaxas  presented a full 3D finite difference solution to simulate the turbulent rotational motion of gas. Because of the inherent instability of the finite difference method with a larger time step, the
528	547	value. 6 MAPPING LBM TO GRAPHICS HARDWARE We briefly review in this section the basic ideas of mapping LBM to graphics hardware, that is, a graphics processing unit (GPU). (For more details, see .) To compute the LBM equations on GPU, we divide the LBM grid and group the packet distributions fqi into arrays according to their velocity directions. All the packet distributions with the same
528	548	the boundary conditions become complicated, such as a fast moving boundary object or a complex geometric structure. In our work, we implemented the boundary conditions based on Mei et al.’s method  for curved boundaries. In their approach, the problem is solved in another way. Instead of directly setting the microscopic values, they calculate the macroscopic variables of density and velocity
528	553	speed. Finally, we outline our implementation and describe several examples in Section 8. 2 PREVIOUS WORK A common approach to simulating gaseous phenomena is procedural modeling , , , where fluid behaviors are described by procedural functions. This method is fast and easy to program, but it is difficult to find the proper parameter settings that achieve realistic results. The
528	554	solution to simulate the turbulent rotational motion of gas. Because of the inherent instability of the finite difference method with a larger time step, the speed of this approach is limited. Stam  devised a fluid solver using a semi-Lagrangian advection scheme and implicit solver for the NS equations. Each term of the equations is handled in turn, starting with external force, then
528	555	equally efficient way to add the small-scale turbulence details into the visual simulation and render these to the screen. One way to model the small-scale turbulence is through spectral analysis . Turbulent motion is first defined in Fourier space and then it is transformed to give periodic and chaotic vector fields that can be combined with the global motions. Another approach is to take
528	558	the fluid animation. We also plan to model the behaviors of objects in the flow, such as the leaves blowing in the wind. Besides gaseous phenomena, our model can also be used to simulate liquid , heat in a solid, and the like, and be extended to model fire . ACKNOWLEDGMENTS This work is partially supported by US Office of Naval Research grant N000140110034, US National Science
528	559	in the flow, such as the leaves blowing in the wind. Besides gaseous phenomena, our model can also be used to simulate liquid , heat in a solid, and the like, and be extended to model fire . ACKNOWLEDGMENTS This work is partially supported by US Office of Naval Research grant N000140110034, US National Science Foundation (NSF) grants IIS-0097646 and CCR-0306438 and NSF CAREER grant
8918500	563	understanding how well humans and robots are able to work together. Thus, we intend to use a variety of metrics and critical incident analysis to characterize and assess human-robot performance (Fong et al. 2004; Scholtz et al. 2004). To evaluate system performance, one metric we will examine is fan out, which measures how many robots can be effectively controlled by a human (Goodrich and Olsen 2003). To
8918500	571	interaction. Moreover, interruption is problematic because humans have cognitive limitations that restrict their ability to work during interruptions and to resume previously interrupted tasks (McFarlane 1990). Computational Cognitive Models In order to take the best possible advantage of the particular skills of humans and of robots in mixed-initiative teams, itsis important that robots be able to
8918500	574	well humans and robots are able to work together. Thus, we intend to use a variety of metrics and critical incident analysis to characterize and assess human-robot performance (Fong et al. 2004; Scholtz et al. 2004). To evaluate system performance, one metric we will examine is fan out, which measures how many robots can be effectively controlled by a human (Goodrich and Olsen 2003). To assess operator
577	578	addressing this problem: algorithmic and architectural. A few pioneering groups of researchers posed the problem, provided complexity bounds, and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the
577	578	Addressable Memory (TCAM) . Some of the most promising algorithmic research embraces the practice of leveraging the statistical structure of filter sets to improve average performance . Several algorithms in this class are amenable to high-performance hardware implementation. We discuss these observations in more detail and provide motivation for packet classification on larger
577	578	to provide significantly better average performance. Gupta and McKeown published a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set
577	578	Classification (RFC) Leveraging many of the same observations, Gupta and McKeown introduced Recursive Flow Classification (RFC) which provides high lookup rates at the cost of memory inefficiency . The authors introduced a unique high-level view of the packet classification problem. Essentially, packet classification can be viewed as the reduction of an m-bit string defined by the packet
577	578	technique like hashing. We probe a tuple for a matching filter by using the bits of the packet field specified by the tuple as the search key. For example, we construct a search key for the tuple  by concatenating the first bit of the packet source address, the first three bits of the packet destination address, the Range ID of the source port range at Nesting Level 2 covering the packet
577	580	addressing this problem: algorithmic and architectural. A few pioneering groups of researchers posed the problem, provided complexity bounds, and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the
577	580	multiple field packet classification algorithms targeted to a hardware implementation. Their seminal technique is commonly referred to as the Lucent bit-vector scheme or Parallel Bit-Vectors (BV) . The authors make the initial assumption that the filters may be sorted according to priority. Like the previously discussed “cutting” algorithms, Parallel BV utilizes a geometric view of the
577	580	technique like hashing. We probe a tuple for a matching filter by using the bits of the packet field specified by the tuple as the search key. For example, we construct a search key for the tuple  by concatenating the first bit of the packet source address, the first three bits of the packet destination address, the Range ID of the source port range at Nesting Level 2 covering the packet
577	580	0 covering the packet destination port 35sTable 5: Example filter set; address fields are 4-bits and port ranges cover 4-bit port numbers. Filter SA DA SP DP Prot Tuple a 0? 001? 2 : 2 0 : 15 TCP  b 01? 0? 0 : 15 0 : 4 UDP  c 0110 0011 0 : 4 5 : 15 TCP  d 1100 ? 5 : 15 2 : 2 UDP  e 1? 110? 2 : 2 0 : 15 UDP  f 10? 1? 0 : 15 0 : 4 TCP
577	580	to at most (2W ? 1) where W is the address length. Each filter mapping to a tuple  leaves a marker in each tuple to its left in its row. For example, a filter (110?, 0111) stored in tuple  leaves markers (11?, 0111) in  and (1?, 0111) in . For all filters and markers in a tuple , we can precompute the best matching filter from among the filters stored in less
577	581	and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the performance constraints discussed in Section 1.1, researchers in industry and academia devised architectural solutions to the problem. This
577	581	set. 5.7 Fat Inverted Segment (FIS) Trees Feldman and Muthukrishnan introduced another framework for packet classification using independent field searches on Fat Inverted Segment (FIS) Trees . Like the previously discussed “cutting” algorithms, FIS Trees utilize a geometric view of the filter set and map filters into d-dimensional space. As shown in Fig21squery 0110, 11 Index Block
577	582	and offered a collection of algorithmic solutions . Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms . Given the inability of early algorithms to meet the performance constraints discussed in Section 1.1, researchers in industry and academia devised architectural solutions to the problem. This
577	582	a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set sizes are small, ranging from tens of filters to less than 5000 filters. It is unclear if the size
577	582	with deeper pipelines and higher link rates. Linear search is a popular solution for the final stage of a lookup when the set of possible matching filters has been reduced to a bounded constant . 4.2 Ternary Content Addressable Memory (TCAM) Taking a cue from fully-associative cache memories, Ternary Content Addressable Memory (TCAM) devices perform a parallel search over all filters in
577	582	e d j Woo independently applied the same approach as HiCuts and introduced a flexible framework for packet classification based on a multi-stage search over ternary strings representing the filters . The framework contains three stages: an index jump table, search trees, and filter buckets. An example data structure for the filter set in Table 4 is shown in Figure 11. A search begins by using
577	582	the HyperCuts algorithm  improves upon the HiCuts algorithm developed by Gupta and McKeown  and also shares similarities with the Modular Packet Classification algorithms introduced by Woo . In essence, HyperCuts is a decision tree algorithm that attempts to minimize the depth of the tree by selecting multiple “cuts” in multi-dimensional space that partition the filter set into lists
577	586	Addressable Memory (TCAM) . Some of the most promising algorithmic research embraces the practice of leveraging the statistical structure of filter sets to improve average performance . Several algorithms in this class are amenable to high-performance hardware implementation. We discuss these observations in more detail and provide motivation for packet classification on larger
577	586	a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set sizes are small, ranging from tens of filters to less than 5000 filters. It is unclear if the size
577	586	5.2 Extended Grid-of-Tries (EGT) Baboescu, Singh, and Varghese proposed Extended Grid-of-Tries (EGT) that supports multiple fields searches without the need for many instances of the data structure . EGT essentially alters the switch pointers to be jump pointers that direct the search to all possible matching filters, rather than the filters with the longest matching destination and source
577	587	Addressable Memory (TCAM) . Some of the most promising algorithmic research embraces the practice of leveraging the statistical structure of filter sets to improve average performance . Several algorithms in this class are amenable to high-performance hardware implementation. We discuss these observations in more detail and provide motivation for packet classification on larger
577	587	with deeper pipelines and higher link rates. Linear search is a popular solution for the final stage of a lookup when the set of possible matching filters has been reduced to a bounded constant . 4.2 Ternary Content Addressable Memory (TCAM) Taking a cue from fully-associative cache memories, Ternary Content Addressable Memory (TCAM) devices perform a parallel search over all filters in
577	587	0 1 Figure 11: Modular packet classification using ternary strings and a three-stage search architecture. 5.5 HyperCuts Introduced by Singh, Baboescu, Varghese, and Wang, the HyperCuts algorithm  improves upon the HiCuts algorithm developed by Gupta and McKeown  and also shares similarities with the Modular Packet Classification algorithms introduced by Woo . In essence, HyperCuts is
577	589	eliminate many of the unfavorable characteristics of current TCAMs . We observe that the community appears to be converging on a combined algorithmic and architectural approach to the problem . In order to lend structure to our discussion, we develop a taxonomy in Section 3 that frames each technique according to its high-level approach to the problem. The presentation of this taxonomy
577	589	Taylor and Turner also performed a battery of analyses on real filter sets, focusing on the maximum number of unique field values and unique combinations of field values which match any packet . They found that the number of unique field values is less than the number of filters and the maximum number of unique field values matching any packet remains relatively constant for various
577	590	eliminate many of the unfavorable characteristics of current TCAMs . We observe that the community appears to be converging on a combined algorithmic and architectural approach to the problem . In order to lend structure to our discussion, we develop a taxonomy in Section 3 that frames each technique according to its high-level approach to the problem. The presentation of this taxonomy
577	590	a number of observations regarding the characteristics of real filter sets which have been widely cited . Others have performed analyses on real filter sets and published their observations . The following is a distillation of observations relevant to our discussion: • Current filter set sizes are small, ranging from tens of filters to less than 5000 filters. It is unclear if the size
577	590	6.5 Parallel Packet Classification (P 2 C) The Parallel Packet Classification (P 2 C) scheme introduced by van Lunteren and Engbersen also falls into the class of techniques using decomposition . The key novelties of P 2 C are its encoding and aggregation of intermediate results. Similar to the Parallel Bit-Vector and RFC techniques, P 2 C performs parallel searches in order to identify
577	592	in Section 6.3. For filters defined by source and destination prefixes, Grid-of-Tries improves upon the directed acyclic graph (DAG) technique introduced by Decasper, Dittia, Parulkar, and Plattner . This technique is also called set pruning trees because redundant subtrees can be “pruned” from the tree by allowing multiple incoming edges at a node. 11sa b c d e f g h 10* 0* 111 11* *
577	594	search engines. The latter is achieved by a novel technique called Field Splitting which we do not discuss in this survey. Using a collection of 12 real filter sets and the ClassBench tools suite , the authors provide analyses of DCFL performance and resource requirements on filter sets of various sizes and compositions. For the 12 real filter sets, they show that the worst-case number of
577	595	is much less than the number of filters in the filter set, Srinivasan, Suri, and Varghese introduced the tuple space approach and a collection of Tuple Space Search algorithms in a seminal paper . In order to illustrate the concept of tuples, we provide an example filter set of filters classifying on five fields in Table 5. Address prefixes cover 4-bit addresses and port ranges cover 4-bit
577	595	discussed in Section 4. 7.1 Tuple Space Search & Tuple Pruning The basic Tuple Space Search technique introduced by Srinivasan, Suri, and Varghese performs an exhaustive search of the tuple space . For our example filter set in Table 5, a search would have to probe seven tuples instead of searching all 12 filters. Using a modest set of real filter sets, the authors found that Tuple Space
577	598	Conflict-Free Rectangle Search W (d?1) d! Warkhede, Suri, and Varghese provide an optimized version of Rectangle Search for the special case of packet classification on a conflict-free filter set . A filter set is defined to be conflict-free if there is no pair of overlapping filters in the filter set such that one filter is more specific than the other in one field and less specific in
577	599	authors observe that in real filter sets conflicts are rare; furthermore, techniques exist to resolve filter conflicts by inserting a small set of resolving filters that resolve filter conflicts . Conflict-Free Rectangle Search begins by mapping the filter set to the W × W tuple space. Using precomputation and markers, the authors prove that a binary search can be performed on the columns
8918535	654	when touching between characters/digits is assumed. There have been three strategies for dealing with the segmentation problem: segmentation-based (dissection), recognition-based,andholistic Daekeun You and Gyeonghwan Kim Dept. of Electronic Engineering Sogang University CPO Box 1142, Seoul 100-611 Korea gkim@ccs.sogang.ac.kr approaches. Correct segmentation is important
8918535	657	could not be considered independently when touching between characters/digits is assumed. There have been three strategies for dealing with the segmentation problem: segmentation-based (dissection), recognition-based,andholistic Daekeun You and Gyeonghwan Kim Dept. of Electronic Engineering Sogang University CPO Box 1142, Seoul 100-611 Korea gkim@ccs.sogang.ac.kr approaches.
8918535	658	characters/digits is assumed. There have been three strategies for dealing with the segmentation problem: segmentation-based (dissection), recognition-based,andholistic Daekeun You and Gyeonghwan Kim Dept. of Electronic Engineering Sogang University CPO Box 1142, Seoul 100-611 Korea gkim@ccs.sogang.ac.kr approaches. Correct segmentation is important since errors
64580	665	Java programmers to use JML specifications as practical and effective tools for debugging, testing, and design by contract. 3.3 Unit testing A formal specification can be viewed as a test oracle , and a runtime assertion checker can be used as the decision procedure for the test oracle . This idea has been implemented as a unit testing tool for Java (jmlunit), by combining JML with the
64580	666	a significant advance over the state of the art in runtime assertion checking as represented by design by contract tools such as Eiffel  or by Java tools such as iContract  or Jass . The jmlc tool also supports advances such as (stateful) interface specifications, multiple inheritance of specifications, various forms of quantifiers and set comprehension notation, support for
64580	668	than can be handled by extended static checking with ESC/Java. A recent paper describing a case study with the LOOP tool, giving the best impression of the state of the art, is now available . A similar program verification tool for JML-annotated code under development is the Krakatoa tool ; it produces proof obligations for the theorem prover Coq, but currently covers only a subset
64580	668	earlier, which is specifically designed for Java Card programs. One of the classes of the electronic purse mentioned above has provided the most serious case study to date with the LOOP tool .s5 Related Work 5.1 Other runtime assertion checkers An Overview of JML Tools and Applications 11 Many runtime assertion checkers for Java exist, for example, Jass, iContract, and Parasoft’s
64580	668	the use of JML gradually, simply by adding the odd assertion to some Java code. – JML can be used for existing (legacy) code and APIs. Indeed, most applications of JML and its tools to date (e.g., ) have involved existing APIs and code. 3. There is a (growing) availability of a wide range of tool support for JML. Unlike B, JML does not impose a particular design method on its users. Unlike
64580	669	same meaning in JML and Java, as does ==, and the same rules for overriding, overloading, and hiding apply. One cannot expect this for OCL. In fact, a semantics for OCL was only recently proposed .s12 Lilian Burdy et al. In all, we believe that a language like JML, which is tailored to Java, is better suited for recording the detailed design of a Java programs than a generic language like
64580	688	to specify and verify a component of a smartcard operating system . ESC/Java has been used with great success to verify a realistic example of an electronic purse implementation in Java Card . This case study was instrumental in convincing industrial users of the usefulness of JML and feasibility of automated program checking by ESC/Java for Java Card applets. This provided the
64580	688	the use of JML gradually, simply by adding the odd assertion to some Java code. – JML can be used for existing (legacy) code and APIs. Indeed, most applications of JML and its tools to date (e.g., ) have involved existing APIs and code. 3. There is a (growing) availability of a wide range of tool support for JML. Unlike B, JML does not impose a particular design method on its users. Unlike
64580	671	3.6 CHASE One source of unsoundness of ESC/Java is that it does not check assignable clauses. The semantics of these frame axioms are also not checked by the JML compiler. The CHASE tool  tries to remedy these problems. It performs a syntactic check on assignable clauses, which, in the spirit of ESC/Java, is neither sound nor complete, but which spots many mistakes made in
64580	672	assertion checking The JML compiler (jmlc), developed at Iowa State University, is an extension to a Java compiler and compiles Java programs annotated with JML specifications into Java bytecode . The compiled bytecode includes runtime assertion checking instructions that check JML specifications such as preconditions, normal and exceptional postconditions, invariants, and history
64580	673	and design by contract. 3.3 Unit testing A formal specification can be viewed as a test oracle , and a runtime assertion checker can be used as the decision procedure for the test oracle . This idea has been implemented as a unit testing tool for Java (jmlunit), by combining JML with the popular unit testing tool JUnit for Java . The jmlunit tool, developed at Iowa State
64580	673	When the method under test satisfies its precondition, but otherwise has an assertion violation, then the implementation failed to meet its specification, and hence the test data detects a failure . In other words, the generated test code serves as a test oracle whose behavior is derived from the specified behavior of the class being tested. The user is still responsible for generating test
64580	674	most important of these below. – To allow specifications to be abstractions of implementation details, JML provides model variables, which play the role of abstract values for abstract data types . For example, if instead of a class Purse, we were specifying an interface PurseInterface, we could introduce the balance as such a model variable. A class implementing this interface could then
64580	675	such as (stateful) interface specifications, multiple inheritance of specifications, various forms of quantifiers and set comprehension notation, support for strong and weak behavioral subtyping , and a contextual interpretation of undefinedness . In sum, the JML compiler brings “programming benefits” to formal interface specifications by allowing Java programmers to use JML
64580	675	of object-oriented systems. When exactly should invariants hold? How should concurrency properties be specified? JML’s specification inheritance forces subtypes to be behavioral subtypes , but subtyping in Java is used for implementation inheritance as well; is it practical to always weaken the specifications of supertypes enough so that their subtypes are behavioral subtypes? There
64580	676	for checking specifications, there are also tools that help a developer write JML specifications, with the aim of reducing the cost of producing JML specifications. • The Daikon tool (Section 3.4, ) infers likely invariants by observing the runtime behavior of a program. • The Houdini tool  uses ESC/Java to infer annotations for code. • The jmlspec tool can produce a skeleton of a
64580	676	then verify code against the specification. Writing the JML specification is left to a programmer. Because this task can be time-consuming, tedious, and error-prone, the Daikon invariant detector  provides assistance in creating a specification. Daikon outputs observed program properties in JML form and automatically inserts them into a target program. The Daikon tool dynamically detects
64580	677	the actual behavior of the program is not necessarily the same as its intended behavior.) However, Daikon uses static analysis, statistical tests, and other mechanisms to suppress false positives . Even if a property is not true in general, Daikon’s output provides valuable information about the test suite over which the program was run. Even with modest test suites, Daikon’s output is
64580	680	the jmldoc tool (authoreds6 Lilian Burdy et al. by David Cok) produces browsable HTML pages containing both the API and the specifications for Java code, in the style of pages generated by Javadoc . This tool reuses the parsing and checking performed by the JML checker and connects it to the doclet API underlying Javadoc. In this way, jmldoc remains consistent with the definition of JML and
64580	683	which has been extended to provide a formal semantics for a large part of JML. The verification of the proof obligations is accomplished using a Hoare Logic  and a weakest-precondition calculus  for Java and JML. Interactive theorem proving is very labor-intensive, but allows verification of more complicated properties than can be handled by extended static checking with ESC/Java. A recent
64580	684	that has has been formalized in PVS, and which has been extended to provide a formal semantics for a large part of JML. The verification of the proof obligations is accomplished using a Hoare Logic  and a weakest-precondition calculus  for Java and JML. Interactive theorem proving is very labor-intensive, but allows verification of more complicated properties than can be handled by
64580	687	smartcards written in the Java Card dialect of Java. Keywords. Formal methods, formal specification, Java, runtime assertion checking, static checking, program verification. 1 Introduction JML , which stands for “Java Modeling Language”, is useful for specifying detailed designs of Java classes and interfaces. JML is a behavioral interface specification language for Java; that is, it
64580	688	smartcards written in the Java Card dialect of Java. Keywords. Formal methods, formal specification, Java, runtime assertion checking, static checking, program verification. 1 Introduction JML , which stands for “Java Modeling Language”, is useful for specifying detailed designs of Java classes and interfaces. JML is a behavioral interface specification language for Java; that is, it
64580	688	uses Java’s expression syntax in assertions, thus JML’s notation is easier for programmers to learn than one based on a language-independent specification language like the Larch Shared Language  or OCL . Figure 1 gives an example of a JML specification that illustrates its main features. JML assertions are written as special comments in Java code, either after //@ or between /*@ ...
64580	688	of object-oriented systems. When exactly should invariants hold? How should concurrency properties be specified? JML’s specification inheritance forces subtypes to be behavioral subtypes , but subtyping in Java is used for implementation inheritance as well; is it practical to always weaken the specifications of supertypes enough so that their subtypes are behavioral subtypes? There
64580	689	uses Java’s expression syntax in assertions, thus JML’s notation is easier for programmers to learn than one based on a language-independent specification language like the Larch Shared Language  or OCL . Figure 1 gives an example of a JML specification that illustrates its main features. JML assertions are written as special comments in Java code, either after //@ or between /*@ ...
64580	689	(time and space), the behavior of the original program is unchanged. The transparency of runtime assertion checking is guaranteed, as JML assertions are not allowed to have any side-effects . The JML language provides a rich set of specification facilities to write abstract, complete behavioral specifications of Java program modules . It opens a new possibility in runtime assertion
64580	689	of specifications, various forms of quantifiers and set comprehension notation, support for strong and weak behavioral subtyping , and a contextual interpretation of undefinedness . In sum, the JML compiler brings “programming benefits” to formal interface specifications by allowing Java programmers to use JML specifications as practical and effective tools for debugging,
64580	692	such as (stateful) interface specifications, multiple inheritance of specifications, various forms of quantifiers and set comprehension notation, support for strong and weak behavioral subtyping , and a contextual interpretation of undefinedness . In sum, the JML compiler brings “programming benefits” to formal interface specifications by allowing Java programmers to use JML
64580	694	since they are used in sensitive applications such as bank cards and mobile phone SIMs. (An interesting overview of security properties that are relevant for Java Card applications is available .) JML, and several tools for JML, have been used for Java Card, especially in the context of the EU-supported project VerifiCard (www.verificard.org). JML has been used to write a formal
64580	695	Section 5 discusses some related languages and tools, such as OCL and other runtime assertion checkers, and Section 6 concludes. 2 The JML Notation JML blends Eiffel’s design-by-contract approach  with the Larch  tradition (and others that space precludes mentioning). Because JML supports quantifiers such as \forall and \exists, and because JML allows “model” (i.e., specification-only)
64580	695	fields, and model methods. Thus the JML compiler represents a significant advance over the state of the art in runtime assertion checking as represented by design by contract tools such as Eiffel  or by Java tools such as iContract  or Jass . The jmlc tool also supports advances such as (stateful) interface specifications, multiple inheritance of specifications, various forms of
64580	696	Daikon’s output provides valuable information about the test suite over which the program was run. Even with modest test suites, Daikon’s output is highly accurate. In one set of experiments , over 90% of the properties that it reported were verifiable by ESC/Java (the other properties were true, but were beyond the capabilities of ESC/Java), and it reported over 90% of the properties
64580	697	example, if Daikon generated 100 properties, users had only to delete less than 10 properties and to add another 10 properties in order to have a verifiable set of properties. In another experiment ,s8 Lilian Burdy et al. users who were provided with Daikon output (even from unrealistically bad test suites) performed statistically significantly better on a program verification task than did
64580	698	with LOOP An Overview of JML Tools and Applications 9 The University of Nijmegen’s LOOP tool  translates JML-annotated Java code into proof obligations for the theorem prover PVS . One can then try to prove these obligations, interactively, in PVS. The translation from JML to formal proof obligations builds on a formal semantics for sequential Java that has has been
64580	699	Java programmers to use JML specifications as practical and effective tools for debugging, testing, and design by contract. 3.3 Unit testing A formal specification can be viewed as a test oracle , and a runtime assertion checker can be used as the decision procedure for the test oracle . This idea has been implemented as a unit testing tool for Java (jmlunit), by combining JML with the
64580	700	has shown that JML is expressive enough to specify a non-trivial existing API. The runtime assertion checker has been used to specify and verify a component of a smartcard operating system . ESC/Java has been used with great success to verify a realistic example of an electronic purse implementation in Java Card . This case study was instrumental in convincing industrial users of
8918537	706	large surfaces by people. We mix horizontal lines and vertical lines and even write text slantingly. Most of the previous publications and systems have been assuming only horizontal lines of text  while we have been trying to relinquish any writing constraint from on-line text input. We proposed a method to recognize mixtures of horizontal, vertical and slanted lines of text with assuming
8918537	706	line direction free recognizer already developed . This recognizer employs segmentation likelihood, character recognition likelihood, context (bi-gram) likelihood and character size likelihood . 3.5. Selection of most plausible interpretation The recognition of text lines described above is applied for every CO-revised text lines for each text line element. This step selects the highest
709	711	possessing the data warehouse. Web data sources are available via the Internet. We represent Web data using a special data model called MIX (Metadata based Integration model for data X-change) . MIX is a self-describing data model in the sense Time Dimension Key Day Month Year Shipping Fact Table Book Shop Fact Table Time-key Customer-Key Book-key List-price Sold-price Units-sold
709	713	of the Third Int’l Workshop Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS’01) 1530-1354/01 $10.00 © 2001 IEEEsquery systems for semistructured/unstructured data, such as . RLTs serve as a view of MIX objects in our approach. Besides, a relational database table can be easily represented as a tree with all leaves having the same depth (fixed-depth tree). The online
709	713	changed, i.e., is application independent. Our transformation rule definition file can be written using a language for querying semi-structured or unstructured data. In our prototype, we use UnQL  as the rule definition language. The reasons are: first, that language is designed for querying semi-structured and unstructured data organized as Rooted Labeled Trees (RLTs) or Labeled Graphs
709	713	Discount Fact Table tuple ... tuple ... Time-key BookBookStore- SoldPrice Discount keykey Figure 7: RLT representation of data warehouse tables Our mapping rule definitions are written in UnQL . UnQL consists of tree constructors, function definitions and query definitions: ¯ tree constructors ??, ?l :t?, Ø ? Ø , ?Ð ? Ø ? ???? ÐÒ ? ØÒ? ¯ functions defined by structural recursion: let sfun
709	713	recursive traverse and ensures that the recursion always terminates. Therefore, the above mapping definitions can be written in the following form using structural recursion following UnQL’s syntax . We will use mapping definitions in the structural recursion form in later sections, because this form can help to understand the semantics of transformation rules, which will be discussed in
709	713	traversal on an object tree and the restructuring of its subtrees. In order to make explicit the exact meaning, i.e., the specified processing of the mapping rules, we use a calculus called UnCAL , which is UnQL’s internal algebra in the sense of lambda calculus (a formalism with variables and functions) rather than in the sense of relational calculus (a logic with variables and
709	716	the comparison of source- and target-schema and tree restructuring. First, Web data is integrated based on a common structural and semantic basis by using a selfdescribing object model, called MIX . We refer to this step as the Web data representation and integration phase, and the MIX model serves as a Web data representation model. Next is the transformation phase. In this step our source
709	720	of the Third Int’l Workshop Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS’01) 1530-1354/01 $10.00 © 2001 IEEEsquery systems for semistructured/unstructured data, such as . RLTs serve as a view of MIX objects in our approach. Besides, a relational database table can be easily represented as a tree with all leaves having the same depth (fixed-depth tree). The online
709	720	are explicitly described as mapping rules, based on which the transformation can be accomplished automatically via tree restructuring. Our approach is related to the work of Milo, Beeri and Zohar . They define common schema and data models for the source and target data. Using a rule-based method, they match components in the source schema with components in the target schema. The matching
709	720	information is already provided as a part of MIX objects, we can use RLTs not only for the schema comparing process but the data mapping task as well. Second, the TranScm system introduced in  uses rules to match schemata. Each rule uses match and descendents functions to handle schema matching, and uses translation functions to handle data translation. In contrast to their descendents
8918544	724	specially designed to increase the performance of the nearest neighbor classification rule. We have not made assumptions on the data distribution, and we don’t force our projection to be orthogonal . The only assumption we impose is that our embedding must be based on a set of simple 1D projections, which can complement each other to achieve better classification results. We have made use of
8918546	733	(35) &quot;Proper Government (PG) cannot apply over governing domains&quot; is falsified: ?BRaD &quot;beard&quot; ? ? ? '( ? ? ? > ? > ? ? ????? ? ? ? ????????????? PGs(23) alternative: CVCV syllable structure (Lowenstamm 1996) closed syllable geminate long vowel O N O N O N O N O N O N ? ? ? ? \ / ? ? \ / C V C ø C V C V (27) PG ????????? ? ? (28) ? ? ? ? ? ? ? ?????????????????? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
740	741	of each text. Basically, there are two kinds of approaches. In the AI-based approach, natural language processing techniques (Sowa, 1984; Srihari and Burhans, 1994) or machine learning techniques (Barletta and Mark, 1988; Baudin et al, 1994) can be used to build or refine an internal representation of each text. Although most of these methods can produce deep conceptual indices, they can only work in restricted
740	742	there are two kinds of approaches. In the AI-based approach, natural language processing techniques (Sowa, 1984; Srihari and Burhans, 1994) or machine learning techniques (Barletta and Mark, 1988; Baudin et al, 1994) can be used to build or refine an internal representation of each text. Although most of these methods can produce deep conceptual indices, they can only work in restricted environments and
740	743	of structure and then let the user navigate through it. In fact, the organizing/navigating paradigm is becoming very popular (Thompson and Croft, 1989; Maarek et al, 1991; Lucarella et al, 1993; Bowman et al, 1994). One main problem of this approach is creating the atomic pieces of information and linking them, which usually requires subjective and time-consuming decisions. We argue that conceptual
740	751	words occurring in a text collection, ignoring punctuation and case. 2. Word stemming We reduce each word to word-stem form. This is done by using a very large morphological lexicon for English (Karp et al, 1992) that contains the standard inflections for nouns (singular, plural, singular genitive, plural genitive), verbs (infinitive, third person singular, past tense, past participle, progressive form),
740	757	of a text database is to identify the content of each text. Basically, there are two kinds of approaches. In the AI-based approach, natural language processing techniques (Sowa, 1984; Srihari and Burhans, 1994) or machine learning techniques (Barletta and Mark, 1988; Baudin et al, 1994) can be used to build or refine an internal representation of each text. Although most of these methods can produce deep
759	760	so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique is dynamic voltagefrequency scaling. Since the power used by a CMOS circuit is proportional to the product f · v 2 , where f
759	761	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	763	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	764	next event requiring the processor will occur is unknown. In some cases, the end of the shutdown time can be predicted, so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique
759	764	by I/O events, such as portable wireless terminals. Furthermore, they don’t implement their 2 techniques on a real system; their results are based on analysis and modeling. C.-H. Hwang’s work  focuses solely on mechanisms for predicting the length of idle periods on event driven applications (X Window System server, Netscape, telnet, and tin). J.R. Lorch and A.J. Smith’s work  is
759	766	so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique is dynamic voltagefrequency scaling. Since the power used by a CMOS circuit is proportional to the product f · v 2 , where f
759	767	work  focuses solely on mechanisms for predicting the length of idle periods on event driven applications (X Window System server, Netscape, telnet, and tin). J.R. Lorch and A.J. Smith’s work  is based on processors that can be put to sleep by turning off the clock signal, which incurs very little latency, and which preserve most of their state when they are sleeping. 1 They also focus
759	768	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	769	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
759	770	so that the processor can be woken up in time for the next (predicted) event . The task of predicting when to shut off a processor offers similarities with that of spinning down a hard disk , however the time scales are very different. Another promising technique is dynamic voltagefrequency scaling. Since the power used by a CMOS circuit is proportional to the product f · v 2 , where f
759	773	to the frequency, the energy required for this task decreases at the same rate as the voltage squared. Policies of when to to modify the voltage and frequency have been extensively studied . Our technique differs from earlier uses of short duration sleep in significant ways. For example, M.B. Srivastava’s work  focuses on devices where all computation is driven by I/O events, such
781	783	2 is therefore interesting to develop specific resolution techniques for these particular fragments. All previous algorithms for solving conjunctive and disjunctive classes, including those from , take at least quadratic time in the size of a boolean equation system in the worst case. For large boolean equations which are typically encountered in model checking and preorder/equivalence
781	783	Definition 3.2. Let G = (V, E, ?) be a dependency graph and k ? V . We define the graph G?k = (V, E?k, ?) by taking • E?k = {?i, j? ? E | i ? k and j ? k}. The following essential lemma comes from . Lemma 3.3. Let GE = (V, E, ?) be the dependency graph of a disjunctive Boolean equation system E. Let xi be any variable in E and let valuation v be the solution of E. Then the following are
781	783	other. Using for instance Lemma 3.22 of  a Boolean equation system can be reordered such that our notion of alternation depth and the notion of  coincide. A note about the open question in . In  the following open question was stated. Given a directed graph of which the vertices are ordered and labelled with either red or green. Is there a sub quadratic algorithm to determine
781	787	O(e 2 ) time solutions, our algorithm computes the solution of such a fixpoint equation system with size e and alternation depth d in O(e log d) time. 1 Introduction A Boolean Equation System (BES)  is a sequence of boolean equations with minimal and maximal fixpoints. It gives a useful framework for the verification of finite state concurrent systems. This is due to the fact that many
781	789	and an empirical comparison between our approach and other related algorithms are left for future work. Our algorithm combines essentially graph theoretic techniques for finding strong components  and hierarchical clustering . King, Kupferman and Vardi  recently found an algorithm in the realm of parity word automata. Their algorithm is very similar to ours and also resorts to the
781	789	and (b) GE contains a cycle of which the lowest index of a vertex on this cycle is j. Finding cycles in graphs can be done in linear time using any algorithm to detect strongly connected components . A strongly connected component (SCC) in a graph G = (V, E, ?) is a set of vertices W ? V such that for each pair of vertices k, l ? W it is possible to reach l from k by following directed edges
1033965	792	of this article is to highlight the research accomplishments of the NetSolve system; additional low-level details including complete usage instructions can be found in the NetSolve User's Guide . 10 THE NetSolve SYSTEM This section offers a high-level overview of the NetSolve system discussing, in turn, architectural, implementation and managerial issues. The architecture An instance of a
1033965	795	(GRID) Figure 4. Proxy architecture. behalf of the client means that different proxies can negotiate for different types of services. So far, we have implemented proxies to negotiate for Globus  services and, of course, the standard NetSolve services. Future work may include integrating other systems like Condor  or Legion  in a similar fashion; however, a primary motivation of the
1033965	799	to optimize the use of Grid resources, especially when bandwidth is of the essence, data sets are very large, or both. Distributed storage infrastructures A distributed storage infrastructure (DSI)  is a technology that allows a program to manage data stored remotely. The Internet backplane protocol (IBP)  is an example DSI that has been 25 incorporated into NetSolve in an effort to
1033965	800	to leverage DSI storage without modifying the standard NetSolve functions for computational requests. A complete taxonomy on the state of the art in DSIs and Data Grid technologies can be found in , which also gives full details regarding NetSolve's DSI integration efforts. Task farming 10 This interface addresses applications that have simple task-parallel structures but require a large
1033965	801	it is to be expected that the availability and load of resources within the server pool will change dynamically. The design and validation of the NetSolve task farming interface is presented in . This article also presents a preliminary adaptive scheduling algorithm for task farming. The initial task farming work in NetSolve has lead to a larger development and integration effort as part
1033965	802	also presents a preliminary adaptive scheduling algorithm for task farming. The initial task farming work in NetSolve has lead to a larger development and integration effort as part of the APST  project 25 (see the section on data persistence). Transparent algorithm selection Through NetSolve, users are given access to complex algorithms that solve a variety of types of problems, one
1033965	802	Several scheduling heuristics were studied in  and experimental results using NetSolve on a wide-area testbed for running large MCell simulations with those heuristics can be 15 found in . Nuclear engineering The goal of this project is to develop a prototype environment for the Collaborative Environment for Nuclear Technology Software (CENTS). CENTS aims to lay the foundation for a
1033965	809	PDF facility, NetSolve has integrated a variety of scientific packages. These PDFs 10 are distributed with the current implementation of NetSolve: BLAS , LAPACK , ScaLAPACK , ItPack , PETSc , AZTEC , MA28 , SuperLU  and ARPACK . When efficient, we include the actual numerical library with NetSolve; in the other cases, these packages are available for a large
1033965	811	scientific packages. These PDFs 10 are distributed with the current implementation of NetSolve: BLAS , LAPACK , ScaLAPACK , ItPack , PETSc , AZTEC , MA28 , SuperLU  and ARPACK . When efficient, we include the actual numerical library with NetSolve; in the other cases, these packages are available for a large number of platforms and are freely distributed.
1033965	814	report information regarding their CPU load so that the agent can determine which servers represent the best choice to service a request. The CPU load manager performs this task; however, the NWS  is a Grid service utility with a component, the CPU sensor, that does this as well. The following section discusses our integration of NWS forecasters 10 and motivates our use of the NWS CPU
1033965	814	is reported. 30 NWS forecasters When allocating resources, the agent's main goals is to choose the best-suited computational server for each incoming request We have employed the use of the NWS  to help us gather the information necessary to make these decisions. The NetSolve agent calls upon the services of NWS forecasters to predict future availability of server hosts based upon
1033965	816	San Diego. The use of NetSolve isolates the scheduler from the resource-management details and allows researchers to focus on the scheduler design. Several scheduling heuristics were studied in  and experimental results using NetSolve on a wide-area testbed for running large MCell simulations with those heuristics can be 15 found in . Nuclear engineering The goal of this project is to
855	861	tufts). It is natural to map the display parameters of the glyphs to the information associated with the data. Parameters that have been demonstrated include color, thickness, opacity, and scale . Multi-sensory modalities such as haptics and sound  have been utilized when the visual approaches have been exhausted—typically the case for multi-dimensional data. Modifications to geometry
855	861	interpolated surface locations. Animated surfaces  can show the range of locations over which a surface might lie as well as the range for other data types. For a more complete review see . 3 Representing Sea Bottom Information Our research investigated representations of depth measurements containing multiple dimensions of uncertainty andsFigure 1: Images show from left to right a
8918568	868	originally stored in the permanent storage, an effect that is certainly not to be desired. While the current R&D efforts have been focusing on how to aggregate (e.g., ) and make use of (e.g., ) distributed computing resources, few addressed the above issues. In this paper, we report on the InstantGrid framework, which is the result of our effort in creating a tool to facilitate the
8918568	869	stored in the permanent storage, an effect that is certainly not to be desired. While the current R&D efforts have been focusing on how to aggregate (e.g., ) and make use of (e.g., ) distributed computing resources, few addressed the above issues. In this paper, we report on the InstantGrid framework, which is the result of our effort in creating a tool to facilitate the
871	872	for trying out, tuning, and analysing different safety policies and logics for different target platforms. Our approach is different from other work in the formal foundation of PCC by Appel et al.   or Hamid et al.  in that it works with an explicit, executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical
871	873	trying out, tuning, and analysing different safety policies and logics for different target platforms. Our approach is different from other work in the formal foundation of PCC by Appel et al.   or Hamid et al.  in that it works with an explicit, executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical
871	875	6 the simplifier and a decision procedure for presburger arithmetic suffice to prove the verification condition. For the client side Isabelle provides (compressed) proof terms and a proof checker . Proofs are encoded as ? terms having a type that corresponds to the theorem they prove (Curry Howard Isomorphism). Proof Checking becomes a type checking problem, which can be handled by a small
871	876	isafeF OD (0,2). 4.4 Code Producer and Consumer The code producer can write annotated programs in Isabelle. To obtain the verification condition one can generate and execute ML code for the VCG  or use the simplifiers12 to evaluate vcg ?. Proving the verification condition is supported by powerful proof tools and a rich collection of HOL theorems. For the example in Fig. 6 the simplifier
871	877	of the PCC system. Proof checkers are relatively small standard components of many logical frameworks. The VCG on the other hand is large (several thousand lines of C code in current PCC systems  ) and complex (it handles annotations, produces complex formulae, and contains parts of the safety policy). Our framework contains a VCG with a formal proof of safety, mechanically checked in
871	877	machine language, safety policy, or safety logic. Additionally to the correctness of VCG and proof checker, we need the safety logic to be sound. As a recent bug  in the SpecialJ system  shows, this is not trivial. It is not even immediately clear, what exactly a safety logic must satisfy to be sound. Our framework makes the underlying assumptions on machine, policy, and logic
871	878	and analysing different safety policies and logics for different target platforms. Our approach is different from other work in the formal foundation of PCC by Appel et al.   or Hamid et al.  in that it works with an explicit, executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical foundations of PCC as the
871	879	our approach is related to other techniques that impose safety policies on machine code statically: Typed Assembly Language , Mobile Ressource Guarantees  or Java Bytecode Verification . There are four levels in our PCC systems. The first level, the PCC framework (§2), provides generic features and minimal assumptions. The second level is the platform (§3). Platform designers can
871	880	is not restricted to any particular machine language, safety policy, or safety logic. Additionally to the correctness of VCG and proof checker, we need the safety logic to be sound. As a recent bug  in the SpecialJ system  shows, this is not trivial. It is not even immediately clear, what exactly a safety logic must satisfy to be sound. Our framework makes the underlying assumptions on
871	881	extensions can then be applied to that sound core. On a broader scale, our approach is related to other techniques that impose safety policies on machine code statically: Typed Assembly Language , Mobile Ressource Guarantees  or Java Bytecode Verification . There are four levels in our PCC systems. The first level, the PCC framework (§2), provides generic features and minimal
871	882	systems by instantiating it to a simple assembly language with procedures and a safety policy for arithmetic overflow. 1 Introduction Proof Carrying Code (PCC), first proposed by Necula and Lee  , is a scheme for executing untrusted code safely. Fig. 1 shows the architecture of a PCC system. The code producer is on the left, the code receiver on the right. Both use a verification
871	883	by instantiating it to a simple assembly language with procedures and a safety policy for arithmetic overflow. 1 Introduction Proof Carrying Code (PCC), first proposed by Necula and Lee  , is a scheme for executing untrusted code safely. Fig. 1 shows the architecture of a PCC system. The code producer is on the left, the code receiver on the right. Both use a verification condition
871	884	the PCC system. Proof checkers are relatively small standard components of many logical frameworks. The VCG on the other hand is large (several thousand lines of C code in current PCC systems  ) and complex (it handles annotations, produces complex formulae, and contains parts of the safety policy). Our framework contains a VCG with a formal proof of safety, mechanically checked in the
871	885	executable, and verified VCG and not directly on the machine semantics or a type system. The focus of the framework is on aiding logical foundations of PCC as the one started by Necula and Schneck  and on encouraging the analysis of safety properties other than the much researched type and memory safety. Necula and Schneck  also present a framework for VCGs. They work with a small,
871	888	This enables us to optimise verification conditions after/during their construction. By now, we also have instantiated the PCC framework to a (downsized) version of the Java Virtual Machine . For this we did not have to change the framework, thus we believe that our framework’s formalisation and its requirements are reasonable, even for real life platforms.sPrototyping Proof Carrying
871	889	or recursive procedures and safety policies about time and memory consumption of programs. Moreover we have instantiated a safety logic based on first order arithmetic in form of a deep embedding . There, formulae are modelled as HOL datatype and can by analysed by other HOL functions. This enables us to optimise verification conditions after/during their construction. By now, we also have
8918569	891	McLain Brigham Young University, Provo, Utah 84602 {weiren,beard}@ee.byu.edu, mclain@byu.edu Much of the research focus in the cooperative control community has been on formation control problems . This focus may be due to the fact that the group control problem can be reduced to well-established single-agent control problems by employing a leader-follower type control strategy. For example,
8918569	891	and stij = qtij otherwise. Also note that detSt() = detQt(), j = 1, · · · , N. Then we know that detSt = = N? (?1) 1+j st1jdetSt() j=1 N? (?1) 1+j qt1jdetSt() j=1 + k1mdetSt() ? (?1) 1+m k1mdetSt() =detQt + k1m(detSt() + (?1) m detSt()). Consider a matrix E = , (i, j) = 1, · · · , N ? 1, given by adding  T to the (m ? 1)th
8918569	891	s3N ? . . . . ? . ? . sN2 sN3 · · · sNm + sN1 · · · sNN Thus e i(m?1) = s (i+1)m + s (i+1)1, i = 1, · · · , N ? 1. Using the properties of determinants, it can be verified that det(tI ? E) = detSt() + (?1) m detSt(). Obviously matrix E has zero row sum and nonpositive diagonal elements. Also matrix E is diagonally dominant. From the Gersgorin disc theorem, we know that E has at least
8918569	893	McLain Brigham Young University, Provo, Utah 84602 {weiren,beard}@ee.byu.edu, mclain@byu.edu Much of the research focus in the cooperative control community has been on formation control problems . This focus may be due to the fact that the group control problem can be reduced to well-established single-agent control problems by employing a leader-follower type control strategy. For example,
8918569	893	, the states of the leader constitute the coordination variable since the actions of the other vehicles in the formation are completely specified once the leader states are known. In , the notion of a virtual structure is used to derive formation control strategies. The motion of each vehicle is causally dependent on the dynamic states of the virtual structure, therefore the
8918569	894	the computation and communication loads. Coordination variables and functions have been applied successfully to UAV cooperative timing missions  and UAV cooperative reconnaissance problems . An illustrative example is given in the next section.s1.1 Example: Cooperative Timing Coordination Variables and Consensus 5 The application of coordination variables and functions can be
8918569	896	information variable is “pushed” toward Aj’s information variable with strength kij. In the case of ?i ? R, Eq. (5) can be written in matrix form assCoordination Variables and Consensus 7 ?? = C?, (6) where ? =  T , C = , (i, j) = 1, · · · , N, with cii = ?( ? j?=i kijGij), i = 1, · · · , N, and cij = kijGij, j ?= i. We say that C is the matrix associated with graph G. Note
8918569	896	is the dynamic state of a virtual leader. Static Consensus It has been shown in  that the group of vehicles A reach consensus asymptotically using the update scheme (5) if matrix C in Eq. (6) has exactly one zero eigenvalue and all the others are in the open left half plane. The following result computes the value of the information variable that is reached through the consensus
8918569	896	ass8 W. Ren, R. W. Beard, T. W. McLain M ? 0, if all its entries are nonnegative. A nonnegative matrix is said to be a stochastic matrix if all its row sums are 1. Lemma 1. If C is given by Eq. (6), then eCt , ?t > 0, is a stochastic matrix with positive diagonal entries. Furthermore, if C has exactly one zero eigenvalue, then eCt ? b?T and ?i(t) ? ?N i=1 (?i?i(0)) as t ? ?, where b = [1, · ·
8918569	896	eigenvalue 1. Thus it can be seen that ? = ?x for some ? ?= 0. Since ? N i=1 ?i = 1, it must be true that ? > 0, which implies that ? ? 0.sCoordination Variables and Consensus 9 The solution to Eq. (6) is given by ?(t) = eCt?(0). Therefore, it is obvious that ?i(t) ? ?N i=1 (?i?i(0)), i = 1, · · · , N, as t ? ?. Note that if we replace matrix C with ?C in Eq. (6), where ? > 0, we can increase
8918569	896	for the group of agents A. Let G2 be the communication graph by adding one more directed link from any node m to node ? to graph G1, where m ?= l. Also let Q and S be the matrices in the update law (6) associated with graphs G1 and G2 respectively. Denote pQ(t) = det(tI ? Q) and pS(t) = det(tI ? S) as the characteristic polynomial of Q and S respectively. Let Qt = tI ? Q and St = tI ? S. Given
8918569	897	by the same time-varying input u(t), which might represent an a priori known feedforward signal. The associated consensus scheme is given by ??i = ? N? kijGji(?i ? ?j) + u(t), i = 1, ?? · · , N. (7) j=1 Eq. (7) can also be written in matrix form as ?? = C? + Bu(t), (8) where C is the matrix associated with graph G and B =  T . We have the following theorem regarding consensus of
8918569	901	For example, in , cooperative task allocation is addressed. Individual vehicle behavior is dependent on the task allocation vector which becomes the coordination variable. Similarly, in , the coordination variable is the dynamic role assignment in a robot soccer scenario. Information necessary for cooperation may be shared in a variety of ways. For example, relative position
8918569	902	of ways. For example, relative position sensors may enable vehicles to construct state information for other vehicles , or knowledge may be communicated between vehicles using a wireless network , or joint knowledge might be pre-programmed into the vehicles before a mission begins . In Section 1 we offer some definitions and general principles regarding coordination variables. For
8918569	902	seeking consensus among a team of vehicles. Section 2 states some results on multi-agent consensus seeking for fixed communication topologies. The consensus problem has recently been addressed in . The work reported in  is particularly relevant to the results reported in thissCoordination Variables and Consensus 3 paper. Ref  addresses the knowledge consensus problem when teams of
8918569	910	two-level decomposition process significantly reduces the computation and communication loads. Coordination variables and functions have been applied successfully to UAV cooperative timing missions  and UAV cooperative reconnaissance problems . An illustrative example is given in the next section.s1.1 Example: Cooperative Timing Coordination Variables and Consensus 5 The application of
8918569	911	for constant coordination variables, which may not be suitable for applications where the coordination variable evolves dynamically. For example, in the context of leader-following approaches (c.f. ), the group leader’s trajectory can act as the coordination variable for the whole group. Dynamic Consensus Suppose that the information variable on each vehicle is driven by the same time-varying
8918569	914	seeking consensus among a team of vehicles. Section 2 states some results on multi-agent consensus seeking for fixed communication topologies. The consensus problem has recently been addressed in . The work reported in  is particularly relevant to the results reported in thissCoordination Variables and Consensus 3 paper. Ref  addresses the knowledge consensus problem when teams of
945	947	THE CASE OF CHINA Shenggen Fan 1 , Cheng Fang, 2 and Xiaobo Zhang 3 1. INTRODUCTION Many studies have shown that investments in agricultural research can yield favorable economic returns (Alston et al. 2000), and contribute to significant reductions in rural poverty (Kerr and Kolavalli 1999; Fan, Hazell, and Thorat 2000; Fan, Zhang, and Zhang 2000; Hazell and Haddad 2001). The links between
945	975	Institutional changes and policy reforms have made important contributions to growth in agricultural and nonagricultural production and poverty reduction in rural China (Fan 1991, and Fan and Pardey 1997). We do not need to estimate these contributions for the purposes of this study, but in order to reduce possible estimation biases that may arise from neglecting them, we added year and province
945	1023	by state governments and were allowed to vary by only a few percentage points across provinces. We therefore assume that price levels were the same for all provinces in 1984. Kanbur and Zhang (1999) and Yang and Cai (2000) have adopted similar methods to calculate real expenditure levels across regions.s12 years for a person who is illiterate and semi-illiterate, 5 years for a person with
945	1023	d. In this report, we use PDLs of degree 2. In this case, we only need to estimate three instead of i+1 parameters for the lag distribution. For more detailed information on this 6 Alston et al. (1999) argue that research lag may be much longer than previously thought, perhaps even infinite. But this argument may be less relevant for most developing countries since their national agricultural
8918572	1023	Concepts By providing a shared and common understanding of a domain that can be communicated between people and applications systems, ontologies facilitate the sharing and reuse of knowledge . An ontology suitable for the integration into the EMMO model has to distinguish between object concepts and relational concepts. Object concepts are used for labeling the nodes, relational
8918572	1028	unique approach to multimedia meta modeling. None of the standards for multimedia document models, such as SMIL  or SVG , and none of the standards for semantic media description, such as RDF  or Topic Maps , addresses all these aspects. Thus, none of the query languages for those standards can fulfil all requirements with respect to the expressiveness of a query language for EMMOs.
8918572	1029	content is to enhance multimedia resources by semantic metamodels and to integrate domain ontologies. To answer these challenges, we have developed Enhanced Multimedia Meta Objects (EMMOs) , a novel approach for semantic multimedia content modeling. EMMOs were created within the EU-project CULTOS to model InterTextualThreads (ITTs), i.e. complex knowledge structures used by the
1059	1062	motion for sustainable operation, 5) logistics capability with energy transport via the cable systems, and 6) node elevation for enhancing wireless link budget relative to surface-distributed nodes . Finally, the need for high fidelity monitoring of large environments will require the simultaneous operation of multiple NIMS nodes that exploit the infrastructure for communication and
1059	1063	the need for high fidelity monitoring of large environments will require the simultaneous operation of multiple NIMS nodes that exploit the infrastructure for communication and coordination . NIMS applications to environmental monitoring range from fundamental science objectives to public health and safety. Examples include natural environment monitoring with requirements for
1059	1069	mapping algorithms have relied primarily on probabilistic as opposed to statistical approaches. These approaches range from the Bayesian filter  to Kalman filter , hidden Markov model , to Bayesian network. These methods apply to mapping unknown environments with known robotic localization or localization with known environments. The Simultaneous Localization and Mapping (SLAM)
695740	1084	the user to explicitly layout the visual components. In this paper, we focus on the added capabilities to the original version of the scripting language AnimalScript built into the Animal system (Rößling and Freisleben, 2001). To avoid confusion, we will always refer to the new implementation as AnimalScript V2, and use AnimalScript for the original implementation. We first review the main features of interest in the
695740	1084	works on String, boolean and integer variables as well as literal Strings. 4 Conclusions and Further Work AnimalScript V2 extends the functionality of the scripting language AnimalScript (Rößling and Freisleben, 2001) by offering important base operations for simplifying animation creation: loops, conditionals, variables and expressions. The additions considerably increase the expressive power of AnimalScript,
8918581	1101	a static generic model to speaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time
8918581	1102	analysis of motion capture data. Figure 3: Shape and appearance changes associated with extreme variations along the first lip component (rounding/spreading) for two speakers. Shape-free textures  have been obtained from image data with colored beads. (a) (b) (c) Figure 4: Rendering and recovering 3D articulation. Figure 5: Subdivision of n elementary volume of the original space and new
8918581	1102	configurations used for estimating the shape model (by warping all images to the neutral configuration). Instead of combining a posteriori separate shape and appearance models as in Cootes et al , we estimate a simple multilinear model that relates RGB colors of each pixel of the shape-free images to shape parameters. Figure 3 illustrates the change of shape-free appearance accompanying the
8918581	1104	variability of articulation, there is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision  or MPEG-4/SNHC coding scheme  require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape
8918581	1106	a static generic model to speaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time
8918581	1107	raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time and with a extreme precision
8918581	1110	of the face with no implicit reference to any articulatory model (e.g. FAP3 open_jaw “does not affect mouth opening” ). FAP ease however specifying constrictions sizes and positions  supposed to be less speaker-dependent than articulatory parameters. On the contrary articulatory models are often used to specify how constrictions sizes and positions are reached by
8918581	1112	in all allophonic variations of  for carrying the tongue front and upwards . (a) (b) (c) Figure 1: (a) Gathering fleshpoint positions; (b) The generic facial mesh developed by Pighin et al. ; (c) The transformed mesh.s(a) jaw down/up (b) jaw back/front (c) lips spread/round (d) upper lip up (e) lip corners up Figure 2: Elementary speech movements extracted from statistical analysis of
8918581	1113	variability of articulation, there is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision  or MPEG-4/SNHC coding scheme  require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape
8918581	1113	raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting  or 3D animation models , all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time and with a extreme precision
8918581	1113	SPEAKERSPECIFIC DATA The deformation of a high definition 3D surface towards a set of low definition 3D data is achieved by an original 3D-to-3D matching algorithm.. The generic 3D mesh used here  has 5826 vertices connected by 11370 triangles (see Figure 1.b). The 3D articulatory model of the female speaker used here drives 304 fleshpoints : 245 beads for the face as in Figure 1.a, 30
8918581	1116	subtle and distributed all over the face, but should not be neglected since interlocutors should be quite sensitive to laws governing biological motion (e.g. the experiments of Runeson et al  with body movements when carrying imaginary versus real loads). Although its crude linear assumptions do not take into account, for now, saturation due to tissue compression, this multilinear
8918581	1117	the second deals with point-to-point distance: a set of 3D fleshpoints {qj} are identified and paired with {rj} vertices of S. The minimization is performed using the Levenberg-Marquardt algorithm . 3.2 Matching a neutral configuration The algorithm described above is applied to the articulatory configuration that provides the same neutral articulation as the static generic model. A minimal
1164	1168	and r-classes and are described at two levels: the object correspondence (OC) level and the value correspondence (VC) level. These mirror the general notions of existence and value dependencies . Finally, the restoration rules specify which actions to be taken if inconsistency occurs. They specify not only the actions needed for restoring consistency, but specify also the conditions that
1164	1164	management. The MRMS maintains consistency based on a specification, a multiple representation schema (MRSchema), formulated in a so-called multiple representation schema language (MRSL) . The MRSchema specifies matching, consistency, and restoration rules. The MRSL is based on the assumption that objects representing the same entity exhibit semantic similarities that enable us to
1164	1164	former is based on the Unified Modeling Language (UML) , and we use the Object Constraint Language (OCL)  to specify constraints. A more thorough description of the MRSL is given elsewhere . An MRSL specification, termed an MRSchema, is used to configure the MRMS. The process is depicted in Figure 1, which shows MRSchema specified in MRSL Translation ObjectRelational MRSchema
1164	1177	multiple representations with matching, consistency, and restoration rules. Finally, it should be noted that multiple representation databases relate in a more general sense to federated databases . A federated schema usually provides an integrated view of the underlying autonomous databases. In contrast, the main purpose of an MRMS is to maintain consistency. At a more general level, related
8918617	1187	mediator do not deal with decentralized systems with participants andsWordNet Exploitation through a Distributed Network of Servers 267 information sources location upredictability. As mentioned in  a P2P approach would be a more appropriate solution, since it lacks a centralized structure and promotes the equality among peers and their collaboration only for the time necessary to fulfill a
8918617	1188	where no a priori knowledge of the location of specific data is possible, the traditional mediator and federated databases approaches are not appropriate. Furthermore, approaches such as  that provide a source- and query-independent mediator do not deal with decentralized systems with participants andsWordNet Exploitation through a Distributed Network of Servers 267 information
8918617	1189	through the WMS. From its definition, WMS falls into the Data Integration framework, being able to manage a distributed, dynamic network of homogeneous data. Previous systems built for this purpose  are often characterized by a centralized system that controls and manages interactions among distributed information sources in order to serve requests for data. As a consequence, in a distributed
8918617	1190	where no a priori knowledge of the location of specific data is possible, the traditional mediator and federated databases approaches are not appropriate. Furthermore, approaches such as  that provide a source- and query-independent mediator do not deal with decentralized systems with participants andsWordNet Exploitation through a Distributed Network of Servers 267 information
8918617	1177	through the WMS. From its definition, WMS falls into the Data Integration framework, being able to manage a distributed, dynamic network of homogeneous data. Previous systems built for this purpose  are often characterized by a centralized system that controls and manages interactions among distributed information sources in order to serve requests for data. As a consequence, in a distributed
1193	1194	extracted from document images. Experimental results confirmed the validity of the proposed approach. 1. Introduction The problem of compressed pattern matching was introduced by Amir and Benson to perform pattern matching directly in a compressed text without any decompressing. For a given text Ì and pattern È , the usual approach to finding the occurrences of È in compressed Ì is to
1193	1197	pattern matching was generally faster than the method of decompressing followed by an ordinary pattern matching. For document image processing, the research of duplicate document detection and OCR on compressed images has been Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE reported recently.
1193	1199	why these documents are stored in the image format. Image based approach becomes an alternative way to directly search keywords in the document images without OCRing the entire document images. Several methods regarding this issue have been reported in the past years. Furthermore, in order to save storage space and speed up the transmission in the Internet, many document images
1193	1204	process is needed. Otherwise, È and É are further matched by the method based on the weighted Hausdorff distance. 4.2. Word Image Matching Based on Weighted Hausdorff Distance The Hausdorff distance measures the degree of mismatch between two point sets, which has been widely applied in two-dimensional image matching, especially in the area of object matching. Dubussion and Jain  presented
1193	1205	distance measures the degree of mismatch between two point sets, which has been widely applied in two-dimensional image matching, especially in the area of object matching. Dubussion and Jain  presented the modified Hausdorff distance(MHD) measure by employing the summation operator over all distance, rather than the maximum operator in the traditional Hausdorff distance. The MHD was
1206	1208	etc., all but the most motivated users will easily give up and save this task for another day. Browsing is an alternative to active search that is used by many online video retrieval interfaces . However, these methods are still limited by what can be displayed on a PC monitor. On the other hand, it’s well known that people can read three times faster than they can listen which suggests
1206	1211	listen which suggests that a paper-based interface could provide a more efficient search mechanism than an online media viewer. Video Paper is a paper-based interface for accessing digital video . The closed caption transcript that’s often provided with a television program is formatted and printed together with key frames selected from the video. Bar codes are also included so that with a
1206	1216	etc., all but the most motivated users will easily give up and save this task for another day. Browsing is an alternative to active search that is used by many online video retrieval interfaces . However, these methods are still limited by what can be displayed on a PC monitor. On the other hand, it’s well known that people can read three times faster than they can listen which suggests
1206	1217	browsing and retrieval was described. In contrast to the prior art, which relies on manual effort by the user to create a multimedia document which links a paper representation to digital data (), Video Paper is a fully automatic solution that creates a paper document which allows users to quickly access information in a long recording by reading the transcript or browsing key frames.
1206	1218	closed caption transcript to suggest key frames that could be useful to someone who browses a Video Paper document. The face detection algorithm first finds skin pixels in the normalized RG-space . Small holes in skincolored regions are removed by a morphological closing Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03
8918623	1228	result was 45% on 100 test documents.  had their best result on accuracy at 51% using Yahoo hierarchy and Yahoo web documents. The non-hierarchical web documents automatic categorization by  that used probabilistic description-oriented had worst result on their preliminary experiment with only an average precision of 2.13%. Later after they had increased the size of their sample
8918623	1229	because if we already choose the wrong node at the top, our topic node will be wrong too. This problem is faced by most of the automatic text classification using hierarchical structure , , . In this experiment we only use a small size of web ontology. This is because we are only interested in how our topic identification system performs on real data regardless of size. However, for
8918623	1232	in many ways. We try to take advantage of these semantics relationships to establish links between the words of Yahoo concepts and WordNet vocabulary. Based on our review on past related works , ,  we decide to use three types of semantics relationships found in the WordNet and they are synonym 1 , hypernym/hyponym 2 and meronym/holonym 3 . Using these three semantics relationships, we
8918623	1233	5 followed by our conclusion in section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to the
8918623	1233	the new document will be identified by computing the similarity between the document feature and each of the node categories feature or the probability that the document belongs to a node category . Other methods are like  that emphasise on representing a document with hypernym density using the WordNet hypernym hierarchy and  which combines clustering method with text categorization.
8918623	1233	mistakes because if we already choose the wrong node at the top, our topic node will be wrong too. This problem is faced by most of the automatic text classification using hierarchical structure , , . In this experiment we only use a small size of web ontology. This is because we are only interested in how our topic identification system performs on real data regardless of size.
8918623	1235	HTML tags and therefore our extraction technique is not appropriate to be used. In this case, alternative way of extracting web document like words frequency ,  , and positional policy  are more applicable. Since in this paper we are interested in extracting out information from the web document based on HTML tag, we consider extracting information from non-structured document as
8918623	1236	the WordNet hypernym hierarchy and  which combines clustering method with text categorization. Our approach of identifying the topic of a document has been inspired by the works of  and .  extends the word frequency counting (the classic way of identifying text topic) to concept counting. He uses the WordNet taxonomy to collect interesting concepts and later generalizes the
8918623	1237	section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to the node categories of the hierarchy. By
8918623	1237	some web documents maybe lack of HTML tags and therefore our extraction technique is not appropriate to be used. In this case, alternative way of extracting web document like words frequency ,  , and positional policy  are more applicable. Since in this paper we are interested in extracting out information from the web document based on HTML tag, we consider extracting
8918623	1237	concepts. Analysis. Our result is quite comparable to that result produced by the topic identification (text classification) that used machine learning techniques on web documents classification.  obtained 37% of accuracy result using Yahoo with 151 classes and 50 documents. Their best attained accuracy result was 45% on 100 test documents.  had their best result on accuracy at 51% using
8918623	1240	section 5 followed by our conclusion in section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to
8918623	1240	by computing the similarity between the document feature and each of the node categories feature or the probability that the document belongs to a node category . Other methods are like  that emphasise on representing a document with hypernym density using the WordNet hypernym hierarchy and  which combines clustering method with text categorization. Our approach of identifying
8918623	1240	way of identifying text topic) to concept counting. He uses the WordNet taxonomy to collect interesting concepts and later generalizes the concepts to identify the main topic of the target text. In , they transform the bag-of-words representation of the text into hypernym density. Using the height of generalization, they can limit the number of steps upward through the hypernym hierarchy for
8918623	1241	in section 6. 2 Related Works We found that the topic identification based on hierarchical structure technique is usually applied in text classification system , , , , , , , , . Most of these works utilize a hierarchical structure to decompose a huge and single classifier task into a set of small classifiers that correspond to the node categories of the
8918623	1241	ways. We try to take advantage of these semantics relationships to establish links between the words of Yahoo concepts and WordNet vocabulary. Based on our review on past related works , ,  we decide to use three types of semantics relationships found in the WordNet and they are synonym 1 , hypernym/hyponym 2 and meronym/holonym 3 . Using these three semantics relationships, we can
8918625	1249	M' B profit margin common standard workstation console This course is derived from the PhD thesis “CAFCR: A Multi-view Method for Embedded Systems Architecting; Balancing Genericity and Specificity”. 1.2 Program The program purposefully alternates process, business and technology views. The table below shows the program of the stakeholder part. Normally this part of the course is given in a
1251	1252	proposed selection methods, such as query by committee  or uncertainty sampling . Applications of active learning techniques are now emerging in the field of multimedia databasesannotation . In the following section we propose a new uncertainty sampling strategy, called partition sampling, that allows to select multiple samples as opposed to classical approaches. 3. PROPOSED APPROACH
1251	1252	that close elements are similar. Thus the knowledge of one sample should induce the knowledge of its neighbors. This is implicitly used in the greedy-like active learning and it is emphasized in , where they proposed to weight the selection function value of a sample with an estimation of its probability density function to increase learning speed. However most ambiguous points are likely
1251	1253	the expensive human intervention to archive and annotate contents. Many researchers are currently investigating methods to automatically analyze, organize, index and retrieve video information . This effort is further stressed by the emerging Mpeg-7 standard that provides a rich and common description tool of multimedia contents. It is also encouraged by Video-TREC which aims at
1251	1255	digit. They can also be selected from a unlabeled set. This approach, called selective sampling, is the most common, and many researchers proposed selection methods, such as query by committee  or uncertainty sampling . Applications of active learning techniques are now emerging in the field of multimedia databasesannotation . In the following section we propose a new uncertainty
1251	1256	to find optimal elements since we do expect teachers to have a rest between rounds. Figure (2) presents results on a real database. It is composed of 40,000 annotated shots from news sequences . Shots are described by HS color histograms and Gabor’s energies of their key frame. We use Latent Semantic Analysis, as described in , to capture local information, remove noise and emphasize
1251	1257	the expensive human intervention to archive and annotate contents. Many researchers are currently investigating methods to automatically analyze, organize, index and retrieve video information . This effort is further stressed by the emerging Mpeg-7 standard that provides a rich and common description tool of multimedia contents. It is also encouraged by Video-TREC which aims at
1251	1258	approaches were proposed to this problem, semi-supervised and active learning. On one hand, a semi-supervised learner combines a small set of labeled samples with a large set of unlabeled samples . The latter set does not provide any direct information but their distribution can be used to boost the performance of the classifier. On the other hand, an active learner starts from a very small
1251	1259	proposed selection methods, such as query by committee  or uncertainty sampling . Applications of active learning techniques are now emerging in the field of multimedia databasesannotation . In the following section we propose a new uncertainty sampling strategy, called partition sampling, that allows to select multiple samples as opposed to classical approaches. 3. PROPOSED APPROACH
1251	1259	fL(x),y)] ? EY |X (4) P To learn the hypothesis f L +(.) of equation (2) for each possible query sample S is now the major problem to compute the estimated error reduction. In , the authors first assume that all losses for any x ? P\L have an equal influence. Hence, the sum over P is reduced over S. Then they can neglect C( f L +(x),y) over C( fL(x),y) since the new
11121203	1293	over long time-scales. Understanding the nature of such fluctuations has important consequences and can reveal the underlying laws of ecosystem dynamics. The biosphere is a complex adaptive system (Levin 1998). As such, it displays some universal features common to other far from equilibrium systems. In this context, beyond the plethora of fine-scale details present in any ecosystem, simple laws can
11121203	1328	Self-organized critical systems require a number of strong constraints in order to operate (Jensen 1998), and it is not clear whether such constraints can operate in a broad range of situations (Solé et al. 1999; Solé & Goodwin 2001). However, partially inspired by ideas of this kind, we propose a mechanism of ecosystem organization in which the observed patterns result from the spontaneous driving of
1337	1339	and utterance types. Recent acquisition research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable
1337	1343	was structured, or the network’s memory was initially limited, and developed gradually. 4 An SRN’s performance with such recursive structures has also been shown to fit well to the human data (Christiansen and Chater, 1999).sSuch networks have also been shown to go beyond the data in interesting ways. Elman (1998) and Morris et al. (2000) showed that SRNs induce abstract grammatical categories which allow both
1337	1348	above sort of evidence, the stochastic information in data uncontroversially available to children is sufficient to allow for learning. Building on recent work with simple recurrent networks (SRNs; Elman 1990), we show that the correct generalization emerges from the statistical structure of the data. Figure 1 shows the general structure of an SRN. The network comprises a three-layer feed-forward
1337	1349	speech that appear to be important for language acquisition, and particularly for the issue at hand. Complexity increases over time which has been shown to be a determinant of learnability (e.g. Elman, 1991, 1993) and there are also arguably meaningful shifts in the distribution of types, and the limitations on forms. The increasing complexity of the child’s input is especially relevant to the problem
1337	1350	i.e., ‘is the boy who is smoking is . . . ’; in fact an auxiliary is predicted as a possible 8 The SRN responsible for these results incorporates a variant of the developmental mechanism from (Elman, 1993). That version reset the context layer at increasing intervals; the version used here is similar, but does not reset the context units unless the network’s prediction error is greater than a set
1337	1358	research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable from positive examples alone, while their
8918655	1440	quadrant-based tree structure, the P-tree . 2.1 Data Representation The vector space model is a simple ubiquitous approach used to represent text documents in machine understandable formats   . It is characterized by being relatively computationally efficient and by having conceptual simplicity; however, it suffers from loss of information related to the original document
8918655	1443	value given to each document, d, to express the referencing importance of d to other documents in the document set. It is calculated as the total number of document references (IDREFs and Xlinks ) to d. All the tags of a document will have the same reference weight since references are usually document based and not tag based. The depth weight reflects the hierarchical structure of an XML
8918655	1445	based on weight voting. We will also present an efficient result sorting method using EIN-rings and P-trees. 4.1 Ranking by Votes Traditional methods for ranking queries results, such as Google  , are based on the use of fixed ranking formulas, which integrate different dimension weights into the ranking score and evaluate documents accordingly. This scheme is straightforward and natural;
8918657	1448	observable domains. They are also first to find an action model at the same time that they determine the agent’s knowledge about the state of the world. They draw on intuitions and results of  for known (nondeterministic) action models. If we assume that our transition model is fully known, then our results reduce to those of  for deterministic actions.sA wide range of virtual domains
8918657	1448	one of the tuples in ? sw-on causes E, sw-on keeps E ? ? sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state)  when the transition model is fully specified. Theorem 3 Let ? = ? × {R}, where ? ? S and R ? S × A × S, and let ?ai, oi? i?t be a sequence of actions and observations. If FilterR(?)
8918657	1448	section illustrates how the explicit representation of transition belief states may be doubly exponential in the number of domain features and the number 1 Filtering semantics as defined in . ??? of actions. In this section we follow the intuition that propositional logic can serve to represents ? more compactly. From here forth we assume that our actions are deterministic. In the
8918657	1449	in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables.  presents an approach that is based on inductive logic programming. Most recently,  showed how to learn stochastic actions with no conditional effects (i.e., the same stochastic change occurs at
8918657	1452	our algorithms in large domains, including over 1000 features (see  for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains.  learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to
8918657	1454	hill-climbing algorithm which is only guaranteed to reach a local optima, and there is no time guarantee for convergence on this local optima. Reinforcement learning in partially observable domains  can be Eyal Amir Computer Science Department University of Illinois, Urbana-Champaign Urbana, IL 61801, USA eyal@cs.uiuc.edu solved (approximately) by interleaving learning the POMDP with solving
8918657	1455	transition models in partially observable domains is hard. In stochastic domains, learning transition models is central to learning Hidden Markov Models (HMMs)  and to reinforcement learning , both of which afford only solutions that are not guaranteed to approximate the optimal. In HMMs the transition model is learned using the Baum-Welch algorithm, which is a special case of EM. It is
8918657	1456	problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows the underlying transition model . Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states , but
8918657	1457	one of the tuples in ? sw-on causes E, sw-on keeps E ? ? sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state)  when the transition model is fully specified. Theorem 3 Let ? = ? × {R}, where ? ? S and R ? S × A × S, and let ?ai, oi? i?t be a sequence of actions and observations. If FilterR(?)
8918657	1458	. Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states , but reinforcement learning has no similar solution known to us. In this paper we present a formal, exact, many times tractable, solution to the problem of simultaneously learning and filtering
8918657	1459	solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) . It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one
8918657	1460	solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) . It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one
8918657	1461	problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows the underlying transition model . Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states , but
8918657	1462	effects and preconditions focused on fully observable domains.  learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables.  presents an approach that is based on
8918657	1463	(in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables.  presents an approach that is based on inductive logic programming. Most recently,  showed how to learn stochastic actions with no conditional effects (i.e., the same stochastic change occurs at every state in which the action is executable). The common theme among these
8918657	1069	to flipping the switch. Learning transition models in partially observable domains is hard. In stochastic domains, learning transition models is central to learning Hidden Markov Models (HMMs)  and to reinforcement learning , both of which afford only solutions that are not guaranteed to approximate the optimal. In HMMs the transition model is learned using the Baum-Welch algorithm,
8918657	1464	change occurs at every state in which the action is executable). The common theme among these approaches is their assumption that the state of the world is fully observed at any point in time.  is the only work that considers partial observability, and it does so by assuming that the world is fully observable, giving approximate computation in relevant domains. 2 Filtering Transition
8918657	1465	our algorithms in large domains, including over 1000 features (see  for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains.  learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators.  uses a general-purpose classification system (in their case, MSDD) to
47718	1469	N can be trained with examples using, e.g., Backpropagation, and using P as background knowledge (Pazzani & Kibler, 1992). The knowledge acquired by training can then be extracted (d’Avila Garcez et al., 2001), closing the learning cycle (as in (Towell & Shavlik, 1994)). For each agent (child), a C-ILP network can be created. Each network can be seen as representing a (learnable) possible world
47718	1472	of knowledge acquisition through inductive learning. 1 Introduction Hybrid neural-symbolic systems concern the use of problem-specific symbolic knowledge within the neurocomputing paradigm (d’Avila Garcez et al., 2002a). Typically, translation algorithms from a symbolic to a connectionist representation and vice-versa are employed to provide either (i) a neural implementation of a logic, (ii) a logical
47718	1472	systems were not able to fully represent, reason and learn expressive languages other than propositional and fragments of first-order logic (Cloete & Zurada, 2000). However, in (d’Avila Garcez et al., 2002b; d’Avila Garcez et al., 2002c; d’Avila Garcez et al., 2003), a new approach to knowledge representation and reasoning in neural-symbolic systems based on neural networks ensembles has been
47718	1472	K1q1, the input neuron associated with K1¬p2, and the input neuron associated with K1¬p3 are all activated (true). The Connectionist Inductive Learning and Logic Programming (C-ILP) System (d’Avila Garcez et al., 2002a; d’Avila Garcez & Zaverucha, 1999) makes use of the above kind of translation. C-ILP is a massively parallel computational model based on an artificial neural network that integrates inductive
47718	1472	paper. By combining a number of simple C-ILP networks, we are able to model individual and common knowledge. Each network represents a possible world or an agent’s current set of beliefs (d’Avila Garcez et al., 2002b). If we allow a number of ensembles like the one of Figure 2 to be combined, we can represent the evolution in time of an agent’s set of beliefs. This is exactly what is required for a complete
47718	1472	of knowledge acquisition through inductive learning. 1 Introduction Hybrid neural-symbolic systems concern the use of problem-specific symbolic knowledge within the neurocomputing paradigm (d’Avila Garcez et al., 2002a). Typically, translation algorithms from a symbolic to a connectionist representation and vice-versa are employed to provide either (i) a neural implementation of a logic, (ii) a logical
47718	1472	systems were not able to fully represent, reason and learn expressive languages other than propositional and fragments of first-order logic (Cloete & Zurada, 2000). However, in (d’Avila Garcez et al., 2002b; d’Avila Garcez et al., 2002c; d’Avila Garcez et al., 2003), a new approach to knowledge representation and reasoning in neural-symbolic systems based on neural networks ensembles has been
47718	1472	K1q1, the input neuron associated with K1¬p2, and the input neuron associated with K1¬p3 are all activated (true). The Connectionist Inductive Learning and Logic Programming (C-ILP) System (d’Avila Garcez et al., 2002a; d’Avila Garcez & Zaverucha, 1999) makes use of the above kind of translation. C-ILP is a massively parallel computational model based on an artificial neural network that integrates inductive
47718	1472	paper. By combining a number of simple C-ILP networks, we are able to model individual and common knowledge. Each network represents a possible world or an agent’s current set of beliefs (d’Avila Garcez et al., 2002b). If we allow a number of ensembles like the one of Figure 2 to be combined, we can represent the evolution in time of an agent’s set of beliefs. This is exactly what is required for a complete
47718	1477	of a logic, (ii) a logical characterisation of a neural system, or (iii) a hybrid learning system that brings together features from connectionism and symbolic artificial intelligence (Holldobler, 1993). Until recently, neural-symbolic systems were not able to fully represent, reason and learn expressive languages other than propositional and fragments of first-order logic (Cloete & Zurada,
47718	1485	? 8 Note that ? is not required to precede every rule antecedent. In the network, neurons are labelled as ?K1L1 or K1L1 to differentiate the two concepts. 2s4 Conclusions In his seminal paper (Valiant, 1984), Valiant argues for the need of rich logic-based knowledge representation mechanisms within learning systems. In this paper, we have addressed such a need, yet complying with important principles
3893559	1487	many SRT applications. AVERAGE The task is willing to except a reservation in which constraints are not met all the time, but are met on average. Most SRT systems provide these kinds of guarantees . STRICT The task may only run if it is guaranteed to meet all timeliness constraints. Hard real-time systems are designed to provide this type of guarantee. Like the above, in many cases we expect
3893559	1489	many SRT applications. AVERAGE The task is willing to except a reservation in which constraints are not met all the time, but are met on average. Most SRT systems provide these kinds of guarantees . STRICT The task may only run if it is guaranteed to meet all timeliness constraints. Hard real-time systems are designed to provide this type of guarantee. Like the above, in many cases we expect
3893559	1490	period workloads, such as media in which the processing depends on the content of the data, the average case execution time of jobs may be known (also known as a variable processing time class ). Knowing a model of the variability, it is possible to provide some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily
3893559	1491	average case execution time of jobs may be known (also known as a variable processing time class ). Knowing a model of the variability, it is possible to provide some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent
3893559	1491	many SRT applications. AVERAGE The task is willing to except a reservation in which constraints are not met all the time, but are met on average. Most SRT systems provide these kinds of guarantees . STRICT The task may only run if it is guaranteed to meet all timeliness constraints. Hard real-time systems are designed to provide this type of guarantee. Like the above, in many cases we expect
3893559	1492	(also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines . {DEADLINE} An imprecise constraint specification states a mandatory deadline constraint for each job, with optional deadlines associated with subprocesses that may be met in best effort fashion
3893559	1493	variability, it is possible to provide some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines .
3893559	1495	some probabilistic guarantees  RATE The task must make a fixed amount of progress during any fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines . {DEADLINE} An imprecise constraint
3893559	1496	period and worst-case execution of any job of a task is known, task constraints may be used for hard guarantees. This constraint type is used to check for schedulability in the periodic task model . DEADLINE+ACE For variable period workloads, such as media in which the processing depends on the content of the data, the average case execution time of jobs may be known (also known as a variable
3893559	1497	{DEADLINE} An imprecise constraint specification states a mandatory deadline constraint for each job, with optional deadlines associated with subprocesses that may be met in best effort fashion . any+u() Any of the above constraints may be paired with a utility function that associates a value with meeting the constraint. Utilities functions may consist of a fixed value or be represented
3893559	1500	fixed-length, but arbitrarily positioned, time interval  (also called the continuous class ). Alternately, an equivalent specification is expressed using an interval and a percentage of CPU . FIRM The task must meet m out of k deadlines . {DEADLINE} An imprecise constraint specification states a mandatory deadline constraint for each job, with optional deadlines associated with
3893559	1501	be specified as of set of multiple constraints—these applications adapt to available resources by adjusting their constraints. These systems adapt allowing the system allowing dynamic QoS control .sTable 3. Types of Processing Guarantees Type Description NONE The task is able to run without any guarantee. This is provided by best-effort systems, and many SRT real-time applications are
3893559	1502	algorithms, i.e. for treating different classes equivalently for scheduling purposes (often independently of the scheduling algorithm). The following theorem is adapted from the HLS framework . Theorem 1 Any schedule that guarantees a rate x over interval y to a task of type a-RATE-b will also guarantee that a task of type a-DEADLINE+WCE-b with a worst-case execution time x and deadline
8918663	1512	frames (translation, zoom and rotation). The affine model is needed since some frames are rotated. The estimation itself is performed 28sthrough M-estimators. Part of the algorithm is described in . We used then the restoration process described in chapter 5 to compensate the frames. The estimation process takes around 3 seconds per frame on a PC and about 10 hours for the whole movie. The
11498811	1536	drastically different kinematic descriptions. The utility of viewing character motion as one or more discrete time series of joint angles and joint positions has been presented in the literature (Gleicher and Litwinowicz 1998). This view naturally suggests the application of signal processing techniques to modify motion signals so as to preserve the desired qualities of the original while modifying it to meet the goals
11498811	1539	attributes when changing from high to low resolution, and disaggregation when moving in the opposite direction. Aggregation and disaggregation have been shown to suffer from several problems (Reynolds et al. 1997), including chain disaggregation, transitional latency, and difficulties in mapping between different levels of resolution. Reynolds et al. (1997) proposed, as an alternative, the concept of
3685242	1550	images. Feature based methods  use features or tokens to get depth information and motion. Flow based methods  assume that optical flow is available. Direct methods  do not require intermediate steps such as feature extraction or flow computation and work directly with spatial and temporal image gradients. Most of the previous approaches assumes locally
3685242	1550	may require a nonsmooth local depth refinement. We show how to use the epipolar constraint and model the parallax field appropriately to deal with such cases. Parallax based approaches proposed in  assume a dominant planar region to be present in the image or the presence of a small planar region for motion estimation . Our approach does not require any such assumptions. Also, many of
3685242	1551	images. Feature based methods  use features or tokens to get depth information and motion. Flow based methods  assume that optical flow is available. Direct methods  do not require intermediate steps such as feature extraction or flow computation and work directly with spatial and temporal image gradients. Most of the previous approaches assumes locally
3685242	1551	or is a parametric function  that imposes smooth flow. The smoothness constraint on depths can be applied by assuming a smooth depth model (constant or planar) over the neighborhood (as in ) and directly using (4) and (3) to estimate Z. However, these assumptions are violated at depth boundaries. Also, the effect of noise in available depth map estimate (from a range finder or from
3685242	1552	images. Feature based methods  use features or tokens to get depth information and motion. Flow based methods  assume that optical flow is available. Direct methods  do not require intermediate steps such as feature extraction or flow computation and work directly with spatial and temporal image gradients. Most of the previous approaches assumes locally smooth
3685242	1552	require a nonsmooth local depth refinement. We show how to use the epipolar constraint and model the parallax field appropriately to deal with such cases. Parallax based approaches proposed in  assume a dominant planar region to be present in the image or the presence of a small planar region for motion estimation . Our approach does not require any such assumptions. Also, many of the
3685242	1555	with ?I. ?1 > 0, ?2 = 0 4. Intensity variation in all directions. No sufficient structure. ?1 > 0, ?2 > 0 Confidence measures based on eigen-values and/or condition number have been proposed in . We use C = ( ?1??2 ?1+?2 )2 as the confidence measure for depth estimation. Homogeneous regions (case 1) can be identified by using a threshold on the sum of eigen-values. Regions where local edge
8918674	1673	our own experience. On the other hand, other limits have also been reported in the literature—for example, Zimny et al. regard an SUV of 6.4 as dealing with a malignancy but accept a range of ?3.6 (32). In a study population of 106 patients, the mean SUV in the case of inflammatory changes is given as 3.4 ? 1.7. Berberat et al. reported a mean SUV of 3.09 for cancer and 0.87 for inflammation
8918674	1673	could explain the excellent results. For their study population of 106 patients, Zimny et al. reported a sensitivity of 89.0% and a specificity of 53.0%, but they evaluated their data only visually (32). The large number of false-negative results was explained as being due to patients with an elevated blood glucose level. After exclusion of this group of patients from the study population, PET
8918674	1673	no general agreement on the benefit of quantitative assessment by means of the SUV; for instance, Zimny et al. dispute the benefitofthe SUV, whereas other authors consider the SUV a valid parameter (32,35). Zimny et al. reported data on PET detection in lymph node and distant metastases, with a correct result of 40.0% in lymph node metastases and 52.0% in distant metastases. Bares et al. compared PET
1054991	1623	single stream when it comes to QoS guarantees, which eliminates the need for per-flow based state and processing. Such an approach is reflected in recent proposals such as the IETF Diff-Serv model . By coarsening the different levels of QoS that the network offers into a small number of service classes, the Diff-Serv model is expected to scale well. However, this gain in scalability through
1054991	1626	selected because it has the lowest mean rate among all the traces in the video library. Several important statistics of the three video sources are given in Table I. We also use voice traces from  in our investigation. In particular, the voice source 1 that we choose corresponds to Fig. 4b in  and was generated using the NeVoT Silence Detector(SD) . We refer to  for a complete
1054991	1629	I. We also use voice traces from  in our investigation. In particular, the voice source 1 that we choose corresponds to Fig. 4b in  and was generated using the NeVoT Silence Detector(SD) . We refer to  for a complete description of the codec configuration and trace collection procedure. In the following table important statistics of the voice source are given. Trace Name Mean
1054991	1631	without this being noticed at the class level. This work was supported in part through NSF grant ITR00-85930.sTECHNICAL REPORT UNIVERSITY OF PENNSYLVANIA 2 This issue was explored in an early work , which focused on the loss probability of an individual user as the relevant metric.  developed a number of explicit models to evaluate the individual loss probabilities and the overall loss
1054991	1631	cases where significant performance deviations can exist, and that user traffic parameters can have a major influence on whether this is the case or not. In this paper, we go beyond the work of  by developing a simple methodology capable of evaluating loss performance deviations in realistic settings. Specifically, we concentrate on the ability to predict performance deviations when
1054991	1631	different configurations that we consider and the performance measures that we use to evaluate loss performance deviation. For completeness, we also summarize the analytical models, developed in , that we use to evaluate the loss probability of an individual user. The voice and video traffic sources and their characteristics are thensTECHNICAL REPORT UNIVERSITY OF PENNSYLVANIA 3 described
1054991	1631	only two sources are aggregated and the other for a system where many sources are aggregated, are described. We only present the main results, while all the relating derivations can be found in . 1) Analytical model when aggregating two sources: The individual loss probability in a two source model can be evaluated simply by specializing an N-source system to the case N = 2. In an N-source
1054991	1631	loss probability. However, we believe that using merely first order statistics to predict loss deviation is a reasonable approach. This assumption is based in part on the experience obtained from , where we saw that deviations were to a large extent a function of the first order statistics of a source, e.g., the peak rate, the utilization, and the average burst duration, among which the
10166220	1633	optimization approach to estimate the overflow probability in three stages by approximating an optimal tilting parameter. The balanced likelihood ratio approach to importance sampling (see Alexopoulos and Shultes 1998, 2001) was developed for analyzing system performance in fault-tolerant repairable systems. This approach has been used to derive importance sampling estimators for limiting system unavailability
10166220	1635	particularly the time until one of these events occurs. Importance sampling is gaining popularity as an efficient method for analyzing rare events in queueing and reliability systems (see Asmussen and Rubinstein 1995, Heidelberger 1995). The application of importance sampling involves simulating the model using an auxiliary distribution designed to make the system experience rare events of Ramya Dhamodaran
8918718	1670	other text in the document conveys some of the images’ semantics. So, it is natural for Web image search systems to use text as part of the process. This approach is similar to that of Brown et al. , who used close-captioned text and speech analysis to index video data. 3 Using HTML Metadata to Find Web Images We studied the effectiveness of HTML metadata (textual content and structure) for
8918718	1671	other text in the document conveys some of the images’ semantics. So, it is natural for Web image search systems to use text as part of the process. This approach is similar to that of Brown et al. , who used close-captioned text and speech analysis to index video data. 3 Using HTML Metadata to Find Web Images We studied the effectiveness of HTML metadata (textual content and structure) for
8918718	1673	image file names. The individual images are assigned to a topic by a human judge. Users can browse or search the topics in the database and can also search usingsimage features. Research on WebSeer  investigated how to classify images into categories such as photographs, portraits and computer-generated drawings. To do this, WebSeer supplemented information from image content analysis with
1677	1683	the three-dimensional nature of anisotropy in tissues. Numerous works have already addressed the problem of the estimation and regularization of these tensor fields. References can be found in , , , , . Motivated by the potentially dramatic improvements that knowledge of anatomical connectivity would bring into the understanding of functional coupling between cortical regions ,
1677	1685	nature of anisotropy in tissues. Numerous works have already addressed the problem of the estimation and regularization of these tensor fields. References can be found in , , , , . Motivated by the potentially dramatic improvements that knowledge of anatomical connectivity would bring into the understanding of functional coupling between cortical regions , the study
1677	1685	our capacity to resolve multiple fibers orientations since local tractography becomes unstable when crossing artificially isotropic regions characterized by a planar or spherical diffusion profile . On the other side, new diffusion imaging methods have been recently introduced in an attempt to better describe the complexity of water motion but at the cost of increased acquisition times. This
1677	1689	planning or tumor growth quantification, various methods have been proposed to tackle the issue of cerebral connectivity mapping. Local approaches based on line propagation techniques ,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , ,
1677	1691	techniques ,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this
1677	1692	techniques ,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation,
1677	1693	,  provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation, they
1677	1696	provide fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation, they enable us to generate
1677	1697	fast algorithms and have been augmented to incorporate some natural constraints such as regularity, stochastic behavior and even local non-Gaussianity (, , , , , , , ). All these efforts aim to overcome the intrinsic ambiguity of the diffusion tensor related to white matter partial volume effects. Bearing in mind this limitation, they enable us to generate
1677	1698	recently introduced in an attempt to better describe the complexity of water motion but at the cost of increased acquisition times. This is a case of high angular diffusion weighted imaging ,  where the variance of the signal could give important information on the multimodal aspect of diffusion. Diffusion Spectrum Imaging ,  provides, at each voxel, an estimation of the
1677	1702	tissues. In favor of these promising modalities, parallel MRI  will reduce the acquisition time in a near future and thus permit high resolution imaging. More global algorithms such as  have been proposed to better handle the situations of false planar or spherical tensors (with fibers crossings) and to propose some sort of likelihood of connection. In , the authors make use
1677	1704	the situations of false planar or spherical tensors (with fibers crossings) and to propose some sort of likelihood of connection. In , the authors make use of the major eigenvector field and in  the full diffusion tensor provides the metric of a Riemannian manifold but this was not exploited to propose intrinsic schemes.sInferring White Matter Geometry from DT-MRI 3 We derive a novel
1677	1710	= {gij} the metric tensor, this writes ? v T Gv. Then, by setting H(x, p) = |p| ? 1, we will work on the following theorem (for details on viscosity solutions on a Riemannian manifold, we refer to ) Theorem 1. The distance function ? is the unique viscosity solution of the Hamilton-Jacobi problem ? |grad?| = 1 in M \ K (3) ?(x) = 0 when x ? K in the class of bounded uniformly continuous
1677	1713	the minimum arrival time problem. This will enable us to solve equation 3 as a dynamic problem and thus to take advantage of the great flexibility of Level Set methods. On the basis of , ,  and , we reformulate equation 3 by considering ? as the zero level set of a function ? and requiring that the evolution of ? generates ? so that ?(x, t) = 0 ? t = ?(x) (4) Osher () showed
1677	1714	arrival time problem. This will enable us to solve equation 3 as a dynamic problem and thus to take advantage of the great flexibility of Level Set methods. On the basis of , ,  and , we reformulate equation 3 by considering ? as the zero level set of a function ? and requiring that the evolution of ? generates ? so that ?(x, t) = 0 ? t = ?(x) (4) Osher () showed by using
1677	1717	has also been done by using WENO schemes in order to increase the accuracy of the method. They consist of a convex combination of nth (we take n = 5) order polynomial approximation of derivatives . A classical narrow band implementation is used to speed up the computations. 4.3 Numerical Scheme for the Geodesics Estimation We finally derive an intrinsic method for geodesics computation in
1718	1719	absorption (see ). Role absorption is important because in ontology derived KBs range and domain constraints will often have been transformed into GCIs. This is because tools such as OilEd  and Protégé  are designed to work with range of DL reasoners, some of which (e.g., FaCT) do not support range and domain axioms. Moreover, these forms of GCI are not, in general, amenable to
1718	1721	algorithm by analysing the perfomance of our FaCT++ implementation when classifying terminologies derived from realistic ontologies. 1 Introduction Many modern ontology languages (e.g., OIL , DAML+OIL  and OWL ) are based on expressive description logics, and in particular on the SHIQ family of description logics . These ontology languages typically support domain and range
1718	1722	for the Racer system to be able to classify large KBs containing many range and domain constraints, it is necessary to give a special treatment to the GCIs introduced by range and domain axioms . The approach used by Racer is to extend the lazy unfolding optimisation so that concepts equivalent to those that would besintroduced by the GCIs are introduced only as necessary. In the approach
1718	1724	is that such GCIs are not amenable to absorption, an optimisation technique that tries to rewrite GCIs so that they can be efficiently dealt with using the lazy unfolding optimisation . Absorption is one of the crucial optimisations that enable state of the art DL reasoners such as FaCT , Racer  and Pellet  to deal effectively with large knowledge bases (KBs), and these
1718	1724	on the same tableaux algorithms as the original FaCT, but has a different architecture and is written in C++ instead of Lisp. Absorption Absorption in FaCT++ uses the same basic approach as FaCT . Given a TBox T , the absorption algorithm constructs a triple of TBoxes ?Tdef, Tsub, Tg? such that: • Tdef is a set of axioms of the form A ? C (equivalent to a pair of axioms {A ? C, C ? A} ? T
1718	1725	GCIs so that they can be efficiently dealt with using the lazy unfolding optimisation . Absorption is one of the crucial optimisations that enable state of the art DL reasoners such as FaCT , Racer  and Pellet  to deal effectively with large knowledge bases (KBs), and these reasoners perform much less well with KBs containing significant numbers of unabsorbable GCIs.
1718	1725	and Empirical Evaluation We have implemented the extended tableaux algorithm and role absorption optimisation in the FaCT++ DL reasoner. FaCT++ is a next generation of the well-known FaCT reasoner , being developed as part of the EU WonderWeb project (see http://wonderweb. semanticweb.org/); it is based on the same tableaux algorithms as the original FaCT, but has a different architecture and
1718	1726	by analysing the perfomance of our FaCT++ implementation when classifying terminologies derived from realistic ontologies. 1 Introduction Many modern ontology languages (e.g., OIL , DAML+OIL  and OWL ) are based on expressive description logics, and in particular on the SHIQ family of description logics . These ontology languages typically support domain and range constraints on
1718	1727	1 Introduction Many modern ontology languages (e.g., OIL , DAML+OIL  and OWL ) are based on expressive description logics, and in particular on the SHIQ family of description logics . These ontology languages typically support domain and range constraints on roles, i.e., axioms asserting that if an individual x is related to an individual y by a role R, then x must be an
1718	1727	of the SHIQ logic, including the semantics of role boxes extended with range and domain axioms. Most details of the logic and the tableaux algorithm are little changed from those presented in . We will, therefore, focus mainly on the parts that have been added in order to deal with range and domain axioms, and refer the reader to  for complete information on the remainder. The
1718	1727	satisfying R and T . Theorem 1 Satisfiability and subsumption of SHIQ-concepts w.r.t. terminologies and role boxes is polynomially reducible to (un)satisfiability of SHIQ-concepts w.r.t. role boxes . 3 Tableaux Reasoning with Range and Domain Here we present an algorithm for deciding the satisfiability of a SHIQ-concept C w.r.t. a role box R; it is an extension of the SHIQ tableaux algorithm
1718	1727	when a clash occurs, and answers “D is satisfiable” iff the completion rules can be applied in such a way that they yield a complete and clash-free completion tree. Note that the only change w.r.t.  is addition of the domain and range-rules that add concepts to node labels as required by domain and range axioms. Lemma 2 Let D be an SHIQ-concept. 1. The tableaux algorithm terminates when
1718	1728	sense to transform GCIs into range and domain axioms. We call this new form of absorption role absorption in contrast to the usual form of absorption we will refer to as concept absorption (see ). Role absorption is important because in ontology derived KBs range and domain constraints will often have been transformed into GCIs. This is because tools such as OilEd  and Protégé  are
1718	1728	on the same tableaux algorithms as the original FaCT, but has a different architecture and is written in C++ instead of Lisp. Absorption Absorption in FaCT++ uses the same basic approach as FaCT . Given a TBox T , the absorption algorithm constructs a triple of TBoxes ?Tdef, Tsub, Tg? such that: • Tdef is a set of axioms of the form A ? C (equivalent to a pair of axioms {A ? C, C ? A} ? T
1718	1729	(see ). Role absorption is important because in ontology derived KBs range and domain constraints will often have been transformed into GCIs. This is because tools such as OilEd  and Protég??  are designed to work with range of DL reasoners, some of which (e.g., FaCT) do not support range and domain axioms. Moreover, these forms of GCI are not, in general, amenable to standard concept
8918719	1739	of ™ onto ‚@ƒ A. However, ? ? and ? need not be in the same quantization cell, and, therefore, ? ? need not be a consistent estimate of ? , as illustrated in this figure. studied in  and  for the case of periodic band-limited signals. Theoretically, these algorithms can be extended to band-limited signals in ; however, these algorithms would not be practical since they require
8299611	1747	rest of the paper describes the BT-tree in detail. 1.1 Background and Previous Work Branched-and-temporal indexing is a relatively unexplored area. However, many access methods (for example, , , ,  and ) have been proposed for temporal data. A survey and comparison of these access methods can be found in . These methods have e ectively solved the problem of providing access
8299611	1748	pages. The paper  does not consider pagination and, although the structure is \branched,&quot; only a current version can be split into branches. Old versions can not be 453 modi ed. Driscoll et al.  develop techniques for making linked data structures (e.g. binary search trees) fully persistent (allversions can be read and updated). Perhaps closest to our work is that of Lanka and Mays ,
8299611	1748	BT-tree structure problematic. We need to be able to decide whether or not one version is a descendent or ancestor of another. 2.1 Ancestor Determination Ancestor determination methods used in  and  for branched data use O(n) space. Their methods are not suitable for branched-and-temporal case where the number of timestamps is large (hence the corresponding total number of the versions
8299611	1753	describes the BT-tree in detail. 1.1 Background and Previous Work Branched-and-temporal indexing is a relatively unexplored area. However, many access methods (for example, , , ,  and ) have been proposed for temporal data. A survey and comparison of these access methods can be found in . These methods have e ectively solved the problem of providing access to versioned record
8299611	1755	that require the support of time-evolving data. Temporal database systems model explicitly the temporal behavior of data, thus providing the ability to store and query temporal data e - ciently . Conventional temporal databases assume a single line of time evolution. As an example, consider an architect's design of a new house (say Joe's house). The house design starts from scratch and
8918722	1804	code. 2.2 OCL description In  we focused on the definition of a special kind of transformations: UML refactorings, i.e. the adaptation to UML of Opdyke’s behavior-preserving transformations . We specified each transformation using pre- and post-conditions, expressed as OCL constraints at the metamodel level. This is illustrated in the following example. Example: attribute privatization
9054621	1812	future. 2 FACTORS AFFECTING SIMULATION COURSES THOUGHTS AND MUSINGS ON SIMULATION EDUCATION The topical composition of simulation courses and the allocation of time to each subject is addressed in (Nance 2000). The results of two surveys (Beckwith 1974, 1976a, 1976b) and (Nance and Overstreet 1976) are compared with a conRichard E. Nance Osman Balci Systems Research Center and Department of Computer
1817	1818	is therefore used as the continuous output of the learning-based evaluator. The Torch3 machine learning library implementation of SVMs for classification is used here with Gaussian kernels (Collobert et al. 2002). 3.2 Features In order to use the general purpose SVM for classifying translations, linguistic objects must be rerepresented in numerical form with a vector of feature values. The features used
1817	1822	final report that the BLEU metric, used for training and evaluation of the team’s MT system, seems insensitive to syntactic changes that should be noticeable to human judges at the sentence level (Och et al. 2003). Here, a metric designed to evaluate single-sentence machine translations based on machineslearning is described and tested. A novel approach leads to a flexible classification-based metric and
1817	1822	thereby improving the metric’s sensitivity to particular aspects of machine translations. For example, it has been observed that BLEU is not particularly sensitive to syntactic improvements (Och et al. 2003); a learning-based metric, on the other hand, could be modified specifically to take syntax into account, providing a much finer-grained error analysis than is currently possible. 6
1817	1823	word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics BLEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a set of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT
1817	1823	the BLEU metric, for example, has been shown to correlate highly with the judgments of bilingual human evaluators (a correlation coefficient of 0.96) when averaged over a 500-sentence corpus (Papineni et al. 2001). However, though useful for system discrimination, metrics correlating with human evaluations only over long texts (which tend to average out the “noise” of evaluation) are relatively useless for
1817	1825	of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT results. The F-Measure has been proposed as a more comprehensible alternative for MT evaluation (Turian et al. 2003), and can be defined as a simple composite of unigram precision and recall. These metrics have demonstrated some success; the BLEU metric, for example, has been shown to correlate highly with the
1817	1825	human evaluation of MT is itself inconsistent and not very reliable, automatic MT evaluation measures are even less reliable and are still very far from being able to replace human judgment.” (Turian et al. 2003) The Syntax for Statistical Machine Translation team from the 2003 Workshop further notes in its final report that the BLEU metric, used for training and evaluation of the team’s MT system, seems
1906	1908	exploration of scientific data, Reeb graphs are used to efficiently compute level sets . Reeb graphs can also function as a user-interface tool aiding the selection of meaningful level sets . A more extensive discussion of Reeb graphs and their variations in geometric modeling and visualization applications can be found in . The first algorithm for constructing the Reeb graph of a
1906	1909	running time at the cost of accuracy has been suggested in . An O(????????? ) time algorithm for loop-free Reeb graphs over manifolds of arbitrary but constant dimension has been described in . For the case of 3-manifolds, this algorithm has been extended to include information about the genus of the level surfaces in . In this paper, we focus on loops in Reeb graphs and study when
1906	1911	????? ? ? ????? ??? ????? ??? ? Figure 9 illustrates only the simple cases. Non-simple cases reduce to simple ones. For example, a ? -fold saddle can be split into ? simple saddles, as described in . Similarly, a boundary minimum/maximum with more complicated lower link than shown in Figure 9 can be split into simple (interior) saddles and a (simple) boundary minimum/maximum. Note that we just
1906	1915	graph have lead to data-base search methods for topologically similar geometric models . In the interactive exploration of scientific data, Reeb graphs are used to efficiently compute level sets . Reeb graphs can also function as a user-interface tool aiding the selection of meaningful level sets . A more extensive discussion of Reeb graphs and their variations in geometric modeling and
1258333	1926	and hybrid caching architectures , . There are three major contributions of this paper. First, unlike the previous analytic work on web caching which is based on statistic analysis, e.g., , , this paper proposes a stochastic model, which allows us to characterize the caching processes for individual documents in a two-level hierarchical caching system. In this model, the LRU
1258333	1929	in the context of hierarchical caching algorithm design. Some of the results of this study are also applicable to other caching architectures, such as distributed and hybrid caching architectures , . There are three major contributions of this paper. First, unlike the previous analytic work on web caching which is based on statistic analysis, e.g., , , this paper proposes a
1258333	1930	algorithm design when caches are arranged in a hierarchical structure. Most of the research papers on cache replacement algorithm design, to date, have focused on a single cache, e.g., , , , , . However, when caches are arranged in a hierarchical structure, running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good
1258333	1931	algorithm design when caches are arranged in a hierarchical structure. Most of the research papers on cache replacement algorithm design, to date, have focused on a single cache, e.g., , , , , . However, when caches are arranged in a hierarchical structure, running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good performance.
1258333	175	design when caches are arranged in a hierarchical structure. Most of the research papers on cache replacement algorithm design, to date, have focused on a single cache, e.g., , , , , . However, when caches are arranged in a hierarchical structure, running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good performance.
1258333	175	this assumption, cache sizes are measured in the unit of document size. Since Zipf-like distribution has been widely tested against the distributions collected from the real traces, e.g., , , , as (1) Fig. 1. Two-level hierarchical web caching structure. well as our own tests in Section IV, we model by Zipf-like distributions for and where is the normalization factor, is the popularity
1258333	1932	running a cache replacement algorithm which is optimized for an isolated cache may not lead to overall good performance. Although results on the optimal hierarchical caching exists, e.g., , they are obtained based on the assumption that global caching information is known to every cache in the cache hierarchy, which is generally unavailable in practice. Moreover, most of the research
1258333	1933	the context of hierarchical caching algorithm design. Some of the results of this study are also applicable to other caching architectures, such as distributed and hybrid caching architectures , . There are three major contributions of this paper. First, unlike the previous analytic work on web caching which is based on statistic analysis, e.g., , , this paper proposes a stochastic
1938	1939	trace, an advantage that is significant in systems that are to be operated in a number of different conditions. A. Experimental setup IV. EXPERIMENTAL RESULTS We used the SimpleScalar toolset  to evaluate the efficiency of the proposed approaches in a set of traces obtained from the execution of a number of programs in the SPEC95 benchmarks. The model used by the simulator is a modified
1938	1941	that it will be used in a large number of actual designs. Techniques that aim at reducing the power dissipated in buses by dynamically reordering and complementing the words to be transmitted  have also been proposed but their actual implementation in hardware is complex and the techniques are applicable only in very special circumstances. III. LOW OVERHEAD BUS ENCODINGS With the
1938	1942	to the problem. One approach is based on the application of transition coding, a method that has already been proposed by other researchers in conjunction with a transformation on the data words . The proposal made here uses transition coding without applying any recoding of the transmitted data, a method that, intuitively, seems to have little advantage over the transmission of the plain
1938	1942	of the Hamming distance between successive bus words requires the use of a non-trivial amount of hardware, that will, in practice, make the approach hard to apply in wide buses. Another approach  is based on the idea of changing the way bus words are encoded as to reduce the probability of occurrence of a § in the bus line (¨?© ) to a value smaller than 0.5. This is obtained by using wider
1938	1942	¨¥????????? and nothing is gained by using transition coding. This can be overcome if the words on the bus are re-encoded, with the objective of reducing the probability of the presence of a § . We must note, however, that the above analysis is not applicable to real traffic on buses. On the first place, ones and zeros are usually not equally likely to appear. On second place, consecutive
3571599	1956	have been developped for experts opinions fusion, including weighted average (see for instance , ), Bayesian fusion (see for instance , ), majority vote (see for instance , , ), models coming from incertainty reasoning: fuzzy logic, possibility theory  (see for instance ), standard multivariate statistical analysis techniques such as correpondence analysis
3571599	1957	since more than three decades (for some review references, see , , ). It has recently been successfully applied to the problem of classifiers combination or fusion (see for instance ). Many different approaches have been developped for experts opinions fusion, including weighted average (see for instance , ), Bayesian fusion (see for instance , ), majority vote (see
3571599	1962	), models coming from incertainty reasoning: fuzzy logic, possibility theory  (see for instance ), standard multivariate statistical analysis techniques such as correpondence analysis , etc. One of these approaches is based on maximum entropy modeling (see , ). Maximum entropy is a versatile modeling technique allowing to easily integrate various constraints, such as
4329218	1983	was necessary to account for the partial volume effect (Figure 1. blue). Two different methods were used to smooth and estimate the curvature function. The first method uses Taubin’s smoothing , a Gaussian filtering without shrinkage, followed by the least-squares estimation. The second method uses quintic splines to estimate the first and second derivatives to compute the curvature: min
1987	1989	Finally, a conclusion is given in Section 6.s2 Background We assume familiarity with the basic concepts of constraint programming. For a thorough explanation of constraint programming, see . A constraint satisfaction problem (CSP) consists of a finite set of variables X = {x1, . . . , xn} with finite domains D = {D1, . . . , Dn} such that xi ? Di for all i, together with a finite set
1987	1990	topological sorts because of the special structure of the graph. That computation can also be made incremental in the same way as in . Recently, that same result was independently obtained in . We however go further by considering edit distance, for which insertions and deletions are allowed as well. For deletions we need to allow “wasting” a value without changing the current state. To
1987	1991	This technique allows the resolution of over-constrained problem within traditional CP solvers. Comparatively few efforts have been invested in developing soft versions of common global constraints . Global constraints are often key elements in successfully modeling real applications and being able to easily and effectively soften such constraints would yield a significant improvement in
1987	1991	C, we introduce a function that measures the violation, and has the following form: violationC : D1 × · · · × Dn ? N. This approach has been introduced in  and was developed further in . There may be several natural ways to evaluate the degree to which a global constraint is violated and these are not equivalent usually. A standard measure is the variable-based cost: Definition 3
1987	1991	to the soft gcc. The soft gcc also inherits from the cost-gcc the time complexity of achieving domain consistency, being O(n(m + n log n)) where m = ?n i=1 |Di| and n = |X|. Note that  also consider the variable-based cost measure for a different version of the soft gcc. Their version considers the parameters l and u to be variables too. Hence, the variable-based cost evaluation
1987	1995	problems, mostly by introducing soft constraints that are allowed to be (partially) violated. The most well-known framework is the Partial Constraint Satisfaction Problem framework (PCSP ), which includes the Max-CSP framework that tries to maximize the number of satisfied constraints. Since in this framework all constraints are either violated or satisfied, this objective is
1987	1996	This technique allows the resolution of over-constrained problem within traditional CP solvers. Comparatively few efforts have been invested in developing soft versions of common global constraints . Global constraints are often key elements in successfully modeling real applications and being able to easily and effectively soften such constraints would yield a significant improvement in
1987	1996	When l = 0 in soft gcc(X, l, u, z), the arc set Aunderflow is empty. In that case, Gval has a particular structure, i.e. the only costs appear on arcs from DX to t. As pointed out in  for the soft alldifferent constraint, constraints with this structure can be checked for consistency in O(nm) time, and domain consistency can be achieved in O(m) time. The result is obtained by
1987	1997	constraints. Since in this framework all constraints are either violated or satisfied, this objective is equivalent to minimizing the number of violations. It has been extended to the Weighted-CSP , associating a degree of violation (not just a boolean value) to each constraint and minimizing the sum of all weighted violations. The Possibilistic-CSP  associates a preference to each
1987	2000	are typically encoded and solved with one of two generic paradigms: valued-CSPs  and semi-rings .sAnother approach to model and solve over-constrained problems involves Meta-Constraints . The idea behind this technique is to introduce a set of domain variables Z that capture the violation cost of each soft constraint. By correctly constraining these variables it is possible to
1987	2000	Even though there are many avenues for combining soft constraints, the objective almost always remains to minimize constraint violations. We propose here a small extension to the approach of , where meta-constraints on the cost variables of soft constraints are introduced. We illustrate this approach with the newly introduced soft gcc. Definition 12 (Soft global cardinality aggregator).
1987	2000	1} |S| ) the MaxCSP approach can be easily obtained by setting l1 = 0, u1 = 0, violation(Z) = ? d?DZ overflow(Z, d) and reading the number of violations in zagg. The sgca could also be used as in  to enforce homogeneity (in a soft manner) or to define other violation measures like restricting the number of highly violated 3 |?| refers to the number of transitions in the
1987	2001	This technique allows the resolution of over-constrained problem within traditional CP solvers. Comparatively few efforts have been invested in developing soft versions of common global constraints . Global constraints are often key elements in successfully modeling real applications and being able to easily and effectively soften such constraints would yield a significant improvement in
1987	2001	to some criteria. For each soft constraint C, we introduce a function that measures the violation, and has the following form: violationC : D1 × · · · × Dn ? N. This approach has been introduced in  and was developed further in . There may be several natural ways to evaluate the degree to which a global constraint is violated and these are not equivalent usually. A standard measure is the
1987	2002	and effectively soften such constraints would yield a significant improvement in flexibility. In this paper we study two global constraints: the widely known global cardinality constraint (gcc)  and the new regular  constraint. For each of these we propose new violation measures and provide the corresponding filtering algorithms to achieve domain consistency. All the constraint
1987	2002	and maximum number of times each value in the union of their domains should be assigned to these variables. Régin developed a domain consistency algorithm for the gcc, making use of network flows . A variant of the gcc is the costgcc, which can be seen as a weighted version of the gcc . For the cost-gcc a weight is assigned to each variable-value assignment and the goal is to satisfy
1987	2002	f is defined as cost(f) = ? w(a)f(a). a?A A minimum-cost flow is a feasible s?t flow of minimum cost. The minimum-cost flow problem is the problem of finding such a minimum-cost flow. Theorem 1 (). A solution to gcc(X, l, u) corresponds to a feasible s ? t flow of value n in the graph G = (V, A) with vertex set and edge set where V = X ? DX ? {s, t} A = As?X ? AX?DX ? ADX ?t, As?X = {(s,
1987	2002	variables. Régin developed a domain consistency algorithm for the gcc, making use of network flows . A variant of the gcc is the costgcc, which can be seen as a weighted version of the gcc . For the cost-gcc a weight is assigned to each variable-value assignment and the goal is to satisfy the gcc with minimum total cost. Throughout this section, we will use the following notation
1987	2002	all variables need to be assigned to a value and the cost function exactly measures the variable-based cost of violation. ? The graph Gvar corresponds to a particular instance of the cost-gcc . Hence, we can apply the filtering procedures developed for that constraint directly to the soft gcc. The soft gcc also inherits from the cost-gcc the time complexity of achieving domain
1987	2002	variables. Régin developed a domain consistency algorithm for the gcc, making use of network flows . A variant of the gcc is the costgcc, which can be seen as a weighted version of the gcc . For the cost-gcc a weight is assigned to each variable-value assignment and the goal is to satisfy the gcc with minimum total cost. Throughout this section, we will use the following notation
1987	2002	all variables need to be assigned to a value and the cost function exactly measures the variable-based cost of violation. ? The graph Gvar corresponds to a particular instance of the cost-gcc . Hence, we can apply the filtering procedures developed for that constraint directly to the soft gcc. The soft gcc also inherits from the cost-gcc the time complexity of achieving domain
1987	2003	been extended to the Weighted-CSP , associating a degree of violation (not just a boolean value) to each constraint and minimizing the sum of all weighted violations. The Possibilistic-CSP  associates a preference to each constraint (a real value between 0 and 1) representing its importance. The objective of the framework is the hierarchical satisfaction of the most important
1987	2004	different from the previous ones since the aggregation operator is a min/max function instead of addition. Max-CSPs are typically encoded and solved with one of two generic paradigms: valued-CSPs  and semi-rings .sAnother approach to model and solve over-constrained problems involves Meta-Constraints . The idea behind this technique is to introduce a set of domain variables Z that
2006	2007	with hierarchy-expressing rewriting rules used to zoom in and out  and to manage and display a derivation ; use of hierarchical graphs in a formal approach to plan generation ; use of hierarchically distributed graph transformations . 2' A A 1' 3' B C B C Cs15 11. Mr. Maggraphen: A lot of our C code performs graph inspections. How can we translate this into
2006	2025	The presence of such hierarchy-crossing edges greatly complicates the construction of tools for hierarchical graph rewriting. Various notations for hierarchical graph structures are described in  . Hierarchical structure assists in the display of a large graph. Zoom-in and zoom-out operations reduce the graph to manageable proportions for viewing, or delimit selected portions of the
2006	2032	processing . A chart-based parser for hierarchical graphs is discussed in . More recently, Klauck reports on a heuristically-driven chart parser and it’s application to CAD/CAM . On a related note, Henderson and Samal discuss efficient parsing of stratified shape grammars, building on the tabledriven methods used for LR(k) string grammars ; these techniques might
2006	2046	can be modularized, with some modules transforming local graphs, others changing interfaces or the global graph, and yet others changing the graph hierarchy (split or join local graphs)  . Inheritance Inheritance is a powerful tool for layering in object-oriented system design. Several forms of inheritance can be used within a graph-rewriting system; some examples are mentioned
2006	2046	out  and to manage and display a derivation ; use of hierarchical graphs in a formal approach to plan generation ; use of hierarchically distributed graph transformations . 2' A A 1' 3' B C B C Cs15 11. Mr. Maggraphen: A lot of our C code performs graph inspections. How can we translate this into graph-rewrite rules? The Maggraphens’ current software freely mixes
2047	2050	implementation and its embedded test cases. With such definition the trustability of a component will be based on the consistency between these three aspects. In a “design-by-contract” approach , the specification is systematically derived in executable contracts (class invariants, pre/post condition assertions for class methods). If contracts are complete enough, they should be violated
2047	2055	after its creation. Suppress a clone or copy instruction. Insert a clone instruction for each reference affectation. The mutation operators AOR, LOR, ROR and NOR are traditional mutation operators , the other operators having been introduced in this paper for the object-oriented domain. The data perturbation operator VCP allows to disturb state of data and to obtain a sensitivity analysis of
2047	2055	has been defined based on the quality of their associatedstests (itself based on fault injection). For measuring test quality, the presented approach differs from classical mutation analysis  as follows: - a reduced set of mutation operators is needed, - oracle functions are integrated to the component, while classical mutation analysis uses differences between original program and
2047	2056	after its creation. Suppress a clone or copy instruction. Insert a clone instruction for each reference affectation. The mutation operators AOR, LOR, ROR and NOR are traditional mutation operators , the other operators having been introduced in this paper for the object-oriented domain. The data perturbation operator VCP allows to disturb state of data and to obtain a sensitivity analysis of
9070367	2081	iPSC/2  and SGI Origin 2000  are examples of commercial systems that are based on the hypercube. Existing multicomputers  have widely used wormhole routing . This is due to its low buffering requirement, and more importantly it makes latency independent of the message distance under light traffic loads. In wormhole routing, a message is divided into
9070367	2082	on the way messages visit the virtual channels; a virtual channel has its own flit queue, but shares the bandwidth of the physical channel with other virtual channels in a timemultiplexed manner . A typical example of a deadlock-free routing widely used in practice is deterministic routing where messages visit dimensions in a predefined order. Deterministic routing has been popular in
9070367	2082	queue by a factor, V V , representing the average degree of V virtual channels multiplexing, that takes place at a physical channel. The factor V V can be estimated using the following formula  V V ? l= 0 V = V l ? l= 0 2 lP P l l (18)swhere P l ) 0 ( ? l ? V is the probability that l virtual channels at a given physical channel are busy (and is computed using equation 10). In the event
9070367	356	proposed algorithms require only one extra virtual channel per physical channel, compared to deterministic routing, allowing for an efficient router implementation. For instance, Duato’s algorithm  divides the virtual channels associated with each physical channel into two classes: a and b. At each routing step, a message visits adaptively any available virtual channel from class a. If all
9070367	356	forced to share the bandwidth of the physical channels. To reduce the effects of virtual channels multiplexing, Duato  has introduced to his original adaptive routing algorithm described in  a time-out mechanism when selecting a particular class of virtual channels. When a message is blocked upon reaching a given router, it waits for a fixed time period for one of the virtual channels
9070367	356	before they can be widely adopted in commercial multicomputers. Except from the models suggested in  for the simple version of Duato’s algorithm (i.e., with no time-out mechanism) , there has not been any model proposed in the literature for any other adaptive routing algorithms, e.g. that of . As a result, most existing studies , including
9070367	356	it suffers time-out, and as a result the message has to wait for the deterministic virtual channel corresponding to the lowest dimension still to be crossed according to deterministic routing . It is assumed that the probability of time-out at a given channel is independent of the subsequent channels. 2.3 The Communication Model Even though the proposed model can deal with different
9070367	2092	networks for practical multicomputers due to its desirable properties, including regularity, symmetry, low diameter and high connectivity. The iPSC/860 , iPSC/2  and SGI Origin 2000  are examples of commercial systems that are based on the hypercube. Existing multicomputers  have widely used wormhole routing . This is due to its low buffering
8918753	2102	With a sufficiently long prefix, inter-symbol interference (ISI) can be completely avoided, thus accommodating high data rate transmission. The performance of OFDM can be greatly enhanced by STBC  through the employment of transmit diversity. Guang-Hua Yang, Dongxu Shen, Victor O.K. Li Department of Electrical and Electronic Engineering The University of Hong Kong Pokfulam Road, Hong Kong
8918753	2102	block coded OFDM system Space-time coding (STC) achieves diversity gain through transmit diversity. An important class of STC is the space-time block code (STBC) which is proposed by Alamouti  and generalized by Tarokh . The employment of STBC requires the channel to be flat. Thus, OFDM is particularly suitable for employing STBC over broadband frequency selective fading channels. The
8918753	2102	for all the sub-channels. In our proposed scheme, sub-channel partitioning is performed at the receiver side based on the estimated channel information, which is essential to the decoding of STBC . The group membership for each sub-channel can be indicated by a number of bits and the whole partition results can be represented by the PV. When there are two groups, a single bit is sufficient
8918753	2102	computer simulations. A. System Parameters and Channel Model In Table I, we list the OFDM parameters used in the simulations. They are identical to those of IEEE 802.11a . Almouti’s STBC scheme  is employed. The delay profile of indoor wideband channel model B (see Table II), provided in ITU-R recommendation , is adopted for the simulation of uncorrelated multipath Rayleigh fading
8918753	2102	0 2 4 6 8 10 12 14 16 18 20 Eb/N0 (dB) Fig. 8. PSNR performance of different schemes. sented in Fig. 9 (a). The improvement due to STBC is dramatic. For each scheme, according to the BER trend , the performance improves as the configuration goes from 1) to 2) to 3), as expected. Further, our proposed UEP scheme has better performance than FEC-based UEP scheme for all the cases. We notice
8918753	2105	Space-time coding (STC) achieves diversity gain through transmit diversity. An important class of STC is the space-time block code (STBC) which is proposed by Alamouti  and generalized by Tarokh . The employment of STBC requires the channel to be flat. Thus, OFDM is particularly suitable for employing STBC over broadband frequency selective fading channels. The STBC-OFDM system we study is
8918753	2105	of sub-channels in an OFDM symbol. Then the pair of OFDM symbols in an STBC block are denoted as 1 Our scheme can be easily extended to multiple receive antennas, as well as other STBC schemes . h2 MB 1 MB 4 Y2 Y4 MB 2 MB 5 MB 6 Slice C B C R B4 B4 B4 B4 B4 B4 B5 B5 B5 B5 B5 VLC1 VLC2 VLC3 VLC4 VLC5 EOB VLC1 VLC2 VLC3 VLC4 EOB h1 B5 EOB Prefix Removal S->P & DFT MB 7 8 8 HP Layer LP Layer
8918753	2105	where si,k ,i =1, 2, k ?  is the modulated symbol on sub-channel k of OFDM symbol i, and T represents the transpose operation. The operation of STBC is given by the transmission matrix  ? ? s1 s2 G2 = (2) 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004 ?s ? 2 s ? 1 (1)swhere ? represents the complex conjugate operation. More specifically, at the first time slot, s1 and s2
3695299	2113	ranges approximately between 1.6 to 1.9 times the optimal solution. VIII. EXTENSIONS AND FUTURE WORK There has been some work on survivability of networks against mulitiple link failures . So far, we have presented our results, for the basic version of the problem, where we want to protect against a single link failure and the links are bidirectional and symmetric in both direction.
3695299	2117	one the over build can be significant and second it is hard to find the smallest cycle cover of a given network . An improvement to these schemes are those based on the notion of p-cycle . Here the main idea is that a cycle can be used to protect not just the links on the cycle but also the chords (spokes) of the cycle, thus showing that far fewer rings may be sufficient for
3695299	2120	extensions and future work. Section VII presents our simulation results. II. BACKGROUND AND RELATED WORK In general the protection schemes for optical and MPLS networks can be classified ( , ), based on whether the protection is local (link based) or end-to-end (path based), and whether the backup resources are dedicated or shared. Fast or local reroute mechanisms, outlined earlier, are
3695299	2121	resources, thus resulting in efficient capacity utilization. Two different techniques for local protection in MPLS networks have been proposed . The one-to-one backup technique    creates bypass LSPs for each protected service carrying LSP, at each potential point (link or node) of local repair. The facility backup technique  creates a bypass tunnel to protect a
3695299	2121	about extensions and future work. Section VII presents our simulation results. II. BACKGROUND AND RELATED WORK In general the protection schemes for optical and MPLS networks can be classified ( , ), based on whether the protection is local (link based) or end-to-end (path based), and whether the backup resources are dedicated or shared. Fast or local reroute mechanisms, outlined
3695299	2123	which ranges approximately between 1.6 to 1.9 times the optimal solution. VIII. EXTENSIONS AND FUTURE WORK There has been some work on survivability of networks against mulitiple link failures . So far, we have presented our results, for the basic version of the problem, where we want to protect against a single link failure and the links are bidirectional and symmetric in both
2133	2134	effective in practice and greatly assisted our support staff in maintaining our Ethernet. 1 Introduction The Ethernet was invented by Bob Metcalfe and others at the Xerox Palo Alto Research Center  and soon developed into an international standard . One aspect that made the Ethernet so attractive was its use of a passive communications medium: a thick coaxial cable into which taps could be
2133	2231	at will. And they were. I was a graduate student at Carnegie Mellon University (CMU) in the 1980’s as the Ethernet was deployed on campus, mostly using the early Digital-Intel-Xerox (DIX) standard . We observed that as soon as an Ethernet coax came near a group’s computers, taps would appear as if by magic. The Ethernet was such an elegantly simple and useful method of connecting computers
2133	2136	operational behavior, using the system’s own equipment is more cost effective. Also, monitoring actual packet delivery between end stations provides an end-to-end check, which is good practice . To verify that stations X and Y can communicate, the straightforward approach is to cause X to send Y a request that causes Y to send X a corresponding response. This is the strategy of Ping ,
2133	2234	request that causes Y to send X a corresponding response. This is the strategy of Ping , which is arguably the network administrator’s most often used tool. Ping uses the IP / ICMP ECHO protocol , but a similar protocol could easily be designed for the Ethernet link layer. Although theoretically a defect could discriminate between application traffic and ping traffic, causing pings to work
2133	2139	transform w? ( u, v) = w( u, v) + h( u) ? h( v) where u and v are nodes and w(u,v) is the edge weight from u to v. It is well-known that this transform leaves the weights of all cycles unchanged . Although one might try to apply alpha-scaling to produce a “cannonical” or “balanced” matrix of probability estimates, we did not do so. We just took whatever the results were that we got from the
2154	2142	This mechanism is also used in . IEEE 802.11 relies on the DCF method to coordinate the transmission of packets. The packet transmission sequence is illustrated in Figure 3. Similar to , we measure the throughput of transmitting a packet ????? ? ? ??? ??? as , ? where is the size of the packet, ??? is the time-stamp that the packet is ready at the MAC layer, and ??? is the
2154	2143	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2143	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2145	and implement our bandwidth management scheme for a wireless network consisting of heterogeneous computers and devices connected together over the IEEE 802.11 wireless MAC protocol. Unlike in , where a basestation determines the schedule of transmission for the entire network and all communication is via the base-station, in our network, transmission is distributed and peer-to-peer. The
2154	2145	reduction. 4 Related Work Past research in wireless bandwidth management has mostly focused on flow scheduling at the base-station to achieve fairness between flows competing for the channel (e.g. ). In contrast, we use a peer-to-peer MAC layer transmission model rather than the base-station model. In , the authors propose an admission control scheme for a peer-to-peer, single-hop, ad hoc
2154	2146	as perceived by different nodes in the network at the same time can also be different. The latter phenomenon is due to errors and interference that are location-dependent. Furthermore, as shown in , the overall throughput of the network can also vary dynamically as flows arrive and depart, and the number of active stations changes. The BM must therefore not just deal with admission control
2154	2148	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2149	RA will again re-negotiate to release the excess channel time proportion. This solution is equivalent to splitting up a VBR stream in the time domain into multiple CBR streams, as has been done in  in the context of ATM networks. Since this scheme only involves re-organizing the traffic rather than the network, it can be directly applied from ATM networks to wireless networks. Our solution is
2154	2150	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2150	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2151	This mechanism is also used in . IEEE 802.11 relies on the DCF method to coordinate the transmission of packets. The packet transmission sequence is illustrated in Figure 3. Similar to , we measure the throughput of transmitting a packet ????? ? ? ??? ??? as , ? where is the size of the packet, ??? is the time-stamp that the packet is ready at the MAC layer, and ??? is the
2154	2152	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2152	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2153	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2153	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2154	for this purpose. The entry for the terminating flow in the BM’s flow table is expunged. A teardown¨ 3 The computational complexity of this algorithm is ? ????? ? . A detailed pseudo-code is in .sacknowledgement message is sent to ¨ ’s RA 4 . Change in Flow’s Perception of Total Network Bandwidth: The RA of every flow periodically obtains from the TBE the flow’s current perceived total
2154	2154	the noise introduced by the measured raw throughput from packets with different sizes. We have also verified that using recent packets to estimate current channel bandwidth is feasible and robust . Throughput (bytes/sec) 250000 200000 150000 100000 50000 raw throughput normalized throughput 0 0 100 200 300 400 500 600 700 800 900 1000 Time (sec) Figure 4. Raw throughput and normalized
2154	2154	from the TBE to the RA. The other is to increase the tolerance to changes in perceived bandwidth © ? ??¨?? . The effect of these optimizations on overhead and performance are discussed in detail in . Our results show that for a small price in terms of performance (we quantify fairness and throughput jitter, the key performance metrics, in ), we can obtain large gains in overhead reduction.
2154	2155	agencies. Samarth H. Shah, Kai Chen and Klara NahrstedtsDepartment of Computer Science University of Illinois at Urbana-Champaign Email: ¡ shshah, klara¢ kaichen, @cs.uiuc.edu scheduling (DWFS)  for the IEEE 802.11 MAC protocol operating in the Distributed Coordination Function (DCF) mode, which is a first step towards providing flows with their desired quality-of-service (QoS). In DWFS,
2154	2155	course of the connection, in addition to admission control at flow startup. Another area of related work is the QoS-aware linklevel scheduling schemes in single-hop and multi-hop wireless networks . As mentioned before, our bandwidth management scheme is a co-operative linkmiddleware scheme that can work independently or assist a QoS-aware link-level scheduler when available. The ability of
2154	2157	this scheme only involves re-organizing the traffic rather than the network, it can be directly applied from ATM networks to wireless networks. Our solution is also similar to that proposed in  for CDMA networks. Frequent bursts could result in an explosion in re-negotiation overhead. We deal with the problem of frequent bursts in one of two ways: (a) adjusting © ????? ??¨?? for VBR flow
8918757	2283	pits and manure, average yields rose by 640 kg/ha compared to the control plots (Table 1). The additional gains due to the addition of inorganic fertilizer proved biggest in years of good rainfall (1994), though in other years (1996) the additional yield would not be sufficient to cover the costs of inorganic fertilizers. Similar trials over two seasons in Mali indicate that zaï pits plus manure
8918757	2283	the compost, the cost of maintenance of compost pit, the cost of emptying the pit as well as the costs of transporting the compost to the fields 18 18 These costs are derived from Sidibe et al. (1994) who measured them in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based
8918757	2292	in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based on Roose et al. (1999). Sidibe et al. (1994) measured the labor requirements for digging the compost pit and filling it. A compost of 10almost 11 m3 is needed to produce 2,5 tons of compost. Crop yields and prices vary
8918757	2292	order to generate sufficient runoff. Because the digging of zaï requires a substantial input of labor, this implies that a relatively high population density would facilitate its spreading. Freeman (1999) has tried to map the range of proven soil management practices in West Africa using digital maps and concluded that there also is a potential for expansion of zaï to, for instance, Eastern Senegal
8918757	2293	in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based on Roose et al. (1999). Sidibe et al. (1994) measured the labor requirements for digging the compost pit and filling it. A compost of 10almost 11 m3 is needed to produce 2,5 tons of compost. Crop yields and prices vary
8918757	2293	order to generate sufficient runoff. Because the digging of zaï requires a substantial input of labor, this implies that a relatively high population density would facilitate its spreading. Freeman (1999) has tried to map the range of proven soil management practices in West Africa using digital maps and concluded that there also is a potential for expansion of zaï to, for instance, Eastern Senegal
8918757	2294	in INERA research villages in the western part of Burkina Faso. Labor requirements for digging the zaï and putting crop residues and other organic material into the pits are based on Roose et al. (1999). Sidibe et al. (1994) measured the labor requirements for digging the compost pit and filling it. A compost of 10almost 11 m3 is needed to produce 2,5 tons of compost. Crop yields and prices vary
8918757	2294	order to generate sufficient runoff. Because the digging of zaï requires a substantial input of labor, this implies that a relatively high population density would facilitate its spreading. Freeman (1999) has tried to map the range of proven soil management practices in West Africa using digital maps and concluded that there also is a potential for expansion of zaï to, for instance, Eastern Senegal
8918757	2299	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2299	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918757	2300	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2300	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918757	2302	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2302	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918757	2303	in the volume and timing of rainfall. This variability is well illustrated by yield data collected in Niger from 1991 to 1996 on the same farmers’ fields (Table 1). 13 Slingerland and Stork (2000) have not been doing research on zaï,as they assume, but rather on a small traditional pit used in the Yako region, called guendo.sTable 1--Impact of planting pits (tassa) plus manure and fertilizer
8918757	2303	(1993) find that zaï pits alone achieved an average gain of only 38 kg/ha in white sorghum yields over two seasons in two locations in Burkina Faso (Table 2). Using a regression analysis, Kabore (2000) found that zaï pits alone increased sorghum yields by 310 kg/ha compared to the non-zaï situation in the village of Donsin, which had recently adopted this 267 11 553 653 542 642 282 125 513 765
8918783	2248	conventional filter methods because of its low inner-class average and its high deviation. In supervised classification problems, various wrapper methods such as Recursive Feature Elimination (RFE)  are proposed for feature selection. Since the main purpose of the wrapper method is to improve the performance of the classification algorithm, the genes chosen by the method have not been paid
8918783	2249	classification algorithm in order to extract the discriminative genes that difficult to be extracted by conventional filter methods. RFE method based on nonlinear Support Vector Machines (SVMs)  is employed to this end because it is successfully applied to classification of gene expression data. We investigate the genes extracted by the RFE method based on SVMs with gaussian kernel
8918785	2274	synsets (e.g. respectively: {local#2} (adj.), {area#1, country#4}, {network#2, communications network#1 }) in order to enrich WordNet with new domain concepts and learn domain-specific ontologies ; ??? disambiguate WordNet glosses ; – disambiguate words in a query for sense-based web query expansion . Semantic disambiguation is performed using a method we have named structural semantic
8918785	2277	algorithm, which is rather complex. A thorough description is in , but a complete reformalization is in progress. SSI is a kind of structural pattern recognition. Structural pattern recognition  has proven to be effective when the objects to be classified contain an inherent, identifiable organization, such as image data and time-series data. For these objects, a representation based on a
2278	2279	traffic. For example, as it is pointed out in , an 18 bytes of search string in a Query message may cause 90 megabytes of data to be forwarded by the P2P network peers. As another example,  states that total number of messages including the responses triggered by a single Query message can be as large as (assuming 4 connections per peer): ? C ? (C ? 1) i = 26240 (1) T T L 2 ? i=0 •
2278	173	have observed the existence of high degrees of free riding in P2P networks and they suggest that free riding may be an important threat against the existence and efficient operation of P2P networks . There may be various reasons and motivations behind free riding. For example, peers with a Network Address Translation (NAT) address may act as a free rider. Bandwidth limitation would be another
2278	173	shared earlier . Moreover, a large number of free riders and their queries will generate a great amount of P2P network traffic, which may lead 2 1% of the peers provides 37% of the content .sto degradation of P2P services. Furthermore, underlying available network capacity and resources will be decreased by free riders, which will cause extra delay and congestion to non-P2P traffic.
2278	173	the possible attacks against the proposed mechanisms by free riders. Finally, the conclusions are presented in section 5. 2 Related Work User traffic on Gnutella network is extensively analyzed in  and it is observed that 70% of peers do not share any file at all. Furthermore, 63% of the peers who share some files do not respond to any queries. Another interesting observation is that 25% of
2278	2281	in the search horizon. As the peers age in the network, they begin not to find interesting files and may leave the system for good with all the files and resources that they have shared earlier . Moreover, a large number of free riders and their queries will generate a great amount of P2P network traffic, which may lead 2 1% of the peers provides 37% of the content .sto degradation of
2278	2281	4 hours, while only 25% of the peers are alive for more than 24 hours. In another work , the median session duration of both Napster and Gnutella clients is about 60 minutes. In a similar work , 90% of average session lengths of Kazaa clients is found to be about 30 minutes. As a result, it can be assumed that peers stay connected long enough to collect statistical information about them
2278	2283	about them and take necessary actions. Another issue is whether a monitoring peer can snoop and monitor enough number of messages that are coming from or going towards the neighboring peers. In , it is reported that the average number of queries per second for three peers located at different geographic locations is about 50. Also, about 30 query responses per second are recorded. This
2278	2284	In a free riding environment, a small number of peers will serve for all other peers. Therefore, many download requests will be directed towards these peers which may lead to scalability problems . Renewal of content or presenting interesting content may decrease in time, thus the number of shared files may become limited or may grow very slowly. Faulttolerance property of P2P networks may
2278	2284	in the search horizon. As the peers age in the network, they begin not to find interesting files and may leave the system for good with all the files and resources that they have shared earlier . Moreover, a large number of free riders and their queries will generate a great amount of P2P network traffic, which may lead 2 1% of the peers provides 37% of the content .sto degradation of
2278	2284	amount of free riding in Gnutella network as well as in Napster . Another interesting observation is that 7% of the peers together provide more files than all of the other remaining peers. In , Ramaswamy and Liu concentrate on how to prevent free riding. They propose to calculate a utility function for each peer in order to estimate its usefulness to all community. According to the
2278	2284	and cheated by writing some malicious client programs. In a recent work , Vishnumurthy et.al. suggest using a single scalar value, called Karma, to evaluate a peer’s utility to a system like in . Each peer has an account consisting of Karma. When a peer uploads a file to a requesting peer, it gets some amount of Karma from the requesting peer. On the other hand, if the peer downloads a
2278	2284	peers, called the bank-set, in order to ensure Karma against loose and tampering. The transfer of Karma between peers is executed through bank-set of each peer. The main difference from the work in  is that the utility value of a peer is not stored at the peer itself but at some other peers. However, to make the scheme work, a group of peers must be known to store Karma value. Whenever a
2278	2285	network peers can join and leave the system at any time. We can find some related work in the literature about the network topology dynamics and peer characteristics of P2P applications. In , it is stated that about 40% of the peers leave the Gunetella network in less than 4 hours, while only 25% of the peers are alive for more than 24 hours. In another work , the median session
2278	188	property of P2P networks may be adversely affected due to the fact that a very small portion of the peers provides most of the content 2 . This also leads to a client-server like paradigm  and decreases P2P network advantages. Quality of search process may degrade due to increasing number of free riders in the search horizon. As the peers age in the network, they begin not to find
2278	188	25% of the peers provide 99% of the whole content in the network. In a more recent work, Saroui et. al. confirm that there is a large amount of free riding in Gnutella network as well as in Napster . Another interesting observation is that 7% of the peers together provide more files than all of the other remaining peers. In , Ramaswamy and Liu concentrate on how to prevent free riding. They
2278	188	of P2P applications. In , it is stated that about 40% of the peers leave the Gunetella network in less than 4 hours, while only 25% of the peers are alive for more than 24 hours. In another work , the median session duration of both Napster and Gnutella clients is about 60 minutes. In a similar work , 90% of average session lengths of Kazaa clients is found to be about 30 minutes. As a
2288	2289	range. Routing protocols must be able to cope efficiently with this mobility. A lot of topology-based routing protocols have been proposed that either establish a route on-demand (e.g. AODV , DSR ) or proactively maintain hop-by-hop information at each node (e.g. OLSR , TBPRF ). In case of link incidents, new routes need to be discovered and updated routing information needs
2288	2289	Protocols that avoid beaconing completely fall into the third group. Several approaches are described in the literature to mitigate the drawbacks of link incidents for topologybased protocols. AODV  implements a local route repair mechanism which aims to replace a particular broken link with an alternate path between the two nodes and to minimize the latency and induced routing overhead of
2288	2289	vmax. For example, TABLE I EXPECTED SPEED DIFFERENCE OF TWO NODES vmin  vmax  ES  1 10 5.69 1 20 9.64 1 40 16.68 10 20 18.83 10 40 29.55 we can see that that for a speed interval  m/s, two arbitrary nodes move relative to each other with almost 10 m/s. Suppose now node A moves a distance d to A’. The size of the area A(r, d), which was initially covered by node A’s
2288	2289	indicate time-out intervals t in seconds and speed intervals  in meters per second, respectively. TABLE II EXPECTED PERCENTAGE OF OUT-DATED NEIGHBORS FOR r = 250 m/r = 100 m  m/s  m/s  m/s 1 1.45/3.62 2.45/6.13 4.25/10.61 3 4.35/10.85 7.36/18.35 12.72/31.52 5 7.24/18.05 12.25/30.39 21.14/51.51 10 14.46/35.73 24.40/58.90 41.67/92.09 Even for a large r = 250 m and for
2288	2289	performance such that the impact of the proposed enhancements can be observed more easily. In Fig. 3 and Fig. 4, the delivery ratio and the end-to-end delay are depicted for a speed interval of  m/s. There are three reasons why we decided to use this high speed interval. First, even though the speed interval may seem high, the average speed of the nodes is only about 10 m/s. Second, we
2288	2292	routing protocols have been proposed that either establish a route on-demand (e.g. AODV , DSR ) or proactively maintain hop-by-hop information at each node (e.g. OLSR , TBPRF ). In case of link incidents, new routes need to be discovered and updated routing information needs to be distributed by (partially) flooding the network, which may cause long latencies and
2288	2293	current node (e.g. provided by GPS), the positions of neighboring nodes (by nodes periodically transmitting a hello message, called beacon) and the destination (e.g. obtained via a location service ). Each packet is routed independently at each node and forwarded to a neighboring node which reduces the distance to the destination. These protocols are inherently more robust to changes in the
2288	2294	All these properties make them e.g. especially suited for sensor and vehicular adhoc networks. An overview of position-based routing algorithms and location services can be found e.g. in  and . In position-based routing protocols, nodes periodically broadcast beacons to announce their presence and location to their neighbors. Each node stores all neighbors and their current
2288	2296	a particular broken link with an alternate path between the two nodes and to minimize the latency and induced routing overhead of link incidents. To avoid complete disruption of communication,  investigated the expected lifetime of routes in order to schedule the route discovery before actual link break. Unlike these protocols using hopcount as the routing metric, several other protocols
2288	2299	in the first place. In ABR  the lifetime of a link is taken into account, whereas SSA  also considers feedback from the link layer about signal strength as primary routing metric. In  and  results from analytical derivations and observations made by simulations are used to design new routing metrics which favor more stable paths. Based on link availability estimations, a
2288	2300	the first place. In ABR  the lifetime of a link is taken into account, whereas SSA  also considers feedback from the link layer about signal strength as primary routing metric. In  and  results from analytical derivations and observations made by simulations are used to design new routing metrics which favor more stable paths. Based on link availability estimations, a metric for
2288	2302	positions of nodes, information about the velocity and direction are also often known and can be utilized to estimate the expiration time of a link and to reconfigure routes timely as proposed in . Unlike , where GPS-information is only applied to maintain routes,  additionally makes use of location information in the routing decision itself to establish paths in a depth-first search
2288	2303	known and can be utilized to estimate the expiration time of a link and to reconfigure routes timely as proposed in . Unlike , where GPS-information is only applied to maintain routes,  additionally makes use of location information in the routing decision itself to establish paths in a depth-first search way. In , factors that influence the utility of hello messages were
2288	2304	, where GPS-information is only applied to maintain routes,  additionally makes use of location information in the routing decision itself to establish paths in a depth-first search way. In , factors that influence the utility of hello messages were studied for determining link connectivity in topologybased protocols. Out-dated and inaccurate neighbor tables in position-based routing
2288	2305	protocols, almost no research has been performed on link incidents and inaccurate neighbor tables in position-based routing protocols. To the best of our knowledge, the only exception is GPSR , where the authors compared the packet delivery ratio and routing overhead for different time intervals between beacons. Even though beaconing was not explicitly studied, the determination of the
2288	2305	has meanwhile left the transmission range. And thirdly, most position-based routing algorithms apply a forwarding strategy which forwards packets to nodes close to the destination, e.g. , . As a consequence, the selected neighbor is close to the border of the transmission range and thus has an even higher probability to have left the transmission range. All these factors contribute
2288	2305	strong are considered as neighbors. Unlike for beacons, data packets received at any power level are processed. VI. PROTOCOLS As a representative of a position-based routing protocol, we use GPSR . GPSR also serves as basis for all proposed enhancements, i.e. only the enhancement under consideration is modified in the original GPSR. We consider also a reactive GPSR were nodes only transmit
2288	2305	information from the global data of the simulator. The aim is to determine the performance limits of any position-based routing protocol. A. GPSR As the underlying routing protocol, we use GPSR  which is basically an extension of GFG  with MAClayer enhancements. A packet is routed in a greedy manner towards the position of the destination. Each node selects the node from its neighbor
2288	2305	it enters perimeter mode and the packet is routed according to the right-hand rule on the faces of a locally extracted planar subgraph to avoid loops and to recover from this local minimum (see  for more details). As soon as the packet arrives at a node closer to the destination than where it entered perimeter mode, the packet switches back to greedy routing. It was shown that this
2288	2307	the Home Location Register to keep track of a node’s position if it has moved to a new cell. Dissemination and replication of data in repositories for mobile ad-hoc networks was studied e.g. in . The authors propose different strategies when to trigger updates. These approaches differ from our investigation in many points, mainly that information is transmitted infrequently to some few
2288	2308	rather frequently. Furthermore, entries do not need to be periodically refreshed to remain valid. Lately, several protocols have been proposed which adopt a new paradigm for position-based routing , , , and . The next hop is not determined at the sender, but in a distributed way at the receivers. Nodes do not rely on information about neighbors anymore and allow disposing
2288	2308	vmax. For example, TABLE I EXPECTED SPEED DIFFERENCE OF TWO NODES vmin  vmax  ES  1 10 5.69 1 20 9.64 1 40 16.68 10 20 18.83 10 40 29.55 we can see that that for a speed interval  m/s, two arbitrary nodes move relative to each other with almost 10 m/s. Suppose now node A moves a distance d to A’. The size of the area A(r, d), which was initially covered by node A’s
2288	2308	indicate time-out intervals t in seconds and speed intervals  in meters per second, respectively. TABLE II EXPECTED PERCENTAGE OF OUT-DATED NEIGHBORS FOR r = 250 m/r = 100 m  m/s  m/s  m/s 1 1.45/3.62 2.45/6.13 4.25/10.61 3 4.35/10.85 7.36/18.35 12.72/31.52 5 7.24/18.05 12.25/30.39 21.14/51.51 10 14.46/35.73 24.40/58.90 41.67/92.09 Even for a large r = 250 m and for
2288	2308	definitely the differences in the results. And finally we consider speed as a proxy for any kind of topology changes as mentioned in section III. We ran the simulation also with a speed interval of  m/s. The results are similar, except that the delay and the packet loss rate are about 30% and 50% lower respectively. As expected the performance suffers in case of low pause times for all
2288	2310	Furthermore, entries do not need to be periodically refreshed to remain valid. Lately, several protocols have been proposed which adopt a new paradigm for position-based routing , , , and . The next hop is not determined at the sender, but in a distributed way at the receivers. Nodes do not rely on information about neighbors anymore and allow disposing beaconing
2288	2311	entries do not need to be periodically refreshed to remain valid. Lately, several protocols have been proposed which adopt a new paradigm for position-based routing , , , and . The next hop is not determined at the sender, but in a distributed way at the receivers. Nodes do not rely on information about neighbors anymore and allow disposing beaconing completely. These
2288	2312	a neighbor has meanwhile left the transmission range. And thirdly, most position-based routing algorithms apply a forwarding strategy which forwards packets to nodes close to the destination, e.g. , . As a consequence, the selected neighbor is close to the border of the transmission range and thus has an even higher probability to have left the transmission range. All these factors
2288	2312	simulator. The aim is to determine the performance limits of any position-based routing protocol. A. GPSR As the underlying routing protocol, we use GPSR  which is basically an extension of GFG  with MAClayer enhancements. A packet is routed in a greedy manner towards the position of the destination. Each node selects the node from its neighbor table which is geographically closest to the
2288	2313	This assumption is valid as nodes move independently of each other and have symmetric transmission ranges. The probability density function fS of the nodal speed s is derived e.g. already in  and is given by fS(s) = s ln 1 ? vmax vmin We first derive the expected value of the relative speed vector, i.e. the difference of two arbitrary speed vectors, in the unbounded random waypoint
2288	2313	placed in a rectangular area of 600m x 3000m. The nodes move according to the random waypoint mobility model. We implemented the stationary distribution of the random waypoint model as described in . Thereby the simulations do not need an initial warm-up phase, whose duration is difficult to predict, to reach a stable state. The simulations last for 900s and data transmission starts at 180s
2288	2315	interval and the time-out interval is set to 1.5 s and 6.75 s respectively. The minimum speed is set unequalsIEEE INFOCOM 2005 8 0 m/s as otherwise the stationary distribution would be static . The following subsection VII-B provides further explanation for the chosen values. The possible combinations and variations for all the parameters of the beaconing strategies, the prediction, and
2350	1623	followings. For example, some routes in the Internet differentiate the service among different classes of data flows by giving priority to specific classes of data flows; see, e.g., Blake, et al. . ??? In general, one would expect that the network be stable under the normal offered load condition. From the above counter-example, we have seen that the bandwidth capacity realization 4 ?sproblem
2350	2351	for more details. A stability result for the network with this class of utility functions is also presented in their paper. Their stability result, as well the earlier one in de Veciana et al. , is a significant step toward a better understanding of the stability issue of the Internet, noting that any utility function is only an approximation to the precise behavior of TCP. Moreover, it
2350	2243	academics and the telecommunication industry. Currently, the majority of the Internet traffic is dominated by various versions of TCP (the Transmission Control Protocol; see for example Jacobson ), and a lot of efforts have been placed on the study of the TCP congestion control. However, due to its complex nature, the behavior of the Internet traffic is not fully understood yet and remains
2350	2354	proportionally fairness criterion are stable under the normal offered load condition. Readers are referred to Bertsekas and Gallager  for detailed descriptions on the max-min fairness, and Kelly  and Kelly, et al.  on the proportionally fairness. In Bonald and Massoulie , a stability result is also established for networks employing a class of (p, ?)-proportionally fair bandwidth
2350	2355	offered load condition, a data network is stable when the bandwidth of the network is allocated so as to maximize a class of general utility functions. Using the microscopic model proposed by Kelly  for a TCP congestion control algorithm, we argue that the bandwidth allocation in the network dominated by this algorithm can be modelled as our bandwidth allocation model, and hence that the
2350	2355	that the network under a bandwidth allocation that maximize a class of more general utility functions is stable under the normal offered load condition; see Section 2. By using a result in Kelly , which models the microscopic behavior of a TCP congestion control algorithm, we show that our bandwidth allocation model does capture some important characteristics of the macroscopic behavior of
2350	2355	light on the connection level stability issue of the Internet, though we should realize that it is impossible to provide a complete and satisfactory answer to such an issue in this study. In Kelly , the bandwidth allocation for TCP is modelled at the microscopic level as follows. , this model approximates the bandwidth allocation of a network
2350	2355	being within the link capacity should be admissible. Finally, this stability result, combined with a recent result in the microscopic modelling of a TCP congestion control algorithm in Kelly , is used to heuristically show the stability of the network dominated by the TCP congestion control algorithm under the normal offered load condition. An assumption of the bandwidth allocation in
2350	2358	and satisfactory answer to such an issue in this study. In Kelly , the bandwidth allocation for TCP is modelled at the microscopic level as follows. , this model approximates the bandwidth allocation of a network dominated by (various versions of) TCP Reno.] For any given number xr of ongoing connections at each route, the bandwidth allocation
2350	2360	result is also established for networks employing a class of (p, ?)-proportionally fair bandwidth allocation policies. This class of bandwidth allocation policies, first proposed by Mo and Walrand , include the bandwidth allocation policies satisfying the proportional fairness criterion and the minimal potential delay criterion as special cases. (We should brief these bandwidth allocation
2361	2363	diversity of classifiers can be achieved on the basis of two approaches: a DT ensemble technique  and an averaging technique based on Bayesian Markov Chain Monte Carlo (MCMC) methodology . Both DT techniques match the above requirements well and have revealed promising results when applied to some real-world problems . By definition, DTs consist of splitting nodes and
2361	2363	some data is calculated for each terminal node . The Bayesian generalization of tree models required to evaluate the posterior distribution of the trees has been given by Chipman et al. . Denison et al.  have suggested MCMC techniques for evaluating the posterior distribution of decision trees. This technique performs a stochastic sampling of the posterior distribution. In this
2361	2363	( | D) , then we can write N N ( ¡ i ¡ ) ¡ ( i) 1 ( i) p( y | x , D) ? p( y | x, , D) p( | D) = p( y | x, , D) . (2) i= 1 N i= 1 This is the basis of the MCMC technique for approximating integrals . To ¡ perform the approximation, we need to generate random samples from p ( | D) by running a Markov Chain until it has converged to the stationary distribution. After this we can draw samples
2361	2363	accepted. As a result, RJMCMC algorithms cannot explore a full posterior distribution. The space which is explored can be extended by using a restarting strategy as Chipman et al. have suggested in . The idea behind the restarting strategy is based on multiple runs of the RJMCMC algorithm with short intervals of burn-in and post burn-in. For each run, the algorithm creates an initial DT with
2385	2388	Schultz, and Adams 1998). As interface development progressed, we saw a need to integrate the language and gestural capabilities of the interface by tracking goals in human/robot interactions (Perzanowski et al 1999). We argued that tracking goals provided us with a means of achieving varying levels of autonomy. Recently, we included a mechanical means of communication with the robots via palm devices
2385	2392	digital assistant (in this case, any of the Palm devices). Speech is initially processed by a speech-to-text system (IBM's ViaVoice), and our natural language understanding system, Nautilus (Wauchope 1994), robustly parses the language input and translates it into a semantic representation which is then mapped to a command after gestural information is incorporated. Gestures can be either distances,
2417	2552	BMS98]. Some of these approaches use the term ‘information architecture’, or ‘architecture of information systems’, while yet others refer to the same concept as ‘enterprise (IT) architecture’. In , the concept of architecture is defined as: “The fundamental organization of a system embodied in its components, their relationships to each other, and to the environment, and the principles
2417	2552	has been decided upon, while responsibilities (such as functionality) have been assigned to the (overall) components of the system. In the conceptual framework for architecture, as defined in , an architectural description can be organised into one or more constituents called architectural views. Each view addresses one or more of the concerns (interests) of the stakeholders of a system.
2417	2422	and architects with the burden of selecting the viewpoints to be used in a specific situation. Some of these frameworks of viewpoints are: The Zachman framework , Kruchten’s 4+1 framework , RM-ODP , ArchiMate  and TOGAF . The aim of this paper is not to provide ‘yet another framework of viewpoints’, but rather to lay a foundation to be able to reason about
2417	2422	of the above three angles in separate sections (sections 2 to 4). To make our results more concrete, section 5 briefly discusses two example frameworks of viewpoints (Kruchten’s 4+1 framework  and RMODP ), from the perspective of our meta-framework. This is followed by a brief discussion on directions of further research and elaboration in section 7. 2 Modelling The aim of this
2417	2422	have been left implicit. Furthermore, the case studies are part of ongoing research efforts. A more detailed elaboration of the cases presented is part of these efforts. 5.1 The ‘4+1’ view model In , Kruchten introduces a framework of viewpoints (a view model) comprising five viewpoints. The use of multiple viewpoints is motivated by the observation that it “allows to address separately the
50485	2433	. This method was generalized to apply more generally to fitting non-convex cost-functions arising in a variety of problems, e.g., finding the optimal wiring for a densely wired computer chip . The method of simulated annealing consists of three functional relationships. 1. g(x): Probability density of state-space of D parameters x ={x i ; i = 1, D}. 2. h(x): Probability density for
50485	2433	examples requiring bona fide ‘‘temperatures’’ and ‘‘energies.’’ Rather, this methodology can be readily extended to any problem for which a reasonable probability density h(x) can be formulated . III. Fast Annealing It was also noted that this methodology can be readily extended to use any reasonable generating function g(x), without relying on the principles underlying the ergodic nature
50485	2436	G + ˆg G i ?i , < ? i? t? ?i t > ?= ? (t ? t?)? ii? , i = 1, ..., ? , G = 1, ..., ? . (25) Expanded sets of equations can represent a field M G (r, t), and the discussion below generalizes as well . Another mathematically equivalent representation is given by the Fokker-Planck equation, in terms of the ‘‘drifts’’ g G and ‘‘diffusions’’ g GG? , ?P ?t = VP + ?(?gG P) ?M G + 1 2 g G = f G + 1 2
50485	2437	diffusions are also parametrized. For example, these parameters can enter as expansion coefficients of polynomials describing accepted models of particular systems, e.g., modeling economic markets , or combat scenarios . In combat systems, such equations appear as ?r = x r bb + y r br br + z r ?r , ?b = x b r r + y b rbrb + z b ? b , (27) where the M G are Red (r) and Blue (b) force
50485	2441	is desired. The above problems provide motivation for the development of a new algorithm. Consider a parameter ? i k in dimension i generated at annealing-time k with the range ? i k ? , (12) calculated with the random variable y i , ? i k+1 = ? i k + y i (B i ? A i), y i ? . (13) Define the generating function gT (y) = D 1 ? i=1 2(|yi | + Ti) ln(1 + 1/Ti) Its cumulative
50485	2443	+ y i (B i ? A i), y i ? . (13) Define the generating function gT (y) = D 1 ? i=1 2(|yi | + Ti) ln(1 + 1/Ti) Its cumulative probability distribution is G T (y) = y 1 ?1 y D ? g i T (y i ) . (14) D ? i=1 ? ... ? dy?1 ...dy? D gT (y?) ? D ? G i T (y i ), ?1 G i T (y i ) = 1 2 + sgn(yi ) 2 ln(1 + |y i |/T i) ln(1 + 1/T i) y i is generated from a u i from the uniform distribution u i ?U
50485	2449	depending on the numerical algorithm used, to find the long-time evolution of the system. Details of the above parameter-fits of Lagrangians to empirical data will be published at a later date . VII. Discussion An algorithm of very fast simulated re-annealing has been developed to fit a empirical data to a theoretical cost-function over a D-dimensional parameter-space. The annealing
423617	2451	supervised methods and bootstrap learning techniques. KNOWITALL uses a novel form of bootstrapping that does not require any manually tagged training sentences. Other bootstrap IE systems such as  still require a small set of domain-specific seed instances as input, then alternately learn rules from seeds, and further seeds from rules. Instead, KNOWITALL begins with a domainindependent set
423617	2452	summary of our contributions. 1.1 Previous Work Whereas search engines locate relevant documents in response to a query, web-based Question Answering (QA) systems such as Mulder , AskMSR , Radev’s work , and others locate potentially relevant answers to individual questions but are not designed to compile large bodies of knowledge. Much of the previous work on Information
423617	2452	of the web for an ample supply of simple sentences that are relatively easy to process. This notion of “redundancy-based extraction” was introduced in Mulder  and further articulated in AskMSR . Several previous projects have attempted to automate the collection of information from the web with some success. Information extraction systems such as Google’s Froogle 1 , Whizbang’s flipdog 2
423617	2455	supervised methods and bootstrap learning techniques. KNOWITALL uses a novel form of bootstrapping that does not require any manually tagged training sentences. Other bootstrap IE systems such as  still require a small set of domain-specific seed instances as input, then alternately learn rules from seeds, and further seeds from rules. Instead, KNOWITALL begins with a domainindependent set
423617	2456	computes semantic tags for a large number of Web pages. KNOWITALL’s task is to automatically extract the knowledge that SemTag takes as input. KNOWITALL was inspired, in part, by the WebKB project  and its motivation. However, the two projects rely on a different architecture and very different learning techniques. Most important, WebKB relies on supervised learning methods that take as input
423617	2457	computes semantic tags for a large number of Web pages. KNOWITALL’s task is to automatically extract the knowledge that SemTag takes as input. KNOWITALL was inspired, in part, by the WebKB project  and its motivation. However, the two projects rely on a different architecture and very different learning techniques. Most important, WebKB relies on supervised learning methods that take as input
423617	2459	bound to predicate arguments. Keywords are formed from literals in the rule, and are sent as queries to search engines. The features chosen are combined using a “naive Bayesian” probability update . Given n observed features f1 . . . fn, which are assumed conditionally independent, the Assessor uses the following equation to calculate the expected truth of an atomic formula ?: P (?|f1, f2, .
423617	2459	effect. Since the naive Bayes formula is notorious for producing polarized probability estimates that are close to zero or to one, the estimated probabilities are often inaccurate. However, as  points out, the classifier is surprisingly effective because it only needs to make an ordinal judgment (which class is more likely) to classify instances correctly. Similarly, our formula produces
423617	2462	are not designed to compile large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements
423617	2463	{“,”}“and other” NP2 NP1 {“,”}“including” NPList2 NP1 “is a” NP2 NP1 “is the” NP2 “of” NP3 “the” NP1 “of” NP2 “is” NP3 Some of our rule templates are adapted from Marti Hearst’s hyponym patterns  and others were developed independently. To see how these patterns can be used as extraction rules, suppose that NP1 in the first pattern is bound to the name of a class in the ontology. Then each
423617	2464	avoids repeated passes over a string by compiling the pattern into a finite-state machine. XScan, our incremental XML query processing algorithm, uses a similar compilation scheme onsstreaming data . Finally, RETE matching has been successfully applied to the problem of matching large sets of production rules against a working memory database . In our case, we could compile rules into a
423617	2465	scaling to the Web due to the diversity of text styles and genres on the web and the prohibitive cost of creating an equally diverse set of handtagged documents. Wrapper induction systems  are able toslearn extraction patterns with a small amount of training, but operate only on highly structured documents and cannot handle the unstructured text that KNOWITALL exploits. The TREC
423617	21199	scaling to the Web due to the diversity of text styles and genres on the web and the prohibitive cost of creating an equally diverse set of handtagged documents. Wrapper induction systems  are able toslearn extraction patterns with a small amount of training, but operate only on highly structured documents and cannot handle the unstructured text that KNOWITALL exploits. The TREC
423617	2466	work and a concise summary of our contributions. 1.1 Previous Work Whereas search engines locate relevant documents in response to a query, web-based Question Answering (QA) systems such as Mulder , AskMSR , Radev’s work , and others locate potentially relevant answers to individual questions but are not designed to compile large bodies of knowledge. Much of the previous work on
423617	2466	relies on the scale and redundancy of the web for an ample supply of simple sentences that are relatively easy to process. This notion of “redundancy-based extraction” was introduced in Mulder  and further articulated in AskMSR . Several previous projects have attempted to automate the collection of information from the web with some success. Information extraction systems such as
423617	2469	the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements corpus, and have difficulty scaling to the Web. These
423617	2470	systems extract information from relatively small corpora of newswire and newspaper articles, while KNOWITALL extracts information from the Web. As a result, top performing systems in TREC (e.g., ) focus on “deep” parsing of sentences and the production of logical representations of text in contrast with the lighter weight techniques used by KNOWITALL. Recent IE systems have addressed
423617	2471	our contributions. 1.1 Previous Work Whereas search engines locate relevant documents in response to a query, web-based Question Answering (QA) systems such as Mulder , AskMSR , Radev’s work , and others locate potentially relevant answers to individual questions but are not designed to compile large bodies of knowledge. Much of the previous work on Information Extraction (IE) has
423617	2473	supervised methods and bootstrap learning techniques. KNOWITALL uses a novel form of bootstrapping that does not require any manually tagged training sentences. Other bootstrap IE systems such as  still require a small set of domain-specific seed instances as input, then alternately learn rules from seeds, and further seeds from rules. Instead, KNOWITALL begins with a domainindependent set
423617	2474	are not designed to compile large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements
423617	2475	large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements corpus, and have difficulty
423617	403	large bodies of knowledge. Much of the previous work on Information Extraction (IE) has focused on the use of supervised learning techniques such as hidden Markov Models , rule learning , or Conditional Random Fields . These methods have typically been applied to small corpora such as a collection of news wire stories or the CMU seminar announcements corpus, and have difficulty
423617	2476	begins with a domainindependent set of generic extraction patterns from which it induces a set of seed instances. Another distinctive feature of KNOWITALL is its use of Turney’s PMI-IR methods  to assess the probability of extractions using “web-scale statistics”. This overcomes the problem of maintaining high precision, which has plagued bootstrap IE systems. Another system that uses hit
423617	2476	the Assessor uses a form of pointwise mutual information (PMI) between words and phrases that is estimated from web search engine hit counts in a manner similar to Turney’s PMI-IR algorithm . For example, suppose that the Extractor has proposed “Liege” as the name of a city. If the PMI between “Liege” and a phrase like “city of Liege” is high, this gives evidence that “Liege” is indeed
423617	2476	for “city of California” than there are for many obscure, but legitimate, cities. In order to compensate for this bias, we considered dividing by the frequency of the instance I. Following Turney , we compute the pointwise mutual information (PMI) between the candidate instance and a discriminator phrases as |Hits(D + I)| PMI(I, D) = (2) |Hits(I)| One potential problem with the PMI approach
423617	2477	that KNOWITALL exploits. The TREC conference has introduced a “list” track where answering a question requires finding all instances of a specific subclass such as “10 movies starring Tom Cruise” . One key difference between the TREC systems and KNOWITALL is that the TREC systems extract information from relatively small corpora of newswire and newspaper articles, while KNOWITALL extracts
2482	2483	However, they do not suggest basis functions and do not implement their method. Primal-Dual representations of the American option problem allow both an upper and lower bound to be calculated . However, the upper bound involves calculating an expectation, which has been done using another Monte Carlo simulation. This simulation on simulation is computationally expensive. Glasserman and
2482	2483	asymptotically to the true value. This method is computationally demanding, but the speed has been improved by using low–discrepancy sequences in . Duality approaches have been suggested in , however these methods tend to converge even more computationally expensive than the low biased estimators. We view and as deterministic functions. Define both M0 = 0 and Each summand is ? + i (.)
2482	2483	basis functions that are martingales for assets driven by stochastic processes other than geometric Brownian motion. A systematic comparison of the duality methods of Andersen and Broadie , Glasserman and Yu , Kogan and Haugh , Rogers , using QuantLib for the implementation, would allow an informed evaluation of their relative merits to be made. Andersen and Broadie
2482	2484	options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include parameterization of the optimal exercise boundary , a quantization tree algorithm , wavelets [21,
2482	2486	European style financial derivatives was first suggested in . Progress in using simulation methods to price American style options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include
2482	2487	sequences . Other methods include parameterization of the optimal exercise boundary , a quantization tree algorithm , wavelets , irregular grid approximations , and sparse grid methods . Regression methods include . The regression method of Longstaff and Schwartz  has proved particularly popular, due to its accuracy and simplicity. The
2482	2497	European style financial derivatives was first suggested in . Progress in using simulation methods to price American style options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include
2482	2497	the American option value. The simplest is simulating each path and choosing the optimal exercise time. This simple estimate uses future information and is biased high. The stochastic mesh method  gives lower and upper bounds which both converge asymptotically to the true value. This method is computationally demanding, but the speed has been improved by using low–discrepancy sequences in
2482	2498	options was stimulated by Tilley . This and other early methods  are reviewed in Boyle et al. . Since then the stochastic mesh method  has been made more efficient  and has been modified to use low-discrepancy sequences . Other methods include parameterization of the optimal exercise boundary , a quantization tree algorithm , wavelets [21,
2482	2498	step and initial continuation value ˆV0 ? max(h0(X0), ?0) the estimated option value ? ? ?1 .sN. P. Firth 3.2. Regression Later. Glasserman and Yu  interpret the method of Broadie et al.  as a regression method, but with the regression taking place one time step ahead. They refer to this as ‘regression later’. To do this Glasserman and Yu  introduce the idea of using basis
2482	2499	International plc. for funding this research. The author would also like to thank Dr. W. T. Shaw, Dr. B. Hambly, A. Dickinson, and particularly Dr. R. A. Stalker Firth. 1 analytical work includes sUpper Bounds for American Option Prices The rest of this paper is structured as follows: We formulate the American option pricing problem, following , as an optimal stopping problem whose
2482	2502	have been found for American option prices 1 . Therefore much work has been done pricing American options numerically. Early examples include finite differences  and the binomial lattice . Grid based methods work well for single asset options, and have been extended to higher dimensions . However, these methods suffer from the ‘curse of dimensionality’, as they become
2482	2507	in . The relationship between the number of basis functions and number of paths required is investigated by Glasserman and Yu . For a comparison of simulation approaches see Fu et al.  or the recent book by Glasserman . Glasserman and Yu  investigate the relative merits of ‘regression now’ versus ‘regression later’. ‘Regression now’ involves using basis functions defined
2482	2514	However, they do not suggest basis functions and do not implement their method. Primal-Dual representations of the American option problem allow both an upper and lower bound to be calculated . However, the upper bound involves calculating an expectation, which has been done using another Monte Carlo simulation. This simulation on simulation is computationally expensive. Glasserman and
2482	2514	asymptotically to the true value. This method is computationally demanding, but the speed has been improved by using low–discrepancy sequences in . Duality approaches have been suggested in , however these methods tend to converge even more computationally expensive than the low biased estimators. We view and as deterministic functions. Define both M0 = 0 and Each summand is ? + i (.)
2482	2514	for assets driven by stochastic processes other than geometric Brownian motion. A systematic comparison of the duality methods of Andersen and Broadie , Glasserman and Yu , Kogan and Haugh , Rogers , using QuantLib for the implementation, would allow an informed evaluation of their relative merits to be made. Andersen and Broadie  produces an upper bound by generating an
2482	2515	of the optimal exercise boundary , a quantization tree algorithm , wavelets , irregular grid approximations , and sparse grid methods . Regression methods include . The regression method of Longstaff and Schwartz  has proved particularly popular, due to its accuracy and simplicity. The paper explains the method, but an introduction to the method is also
2482	2515	Convergence results are given in . Algorithm 1. We make the method concrete by describing the computation in algorithm 1. This describes the Least Squares Monte Carlo (LSMC) method of  in this setting, as well as the method developed in . In Glasserman and Yu  discounting is not explicitly considered. We explicitly denote the discount factor between times i and i+1 by
2482	2515	8.508 7.700 8.488 (.024) 8.495 (.023) 8.495 (.023) Table 1: Comparison of ‘regression now’ and ‘regression later’ with the results for American style put option in Table 1 in Longstaff and Schwartz . X0 is the initial asset price, ? is the volatility of returns, and T is the number of years until the option expiry date. The continuously compounded short–term interest rate is 0.06, and the
2482	2515	‘regression later’ provides good estimates for the option value. The results in table 1 compare ‘regression now’ with ‘regression later’, following the results in table 1 in Longstaff and Schwartz . Only in-the-money paths are used in the regression . Both regressions use the first three martingale basis functions described above. We find that ‘regression later’ does indeed provide a good
2482	2515	and Future Directions. The numerical results show that regression estimates using ‘regression later’ are more accurate than existing methods using ‘regression now’, such as Longstaff and Schwartz . However, the ‘regression later’ method depends, through condition (C3), on the availability of basis functions that are martingales. We have obtained results for single asset options on underlying
2482	2524	of the optimal exercise boundary , a quantization tree algorithm , wavelets , irregular grid approximations , and sparse grid methods . Regression methods include . The regression method of Longstaff and Schwartz  has proved particularly popular, due to its accuracy and simplicity. The paper explains the method, but an introduction to the method is also
2482	2524	optimal exercise strategy the estimate has mixed bias. Estimators with definite low bias and definite high bias will be given in sections 6 and 7, respectively. Convergence results are given in . Algorithm 1. We make the method concrete by describing the computation in algorithm 1. This describes the Least Squares Monte Carlo (LSMC) method of  in this setting, as well as the method
2550	2551	of the searches outlined above, but have significant limitations that prevent their extension to all domains. Recently, Nene and Nayar proposed a method for effective NN search in high dimensions . Their method, while providing a good search time, generates a static structure that prevents the insertion of additional elements without an expensive tree rebuilding. Such a solution would not be
2550	2551	reason to use a tree structure is the ability to add elements. If an a priori data analysis is always possible, and insertions and deletions are infrequent, a structure such as that presented in  is likely a better choice than a k-d tree. Nearest neighbor searches generally begin with the construction of a “clipping window,” which defines a region of the tree space to search. For example, a
2550	2551	an initial value for ? based on the probability that at least one element will lie within the hyper-sphere around ? bounded by ?. The method for choosing an appropriate initial ? is outlined in , and has proven successful in their fixed technique. While the results of those experiments are beyond the scope of this paper, we present two critical findings from our additional work on the
2550	2552	hashing and indexing, various types of trees, and many hybrid and novel approaches. Proposed tree solutions alone include k-d, B+, R+, BBD, VAMSplit k-d, red-black, Patricia and other variants . Tree-based search strategies are popular for many reasons, including, for n cases, O(log n) search and insertion time, O(n log n) construction time, and reasonable space requirements. Tree
2550	2552	but not exhaustively, since its introduction in 1975 . Friedman, Bentley, and Finkel presented good search and construction algorithms for NN searching with kd trees as early as 1977 . More recently, Arya and Mount have presented refined search tactics that have been especially effective for approximate searches . The refinements present here are effective for all thessearch
2550	2552	of the tree. It is this absence of information that the tracking node solves for. Other complicated methods have been proposed for tracking, primarily variants of the Bounds-Within-Ball (BWB) test . Our tracking nodes achieve the same effect as the BWB test with just one simple structure and a single comparison at each branch. Also, it is our claim that the code for the tracking node solution
2550	2553	hashing and indexing, various types of trees, and many hybrid and novel approaches. Proposed tree solutions alone include k-d, B+, R+, BBD, VAMSplit k-d, red-black, Patricia and other variants . Tree-based search strategies are popular for many reasons, including, for n cases, O(log n) search and insertion time, O(n log n) construction time, and reasonable space requirements. Tree
2550	2553	algorithms for NN searching with kd trees as early as 1977 . More recently, Arya and Mount have presented refined search tactics that have been especially effective for approximate searches . The refinements present here are effective for all thessearch types outlined and provide the maximum speed-up for exact NN queries. 2 Traditional Search Many papers have examined the best ways to
2550	2559	types of search are of interest to different disciplines. Nearest neighbor (NN) search is important to many case-based reasoning (CBR) as well as various classification and matching problems . Approximate nearest neighbor search is important to many AI systems, and in systems where there is an acceptable trade-off between exact answers and performance. K-NN and other multivariate range
2550	2559	hashing and indexing, various types of trees, and many hybrid and novel approaches. Proposed tree solutions alone include k-d, B+, R+, BBD, VAMSplit k-d, red-black, Patricia and other variants . Tree-based search strategies are popular for many reasons, including, for n cases, O(log n) search and insertion time, O(n log n) construction time, and reasonable space requirements. Tree
2550	2559	method, but present our method only for future comparisons. In fact, our search and pruning techniques work equally well with a tree that has been built using one of the bulk-loading techniques  as with our incremental construction technique, outlined below. We start with a randomized set of elements of dimensionality d, which are added to the tree one at a time. The single parameter
2550	2559	of the tree. It is this absence of information that the tracking node solves for. Other complicated methods have been proposed for tracking, primarily variants of the Bounds-Within-Ball (BWB) test . Our tracking nodes achieve the same effect as the BWB test with just one simple structure and a single comparison at each branch. Also, it is our claim that the code for the tracking node solution
2550	2559	has dramatic results, as has been shown previously. But a critical aid to DFBB search, appropriate path ordering, has been neglected in the same studies. The critical code fragment presented in  demonstrates an algorithm for ordering that examines tree leaves in a static order. The algorithm operates independently of freely available information from tracking nodes. Such information can be
2562	2566	On the other hand we claim that agent communication languages can be successfully used for this task, especially if they provide support for fault tolerant communication primitives as suggested in . In the following we outline the main features of our Knowledge Level OSA. We assume knowledge level agents , that is, they should concern with the use, request and supply of knowledge without
2562	2566	as in IRS-II. Agent Communication. Agents access services and communicate each other using a fault tolerant Agent Communication Language (ACL) which provides one-to-one and one-to-many primitives . We assume an asynchronous communication and a reliable message passing, i.e., whenever a message is sent it must be eventually received by the target agent (thus we do not handle communication
2562	2566	Knowledge Base (KB) with a new capability (for example adding a task Tk), it forwards this information to all the other agents, for example by means of an agent primitive register(myself, Tk) as in . We also assume that agents communicate only their capabilities, not their PSMs. Therefore each agent knows all the problems which can be solved by the OSA, but it knows how to solve a task only if
2562	2566	language or knowledge representation formalisms it adopts, but it reacts to a well defined protocol based on the standard primitives of an agent communication language. The primitives of this ACL  can be divided into four categories as shown in Table 1. Contents based services requests are realized as one-to-many primitives: whenever an agent needs a given service which solve a task T it can
2562	2566	from another agent in the network. At the facilitator level some primitives, like ask-one and tell, have an extra parameter used to identify the right answer of the message, as specified in . To deeply understand how the communication is implemented, suppose that agent A would communicate with agent B. Suppose that the selected communication primitive is the ask-one(B, A, p). Agent B
2562	2568	and any IRS-II service automatically appears as a standard web service to other web service infrastructures. 2.1 IRS-II Approach Semantic Web Services are described by means of the UPML framework  developed within the IBROW project . Figure 1 shows how a single knowledge based application would be UBLCS-2004-14 2sdescribed in UPML terms. The UPML framework partitions knowledge into four
2562	2570	A significant challenge to address is the design of distributed reasoning infrastructures tightly integrated with current Internet components and technologies that would allow semantic Web Services  to be exploited in the large. While new standards are emerging such as the Ontology Web Language (OWL) , and the community is currently addressing many central issues as the design of a Web
2562	2570	facilities . We describe here a Knowledge-Level extension of this architecture which extends Web servers with the IRS functionalities and agents’ capabilities providing semantic Web Services . Agents becomes the main building block of this architecture. They are geographically distributed (as Web servers are) and can provide a set of semantic Web Services to the outside world which
2562	2570	A significant challenge to address is the design of distributed reasoning infrastructures tightly integrated with current Internet components and technologies that would allow semantic Web Services  to be exploited in the large. While new standards are emerging such as the Ontology Web Language (OWL) , and the community is currently addressing many central issues as the design of a Web
2562	2570	facilities . We describe here a Knowledge-Level extension of this architecture which extends Web servers with the IRS functionalities and agents’ capabilities providing semantic Web Services . Agents becomes the main building block of this architecture. They are geographically distributed (as Web servers are) and can provide a set of semantic Web Services to the outside world which
2562	2572	possible role for agents is to enhance the capabilities of servers using their intelligence to provide more complex services and behaviors. For example the Internet Reasoning Service (IRS-II)  is a knowledge based server which supports the publication, location, composition and execution of semantic Web Services. The IRS provides facilities to achieve complex tasks, if all the needed
2562	2572	of the proposed OSA. Then in Section 5 we present a Tomcat based implementation of the OSA. The final Section of the paper contains our conclusions and highlights our future work. 2 IRS-II IRS-II  is a framework and implemented infrastructure which is aimed at supporting the publication, location, composition and execution of semantic Web Services. IRS-II has three main classes of features
2562	2572	1. The UPML framework. The application of the UPML framework to semantic Web Services provides a number of advantages. In particular, the explicit separation between tasks and methods provides : ??? A task-based mechanism for aggregating services. It is possible to specify service types (i.e. tasks) independently from specific service providers. • A basic model for dealing with ontology
2562	2712	current Internet components and technologies that would allow semantic Web Services  to be exploited in the large. While new standards are emerging such as the Ontology Web Language (OWL) , and the community is currently addressing many central issues as the design of a Web Service Modelling Ontology (WSMO) , there is still not a widely accepted architecture for the underlying
2576	2728	performance, then it is difficult to know whether to attribute differences in performance to project activities or to the effects of pre-existing village characteristics. For example, Pitt et al. (1993) describe a case in Indonesia that showed that villages covered for several years under a major family planning program actually had higher fertility rates than those outside of the program. One
2576	2728	there was insufficient data to match them in a more rigorous manner. A nonexperimental design was used instead. Several nonexperimental approaches are possible. One way comes from the Pitt et al. (1993) study of Indonesian poverty programs referred to in Section 1. Because those programs were intentionally located in the poorest areas, purely cross-sectional analysis would have suggested
2576	2728	and if program placement is independent of treatment effect. Since this is unlikely to be the case, the estimated effect of the treatment is likely to be biased. In the case analyzed by Pitt et al. (1993), the project coefficient b in equation 1 had a negative coefficient; i.e., the analysis suggested incorrectly that the project contributed to poverty. Pitt et al. approached this problem by
8918831	2634	provided by rating agencies to be useful and have incorporated it into their investment decisions as indicated by movements in yields on sovereign bonds in response to changes in credit ratings (Cantor and Packer 1996, Clark and Lakshmi 2003, Larrain, Reisen, and von Maltzan 1997). The Basel II capital accords may further increase the importance that markets attach to the assessments of credit rating agencies.
8918831	2634	ratings are influenced by the same fundamentals as investors’ private information, ratings will still affect investors’ decisions. This is consistent with the findings of several empirical studies (Cantor and Packer 1996, Larrain, Reisen, and von Maltzan 1997). Credit ratings have this effect in part because the revision of the rating agency’s signal in response to changes in fundamentals causes investors to revise
12120132	2663	increments process and then generate a realization of fBM by calculating the cumulative sums process of fractional Gaussian noise, such as those of Levinson (Coeurjolly 2000) and of Wood and Chan (Dietrich and Newsam 1997; Wood and Chan 1994). These methods take advantage of the fact that the increment process is a stationary one and its covariance matrix is a Toeplitz matrix. Finally, there are methods that rely on
12120132	2663	and Bottom Panels: Realizations of Fractional Stable Motions for ? = 1.2 and ? = 0.6. In all cases, Left Panel H = .1, Middle Panel: H = .5 and Right Panel H = .9 is a m × m circulant matrix (Dietrich and Newsam 1997). Let SY n (t), 0 ? t ? 1 be a stepwise constant function such that SY n (k/n) = ?k i=1 Ym,i, 0? k ? n. In the sequel S ? n(t), SX n (t) and SU n (t) etc are similarly defined. Let also ? =[(?? +
12120132	2663	Let a = (a1,...am) and e = (?0,?1?m,?2?m,...,??1) and let ? =FFT(a) and ? =FFT(e) be their corresponding discrete Fourier transforms. Then due to the fact that the m × m matrix A is circulant (Dietrich and Newsam 1997) we have Y =IFFT(v), where v = (v1,...,vm) and vj = ?j ?j . (7) The proposed algorithm can be summarized as follows: Step 1: Compute FFTA; i.e. the FFT of the vector  Step 2: Compute
12120132	2673	computing the wavelet coefficients corresponding to wavelet transform of fBM and then synthesize fBM through the inverse wavelet transformation, while the random midpoint displacement method (Leland et al. 1994) progressively subdivides the interval over which a sample path is generated and at each subinterval a Gaussian displacement is used to determine the value of the process at the midpoint. Other
12120132	2675	the behavior of such networks (Park and Willinger 1995; Yuksel et al. 2000). One then must be able to generate traffic that exhibits the necessary temporal behavior over large time scales (Norros 1995; Norros, Mannersalo and Wang 1999). One of the simplest models exhibiting long-range dependence is fractional Brownian motion (fBM) introduced by Kolmogorov (1940) and further developed by
12120132	2678	the number of points being generated, see Coeurjolly (2000), or the fact that due to the approximate nature of the method, the quality of the generated process can not be accurately assessed (e.g. Paxson 1997). However, the simulation study of Coeurjolly (2000) suggests that the method of Wood and Chan (1994) with a time complexity of O(n log n) and memory complexity of O(n) performs satisfactorily. Our
12120132	2679	of EECS University of Michigan Ann Arbor, MI 48105, U.S.A. George Michailidis Department of Statistics University of Michigan Ann Arbor, MI 48105, U.S.A. porting a heavy tailed assumption (Paxson and Floyd 1995) backed by theoretical work that explains how the former assumption induces through an appropriate mechanism long range dependence in the aggregate traffic (Konstantopoulos and Lin 1998).
8918845	2691	the challenge of creating a better model. Bounded Optimality Russell et al. define a model of rationality they believe is appropriate for determining if an AI system exhibits rational behaviour(Russell & Subramanian 1993). To motivate the approach taken four possible definitions of rationality are given. The first three are currently used in AI and the fourth is the proposed definition upon which the paper is
8918845	2691	successful in many applications. An agent function is a mapping from an agent’s perceptions to its actions. An agent’s program is what generates its actions. The agent function is defined in (Russell & Subramanian 1993) such that the action may be a null action if the agent is still calculating. Russell et al. construct asframework for designing bounded optimal agents in specific environments. A bounded optimal
2702	2703	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	2704	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	1939	and on caches employing the drowsy cache and cache decay technique for energy savings. In order to perform the experiments, weshave built a simulation framework on top of the SimpleScalar simulator  that permits the injection of soft errors into the cache and also allows the modeling of different cache interleaving schemes and error detection/correction schemes. Our experimental results show
2702	1939	the different soft error cases to provide a realistic evaluation. Qss2.2 Soft Error Injection In this work, we implement a random soft error injection in the cache memories using the SimpleScalar  simulator. A random variable generated by the simulator along with the SERs provided in Table 1 is used to determine whether a soft error happens and the number of bits influenced by the error. In
2702	1939	employing the commonly used write-back approach. 3.3 Error Behavior of Different Approaches We implement the soft error injection, drowsy cache, and cache decay in the simulator SimpleScalar 3.0 . Our benchmarks are from SPEC CINT2000 . Each benchmark is first fast forwarded 300 million instructions and then simulated 1 billion instructions. Table 2 gives the total number of cycles taken
2702	2706	voltages of transistors have reduced. Though lowering supply voltage helps to reduce the dynamic power consumption, the consequent decrease in threshold voltage has increased the leakage energy . As cache memories constitute a significant portion of the transistor budget of current microprocessors, leakage reduction for cache memories has been of particular importance. It has been
2702	2707	are not accessed for a long period of time. Different designs of low power cache memories can potentially have different immunity to soft errors due to supply voltage changes and circuit structure . These differences in soft error vulnerability are important as they influence the complexity of error detection and correction circuitry employed in on-chip memories. We believe that the issues of
2702	2707	by Equation 1, error rates for low power mode was fixed as one order of magnitude higher than the corresponding high power mode, as the error rate is exponential dependent on the supply voltage . During read and write operations, the error can manifest in either the SRAM cell or the peripheral circuits like sense amps. Also, when the wordline in SRAM is asserted, charge sharing occurs and
2702	2708	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	2708	optimizations: drowsy cache and cache decay. 3.1 Drowsy Cache Drowsy cache is based on controlling the leakage by using dynamic voltage scaling (DVS) while using the standard 6T SRAM cell structure . This method makes use of the fact that to retain a value in the SRAM cell, the source voltage of the cell can be just about 1.5 times of Vt. Thus, for a 70nanometer technology based SRAM cell that
2702	2708	be reduced to up to 0.3V when accesses are not required while still retaining the data. Substantial leakage energy saving can be gained when placing the cache line in this low voltage drowsy state . However, one cycle penalty is incurred when accessing a drowsy cache line, as the supply rails have to be restored to 1.0V before a read or write operation. For voltages less than 0.3V it was
2702	2710	the impact of leakage energy optimizations on the soft errors. Next, we investigate the energy implications due to providing protection against soft errors. There have been several efforts  at the architectural level reducing the cache leakage energy when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the
2702	2712	when it is idle. Drowsy cache  controls the leakage of memory cells by using dynamic voltage scaling (DVS) while maintaining the values of cache blocks. In contrast, the cache decay scheme  completely turns off the cache blocks, losing the stored data, when they are not accessed for a long period of time. Different designs of low power cache memories can potentially have different
2702	2712	capacitance is also a function of the supply voltage). Thus, the use of DVS provides an interesting opportunity for trade off between leakage reduction and soft error immunity. 3.2 Cache Decay In , Kaxiras et al. present a leakage energy reduction technique, called Cache Decay, for cache memories. Cache Decay exploits the temporal behavior of cache blocks to reduce leakage consumption. This
2702	2714	reduction for cache memories has been of particular importance. It has been estimated that leakage energy accounts for 30% of L1 cache energy and 70% of L2 cache energy for a 0.13 micron process . Another consequence of the technology scaling is the smaller supply voltages and reduced capacitive values of the circuit nodes. This has raised reliability concerns due to the increased
2702	2716	Scheme 1, 2, and 3). in L1 data cache. This method keeps the blocks in L1 cache in clean mode, but it significantly increases the number of L2 cache accesses. Therefore, a coalescing write-buffer  is always employed with write-through cache to merge the writebacks to L2 cache. But the total number of L2 cache accesses of write-through cache is still significantly larger than that of
2702	2717	scheme for all data. This is also different from other schemes provided for energy-efficient data protection through duplication of data , or protecting only the frequently used cache lines . The approach involving cache line duplication is orthogonal to our approach and has been investigated using a cache using both ECC and parity. In contrast, our work focuses on providing different
2702	2719	commercial caches that use a uniform error protection scheme for all data. This is also different from other schemes provided for energy-efficient data protection through duplication of data , or protecting only the frequently used cache lines . The approach involving cache line duplication is orthogonal to our approach and has been investigated using a cache using both ECC and
2721	2722	Communications Systems Laboratory Nokia Research Center Mountain View, CA 94303 Email: charliep@iprg.nokia.com destinations for which traffic exists. A couple of examples of such protocols are DSR  and AODV  . For the purposes of this paper, the main feature of these protocols is that a node uses a series of network-wide all-node broadcasts to disseminate its route request to discover a
2721	2722	Subsequent work  provides more advanced algorithms and more sophisticated methods to handle node movement, shutdown and power-on.  also suggests a way to run Dynamic Source Routing (DSR)  over a connected dominating set of the network. The dominating set of a network is a subset of nodes such that each node is either in the dominating set, or is adjacent to a node in the dominating
2721	2289	Systems Laboratory Nokia Research Center Mountain View, CA 94303 Email: charliep@iprg.nokia.com destinations for which traffic exists. A couple of examples of such protocols are DSR  and AODV  . For the purposes of this paper, the main feature of these protocols is that a node uses a series of network-wide all-node broadcasts to disseminate its route request to discover a route to an
2721	2724	algorithm suggested in  uses a greedy approach to minimize the number of repeater nodes while trying to achieve the desired MPR COVERAGE. The Topology Broadcast Reverse Path Forwarding (TBRPF)  routing mechanism uses broadcasts and limits flooding through a packet cache similar to AODV. It is based on , which has the potential to limit the default blind flooding but does not have any
2721	2727	not originate any. Thus, no other node will create a route through the passive member. There have been a few proposals for establishing a virtual backbone over which routing takes place (e.g., , ). In  a spine is used for all communications, while in  the backbone is used as a secondary route in case shortest-pathsroutes fail. These approaches assume a perfectly scheduled MAC layer.
2721	2728	is used for all communications, while in  the backbone is used as a secondary route in case shortest-pathsroutes fail. These approaches assume a perfectly scheduled MAC layer. Subsequent work  provides more advanced algorithms and more sophisticated methods to handle node movement, shutdown and power-on.  also suggests a way to run Dynamic Source Routing (DSR)  over a connected
2721	2728	in the dominating set. Obtaining the minimum connected dominating set of a graph is known to be NP-hard   even when the complete network topology is available. Our work is distinct from  and OLSR’s MPR scheme in that we only use dominating sets for flooding control, not packet routing, and we construct a more robust connected dominating set through several heuristics. In
2721	2729	subset of nodes such that each node is either in the dominating set, or is adjacent to a node in the dominating set. Obtaining the minimum connected dominating set of a graph is known to be NP-hard   even when the complete network topology is available. Our work is distinct from  and OLSR’s MPR scheme in that we only use dominating sets for flooding control, not packet routing, and we
2721	2731	describes how DP is applied to the forwarding of route request (RREQ) packets in AODV. To facilitate DP, each node needs two-hop neighbor information. We use a neighbor-exchange protocol called NXP . Applying DP directly to AODV does not result in better performance from that obtained by the conventional AODV in many situations, due to the loss of RREQ packets. We present several heuristics to
2721	2732	Section III presents the results of our simulation performance analysis between conventional AODV, AODV with DP, and AODV-DS. The results from our analysis are consistent with the findings for OLSR , which show that multi-point relays (MPR) significantly reduce the protocol overhead, but also results in a lower route availability and packet delivery rates, except for one topology. Our results
2721	2733	the destination from either an active route or broken route, we add the listed next-hop to the forwarder list. III. SIMULATION RESULTS We implemented AODV draft 10 and Dominant Pruning in GlomoSim . We did not use the version of AODV distributed with Glomosim, but rather made a new version to conform with recent AODV specification. This section first reviews our implementation of AODV, then
2721	2734	packets – broadcast and unicast – are jittered by an exponential delay with mean value of 10 milliseconds, with a minimum of 1 ms and a maximum of 100 ms. Our simulations generally replicate  for a 50-node network. We have scenarios with 10 source nodes and 30 source nodes, transmitting 4 packets/sec CBR traffic of 512 byte UDP packets. Nodes begin transmitting at 50 seconds plus an
2721	2734	number seeds. Each data point represents the mean over the 10 trials. We show 95% confidence intervals all graphs except some cumulative distribution plots. Our performance metrics are similar to . We measure the delivery ratio of CBR packets received to packets transmitted, the latency of received CBR data packets, and the control overhead. The control overhead is the ratio of the total
2721	2734	200 300 500 700 900 pause time aodv-1s aodv-2s dp-1s dp-2s aodv-ds-1s aodv-ds-2s Fig. 4. Average # RREQs transmitted, 30 sources AODV-DS is close to AODV. When we compare our results with those in , there are three significant differences between AODV in  and our implementation of AODV.  uses link-layer feedback for failed links. When the MAC layer fails an RTS/CTS/ACK handshake and
2721	2734	retries, it notifies AODV that a specific packet failed. AODV immediately breaks the link.  does not use Hello packets, which we rely on to detect link failures and exchange two-hop data.  broadcasts all RERR packets while our implementation sometimes unicasts them. In terms of delivery ratio, our simulations of AODV showsfraction of RREQ transmissions 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3
2721	2734	higher delivery ratio. For 30 source nodes, our work shows between 68% and 89% delivery for conventional AODV and between 75% and 92% delivery for AODV-DS. For the control load (“routing load” in ), one would expect a large difference because we used Hello packets. For 10 source nodes,  reports a routing load of 1.0 or less, dropping towards zero as the pause time increases. For 30
2796	2961	Energy curve of a signal in decibel. 4.1.6 Pitch Inherits from Sequence, template parameter set to double Subclasses: StPitch Pitch of a signal. Computation of pitch is performed by ESPS library . 4.1.7 StPitch Inherits from Pitch Subclasses: none 61sPitch of a signal in semitones. 4.1.8 LPCraw Inherits from Sequence, template parameter set to double Subclasses: none This class provides LPC
2796	2806	tune For simulated annealing, for example, parallelization can be performed in various ways: simultaneous independent annealing , speculative decisions , periodically interacting searches . As the training set is composed of a set of separate signal files that can be processed separately without any contradiction the latter has been chosen. Every tuning strategy (annealing, genetic,
2796	2814	Fujisaki’s parameter from natural pitch contour. Other systems were developed to identify syllable boundaries in speech signals. This segmentation can be interesting both for speech recognition  and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also
2796	2815	average over nine critical frequency bands every 10ms is also cosidered. The resulting vector was concatenated with log-RASTA features and used as input to a multilayer perceptron. Greenberg et al.  introduced the speech modulation spectrogram, a system for the research of invariant features related to frequency portions of the speech spectrum distributed across critical band-like channels.
2796	2816	in phones, but they will be computationally more expensive than syllabification and furthermore syllables are, according to many author, pointed as an important unit for speech processing , because many linguistic phenomena (such as co-articulation) are confined into syllables. Pitch stylizations of are provided in a number of way to take in account or not breaks at pauses and to
2796	2816	as source of useful information. Recent ASR systems try to use some prosodic feature to improve their analysis. Most of their effor only focused on syllable segmentation and recognition. Greenberg  showed how syllables can be important in speech recognition according to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system.
2796	2818	and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also developed . A possible use of such a system is to provide a new feature useful to disambiguate some utterances. The following paragraphs show more in detail some of these systems. 1.3 Syllables
2796	2818	can be used to distinguish among them. 26sStress is tied both to duration of syllable, energy and pitch. Various efforts were devoted to automatic stress detection. Hieronymus and Williams  used a combination of these quantities to locate stressed syllables, while Silipo and Greenberg  showed a comparison between a wide range of combinations of these quantities. They found better
2796	2825	parameter from natural pitch contour. Other systems were developed to identify syllable boundaries in speech signals. This segmentation can be interesting both for speech recognition  and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also
2796	2825	27sChapter 2. ALGORITHMS 2.1 Syllabification 2.1.1 Why syllables? Syllable segmentation is important in APA for tempo determination, but it can also be used as basic unit in recognition . The definition of what one call “syllable” is controversial. Some author bound syllables to jaw movement , to chest burst  or they consider syllables as the basic unit of speech
2796	2825	in phones, but they will be computationally more expensive than syllabification and furthermore syllables are, according to many author, pointed as an important unit for speech processing , because many linguistic phenomena (such as co-articulation) are confined into syllables. Pitch stylizations of are provided in a number of way to take in account or not breaks at pauses and to
2796	2834	it is also necessary to develop systems able to detect accent and phrase command from natural speech to mimic them in synthetic utterances. It was the goal of many efforts such those from Mixdorff  and Salvo Rossi et alii . The former work starts its analysis from the quadratic spline stylization of pitch contour. It was done in order to ignore short time disturbing variations and to
2796	2834	made via algorithmic methods, the problem of determining the right value of some parameters arise. In many previous work, authors simply refer to “empirically determined parameter” (e.g., see ), while when the number of parameters is quite small, say less than four, some author  tried to perform an exhaustive search among every plausible combination of parameter values.
2796	2836	by assigning a weight to each element within a set of spectral bands.An algorithm evaluating the shape of the loudness pattern (convex-hull) is used to find syllable boundaries. Pfitzinger et al.  process the speech signal in three steps: firs. They used a bandpass filter, then they compute the energy pattern using a short term window and finally they low-pass filtered this energy function.
2796	2836	many previous work, authors simply refer to “empirically determined parameter” (e.g., see ), while when the number of parameters is quite small, say less than four, some author  tried to perform an exhaustive search among every plausible combination of parameter values. Tools are shown in this work that help to fix these parameters with a more motivated approach.
2796	2836	function of the number of values tried in most cases the considered number of values is too small to find a reasonable solution. The exhaustive search was formerly used by Pfitzinger et alii , as they only had three parameters with an average of 4 values each: their search was then in a space of only 64 configurations. In the syllabification case, exhaustive search can not be used. 3.1
2796	2836	found marker is 13.4ms on both training and test corpora. Results in Italian seem to be close to those from other authors (cfr.  for boundary accuracy on spontaneous speech, and  on nuclei detection), while the training strategies seem to be roughly equivalent even with slightly better performances from simulated annealing, especially in the modified version. The comparison
2796	2836	be distributed differentially. The error rate obtained after training is quite equivalent to those obtainable with more complex techniques , or better than other algorithm based techniques . Further modifications to syllabification algorithm are required for English. Results about tone unit seem poor, but the agreement among human experts is not so far from the error rate
2796	2839	model Fujisaki and his co-workers proposed, between the 70s and the 80s, an analytical model describing the fundamental frequency (F0) variations . The model, tested on many languages , assumes that (in a logarithmic scale) the contour is the superposition of two contributions, namely a phrase component yp and an accent component ya, obtained by filtering two signals. The first
2796	2842	considering two kinds of factors: speech rhythm parameters and the auditory temporal integration of the slowest spectral components. Starting from the modulation spectrogram Shastri et al.  used a different kind of artificial neural network, the temporal flow network introduced by Watrous . With this tool they compute a function having local peaks at syllabic nuclei. The main
2796	2842	deviation of distances between reference markers and automatically found marker is 13.4ms on both training and test corpora. Results in Italian seem to be close to those from other authors (cfr.  for boundary accuracy on spontaneous speech, and  on nuclei detection), while the training strategies seem to be roughly equivalent even with slightly better performances from simulated
2796	2842	number of deletion and insertion, basically errors can be distributed differentially. The error rate obtained after training is quite equivalent to those obtainable with more complex techniques , or better than other algorithm based techniques . Further modifications to syllabification algorithm are required for English. Results about tone unit seem poor, but the agreement
2796	2844	The main reason to provide a TCL/TK interface to APA is the need of a Graphical User Interface (GUI). In place of writing a new one, it was chosen to use Wavesurfer as GUI for APA. WaveSurfer  is an Open Source tool for sound visualization and manipulation. It has been designed to suit both novice and advanced users. WaveSurfer has a simple and logical user interface that provides
2796	2846	to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for automatic prosodic analysis Many systems have already been developed to analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main
2796	2847	analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main efforts in prosodic analysis were spent on intonation modelization. Examples are the Tilt Model , that directly detect those pitch variations related to linguistic events and give their description; while the MOMEL algorithm performs a stylization of the pitch contour where non-significant
2796	2847	which affect the pitch contour but are not important in this sense (e.g. segmental perturbations). Examples of an Intonation Model for which some automatic system is available are: the Tilt model  from Taylor, Fujisaki model , the Hirst model . The main desired properties of such representations are: • The representation should be as compact as possible, with few degrees of freedom.
2796	2847	amenable to automatic analysis and synthesis, if no care is devoted linguistic relevance of the representation. 1.4.1 Tilt model The basic unit in the Tilt model is the intonational event. Taylor  found two types of intonational events: pitch accents and boundary tones. Pitch 19saccents are pitch excursions associated with syllables which are used to give some degree of emphasis to a
2796	2848	known as tilt parameters, are determined from examination of the local shape of the event's F0 contour. The tilt model is built on a simpler model, the Rise/Fall/Connection (RFC) model. In the RFC  model, each event is modeled by a rise part followed by a fall part. Each part has an amplitude and duration, and two parameters are used to give the time position of the event in the utterance and
2796	2850	to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for automatic prosodic analysis Many systems have already been developed to analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main
2796	2855	this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for automatic prosodic analysis Many systems have already been developed to analyze some prosodic feature of speech. Most of them focused on just one of those aspects. The main efforts
2796	2856	analysis. Most of their effor only focused on syllable segmentation and recognition. Greenberg  showed how syllables can be important in speech recognition according to this scheme Wu et alii , tried to incorporate information from syllable-length time scale in a conventional ASR system. Some other author are also trying to use other prosodic feature . 1.2 Systems for
2796	2856	Fujisaki’s parameter from natural pitch contour. Other systems were developed to identify syllable boundaries in speech signals. This segmentation can be interesting both for speech recognition  and prosodic analysis because syllables can be used as the main unit for tempo evaluation and because stress and accents mainly refers to syllables. 16sStress detection system were also
2796	2856	approach. 27sChapter 2. ALGORITHMS 2.1 Syllabification 2.1.1 Why syllables? Syllable segmentation is important in APA for tempo determination, but it can also be used as basic unit in recognition . The definition of what one call “syllable” is controversial. Some author bound syllables to jaw movement , to chest burst  or they consider syllables as the basic unit of speech
2796	2856	of Bark17sscaled loudness spectra calculated every 10 ms. Two kinds of artificial neural networks were compared, a multilayer perceptron and a radial basis function neural network. Wu et al.  based their analysis on smoothed speech spectra computed by two dimensional filtering techniques. In this way energy changes of the order of 150ms are enhanced, they apply also other techniques
2796	2856	but not least, the main aim of this work is to create the premises for the integration of data computed by APA in an Automatic Speech Recognizer. Syllables were already used in such an application , but stresses and intonational patterns are relatively not used, APA could provide interesting news in this field. 88sAppendix A. SIMULATED ANNEALING PSEUDOCODE T := sequence of temperatures
3847999	2859	the CP, so that a time-domain equalizer (TEQ) is placed in the receiver with the purpose of appropriately shortening the overall impulse response (OIR). Several TEQ designs have been proposed , , , , . Melsa et al.  introduced the Shortening SNR (SSNR) as the ratio of the OIR energy inside a window of size to that outside the window and suggested to choose the TEQ in order to
3847999	2859	in the minimization of alone. Fig. 1 shows the delay spread obtained with the MDS TEQ (8) of various lengths and for different values of . The channel was the standard DSL test loop CSA 1  combined with a POTS splitter and a twelfth-order Chebyshev bandpass filter for the 30–1000 kHz frequency band, and truncated to 512 samples (see Fig. 2). The sampling frequency was 2.208 MHz. The
2867	2868	artificial intelligence, and databases. Initially, mining problems have been grouped in three categories: identifying classifications, finding sequential patterns, and discovering associations . Intelligent solutions for such problems are application-dependent and different applications usually require different mining techniques. A field where artificial intelligence (AI) has the
2867	2878	a statistic for many pairwise tests) is usually handled by estimating corrected p-values for clusters. Although approaches have been proposed that seek to overcome the multiple comparison problem , they are based on a linearization of the 3D domain that might fail to preserve 100% the spatial locality of the ROIs. Another approach to detect functional associations in the human brain is to
2867	2878	domain to discover discriminative areas. This technique reduces the multiple comparison problem encountered in voxel-based analysis by applying statistical tests to groups of voxels. Compared with  this step of analysis is performed directly on the 3D domain (hyper-rectangles) without any loss of spatial locality. For classification, to avoid problems with distribution estimation techniques
2867	2879	of the ROIs. Another approach to detect functional associations in the human brain is to model (estimate) their underlying distributions when distinct classes are present (controls vs. patients) , , utilizing parametric, non-parametric or semi-parametric techniques. EM and k-means algorithms  have been employed for this purpose, and statistical distance based methods have been used
2867	2884	the first step of the analysis we employ Adaptive Recursive Partitioning (ARP) that has been so far applied mainly to realistic and synthetic 3D region datasets of discrete (binary) voxel values . Some initial results from attempts to apply the technique on real fMRI datasets have been presented in . The main idea of this technique is to treat the initial 3D volume as a hyper rectangle
2867	2885	mainly to realistic and synthetic 3D region datasets of discrete (binary) voxel values . Some initial results from attempts to apply the technique on real fMRI datasets have been presented in . The main idea of this technique is to treat the initial 3D volume as a hyper rectangle and search for informative regions by partitioning the space into sub-regions. The intelligence of the tool
2867	2888	We apply a method that efficiently extracts a k-dimensional feature vector using concentric spheres in 3D (or circles in 2D) radiating out of the ROI’s center of mass, initially presented in  and applied on artificially generated data. Here we demonstrate the potential of the technique to be utilized for characterizing real ROIs. The proposed technique extends the original idea of
2867	2888	technique has been shown to be two orders of magnitude faster than mathematical morphology (namely the “pattern spectrum”) although it achieves comparable to or even better characterization results . The purpose of extending these two approaches to be applicable on real data and combining them in the context of a unified approach is to create an intelligent brain informatics tool. This can be
2867	2888	following similar behavior and the two classes barely overlap. The curvature of the signatures conveys information about the activation patterns of the original data. As demonstrated initially in  with synthetic data, using morphological operators for such an analysis is two orders of magnitude slower than the approach employed here. As illustrated, patient samples exhibit positive
1884	2905	tails of the function in the range is very small, the effect of aliasing is essentially negligible. Note that in every case the energy terms are proportional to the sampling rate. It can be shown  that the energy of any uniformly (super-critically) sampled version of a band-limited signal is proportional to the sampling rate. APPENDIX C IS A REASONABLE ASSUMPTION? Suppose that we first wish
1884	2906	we can easily compute the following terms: (71) (72) (73) (74) (75) (76) (77) (78) 12 To recover exactly ?@?Y ?A would mathematically require an infinite number of measurements (or samples) ???@?Y ?A . But since we have considered a fairly large range (0IH to 10) for sampling, and since the energy in the tails of the function in the range is very small, the effect of aliasing is essentially
1884	2907	we discuss a more general case where neither the intensities and , nor the distance , are known. 8 Equation 7 Similar analysis for the two-dimensional extension of this problem is presented in . 8 But we assume that C aPis known to the detector.s682 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 5, MAY 2004 (22) leads to a detection problem defined in terms of a linear model over the
1884	2908	for indirect imaging such as in computed tomography. As for other extensions and applications in optical imaging, an appealing direction is to study the limits to super-resolution from video ???. The analysis presented here can help answer questions regarding the ability of image super-resolution methods to integrate multiple low resolution frames to produce a high resolution image
1884	2910	for indirect imaging such as in computed tomography. As for other extensions and applications in optical imaging, an appealing direction is to study the limits to super-resolution from video ???. The analysis presented here can help answer questions regarding the ability of image super-resolution methods to integrate multiple low resolution frames to produce a high resolution image from
2953	2957	methods. Feature-image methods are the focus of Petra van den Elsen’s work and our own work . For ultrasound to MR/CT registration, a featurefeature image registration method is proposed in . Vessel voxels are identified in both images and the distance between those sets of voxels is minimized. In  a deformable surface method is used to align ultrasound images and quantify liver
1194777	2981	connections. In contrast to the first approach, this can be done at the data middleware or application level. This approach has been implemented several times, including PSockets  and GridFTP . It has been observed that effectively utilizing high performance links can require dozens to hundreds of sockets. This can create an overhead, limiting the usefulness of this approach. In
1194777	2982	transmission mechanisms. SABUL implements these control functions in a separate TCP control channel. This approach is in contrast to the approach of other high performance protocols such as NETBLT  which combine the data and control channels. In SABUL, the packets on the UDP channel consist of the usual UDP header plus a 32 bit field for a sequence number. On the TCP channel, each packet
1194777	2990	distributed mining of geoscience data. Section 10 is the summary and conclusion. We are currently continuing the experiments described in this paper and preparing an expanded version of this paper . 2 Background and Related Work In this paper, we are concerned with supporting remote data analysis and distributed data mining applications with high performance data transport services. In
1194777	2993	TCP network connections. In contrast to the first approach, this can be done at the data middleware or application level. This approach has been implemented several times, including PSockets  and GridFTP . It has been observed that effectively utilizing high performance links can require dozens to hundreds of sockets. This can create an overhead, limiting the usefulness of this
1194777	2993	NSF’s vBNS network or the Internet 2’s Abilene network. In practice, very large bandwidth applications have to be scheduled on these networks and require the use of specialized transport protocols . As optical networking architectures become more common, a new possibility is emerging. A bandwidth demanding application can request an optical connection between the data sources and the data
1194777	2994	with this problem. One approach to improving TCP performance for data intensive applications is to adjust the TCP window size to be the product of the bandwidth and the RT T delay of the network . This approach requires modifying and tuning the kernel of each of the operating systems transporting the packets and ensuring that the networking hardware can support these large or jumbo packets.
1194777	2995	as an application library without making any changes to the existing network infrastructure. Another approach is to improve TCP in various ways. High Speed TCP , Scalable TCP , and FAST  are examples of this approach. Although these approaches all appear to be promising, work is still required to understand their friendliness, performance, and scalability. In addition, new TCP
1194777	2996	sites, and to collect the intermediate results and models. Systems with this architecture include the JAM system developed by Stolfo et al. , the BODHI system developed by Kargupta et al. , the Kensington system developed by Guo et al. , and the Papyrus system developed by Grossman et al. . The second approach is to use cluster middleware. Systems with this architecture
1194777	2997	In addition, new TCP variants require significant changes to the current network infrastructure. Another approach is to create entirely new protocols, such as Explicit Control Protocol (XCP)  and the Datagram Congestion Control Protocol (DCCP) . Again, deploying these new protocols will take some time due to the significant changes required to the current network infrastructure. 4s3
1194777	2998	it can be deployed as an application library without making any changes to the existing network infrastructure. Another approach is to improve TCP in various ways. High Speed TCP , Scalable TCP , and FAST  are examples of this approach. Although these approaches all appear to be promising, work is still required to understand their friendliness, performance, and scalability. In
1194777	3006	control the data mining algorithms at the different sites, and to collect the intermediate results and models. Systems with this architecture include the JAM system developed by Stolfo et al. , the BODHI system developed by Kargupta et al. , the Kensington system developed by Guo et al. , and the Papyrus system developed by Grossman et al. . The second approach is to use
3008	3188	each node is also annotated with a location field to describe the 2 We assume that the surrogate can be discovered in the local environment by some discovery service such as the Jini lookup service.sClass: A; Memory: 5KB; AccessFreq: 10; Location: surrogate; isNative: false; InteractionFreq: 12 BandwidthRequirement: 1 KB Figure 2. Illustration of our application program execution graph model.
3008	3009	computing environment. To support application-specific adaptation, application developers can use meta-level programming tools for deploying their applications in pervasive computing environments . One of the key differences between our dynamic offloading system and the above work is that pervasive application delivery can be realized without modifying the application or assuming that the
3008	3010	Unfortunately, these solutions are often associated with high levels of resource. Different approaches have been proposed to solve the problem by using application-based or system-based adaptations . However, these approaches often require degrading an application’s fidelity to adapt it to resource-constrained mobile devices. Moreover, adaptation efficiency is often limited by coarse-grained
3008	3011	without modifying applications. However, they assume that the application is already written in a component-based fashion and has exported component interfaces to the system. In the Gaia project , we proposed a dynamic service composition and distribution framework for delivering component-based applications in a pervasive computing environment. To support application-specific adaptation,
3008	3012	computing environment. To support application-specific adaptation, application developers can use meta-level programming tools for deploying their applications in pervasive computing environments . One of the key differences between our dynamic offloading system and the above work is that pervasive application delivery can be realized without modifying the application or assuming that the
3008	3013	partitioned at runtime based on its execution history and system/network resource information. Other closely related work includes application partitioning under different contexts. The Coign  project proposed a system to statically partition binary applications built from COM components. Unlike Coign, our approach performs dynamic runtime partitioning without any offline profiling.
3008	3015	to the surrogate and which program objects should be pulled back to the mobile device during an offloading action. To achieve both flexibility and stability, OLIE employs the Fuzzy Control model  for making offloading decisions. The Fuzzy Control model has previously been applied to coarse-grained application adaptations. The novelty of our approach is to apply the model to fine-grained
3008	3015	offloading, and (2) intelligent selection of an application partitioning policy. 3.1 Triggering of Adaptive Offloading OLIE makes the offloading triggering decision based on the Fuzzy Control model . The Fuzzy Control model includes: (1) a generic fuzzy inference engine based on fuzzy logic theory, and (2) decision-making rule specifications provided by system or application developers. For
3008	3017	expensive to rewrite an application according to the capacity of each mobile device. Hence, a fine-grained runtime offloading system, called adaptive infrastructure for distributed execution (AIDE) , has been proposed to solve the problem without modifying the application or degrading its fidelity. The key idea is to dynamically partition the application during runtime, and migrate part of the
3008	3017	dynamic offloading system architecture. device 2 . AIDE is responsible for properly transforming method invocations to objects that were offloaded to the surrogate into remote invocations . OLIE does not require any prior knowledge about an application’s execution or the resources of the system and the network to make offloading decisions. OLIE collects and analyzes all of the
3008	3018	Unfortunately, these solutions are often associated with high levels of resource. Different approaches have been proposed to solve the problem by using application-based or system-based adaptations . However, these approaches often require degrading an application’s fidelity to adapt it to resource-constrained mobile devices. Moreover, adaptation efficiency is often limited by coarse-grained
3008	3018	both application-based and system-based adaptations have been proposed to overcome resource constraints and environmental changes (e.g., wireless network fluctuations). The Odyssey project  introduced an application-aware adaptation service within the end host to accommodate resource changes, such as wireless network bandwidth fluctuations. Fox et. al. proposed an applicationbased
3008	3019	Unfortunately, these solutions are often associated with high levels of resource. Different approaches have been proposed to solve the problem by using application-based or system-based adaptations . However, these approaches often require degrading an application’s fidelity to adapt it to resource-constrained mobile devices. Moreover, adaptation efficiency is often limited by coarse-grained
3008	3019	both application-based and system-based adaptations have been proposed to overcome resource constraints and environmental changes (e.g., wireless network fluctuations). The Odyssey project  introduced an application-aware adaptation service within the end host to accommodate resource changes, such as wireless network bandwidth fluctuations. Fox et. al. proposed an applicationbased
3008	3021	profiling. Furthermore, we do not assume a componentbased application and enable mobile delivery for any application, even a complex monolithic application. More recently, Teodorescu et. al.  presented a system to support mobile Java program deployment by partitioningsJava program execution between system nodes and mobile devices. The system nodes prepare a Java application for
3008	3022	computing environment. To support application-specific adaptation, application developers can use meta-level programming tools for deploying their applications in pervasive computing environments . One of the key differences between our dynamic offloading system and the above work is that pervasive application delivery can be realized without modifying the application or assuming that the
3025	3026	on Internet-like topologies. We first devise a scenario where requests are homogeneously distributed across a chosen topology. The two topologies used in this section were generated by BRITE , a topology generator with the ability to create AS level topologies. Homogeneous Traffic (HT) This experiment exemplifies BGRP, BPS, and WDS behavior in the topology illustrated in Fig. 6 (b),
3025	3027	count, e.g., based on a given AS path size distribution. To obtain realistic values for ¤ , we use the values collected by Telstra  and presented in  1 . This model, an AS-level dumbbell  topology, is simple and sufficient to explain the state accounting methodology we use, since state along any path differs as a function of an AS location: sources, destinations, and intermediate
3025	3028	Data path Quality of Service (QoS) issues are by now reasonably well understood, and a number of different alternatives have been proposed and investigated, e.g., Integrated Services (IntServ)  and Differentiated Services (DiffServ) , each representing a different trade-off in terms of capability and scalability. However, the same understanding is not really available for control path
3025	3029	of mechanisms for reserving and maintaining the necessary data path resources, as embodied in proposals such as Internet Streaming Protocol (ST-II)  and Resource Reservation Protocol (RSVP) , with the latter being the current solution of ¡ This work was supported by the program Programa Operacional Sociedade de Informação (POSI), of the Portuguese Fundação para a Ciência e Tecnologia
3025	3032	by means of simulations, the performance of the algorithms for different network topologies. Finally, Section 5 summarizes findings and outlines future work. 2. Related Work Guérin et al.  present a survey of possible approaches to aggregate RSVP requests assuming unicast scenarios and covering issues such as RSVP state management and path characterization. The authors propose the
3025	3025	changes. Also, to achieve statistically meaningful results, each experiment has been repeated several times using different random number seeds. Further and more detailed results can be found in . 9 10 4 3 2 11 1 12 0 5 (a) 4.1. Tree Topology 6 5 18 31 44 6 41 24 46 22 34 14 29 9 7 1 0 20 25 4 38 21 (b) 11 2 13 27 3 33 23 3 42 16 16 7 40 48 39 8 49 0 10 30 8 10 43 26 35 13 36 17 14 28 19 19
3025	3034	tunnels, i.e., pipes between entry and exit points of a defined aggregation region, i.e., a cloud of routers where regular RSVP messages are ignored. A similar approach is followed by Berson et al. . They consider unicast and multicast scenarios, focusing also on RSVP aggregation within an aggregation region. These two approaches are concerned with RSVP scalability: RSVP requires all the
3025	3039	accounting is done both on the AS and edge router level by 2 We chose a Poisson process to model the arrival of requests since it is known to describe well user session arrivals, as mentioned in .scollecting statistics dynamically for incoming and outgoing reservations: minimum, average and maximum values are updated each time the corresponding variable changes. Also, to achieve
4470241	3045	who would consistently act contrary to his or her assignment. Under this assumption, which Imbens and Angrist (1994)call monotonicity, the inequalities in Equation (3.26) can be tightened (Balke and Pearl, 1997) to give P (y, X = 1|Z = 1) ? P (y, X = 1|Z = 0) P (y, X = 0|Z = 0) ? P (y, X = 0|Z = 1) (3.27) for all y ? {0, 1}. Violation of these inequalities now means either selection bias or a direct
4470241	3051	bounds as a function of sample size. Testable implications The two assumptions embodied in the model of Figure 5(a), that Z is randomized and has no direct effect on Y , are untestable in general (Bonet, 2001). However, if the treatment variable may take only a finite number of values, the combination of these two assumptions yields testable implications, and these can be used to alert investigators to
4470241	3057	maintained if the errors associated to X and to Y given X are not independent? This causal interpretation is more evident in procedures using observational and experimental data at the same time (Cooper and Yoo, 1999), in which the truncated factorization is used in the score derivation. The discovering of causal relationships from observational data has been discussed in learning literature. But I think that
4470241	3077	based on decision theory—in particular, the work of Savage (1954). That is, we defined Pearl’s causal model as well as the do operator and cause and effect in terms of Savage’s primitives (Heckerman and Shachter, 1995). As an additional benefit, we showed how Pearl’s causal model can be equivalently represented as an influence diagram—a graphical representation of decision making under uncertainty used now for
4470241	3101	Angrist et al., 1996; Greenland et al., 1999b), structural equation models Heckman and Smith (1998), and a more recent formulation, which unifies these approaches under a single interpretation (Pearl, 1995a, 2000). This paper aims at making these advances more accessible to the general research community 1 . To this end, Section 2 begins by illuminating two conceptual barriers that impede the
4470241	3101	that the corresponding background variables, UY and {UZ1 , . . . , UZk }, are independent in P (u). These assumptions can be translated into the potential-outcome notation using two simple rules (Pearl, 1995a, p. 704); the first interprets the missing arrows in the graph, the second, the missing dashed arcs. 1. Exclusion restrictions: For every variable Y having parents P A Y and for every set of
4470241	3122	formalized, can be extremely powerful in refining assumptions (Angrist et al., 1996), deriving consistent estimands (Robins, 1986), bounding probabilities of necessary and sufficient causation (Tian and Pearl, 2000), and combining data from experimental and nonexperimental studies (Pearl, 2000). The Section (4.3) presents a way of combining the best features of the two approaches. It is based ons316 J. Pearl
3130	3132	consider variations of Problem 1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the
3130	3133	and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer
3130	3134	variations of Problem 1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid
3130	3137	and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic
3130	3139	of Problem 1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state
3130	3140	switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming
3130	3142	mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or
3130	3143	mixed-integer quadratic programming  or iteratively by alternating between assigning data points to models and computing the model parameters starting from a random or ad-hoc initialization , . The first algebraic approach to the identification of PWARX models appeared in , where it was shown that one can identify the model parameters in closed form when the ARX models are of known
3130	3144	are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or
3130	3145	n) ?Mn?j?1 k=1 ?2 k (Ljn) + µ(Mn ? j), (14) where ?k(L j n) is the k th singular value of L j n and µ is a parameter. The above formula for estimating m is motivated by model selection techniques  in which one minimizes a cost function that consists of a data fitting term and a model complexity term. The data fitting term measures how well the data is approximated by the model – in this case
3130	3146	are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or iteratively
3130	3147	known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been recently addressed using mixed-integer quadratic programming  or iteratively by
3130	3151	1 in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has
3130	3152	in which the model parameters, the discrete state and/or the switching mechanism are known, and concentrate on the analysis of the observability of the hybrid state , , , , , ,  and the design of hybrid observers , , , , , , , , , . The more challenging case in which both the model parameters and the hybrid state are unknown has been
3130	3154	. , zK] T ??  T , (6) with I chosen in the degree-lexicographic order; and ? ? ? ? n + K ? 1 n + K ? 1 Mn = = K ? 1 n is the total number of independent monomials. One can show  that the vector of coefficients hn ? RMn is simply a vector representation of the symmetric tensor product of the individual model parameters {bi} n i=1 , i.e. ? b?(1) ? b?(2) ? · · · ? b?(n), (8)
3130	3155	data points to models and computing the model parameters starting from a random or ad-hoc initialization , . The first algebraic approach to the identification of PWARX models appeared in , where it was shown that one can identify the model parameters in closed form when the ARX models are of known and equal order and the number of discrete states is less than or equal to four. In
3130	3155	II-A and II-B show how to decouple the identification of the model parameters from the estimation of the discrete state via a suitable embedding into a higherdimensional space, as proposed in . Sections II-C and IID show how to identify the orders and the model parameters from the derivatives of a polynomial whose coefficients are obtained from a rank constraint on the embedded data.
3130	3155	i = 1, . . . , n, then we have that for all t ? na there exists a discrete state ?t?1 = i ? {1, . . . , n} such that b T i xt = 0. (2) Therefore, the following hybrid decoupling constraint (HDC)  must be satisfied by the model parameters and the input/output data regardless of the value of the discrete state and regardless of the switching mechanism generating the evolution of the discrete
3130	3155	?n(xna+1) T . ?n(xT ) T ? ? ? hn = 0 ? R T ?na+1 , (9) where Ln ? R (T ?na+1)×Mn is the matrix of embedded input/output data. We are now interested in solving for hn from (9). In our previous work  we considered ARX models of equal and known orders na(1) = na(2) = · · · = na(n) = na and showed that if the number of measurements is such that T ? Mn+na?2 and at least 2na measurements correspond
3130	3155	hn from input/output data {ut, yt} T t=0. The rest of the problem is to recover the model parameters {bi} n i=1 and the orders of the ARX models {na(i)} n i=1 from hn. In our previous work , which deals with the case of ARX models of equal and known orders, we showed that one can identify the model parameters directly from the derivatives of pn(z) at a collection of n regressors {zi}
8918942	3165	type of learning has found many useful applications in domains with large amount of data where labeling of a training set for supervised learning is cost prohibitive or where autonomy is essential . However, clustering algorithms generally rely on some prior knowledge of the structure present in a data set. For instance, one needs to know whether or not clusters actually exist in data prior
8918942	3166	type of learning has found many useful applications in domains with large amount of data where labeling of a training set for supervised learning is cost prohibitive or where autonomy is essential . However, clustering algorithms generally rely on some prior knowledge of the structure present in a data set. For instance, one needs to know whether or not clusters actually exist in data prior
8918942	3166	and autonomously creates a new category. Another ad2 Louis Massey vantageous and distinguishing feature of ART is its ability to discover concepts in data at various levels of abstraction . This is achieved with the vigilance parameter ? ? (0,1]. First, a similarity measure S (Eq. 1) is computed to determine if an existing cluster prototype T j appropriately represents the critical
8918942	3166	obtained from HotBits (http://www.fourmilab.ch/hotbits/) ). Other similar experiments have been conducted with several real-life or benchmark data sets. One of these experiments is documented in . 6. Conclusion and Future Work 0.5 Vigilance 0.7 Non-Random random 0.9 Pseudo-Random Vs Random We have shown how the vigilance parameter of a binary ART neural network can be used to determine the
8918942	3167	We have described a method to establish if natural groups occur in the data. The residual question is whether those groups are the result of mere coincidence. Indeed, it can easily be demonstrated  both analytically and empirically that clusters do occur in a random data set. Evidently, such clusters are meaningless and clustering tendency of such origin must be appropriately detected. We now
8918942	3167	qualitative approach may not be ideal, but it suits our current objective by giving an idea of whether or not clusters may be due to mere chance. Elements of a quantitative approach are given in . 5. Empirical Validation Patterns are bit strings of length N. In the first experiment, we consider the case where tendency is determined by failing to reach M=1 at ?< ? min . The data set with
634091	3175	may replace its set toBeMoved by the intersection of that set with the set toBeMoved of another one. We do not give a preferred strategy here, one can refer to algorithms for the write-all problem . 3.5.8 MoveElement The procedure moveElement moves a value to the new hashtable. Note that the value is tagged as outdated in moveContents before moveElement is called. proc moveElement(v : Value \
634091	3176	store huge but sparsely filled tables. As far as we know, no wait- or lock-free algorithm for hashtables has ever been proposed. There are very general solutions for wait-free addresses in general , but these are not efficient. Furthermore, there exist wait-free algorithms for different domains, such as linked lists , queues  and memory management . In this paper we present an
634091	3176	H?index > 0 Ha2: H(i) < H?index Ha3: i ?= j ? Heap(H(i)) ?= ? ? H(i) ?= H(j) 23sHa4: index ?= currInd ? H(index) ?= H(currInd) Invariants about counters for calling the specification. Cn5: pc ?  ? cntfi = 0 Cn6: pc ?  ? pc ?  ? returngA = 10 ? pc ?  ? (returnrA = 59 ? returngA = 10 ? returnrA = 90 ? returnref = 10 ? pc ? 90 ? returnref = 10 ? cntfi = ?(rfi = null ?
634091	3176	?= null}]) f is injective, where h = H(next(currInd)) Invariants concerning procedure find (5. . . 14) fi1: afi ?= 0 fi2: pc ? {6, 11} ? nfi = 0 fi3: pc ? {7, 8, 13} ? lfi = hfi.size fi4: pc ?  ? pc ?= 10 ? hfi = H(index) fi5: pc = 7 ? hfi = H(currInd) ? nfi < curSize fi6: pc = 8 ? hfi = H(currInd) ? ¬Find(rfi, afi) ? rfi ?= done ? ¬ Find(Y, afi) fi7: pc = 13 ? hfi
634091	3177	a few extra pointer indirections suffices. Sometimes, however, the time and space complexities of a lock-free algorithm is substantially higher than its sequential, or ‘synchronized’ counterpart . Furthermore, some machine architectures are not very capable of handling shared variables, and do not offer compare-and-swap or test-and-set instructions necessary to implement lock-free
634091	3177	H?index > 0 Ha2: H(i) < H?index Ha3: i ?= j ? Heap(H(i)) ?= ? ? H(i) ?= H(j) 23sHa4: index ?= currInd ? H(index) ?= H(currInd) Invariants about counters for calling the specification. Cn5: pc ?  ? cntfi = 0 Cn6: pc ?  ? pc ?  ? returngA = 10 ? pc ?  ? (returnrA = 59 ? returngA = 10 ? returnrA = 90 ? returnref = 10 ? pc ? 90 ? returnref = 10 ? cntfi = ?(rfi = null ?
634091	3187	out a performance bottleneck, and failure of a single process can force all other processes to come to a halt. Therefore, wait-free, lock-free, or synchronization-free algorithms are of interest . An algorithm is wait-free when each process can accomplish its task in a finite number of steps, independently of the activity and speed of other processes. An algorithm is lock-free when it
634091	3187	very general solutions for wait-free addresses in general , but these are not efficient. Furthermore, there exist wait-free algorithms for different domains, such as linked lists , queues  and memory management . In this paper we present an almost wait-free algorithm for hashtables. Strictly speaking, the algorithm is only lock-free, but wait-freedom is only
634091	3187	hashing with open addressing, cf. . We do not use direct chaining, where colliding entries are stored in a secondary list, because maintaining these lists in a lock-free manner is tedious , and expensive when done wait-free. A disadvantage of open addressing with deletion of elements is that the contents of the hashtable must regularly be refreshed by copying the non-deleted elements
634091	3188	solutions for wait-free addresses in general , but these are not efficient. Furthermore, there exist wait-free algorithms for different domains, such as linked lists , queues  and memory management . In this paper we present an almost wait-free algorithm for hashtables. Strictly speaking, the algorithm is only lock-free, but wait-freedom is only violated when a
3190	3372	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3372	in . 3 Related work There are currently several peer-to-peer systems in use, and many more are under development. Among the most prominent are file sharing facilities, such as Gnutella  and Freenet . The Napster  music exchange service provided much of the original motivation for peer-to-peer systems, but it is not a pure peer-to-peer system because itssdatabase is
3190	3191	the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional, persistent storage service that supports
3190	3192	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3192	of systems like FreeNet with the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional, persistent storage
3190	3193	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3193	3 Related work There are currently several peer-to-peer systems in use, and many more are under development. Among the most prominent are file sharing facilities, such as Gnutella  and Freenet . The Napster  music exchange service provided much of the original motivation for peer-to-peer systems, but it is not a pure peer-to-peer system because itssdatabase is centralized. All three
3190	3194	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3194	like FreeNet with the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional, persistent storage service that
3190	3195	the system. 1 Introduction There are currently many projects aimed at constructing peer-to-peer applications and understanding more of the issues and requirements of such applications and systems . Peer-to-peer systems can be characterized as distributed systems in which all nodes have identical capabilities and responsibilities and all communication is symmetric. We are developing PAST, an
3190	3195	of systems like FreeNet with the strong persistence and reliability expected of an archival storage system. In this regard, it is more closely related with projects like OceanStore , FarSite , FreeHaven , and Eternity . FreeNet, FreeHaven and Eternity are more focused on providing strong anonymity and anti-censorship. OceanStore provides a global, transactional,
3190	3197	of network hops, while retaining the scalability of FreeNet and the self-organizing properties of both FreeNet and Gnutella. Pastry and Tapestry bear some similarity to the work by Plaxton et al . The approach of routing based on address prefixes, which can be viewed as a generalization of hypercube routing, is common to all three schemes. However, the Plaxton scheme is not self-organizing
3190	939	a distributed directory service to locate content; this is different from PAST’s Pastry scheme, which integrates content location and routing. Pastry, along with Tapestry , Chord  and CAN , represent a second generation of peer-to-peer routing and location schemes that were inspired by the pioneering work of systems like FreeNet and Gnutella. Unlike that earlier work, they guarantee
3190	940	a file, which does not guarantee that the file is no longer available. These weaker semantics avoid agreement protocols among the nodes storing the file. An efficient routing scheme called Pastry  ensures that client requests are reliably routed to the appropriate nodes. Client requests to retrieve a file are routed to a node that is “close in the network” 1 to the client that issued the
3190	940	shown that the average distance traveled by a message, in terms of the proximity metric, is only 50% higher than the corresponding “distance” of the source and destination in the underlying network . Moreover, since Pastry repeatedly takes a locally “short” routing step, messages have a tendency to first reach a node, among the k nodes that store the requested file, that is near the client,
3190	940	One experiment shows that among 5 replicated copies of a file, Pastry is able to find the “nearest” copy in 76% of all lookups and it finds one of the two “nearest” copies in 92% of all lookups . Node addition and failure A key design issue in Pastry is how to efficiently and dynamically maintain the node state, i.e., the routing table, leaf set and neighborhood sets, in the presence of
3190	940	their current leafs sets, updates its own leaf set and then notifies the members of its presence. Routing table entries that refer to failed nodes are repaired lazily; the details are described in . Fault-tolerance The routing scheme as described so far is deterministic, and thus vulnerable to malicious or failed nodes along the route that accept messages but do not correctly forward them.
3190	3198	scheme in PAST ensures that the global storage utilization in the system can approach 100%, despite the lack of centralized control and widely differing file sizes and storage node capacities . In a decentralized storage system where nodes are not trusted, an additional mechanism is required that ensures a balance of storage supply and demand. Towards this end, PAST includes a secure
3190	3198	sufficient availability. In the event of storage node failures that involve loss of the stored files, the system automatically restores k copies of a file as part of a failure recovery procedure . Data privacy and integrity Users may use encryption to protect the privacy of their data, using a cryptosystem of their choice. Data encryption does not involve the smartcards. Data integrity is
3190	3198	show that PAST can achieve global storage utilization in excess of 95%, while the rate of rejected file insertions remains below 5% and failed insertions are heavily biased towards large files . Any PAST node can cache additional copies of a file, which achieves query load balancing, high throughput for popular files, and reduces fetch distance and network traffic. Storage management and
3190	943	Farsite uses a distributed directory service to locate content; this is different from PAST’s Pastry scheme, which integrates content location and routing. Pastry, along with Tapestry , Chord  and CAN , represent a second generation of peer-to-peer routing and location schemes that were inspired by the pioneering work of systems like FreeNet and Gnutella. Unlike that earlier work,
3190	3199	that certain operations were initiated by the same user. To provide stronger levels of anonymity and other properties such as anti-censorship, additional mechanisms may be layered on top of PAST . 2 PAST design Some of the key aspects of PAST’s architecture are (1) the Pastry routing scheme, which routes client requests in less than ?log16N? steps on average within a selforganizing, fault
3190	3200	that certain operations were initiated by the same user. To provide stronger levels of anonymity and other properties such as anti-censorship, additional mechanisms may be layered on top of PAST . 2 PAST design Some of the key aspects of PAST’s architecture are (1) the Pastry routing scheme, which routes client requests in less than ?log16N? steps on average within a selforganizing, fault
3190	3201	that certain operations were initiated by the same user. To provide stronger levels of anonymity and other properties such as anti-censorship, additional mechanisms may be layered on top of PAST . 2 PAST design Some of the key aspects of PAST’s architecture are (1) the Pastry routing scheme, which routes client requests in less than ?log16N? steps on average within a selforganizing, fault
3203	3219	diffusion path of ions. The diffusion field in these restricted environments changes by a tortuosity factor ? that characterizes the diffusivity in porous media. In the recent summary of this topic  the evolution of the diffusion actions of Liesegang rings formation, BelousovZhabotinsky waves and the cAMP (cyclic adenosine 3´,5´monophosphate) waves were analyzed. The main trend for all three
3229	3230	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3232	this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author, e-mail: uhl@cosy.sbg.ac.at School of Telematics & Network Engineering Carinthia
3229	3233	this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author, e-mail: uhl@cosy.sbg.ac.at School of Telematics & Network Engineering Carinthia
3229	3235	environment is mandatory for multimedia security applications. An overview about current requirements and implementations of contents protection systems for digital multimedia data is given in . Selective or partial encryption (SE) of visual data is an example for such an approach. Here, application specific data structures are exploited to create more efficient encryption systems (see
3229	3242	of visual data is an example for such an approach. Here, application specific data structures are exploited to create more efficient encryption systems (see e.g. encryption of MPEG video streams ). Consequently, selective encryption only protects the visually most important parts of an image or video representation relying on a secure but slow “classical” cipher. See  for a discussion
3229	3243	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3244	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3246	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
3229	3247	selective video encryption schemes resistant to bit errors Corresponding author, e-mail: uhl@cosy.sbg.ac.at School of Telematics & Network Engineering Carinthia Tech Institute, Klagenfurt, AUSTRIA  and compliant to video formats  have been proposed for wireless environments. In this work we propose and evaluate selective bitplane encryption for confidential transmission of image data in
3229	3251	cipher. See  for a discussion of sensible application scenarios for this approach. The first attempts in this direction have been made to secure DCT-based multimedia representations (see e.g. ), wavelet based  and quadtree based representations  have been considered also. Recently, selective video encryption schemes resistant to bit errors Corresponding author,
715250	3255	(IntServ) model or in RSVP. It refers to all packets moving between each pairs? of source and destination. For instance, Flow ?? indicates all packets being transmitted from node ? to node ?.s, . In DiffServ networks, traffic is classified into three service classes: premium, assured and best-effort. The premium class traffic has the highest priority in comparison to other classes of
715250	3255	at each node along a path account for the most significant part of the whole end-to-end delay for that path. With the highest priority, premium class traffic experiences almost no queueing delays , . Therefore, choosing a fairly longer (in terms of hop-count) path from a source to a destination does not compromise its delay requirements. B. Optimal Premium-class Routing (OPR) Problem In
715250	3256	into consideration, the premium class traffic imposes very negative influences on other classes of traffic, especially when the network is highly loaded. We call this the inter-class effects . In , the authors presented simple performance models and analysis for DiffServ schemes. However, to make a strong case for the negative impacts the premium class traffic may impose on all other
715250	3256	and expediency of problem definition, we assume each node in the topology tries to reserve More simulation details and extensive results and measurements are presented in our technical report . Loss Probability 1 0.8 0.6 0.4 0.2 0 0.4 0.6 0.8 1 1.2 Offered load (Lambda) 1.4 1.6 1.8 Packet Delay (in packet) 100 80 60 40 20 2 0.1 (a) Packet Loss Probability 0 0.3 0.4 0.5 0.6 0.7 Offered
715250	3256	each node along a path account for the most significant part of the whole end-to-end delay for that path. With the highest priority, premium class traffic experiences almost no queueing delays , . Therefore, choosing a fairly longer (in terms of hop-count) path from a source to a destination does not compromise its delay requirements. B. Optimal Premium-class Routing (OPR) Problem In
715250	3257	consideration, the premium class traffic imposes very negative influences on other classes of traffic, especially when the network is highly loaded. We call this the inter-class effects . In , the authors presented simple performance models and analysis for DiffServ schemes. However, to make a strong case for the negative impacts the premium class traffic may impose on all other service
715250	3258	may be reduced). Actually, if we look at the problem from the second perspective, ?× becomes a load-balancing or fairness index of a network, which is similar to the max-min fairness in , , . Together with the assumption of hop-by-hop routing, an interesting problem is what is the optimal value of ? × (denoted as ?Ñ?Ü) we can get. This problem is closely related to the routing
715250	3258	An optimal local solution for a single source may not be the solution to the whole OPR problem. It is not difficult to show that this problem is a NP-Complete problem by following the proof in . Therefore, the SP routing algorithm may not always be the optimal one. By using the same topology in Figure 1, but applying a different routing algorithm, Figure 4 illustrates a simple case where
715250	3258	the Optimal Premium-class Routing (OPR) problem - to find an optimal routing algorithm to efficiently service premium class traffic and reduce negative inter-class effects simultaneously. In , J. Kleinberg addressed an NP-Complete problem which combined the selecting paths for routing and allocating bandwidth fairly among connections in the max-min sense. Following their proof of
715250	3259	may be reduced). Actually, if we look at the problem from the second perspective, ?× becomes a load-balancing or fairness index of a network, which is similar to the max-min fairness in , , . Together with the assumption of hop-by-hop routing, an interesting problem is what is the optimal value of ? × (denoted as ?Ñ?Ü) we can get. This problem is closely related to the routing
715250	3260	IV. Simulations and results are illustrated in Section V. Finally, Section VI concludes this paper. II. RELATED WORK Extensive researches have been conducted on QoS routing issues recently. In , S. Chen and K. Nahrstedt did a thorough survey on QoS routing algorithms. But they focused on network models in virtual circuit mode. Our work in this paper is based on the hop-by-hop routing
715250	3261	in virtual circuit mode. Our work in this paper is based on the hop-by-hop routing scheme. Issues on hop-by-hop routing algorithms, such as isotonicity, search of optimal paths, were studied in . The author provided an elegant algebra basis to study the QoS routing issues in the Internet. In our paper, although we will borrow some definitions and theorems from the work in , the problem
715250	3261	? to , then there will be a forwarding loop between ? and ? in the network, although the two paths are loop-free individually. Formal definitions of isotonicity and strict isotonicity were given in , and they state that the order relation between the weights of any two simple paths is preserved if both of them are either prefixed or appended by a common, third, simple path. Since both of them
715250	3261	smallest identifier to break the tie. However, the WSP always chooses the widest path among the set of shortest paths between any pair of source and destination. The WSP has been well-studied in , . Although it does not have a strict isotonicity, the isotonicity still holds. Therefore, by using the Dijkstra-oldtouch-first (Dijkstra-OTF) algorithm proposed in , we cansguarantee that
715250	3261	isotonicity property holds, the BSP algorithm with this weight function can guarantee that packets are transmitted through the lightest path between any pair of source and destination without loop . Therefore, it can be used as a hop-by-hop routing algorithm as well. If there are more than one lightest paths between the source and destination, the path with the least hop count is selected.
715250	3262	whole network topology and tries to address the OPR problem in a global view. In our work we show that an optimal local solution for a single source may not be the solution to the OPR problem. In , the authors addressed the QoS routing from the precomputation perspective and proposed a hierarchical algorithm to solve the All-Hops Problem. In , the authors discussed path selection
715250	3263	identifier to break the tie. However, the WSP always chooses the widest path among the set of shortest paths between any pair of source and destination. The WSP has been well-studied in , . Although it does not have a strict isotonicity, the isotonicity still holds. Therefore, by using the Dijkstra-oldtouch-first (Dijkstra-OTF) algorithm proposed in , we cansguarantee that WSP
715250	3263	gain, we need to choose a “best” path from a broader range. Therefore, the BSP algorithm is introduced. The BSP algorithm was studied and called the “Shortest-distance path algorithm” in . It was used in a call-based or connection-oriented network (such as the ATM network). The BSP is basically a shortest-path algorithm with the distance or weight function defined as and Û Ú??Ú? ?
3294	3296	problem is, in general, undecidable.  defines a scheme based on abstract interpretation for the (static) analysis of the unsatisfiability of equation sets, and show how various analyses such as  can be seen as instances of the scheme. In this work, we are concerned with equation solving or E-unification with respect to a given set E of equations. We define an optimization of (basic)
3294	3296	been dropped using either of the standard simplifying strategies of narrowing which rest on the use of loop-checking , unification rules , operator joinability  or eager normalization  techniques. The following theorem proves that our approach is sound and complete. Theorem 4.4. correctness and completeness of refined Conditional Narrowing Let E be a level-canonical Horn
3294	3297	has been implemented. In spite of the fact that it does not have the simplicity of simple basic narrowing, we have chosen to implement an innermost selection basic conditional narrowing procedure  since it further reduces the size of the search space. It is difficult to state a general result for the efficiency improvement of the optimization. Table 1 gives an impression of some achievable
3294	3304	occurring in the syntactic object s. A free variable is a variable that appears nowhere else. The symbol ? denotes a finite sequence of symbols. We describe the lattice of equation sets following . We let Eqn denote the set of possibly existentially quantified finite sets of equations over terms. We let fail denote the unsatisfiable equation set, which (logically) implies all other equation
3294	3304	bCN we introduced above. For this purpose, we follow the top-down approach to abstract interpretation which is based on constructing and examining abstract transition systems as defined in . Some of the following definitions are already in  and are reported for completeness. A difference with respect to  is in the definition of abstract most general unifier (Definition 3.5)
3294	3304	states which are equivalent modulo variable renaming. To ensure finiteness of the analysis, we collapse states which are variable renamings of each other into a single ‘equivalent’ state, as in . In the following, we formalize the idea that abstract narrowing reduction approximates narrowing reduction with abstract states, abstract unification and abstract term rewriting systems replacing
3294	3305	missing proofs can be found in . 2 Preliminaries Let us first summarize some known results about equations, conditional rewrite systems and equational unification. For full definitions refer to . Throughout this paper, V will denote a countably infinite set of variables and ? denotes a set of function symbols, each with a fixed associated arity. ?(? ?V) and ?(?) denote the sets of terms
3294	3306	execution. Keywords: Abstract interpretation, equational logic programming, term rewriting systems, universal unification. 1 Introduction The recent interest in logic programming with equations  has promoted much work on equational unification  and narrowing . Equational unification (E-unification) characterizes the problem of solving equations modulo an equational theory
3294	3306	problem is, in general, undecidable.  defines a scheme based on abstract interpretation for the (static) analysis of the unsatisfiability of equation sets, and show how various analyses such as  can be seen as instances of the scheme. In this work, we are concerned with equation solving or E-unification with respect to a given set E of equations. We define an optimization of (basic)
3294	3306	removed subtree could not have been dropped using either of the standard simplifying strategies of narrowing which rest on the use of loop-checking , unification rules , operator joinability  or eager normalization  techniques. The following theorem proves that our approach is sound and complete. Theorem 4.4. correctness and completeness of refined Conditional Narrowing Let E be
3294	3312	missing proofs can be found in . 2 Preliminaries Let us first summarize some known results about equations, conditional rewrite systems and equational unification. For full definitions refer to . Throughout this paper, V will denote a countably infinite set of variables and ? denotes a set of function symbols, each with a fixed associated arity. ?(? ?V) and ?(?) denote the sets of terms
3294	3312	assumptions hold for all theories we consider in this paper. The equational theory E is said to be canonical if the binary one-step rewriting relation ?R defined by R is noetherian and confluent . A function symbol f ? ? is irreducible iff there is no rule (? ? ? ? e1,e2,...,en) ? R such that f occurs as the outermost function symbol in ?, otherwise it is a defined function symbol. In
3294	3315	rewriting systems, universal unification. 1 Introduction The recent interest in logic programming with equations  has promoted much work on equational unification  and narrowing . Equational unification (E-unification) characterizes the problem of solving equations modulo an equational theory E. The narrowing mechanism is a powerful tool for constructing complete
3294	3315	narrowing has quite a large search space, several strategies to control the selection of redexes have been devised to improve the efficiency of narrowing by getting rid of some useless derivations . Narrowing at only basic positions has been proven to be a complete method for solving equations in the theory defined by a level-canonical conditional term ? This work has been partially supported
3294	3315	de Valencia, Camino de Vera s/n, Apdo. 22012, 46020 Valencia, Spain. ‡ Dipartimento di Elettronica e Informatica, Università di Padova, Via Gradenigo 6/A, 35131 Padova, Italy.srewriting system . In , a further refinement is considered which derives from simulating SLD-resolution on flattened equations and in which the search reduces to an innermost selection strategy. The ability
3294	3315	of E. The set of all E-unifiers of E is recursively enumerable . Conditional narrowing has been shown to be a complete E-unification algorithm for theories satisfying different restrictions . 3 Abstract Basic Conditional Narrowing Basic (conditional) narrowing is a restricted form of (conditional) narrowing where only terms at basic occurrences are considered to be narrowed .
3294	3315	of basic is to avoid narrowing steps on subterms that are introduced by instantiation. Basic Conditional Narrowing is a complete E-unification algorithm for level-canonical Horn equational theories . Let R be a level-canonical TRS. We formulate a Basic Conditional Narrowing calculus according to the partition of equational goals into a skeleton and an environment part, as in . The skeleton
3294	3317	of ? is a conjunction of equations and the second occurrence is the application of a substitution. We consider the usual preorder on substitutions ?: ? ? ? iff ??. ? ? ??. Note that ?? ? ? iff ? ? ? . A substitution {x1/t1,...,xn/tn} is a unifier of an equation set E iff {x1 = t1,...,xn = tn} ? E. We denote the set of unifiers of E by unif(E) and mgu(E) denotes the most general unifier of the
3294	3317	we show how the test ? of ‘compatibility’ on abstract substitutions can be effectively checked by providing an algorithm to decide it. First we need to extend the notion of parallel composition  from substitutions to abstract substitutions. Parallel composition was proposed in  in order to provide a compositional characterization of the semantics of Horn Clause Logic. The formalization
3294	3317	combined afterwards in order to get the final result. Roughly speaking, parallel composition is the operation of unification generalized to substitutions. It can be performed in the following way . Given two idempotent substitutions ?1 and ?2, consider the set of all pairs corresponding to the bindings of both ?1 and ?2. Then, compute the most general unifier of such a set. Note that the
3320	3325	retrieval purposes. Furthermore a lot of work on this subject is done in the Semantic Web community. The development of annotation formalisms and reasoning in Web environments (e.g. Motta et al., ), language technology for the Semantic web (most notable the OntoWeb SIG-5 4 ), the development of tools to support manual 4 For more information on the OntoWeb cf
3320	3326	retrieval purposes. Furthermore a lot of work on this subject is done in the Semantic Web community. The development of annotation formalisms and reasoning in Web environments (e.g. Motta et al., ), language technology for the Semantic web (most notable the OntoWeb SIG-5 4 ), the development of tools to support manual 4 For more information on the OntoWeb cf
3320	3331	to describe words that belong to distinct categories as synonyms for one concept. In this sense the lexicon reflects the EuroWordNet strategy to allow for cross-category listing of synonyms . – Regular expression-like syntax can be used to combine entries with the same structure that differ for example only in the choice of a word (such as alternative prepositions expressing the same
3320	3335	further in this section. 6.1 Annotation of Video Recordings The only textual sources that can be directly matched to the video recordings are the transcripts of the speech in those recordings . Even though Information Extraction results for these transcripts is possible, the transcripts themselves contain more errors than the other sources. Furthermore they contain relatively few actual
3320	3337	that aim at semantic access of Web content. Nicolas et al.  describe an Information Retrieval system in which both text documents and queries are translated to a CG representation, Zhong et al.  use CG’s to retrieve online descriptions of garments, Ounis and Pasca , Myaeng  and Montes-y-Gomez  also use CG’s for information retrieval purposes. Furthermore a lot of work on this
3342	3344	of arguments for methods and constructors not only allow the semantics of classes to be expressed more concisely but also enables more energy-efficient execution in power-stringent platforms  23 The reduction of conditional statements improves branch predictions and achieves better cache performance. The reduction of attributes of classes simplifies the runtime stack of programs and
3342	3346	in the aspect oriented way. Our definition of aspects is more aggressive and more precise as compared to ilities , non-functional requirements , or general distributed computing concerns . For instance, the customization of communication protocols could be described as both an ility (customizability) and a distributed concern, and, hence, could be classified as an aspect. However,
3342	3347	of the middleware through static or dynamic policy selection , approaches that adapt the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions and interpretations . Many of these projects use several of these techniques in combination. Below, we discuss each
3342	3347	concerns from the middleware implementation. Several projects exploit reflective programming techniques to allow the middleware platform to adapt itself dynamically to changing runtime conditions . This includes projects such as openORB , openCORBA , dynamicTAO , the OpenOrb project , and also the CompOSE—Q project . In these approaches, the reflective middleware
3342	3350	options for users, and, as a result, imperatively partition the application domains on behalf of the users. The mainstream of middleware implementations is “heavyweight, monolithic and inflexible” . As many researchers have pointed out , the effective solution to the middleware architectural problems described above is a high degree of configurability, adaptability and
3342	3544	programming models such as reflection  and component frameworks . Component frameworks operate within the modularization capabilities 1 These specification are collectively defined in  except for Data-parallel CORBA, which is located at http://www. omg.org/technology/documents/specialized_corba.htm 2 The IONA product line includes Orbix Enterprise, Orbix standard, Orbix mainframe
3342	3544	do not have clear modular boundaries within the middleware code space and, more seriously, often tangle with one another. This prohibits these functionalities from being pluggable. Let us use CORBA  as an example. CORBA implementations typically support interceptor mechanisms and the dynamic programming style 14 . Figure 2 illustrates the convolution among these two features as well as with
3342	3360	apply to any orthogonal functionality. Moreover, we introduce and evaluate a set of principles to guide the aspect oriented design of systems and the refactoring of such systems. Hunleth et al. , as well as its extended work, FACET 28 , take a similar position as we do and aim at customizing middleware with aspect oriented techniques. It is suggested that aspects could be used for
3342	3360	the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions and interpretations . Many of these projects use several of these techniques in combination. Below, we discuss each category in turn and point out how our approach is distinguished. Upfront we can say that the key
3342	3361	classdirectional towards the core. Class-directional is a category of relationships between base modules (classes) and aspects in which the base module “knows about the aspects but not vice-versa” . Classdirectional in HD means the system core does not have the knowledge of aspect implementations. Our previous work shows that middleware aspects, such as the interception support and the
3342	3362	libraries of Java provide a rich set of middleware functionality including RMI and CORBA.sof conventional programming languages, therefore, they cannot effectively address concern crosscutting . Reflection typically requires the setup of the reflection infrastructure which is not always desirable. Moreover, reflection operates at a level of abstraction above programming languages, which
3342	3362	Invocation Mixing features Additional class types are created Figure 2: Implementation Convolution in ORBacus. two properties being programmed must compose differently and yet be coordinated??? . We extend this AOP term and use implementation convolution to describe this large scale N-by-N crosscutting phenomenon. Implementation convolution means the loss of modularity and configurability.
3342	3364	partition the application domains on behalf of the users. The mainstream of middleware implementations is “heavyweight, monolithic and inflexible” . As many researchers have pointed out , the effective solution to the middleware architectural problems described above is a high degree of configurability, adaptability and customizability. Its ultimate goal is to customize middleware
3342	3364	of the middleware through static or dynamic policy selection , approaches that adapt the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions and interpretations . Many of these projects use several of these techniques in combination. Below, we discuss each
3342	3364	concerns from the middleware implementation. Several projects exploit reflective programming techniques to allow the middleware platform to adapt itself dynamically to changing runtime conditions . This includes projects such as openORB , openCORBA , dynamicTAO , the OpenOrb project , and also the CompOSE—Q project . In these approaches, the reflective middleware
3342	3365	or, more broadly, non-functional properties. The QuO project at BBN Technologies constitutes a framework supporting the development of distributed applications with QoS requirements (see , for example). QuO uses quality description languages (QDL) to specify client-side QoS needs, regions of possible level of QoS, system conditions that need to be monitored, certain behavior desired
3342	3365	the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions and interpretations . Many of these projects use several of these techniques in combination. Below, we discuss each category in turn and point out how our approach is distinguished. Upfront we can say that the key
3342	3366	of the middleware through static or dynamic policy selection , approaches that adapt the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions and interpretations . Many of these projects use several of these techniques in combination. Below, we discuss each
3342	3366	concerns from the middleware implementation. Several projects exploit reflective programming techniques to allow the middleware platform to adapt itself dynamically to changing runtime conditions . This includes projects such as openORB , openCORBA , dynamicTAO , the OpenOrb project , and also the CompOSE—Q project . In these approaches, the reflective middleware
3342	3368	2.2 Vertical Decomposition We use the term “vertical decomposition” to denote the hierarchical decomposition advocated by many pioneers of software architecture including Dijkstra  and Parnas . It is based on levels of abstractions and step-wise refinements to divide-and-conquer complex problems. Hierarchical decomposition is mostly suitable for implementing a single independent function
3342	3369	with our approach is difficult. 27 JBoss URL:http://www.jboss.org 28 FACET URL: http://www.cs.wustl.edu/~doc/RandD/ PCES/facet/ 15 7.2 Feature Oriented Programming Feature oriented programming  is an alternative programming paradigm for increasing the flexibility of conventional inheritance-based typing in object oriented systems. In FOP, base objects, features which “crosscut” base
3342	3372	the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions and interpretations . Many of these projects use several of these techniques in combination. Below, we discuss each category in turn and point out how our approach is distinguished. Upfront we can say that the key
3342	3373	of separation of concerns for the design of middleware platforms can be broadly classified into approaches that provide customization of the middleware through static or dynamic policy selection , approaches that adapt the operation of the middleware to changing runtime conditions through the use of reflection , and approaches based on various forms of aspect definitions
3342	3373	paradigm. Astley et al.  achieve middleware customization through techniques based on separation of communication styles from protocols and a framework for protocol composition. The CompOSE—Q  project uses an actor-based model for runtime adaptation. Both approaches do not employ aspect orientation to isolated cross-cutting design concerns from the middleware implementation. Several
3342	3373	to changing runtime conditions . This includes projects such as openORB , openCORBA , dynamicTAO , the OpenOrb project , and also the CompOSE—Q project . In these approaches, the reflective middleware implementation observes and reacts to changing environmental conditions by selecting different implementation strategies. The platform adapts itself
3342	3374	invocation of local servers is a crosscutting concern (cf. Section 4.3). However, in the context of non-distributed applications, the remote invocation mechanism can be implemented as an aspect . We use these examples to highlight the possible ambiguity for the semantics of aspects in large and complex systems. This ambiguity should be clarified as much as possible because we believe
3342	3375	We refer to this referential architecture as the core decomposition, or just simply “core”. In large software systems, such as middleware, the core consists of several conceptual components  15 . Each of the components focuses on a single task and they are logically coherent in supporting the primary system functionality or its most typical usage. For this reason, it is minimal and
3342	3379	middleware applications of AOP  primarily focus on modularizing non-functional properties  as aspects and treat the middleware core as a monolithic building block. Our observation  reveals that the poor modularization of crosscutting concerns is an inherent phenomenon within this monolithic core of current middleware implementations. By “inherent” we mean that, for a certain
3342	3379	with support for both interceptors and the dynamic programming style. This type of problem is not due to the design limitations of ORBacus. Our examination of three different CORBA implementations  shows that around 50% of the classes coordinate with a second design concern. Moreover, 10% of the classes coordinate with three and more concerns. The phenomenon of crosscutting arises “whenever
3342	3379	based aspect oriented middleware architecture approach. 4.2 Horizontal Decomposition Principles The horizontal decomposition method consists of five principles synthesized from our past experience  and the ongoing application of AOP to middleware architecture. These principles are listed following a logical order in which they can be sequentially applied. Principle 1: Recognize the relativity
3342	3379	shows that middleware aspects, such as the interception support and the dynamic invocation semantics, can be completely separated from the core implementation and transparently super-imposed back . Later in this paper, we show that maintaining class-directional can even be achieved for crosscutting concerns of a much larger scale. The property of class-directional does imply a strong
3342	3379	relationships among the aspects that we have identified so far. Each “o” in the table means the row aspect crosscuts the column aspect. We also include three aspects identified in our previous work , including portable interceptor (3) Adding oneway to DIIssupport (PI), local invocation (LI), and the dynamic programming interface (DPI), as these relationships were not explored previously. We
3342	3381	middleware applications of AOP  primarily focus on modularizing non-functional properties  as aspects and treat the middleware core as a monolithic building block. Our observation  reveals that the poor modularization of crosscutting concerns is an inherent phenomenon within this monolithic core of current middleware implementations. By “inherent” we mean that, for a certain
3342	3381	based aspect oriented middleware architecture approach. 4.2 Horizontal Decomposition Principles The horizontal decomposition method consists of five principles synthesized from our past experience  and the ongoing application of AOP to middleware architecture. These principles are listed following a logical order in which they can be sequentially applied. Principle 1: Recognize the relativity
3342	3381	such as DII or DSI; and the asynchronous invocation style denoted by the oneway keyword in CORBA’s IDL. These features are “woven” into the stubs and skeletons by an aspect-aware IDL compiler . 2. Messaging Layer: Client-side and Server-side. This layer consists of two conceptual components: the clientside “downcall” mechanism responsible for marshalling the requests and the server-side
3382	3383	(like coverage of a certain area) and communications connectivity (for routing data back to users) are maintained. There are several examples of prototype and commercial sensors of this type . In this work, we assume that sensor units consist of different components such as processors, memory, Global Positioning System (GPS) receivers, radio transmitter and various sensing modalities.
3382	3386	positions of non-GPS equipped sensors deployed on a disk area ¡s? ? ¦ ¥ following a top-down methodology for the design of desired emergent behaviors in multiple agent-based systems (MAS) , , . Assume that we are given a placement, , of sensors from . Without loss of generality, let . This means that ??????? the ? first sensors are GPS-enabled and the remaining ones ? ??? ?
3382	3387	to the true values, ????? ? ??????? ? ????????????? . Our technique is based on a global potential function to be minimized with a decentralized, possibly asynchronous, gradient descent method . The effectiveness of the approach relies upon the fact that the gradient vector is locally computable. This is one of the key aspects of our top-down methodology in which the computation of a
8918961	3604	through aspect recomposing or overriding. The security aspect library (JSAL) is simple, flexible and extensible. It can be integrated seamlessly with software development. JSAL is built upon JCE  and JAAS  in AspectJ. As mentioned above, abstract aspect with abstract pointcut in AspectJ provides the basic mechanism for reusing aspects. Furthermore, popular security packages in Java, upon
8918961	3418	application in the process shown above. 3 JSAL Example We have simply assessed the usability of JSAL through a practical case upon what Bart De Win, Wouter Joosen and Frank Piessens have done in . They used AOSD techniques to modularize the security features within the jFTPd . Four new aspects were added by them to deal with crosscutting behavior: two aspects (FTPSession,
3454	3455	sort have been given when the constraints on the predictors are sequential. Variations of HMMs, conditional models and sequential variations of random Markov fields all provide efficient solutions . However, in many important situations, the structure of the problem is more general, resulting in a computationally intractable problem. Problems of these sorts have been studied in computer
3454	3455	although different in its technical approach and generality to other approaches that attempt to learn several different classifiers and derive global decisions by inference over their outcomes . It could be contrasted with other approaches to sequential inference or to general Markov random field approaches . The key difference is that in these approaches, the model is learned
3454	3455	that prevents an inference of the type: “C lives in A; C does not live in B”). We note that a large number of problems can be modeled this way. Examples include problems such as chunking sentences , coreference resolution and sequencing problems in computational biology. In fact, each of the components of our problem here, the separate task of recognizing named entities in sentences and the
3454	3455	loc (HP, Palo Alto) live in per loc (Bush, US) kill per per (Oswald, JFK) In order to focus on the evaluation of our inference procedure, we assume the problem of segmentation (or phrase detection)  is solved, and the entity boundaries are given to us as input; thus we only concentrate on their classification.sWe evaluate our linear programming based global inference procedure, LPR against two
3454	3456	sort have been given when the constraints on the predictors are sequential. Variations of HMMs, conditional models and sequential variations of random Markov fields all provide efficient solutions . However, in many important situations, the structure of the problem is more general, resulting in a computationally intractable problem. Problems of these sorts have been studied in computer
3454	3456	classifiers and derive global decisions by inference over their outcomes . It could be contrasted with other approaches to sequential inference or to general Markov random field approaches . The key difference is that in these approaches, the model is learned globally, under the constraints imposed by the domain. In our approach, predictors do not need to be learned in the context of
3454	3457	intractable problem. Problems of these sorts have been studied in computer vision, where inference is typically done over low level measurements rather than over higher level predictors . In the context of natural language, the typical processing paradigm is the “pipeline” approach, where learners are being used at one level, and their outcomes are being used as features for a
3454	3458	intractable problem. Problems of these sorts have been studied in computer vision, where inference is typically done over low level measurements rather than over higher level predictors . In the context of natural language, the typical processing paradigm is the “pipeline” approach, where learners are being used at one level, and their outcomes are being used as features for a
3454	3459	use all these when learning to identify relations between entities. Sometimes a sequential type approach is being used within the pipeline paradigm, when a Viterbi-like algorithm can be used . In addition to accumulating errors, it is clear that the sequential processing is a crude approximation to a process in which interactions occur across levels and down stream decisions often
3454	3460	possible assignments, which is too large even for a small n. The key insight to the technical solution we suggest comes from recent techniques developed in the context of approximation algorithms . Following this work, we develop a linear programming formulation and show how to cast our problem in it. However, we still need an integral solution, and this formalisms does not guarantee it. In
3454	3460	to “round” the solutions to integer solutions is needed. Under some assumptions on the cost function, which do not hold in our case, there exist rounding procedures that guarantee some optimality . However, such rounding procedures do not always exist. We discuss the issue of integer solutions to linear programs and provide evidence that for our target problems, rounding is not required –
3454	3460	procedure may not be a legal solution to the problem, although under some conditions that do not hold here, it can be shown that the rounded solution is a good approximation to the optimal solution . Instead of studying rounding, we take a different route and study the theory of integer solutions to linear programs. It turns out, that under some conditions on the coefficient matrix of the
3454	3461	although different in its technical approach and generality to other approaches that attempt to learn several different classifiers and derive global decisions by inference over their outcomes . It could be contrasted with other approaches to sequential inference or to general Markov random field approaches . The key difference is that in these approaches, the model is learned
3454	3463	The significance of this is clearly shown in our experimental results. 2 The Relational Inference Problem We consider the relational inference problem within the reasoning with classifiers paradigm . This paradigm investigates decisions that depend on the outcomes of several different but mutually dependent classifiers. The classifiers’ outcomes need to respect some constraints that could
3454	3465	formalism of Markov Random Field (MRF)stheory . Rather than doing that, for computational reasons, we first use a fairly standard transformation of MRF to a discrete optimization problem (see  for details). Specifically, under weak assumptions we can view the inference problem as the following optimization problem, which aims to to minimize the objective function that is the sum of the
3454	3465	Computational Approach to Relational Inference Unfortunately, it is not hard to see that the optimization problem 1 is computationally intractable even when placing assumptions on the cost function . The computational approach we adopt is based on a linear programming formulation of the problem. We first provide an integer linear programming formulation to Eq. 1, and then relax it to a linear
3454	3465	to “round” the solutions to integer solutions is needed. Under some assumptions on the cost function, which do not hold in our case, there exist rounding procedures that guarantee some optimality . However, such rounding procedures do not always exist. We discuss the issue of integer solutions to linear programs and provide evidence that for our target problems, rounding is not required –
3454	3468	loc (HP, Palo Alto) live in per loc (Bush, US) kill per per (Oswald, JFK) In order to focus on the evaluation of our inference procedure, we assume the problem of segmentation (or phrase detection)  is solved, and the entity boundaries are given to us as input; thus we only concentrate on their classification.sWe evaluate our linear programming based global inference procedure, LPR against two
3454	3469	the first and second argument entities respectively. In addition, Table 1 presents some patterns we use. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW , a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (entity classes or relation classes,
3454	3470	the first and second argument entities respectively. In addition, Table 1 presents some patterns we use. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW , a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (entity classes or relation classes,
8918985	3507	are numerous. Magnenat-Thalmann et al. have researched clothes , hair  and wrinkles . Finger nails and clothes are also discussed in DeRose et al. . Baraff and Witkin  study the numerical stability problem for a mass-spring model of clothes. Research in this area also overlaps with medical applications . Interactive systems that use soft tissue models
8918988	3514	of the readback frame, thereby incurring extra overhead. The amount of inter-bitstream regularities present relies heavily on the amount of static configuration data and placement of static kernels . For Benchmark1, which has no static kernel, inter-bitstream techniques performs marginally better than the corresponding intra-bitstream techniques by taking advantage only of the random
8918988	3516	regularities can be leveraged. This is the best configuration compression method known to date. Besides LZSS, other dictionary compression schemes such as the Lempel-Ziv-Welch (LZW) method , and LZ77  were also reported in the literature. However, they tend to fair less well. DIFFERENCE VECTOR (DV) COMPRESSION An analysis of configuration bitstreams reveals a high degree of
8918988	3517	largely unexplored. In this paper, we first propose a new intra-bitstream compression technique. Our results show that this technique competes favorably with the best known previous technique. We then extend the technique to take advantage of interbitstream redundancies as well as partial reconfigurability supported by modern FPGAs such as Xilinx Virtex family. To the best of our
8918988	3517	by dictionary-based methods. Dictionary-based algorithms depend less on specific features of the FPGA configuration architecture, providing for greater flexibility. Configuration compression  based on the Lempel-Ziv-StorerSzymanski (LZSS)  compression scheme has been shown to be effective for the Xilinx Virtex family of FPGAs. These algorithms require that an extended version of the
8918988	3517	CLBs may share high regularity. Between such frames, one frame may be converted into another simply by flipping a few bits. LZSS compression produces good results in highly regular bitstreams . However, when the lengths of the matches are small, LZSS compression proves to be less effective as the savings is offset by the encoding overhead. An analysis of our benchmark bitstreams reveals
8918988	3517	and DV compression require a suitable configuration sequence of frames such that similar frames are next to each other thereby improving the compression ratio. We use the algorithms presented in  to generate the configuration sequence for LZSS compression and modify it suitably to adapt it to DV compression. Inter-frame Regularity Graph (IRG) An inter-frame regularity graph (IRG)  is a
8918988	3517	configuration sequence that does not have readback corresponds to the shortest Hamiltonian path (i.e., a path that visits each vertex exactly once) in the IRG. A greedy algorithm is presented in  that starts with the minimum cost edge and expands its both ends to cover all the vertices. This algorithm generates close to optimal results. For LZSS compression, this scheme requires extension
8918988	3519	cells sharing common configurations, the regularities upon which the algorithm leverages upon may not be present. Runlength encoding techniques have also been proposed for this type of architecture . On the whole, however, this architecture is clearly not appropriate for large FPGAs, bringing into question this entire class of methods. Configuration cloning  exploits regularity and
8918988	3523	for this type of architecture . On the whole, however, this architecture is clearly not appropriate for large FPGAs, bringing into question this entire class of methods. Configuration cloning  exploits regularity and locality in bitstreams by copying configuration data from one region of the FPGA to several other regions. Without loading the entire bitstream, the whole FPGA can be
5102421	3531	in mixed reality environments, presents a largely different set of issues especially those having to deal with tangible interaction as we address in the present paper. In 1997 Hinckley et al  presented a formal user study of interactive 3D rotation comparing mouse-driven Virtual Sphere with multidimensional input devices and found out that multidimensional input tasks presented a clear
5102421	3532	devices. While user interfaces in immersive and mixed reality environments are not new, not many usability tests have been systematically conducted so far. Kato et al in a usability study  present five main objectives for tangible interfaces. Object affordances should match physical constraints to task requirements. Moreover, interfaces should support parallel activities when
5102421	3536	display scenarios, they identify some problems such as marker cluttering that arise in supporting complex interactions in semi-constrained environments. In a similar application Shelton and Hedley  report usability findings for the use of Mixed Environments in classroom settings. Their findings underscore the importance of physical inspection and direct “control” over the environment. In a
3563154	3544	a mobile computing environment, where hosts are partially or intermittently connected to other hosts. While there has been some discussion of de-centralized network management using mobile agents , the problem of mobile nodes (and so strongly time-varying topology) has received little attention. However, we will argue below that ad-hoc networks provide a useful framework for discussing the
3563154	3547	where it fails to meet the needs of the local environment. This kind of authority delegation is not catered for by SNMP-like models. Policy based management attempts to rectify some of these issues. What we find then is that there is another kind of networking going on: a social network, superimposed onto the technological one. The needs of small clusters of users override the broader strokes
3563154	3548	onto the technological one. The needs of small clusters of users override the broader strokes painted by wide area management. This is the need for a scaled approach to system management. 5 Configuration management in ad hoc networks Configuration management deals with the problem of establishing and maintaining a policy conformant configuration on workstations and other hosts
3563154	3549	case, then the temporary unavailability of a node to a central resource would not necessarily imply its isolation from fresh, critical data. This kind of data distribution has been discussed before in connection with the scalability of software distribution. On the down side, peer to peer reliance is clearly an open invitation to engage in denial of service activity. 6 Predictability and
3563154	3550	not reaching every host. If hosts hear policy, they must accept and comply, if not, they fall behind in the schedule of configuration. Monitoring in distributed systems has been discussed in ref. . ? The capacity of the ? ? central manager is ??? now shared between the average number of hosts able, thus £ ? ??¤ ? ¢ ? ? ?s£¢ ? ? ??© ??? ? ? ? ? ? ? that is avail(17) 7 Controller Figure 2: Model
3563154	3551	of a datagram multicast is not necessarily required, but the effective dissemination of the policy around the network is an application layer flood.sinternode capacity. The latter scales as ? ? ? ? . Hence, for sufficiently large ? , the controller and AHN will fail collectively to convey updates to the net. This failure will occur at a threshold value defined by £?¦ ?¥¤ ¨ ¨ ? ? ? ¢s?¢¨¢ ? ?s£
3563154	3552	of a datagram multicast is not necessarily required, but the effective dissemination of the policy around the network is an application layer flood.sinternode capacity. The latter scales as ? ? ? ? . Hence, for sufficiently large ? , the controller and AHN will fail collectively to convey updates to the net. This failure will occur at a threshold value defined by £?¦ ?¥¤ ¨ ¨ ? ? ? ¢s?¢¨¢ ? ?s£
11100072	2515	Avramidis and Hyden (1999) do further work on improving stochastic mesh estimators. Another line of research combines simulation with regression (Carrière 1996, Tsitsiklis and Van Roy 1999, Longstaff and Schwartz 2001). These papers differ in their details; what follows is an algorithm in their spirit. The basic idea is to approximate the continuation value Ct(St) in condition (4) by regressing the simulated
2909851	3588	events have upon the nal outcomes. 3sSince the early 1980's, in uence diagrams have been used in a wide variety of di erent applications. Some examples are medical diagnosis ; clinical decisions ; optimisation of oil spill response ; and the e ect of res on biodiversity . 3.2 Constructing In uence Diagrams There are three di erent types of node in an in uence diagram. These three
2909851	3781	consequences of making any changes to the diagram, some form of computer automation is useful. For this work the in uence diagram was implemented using a proprietary software package called HUGIN . Tables are constructed in Hugin to specify how the Feasibility node and the Utility node calculate their values from the values of the nodes directly connected to them. For each node, a decision
5442650	1228	bagof-words as the representative of the web documents.  only obtained 37% of accuracy using Yahoo with 151 classes and 50 documents. Their best attained result was 45% on 100 test documents.  had the worst result with only 2.13% at their preliminary experiment. Later, after they had increased the size of the sample training, the result improved to 36.5% accuracy (average). The poor
5442650	1237	Our accuracy result (precision on topic node) is quite comparable to the results produced by the other topic identification systems that use bagof-words as the representative of the web documents.  only obtained 37% of accuracy using Yahoo with 151 classes and 50 documents. Their best attained result was 45% on 100 test documents.  had the worst result with only 2.13% at their preliminary
5442650	1240	amount of information available in Internet has attracted many IR researchers to focus their works on web documents. In IR, most of the document categorization or classification use bags-of-words 1  to represent the documents that needed to be categorized or classified. This kind of representation is not very suitable to be used to analyze the web document since a web document is more complex
5442650	1241	sources of words by using NLP technique. But, before we describe our work further, we have to get a clear view on the impact of NLP technique towards this problem by looking at others’ works. In ???s paper, the effectiveness of the IR process is done by adding words that have lexical relationships to the query vector. Based on the experimental results, the most effective way of improving the
8919004	3787	k-? model. Each of these models offers some degree of improvement for modelling flows involving pressure gradients and strong streamline curvature. The RNG model was developed by Yakhot et al.  and is based on renormalised group theory arguments. It contains an additional source/sink term in the ? equation and takes better account of the physics. McKenzie has used the RNG k-? model in the
1473182	3597	with new candidate requirements . Commonly, market-driven software developing organisations provide successive releases of the software product and release planning is an essential activity . A major challenge in market-driven RE is to prioritise and select the right set of requirements to be implemented in the next release , while avoiding congestion in the selection process .
1473182	3794	eventual architectural decisions and the knowledge gained from the actual design of the subsequent releases. By using, for example, a cost-value prioritisation approach with pairwise comparisons , an ordered priority list can be obtained where the requirements with a higher market value and a lower cost of development are sorted in the priority order list before the requirements with a
1473182	3794	the bar chart may not show the appropriate requirements priorities. Nevertheless, the consistency check proved that the prioritisation was performed carefully and few judgment errors were made . Finally, the decision categories that emerged during the root cause analysis may not reflect the typical kinds of decisions. A different set of requirements would probably generate a different set
1473182	3604	example in the characteristics of stakeholders and schedule constraints . Requirements are often invented by the developers as well as elicited from potential customers with different needs , and it is common to use a requirements 2 Focal Point AB, Linköping, Sweden {joachim.karlsson, stefan.olsson}@focalpoint.se database that is continuously enlarged with new candidate requirements
1473182	3604	product and release planning is an essential activity . A major challenge in market-driven RE is to prioritise and select the right set of requirements to be implemented in the next release , while avoiding congestion in the selection process . This decision-making is very challenging as it is based on uncertain predictions of the future, while crucial for the product’s success on
6403918	3637	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3639	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3640	Then, we only need to consider consistency problems when distributing those objects. This paper did not address further analyses to optimize communication or execution scheduling. The Orca language  is a pioneering effort on object-based program execution in distributed environments. Hawk  runtime system supports partitioned objects for distributed applications. Such efforts are very
6403918	3641	Then, we only need to consider consistency problems when distributing those objects. This paper did not address further analyses to optimize communication or execution scheduling. The Orca language  is a pioneering effort on object-based program execution in distributed environments. Hawk  runtime system supports partitioned objects for distributed applications. Such efforts are very
6403918	3643	work, we have experimented with general graph partitioning and mapping and we have explored the issues of communication generation, partial replication, distributed consistency and synchronization . Parting from general graph representation and partitioning, these aspects are similar in our present work. However, in our previous work, we only looked at data parallel applications and specific
6403918	3644	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3645	Our approach is static, and it considers also class instance interaction. An approach similar to the distributed shared memory paradigm is to implement a distributed JVM as global object space . This approach virtualizes a single Java object heap across machine boundaries, implicitly ensuring location transparency. There are two types of objects. Node local objects are reachable from a
6403918	3647	is to find these instances and their relations. The appendix lists the quad internal representation for the Student class. We use rapid type analysis to compute the call graph and the program types . Then, for each method in the graph we compute the class relations by looking at field access and method call statements. As in , a usage relation between two classes occurs when one class
6403918	3648	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3649	not address further analyses to optimize communication or execution scheduling. The Orca language  is a pioneering effort on object-based program execution in distributed environments. Hawk  runtime system supports partitioned objects for distributed applications. Such efforts are very inspiring for work on program partitioning, scheduling and execution consistency. 6.3 Java Program
6403918	3650	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3652	repartition the application dynamically.s6.2 Distributed Shared Memory Another approach to transparently schedule applications onto multiple resources is the distributed shared memory approach . The main problem in this approach is to ensure consistency between address spaces. Since the approach is at the memory level, each read or write to memory has to be synchronized. Our approach is
6403918	3653	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3655	repartition the application dynamically.s6.2 Distributed Shared Memory Another approach to transparently schedule applications onto multiple resources is the distributed shared memory approach . The main problem in this approach is to ensure consistency between address spaces. Since the approach is at the memory level, each read or write to memory has to be synchronized. Our approach is
6403918	3656	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3017	the user would only run an application on a (possibly mobile) device (in a network) and would not be aware of how the application is actually executed. An approach closer in goals with ours is . The idea is to transparently off-load portions of service to relieve memory and processing constraints on resource-constraint devices. The partitioning is dynamic, based on application monitoring
6403918	3659	instead of arrays and affine array accesses, we deal with heap allocated objects. We also assign statically each object to a partition using multi-objective, general graph partitioning algorithms . Instead of computing the location, we keep track of abstract locations based on processor and object identifiers. Therefore, we can also generate communication statically. However, this is only a
6403918	3661	coupled distributed execution (processes loosely synchronizing via message exchange). ¢ Analysis: An improved implementation of an object dependence graph construction algorithm by Andre Spiegel . We improve the original analysis in the following aspects: – We analyze programs at byte-code level instead of sourcecode level. – We use an intermediate representation and a reduced set of
6403918	3661	object dependence graph construction from Java byte-code. 3. OBJECT DEPENDENCE GRAPH CONSTRUCTION Our graph construction algorithm builds on an existing object graph analysis algorithm described in . We preserve some of the semantic of the original algorithm, while changing significantly the algorithmic aspects. The most significant differences are: ¢ The original analysis targets one
6403918	3661	analysis to compute the call graph and the program types . Then, for each method in the graph we compute the class relations by looking at field access and method call statements. As in , a usage relation between two classes occurs when one class calls methods or accesses fields of another class. Export or import relations occur when new types may propagate from one class to
6403918	3661	experiment with various partitioning strategy to respond to various optimization targets in heterogeneous environments. We have presented an improved implementation of an existing static algorithm  to construct the object dependence graph for an application. Our implementation of the algorithm constructs the object dependence graph directly from Java byte-code. Also, we target a flexible
6403918	3662	of the call graph. Thus, the single and summary class instance approximations are more precise. This implementation leads to more accurate results than both, the byte-code and original, Pangaea  implementation. Both of the latter approaches fail to identify control independence inter-procedurally, leading to a larger set of summary class instances and a smaller set of single class
6403918	3662	and adaptive repartitioning. The results in Table 2 show the number of nodes and edges for the class and class instance graphs in the three implementations: Pangaea original implementation , and our implementations in both quad and byte-code versions. The sizes of the class relation graphs in Pangaea are sometimes bigger. This is because, in our versions of the algorithm, we treat
6403918	3662	First, it classifies objects as anchored and mobile. Second, it converts all references into indirect references. The communication middleware is RMI and the approach is quite expensive. Pangaea  is a system that can distribute Java programs using arbitrary middleware (Java RMI, CORBA) to invoke objects remotelly. The system is based on the original algorithm by Spiegel that we also use.
6403918	3665	objects can reside anywhere and thus can escape the local address space. We control the partitioning and analyze the dependences and thus, we know exactly where each object resides. J-orchestra  transforms Java byte-code into distributed Java applications. This is also an abstract shared memory implementation, consisting of two steps. First, it classifies objects as anchored and mobile.
6403918	3666	apply an iterative scheduling that tries to optimize their execution. Another approach  is to find optimal schedules for special DAGs (fork, join, coarse-grain trees, some fine-grain trees). In  the control flow graph structure contains information on merge nodes, distribution flow edges and relationship edges. The relationship edges use reaching definition data flow information. The
6403918	3667	is to find these instances and their relations. The appendix lists the quad internal representation for the Student class. We use rapid type analysis to compute the call graph and the program types . Then, for each method in the graph we compute the class relations by looking at field access and method call statements. As in , a usage relation between two classes occurs when one class
6403918	3668	to incorporate new strategies. Thus, we can plug in any partitioning strategy to our partitioning interface, without changing the system. We build our infrastructure on an existing compiler, Joeq . We use Joeq front-end to transform the byte-code into an intermediate representation suitable for our analyses and transformations. However, our approach is not limited to handling Java byte-code.
6403918	3670	concurrency support) is one research area that has investigated the partitioning problem mainly for scientific programs typically targeting a significant reduction in CPU or memory consumption . There are two main differences between partitioning for scientific applications and our work. Most of the previous work focuses on array partitioning, or loop iteration partitioning in scientific
6403918	3671	the communication and computation. The idea is to find the dominant and sensitive program components and then apply an iterative scheduling that tries to optimize their execution. Another approach  is to find optimal schedules for special DAGs (fork, join, coarse-grain trees, some fine-grain trees). In  the control flow graph structure contains information on merge nodes, distribution
9308593	3678	(and not vice versa). The promise is said to be onesidely dependent on both the promiser and the promisee, since without these the promise could not exist, whereas the converse is not the case . The promiser and promisee as persons do not require the existence of the promise in order to exist. Having said this, there are certain things that must be present, call them founding relations,
9308593	3678	hand there are two lines drawn from a box with a broken edge to another box with a broken edge (e.g., between the obligation and claim boxes), it indicates that there exists a mutual dependence . Two objects are mutually dependent on one another if neither one can exist without the other. The promise itself is composed of a three-part structure: the act of speaking, the act of registering,
3680	3875	but to present a comparison of the major characteristics of the two main protocols for short-range terrestrial communications. 2 A SURVEY OF BLUETOOTH AND IEEE 802.11 2.1 BLUETOOTH Bluetooth  is a standard for wireless communications based on a radio system designed for short-range, cheap communications devices suitable for substituting cables for printers, faxes, joysticks, mice,
3680	3681	has adopted the work done for Bluetooth (without any major changes) and made it an IEEE standard, namely IEEE 802.15.1 (Figure 1). The future of Bluetooth may be based on ultra-wide band (UWB) . UWB systems use very high-speed, precisely timed impulses for transmitting information over a very wide spectrum; this is very different from most other transmission schemes, which modulate a sine
3680	3685	vector) is a counter set by the station to compute the expected end of the current transmission. Figure 7: How PCF and DCF alternate 2 . The PCF, as described in the standard, has many drawbacks ; in fact, it is not implemented in any commercial device. The IEEE 802.11e amendment corrects this situation by redefining the QoS aspects of the multiple access protocol. The new coordination
8919029	3716	rates. Indeed, an increase in the quantum yield of photosynthesis (Fv/Fm) has always been observed within a day after Fe addition during in situ fertilization experiments in the Antarctic Ocean (Boyd and Abraham, 2001; Barber, 2002; Gervais et al., 2002) and elsewhere (Behrenfeld et al., 1996). This increase is directly reflected in the rate of carbon fixation measured by 14 C incubations (Kolber et al., 1994;
8919029	3730	algal carbon fixation in the ocean is a link between the atmospheric andsJOURNAL OF PLANKTON RESEARCH j VOLUME 26 j NUMBER 8 j PAGES 885–900 j 2004 oceanic compartment of the global carbon cycle (Falkowski et al., 1998). Specifically, diatom blooms in the Southern Ocean are assumed to be followed by a significant export flux of carbon out of the euphotic zone and carbon dioxide (CO 2) drawdown (De Baar and Boyd,
8919080	3821	perform poorly wheneversthere are no prominent point scatterers, such as with bland sea-floor images from mud-bottomed shallow harbours. In these environments, the Shear Average based algorithms  perform better, except in the presence of strong scatterers. Hybrid algorithms, such as Phase Matching Autofocus (PMA) , and global optimisation techniques based on image quality measures
8918928	3836	effects. 1. INTRODUCTION We are working on an Event Based Speech recognition system (EBS) that combines knowledge-based acoustic parameters (APs) with a statistical framework for recognition . In EBS, the speech signal is first segmented into the broad classes: vowel, sonorant consonant, strong fricative, weak fricative and stop. This segmentation is based on acoustic events (or
8918928	2249	of nasals), and an energy ratio, a low spectral peak measure and a formant density measure to capture the nasal murmur. The four APs are combined using a separate Support Vector Machine (SVM) ,  based classifier for the three different cases of prevocalic, postvocalic and intervocalic sonorant consonants. Note that we do not consider cues for vowel nasalisation in the present study.
678683	3840	interior and exterior hexahedral meshes are required, for example, the interior mesh of the volume inside the solvent accessibility surface of the biomolecule mouse acetylcholinesterase (mAChE) , and the exterior mesh between the solvent accessibility surface and an outer sphere. Since the most important part in the geometric structure of mAChE is the cavity, we need to generate finer mesh
678683	3840	are interested in some special areas based on their physical or biological applications. For example, there is a cavity in the structure of the biomolecule called mouse acetylcholinesterase (mAChE) . A finer mesh is required around the cavity area while a coarse mesh needs to be kept in other regions. In this situation, the error function should be defined by regions. Figure 1 shows the
678683	3841	coarser meshes are kept in other areas. The cavity is shown in the red boxes. starting octree level using a bottom-up surface topology preserving octree-based algorithm. An approach provided in  is used to check whether a fine isosurface is topologically equivalent to a coarse one or not. Generally correct topology is guaranteed in the uniform mesh. The dual contouring method  proposes
678683	3841	part of CUBIT , the paving algorithm places elements starting from the boundary and works in . Different from the decomposition and the advancing front techniques, the dual contouring method  extracts uniform quadrilateral meshes from volumetric imaging data to approximate isosurfaces which can be an arbitrary geometry. Unstructured Hex Mesh Generation: Eppstein  started from a
678683	3841	hex meshes for some geometry, but have not been proven to be robust and reliable for an arbitrary geometric domain. Zhang et al.   extended the dual contouring isosurface extraction method  to uniform hexahedral mesh generation. This method is robust and reliable for an arbitrary geometry, but adaptive meshes are preferable and mesh quality needs to be improved. Quality Improvement:
678683	3841	We assign a sign to each grid point in the volumetric data. If the function value at a grid point is greater than the isovalue, then the sign is 1, otherwise it is 0. An approach is described in  to check whether a fine isocontour is topologically equivalent to a coarse one or not. The fine and coarse isocontour is topologically equivalent with each other if and only if the sign of the
678683	3841	meshes. It is more challenging to generate quadrilateral meshes since not every polygon can be decomposed into quads directly. The uniform quadrilateral mesh extraction algorithm is simpler , but adaptive meshes are more preferable than uniform ones. There are two main problems in adaptive quadrilateral mesh extraction. 1. How to decompose a quad into finer quads. 2. How to calculate
678683	3844	mesh can be refined adaptively by using those templates. 5.2 3D Mesh Decomposition Indirect Method: Adaptive and quality tetrahedral meshes have been generated from volumetric imaging data  , therefore we can obtain hexahedral meshes by decomposing each tetrahedron into four hexahedra. Figure 8: Adaptive hexahedral mesh decomposition (Method 1). Left - a 2D example; Middle - a small
678683	3844	meshes of mAChE and the human head. • The feature sensitive error function. • Areas that users are interested in. • Finite element calculation results. The feature sensitive error function   is defined as the difference of trilinear interpolation functions between coarse and fine octree levels normalized by the gradient magnitude. It is sensitive to areas of large geometric feature
678683	3844	which can generate adaptive and quality 2D (triangular/quadrilateral) and 3D (tetrahedral/hexahedral) meshes from volume data. The algorithm of tetrahedral mesh generation is described in  . In this software, error tolerances and isovalues can be changed interactively. Our results were computed on a PC equipped with a Pentium III 800MHz processor and 1GB main memory. Our algorithm has
678683	3844	preserving octree-based algorithm is used to select a starting octree level, at which we extract uniform meshes with correct topology using the dual contouring isosurface extraction method   . Then we extended it to adaptive quadrilateral and hexahedral mesh generation using some predefined templates without introducing any hanging nodes. The position of each boundary vertex is
678683	3847	In the CUBIT project  at Sandia National Labs, a lot of research has been done to automatically recognize features and decompose geometry into mapped meshable areas or volumes. As reviewed in  , there are indirect and direct methods for unstructured quad/hex mesh generation. The indirect method is to generate triangular/tetrahedral meshes first, then convert them into quads/hexes. The
678683	3854	and isosurface extraction. The grid-based approach generates a fitted 3D grid of hex elements on the interior of the volume, and hex elements are added at the boundaries to fill gaps   . The grid-based method is robust, but tends to generate poor quality elements at the boundaries. Medial surface methods are to decompose the volume to map meshable regions, and fill the volume with
678683	3854	does not consider its neighboring information, each quad is refined independently. If a quad needs to be refined, then the resulting mesh has 5 elements and 4 newly inserted vertices. In Method 2  and 3, various decomposition methods are chosen according to the cell which generates a quad node and also needs to be refined. Method 2 and 3 are only different in Case (2b), Method 2 generates
678683	3854	there are 22 necessary templates , but not all the templates can be decomposed into hexahedra. Figure 9 shows five templates for adaptive hexahedral decomposition and the detailed view , which are much more complicated than the templates of 2D quadrilateral decomposition. Figure 10 lists the number of elements and the number of newly inserted vertices for each template. We set a
3855	3858	mechanism is new, other work has examined dynamically varying associativity to balance power and performance. Albonesi examines turning off “ways” of each set to save power when cache demand is low . Powell et al. evaluate the balance between incremental searches of the sets to balance power and performance, as we do with our multicast versus increIPC 1.0 0.5 0.0 1 MB 2 MB 4 MB 8 MB 16 MB
3855	3859	times the transistor’s drawn gate length in microns . floating-point benchmarks from the SPEC2000 suite , six SPEC2000 integer benchmarks, three scientific applications from the NAS suite , and one speech recognition benchmark called Sphinx . For each benchmark we simulated the sequence of instructions which capture the core repetitive phase of the program. The phases were
3855	3860	, and as Kessler et al. did to optimize for speed . Our concept of bank sets does not lend itself well to more creative set mappings to reduce conflicts, such as skewed associativity . Other researchers have examined using multiple banks for high bandwidth, as we do to reduce contention. Sohi and Franklin  proposed interleaving banks to create ports, and also examined the
3855	3864	and Hwu ). While the D-NUCA scheme leaves data with low locality in banks far from the processor, an alternative approach is not to cache those lines at all. González, Aliagas, and Valero  examined a cache organization that could adaptively avoid caching data with low locality and a locality detection scheme to split the cache into temporal and spatial caches. Tyson et al.  also
3855	3866	the physical topology of a bank set. Generational replacement was recently proposed by Hallnor et al. for making replacement decisions in a software-managed UCA called the Indirect Index Cache . In our scheme, when a hit occurs to a cache line, it is swapped with the line in the bank that is the next-closest to the controller from the bank holding the accessed line. With that policy,
3855	3866	studied issues associated with large caches. Kessler examined designs for multi-megabyte caches built with discrete components, and not in a wire-dominated technology . Hallnor and Reinhardt  studied a fully associative software-managed design for large, on-chip L2 caches, but not did not consider non-uniform access times in future technologies. The concept of bank sets permits flexible
3855	3868	inverter driving four copies of itself. Delays measured in FO4 are independent of technology, and one FO4 roughly corresponds to 360 pico-seconds times the transistor’s drawn gate length in microns . floating-point benchmarks from the SPEC2000 suite , six SPEC2000 integer benchmarks, three scientific applications from the NAS suite , and one speech recognition benchmark called Sphinx
3855	3869	Sciences Tech Report TR-02-10 The University of Texas at Austin cart@cs.utexas.edu — www.cs.utexas.edu/users/cart expected to continue to increase as the bandwidth demands on the package grow , as smaller technologies permit more bits pers¡ £¢ , and as larger workloads produce correspondingly larger working sets. Demonstrating the likely trend toward even larger on-chip memory systems is
3855	3870	an updated version of Cacti 3.0 which enables the user to optimize for both banking and sub-banking. However, to more realistically model large caches, we replace the Rubenstein RC wire delay model  in Cacti 3.0 with a more aggressive repeater and scaled wire model of Agarwal et al.  for the long address and data busses to the banks. Our complete cache delay model includes the access time
3855	3871	in direct-mapped on-chip caches by virtually binding regions of the address space to portions of the cache, as well as adapting the block size to different workload needs (as did Johnson and Hwu ). While the D-NUCA scheme leaves data with low locality in banks far from the processor, an alternative approach is not to cache those lines at all. González, Aliagas, and Valero  examined a
3855	3872	a tremendous area penalty. Popular cache modeling tools, such as Cacti, enable fast exploration of the cache design spaces by automatically optimizing for sub-bank count, size, and orientation . However, as cache capacities grow, existing cache architectures and the tools used to model them break down. First, large caches are much more sensitive to wire delays and great care must be taken
3855	3875	shows the anticipated load on the L2 cache by listing the number of L2 accesses per 1 million instructions given 64KB level-1 instruction and data caches (this metric was proposed by Kessler et al. ). 3 Uniform Access Caches Modern level-two caches no longer employ a single monolithic data array, and instead are subdivided into multiple smaller sub-banks to minimize the access time. In
3855	3876	we do with our multicast versus increIPC 1.0 0.5 0.0 1 MB 2 MB 4 MB 8 MB 16 MB 180nm 130nm 100nm 70nm 50nm (c) All Benchmarks mental policies , and as Kessler et al. did to optimize for speed . Our concept of bank sets does not lend itself well to more creative set mappings to reduce conflicts, such as skewed associativity . Other researchers have examined using multiple banks for
3855	3876	processors. Maintaining coherence among different logical caches presents a different set of challenges for NUCA architectures. A variant of the partial tag compare scheme of Kessler et al.  may permit fast discovery of shared blocks without necessitating slow, huge centralized tag banks. Acknowledgments This project is supported by the Defense Advanced Research Projects Agency under
3855	3877	floating-point benchmarks from the SPEC2000 suite , six SPEC2000 integer benchmarks, three scientific applications from the NAS suite , and one speech recognition benchmark called Sphinx . For each benchmark we simulated the sequence of instructions which capture the core repetitive phase of the program. The phases were determined empirically by plotting the L2 miss rates over one
3855	3880	of the sets to balance power and performance, as we do with our multicast versus increIPC 1.0 0.5 0.0 1 MB 2 MB 4 MB 8 MB 16 MB 180nm 130nm 100nm 70nm 50nm (c) All Benchmarks mental policies , and as Kessler et al. did to optimize for speed . Our concept of bank sets does not lend itself well to more creative set mappings to reduce conflicts, such as skewed associativity . Other
3855	3883	1 lists the number of instructions skipped to reach the phase (FFWD) and the number of instructions simulated (RUN). A more rigorous method of choosing simulation phases will be used in future work . Finally, Table 1 shows the anticipated load on the L2 cache by listing the number of L2 accesses per 1 million instructions given 64KB level-1 instruction and data caches (this metric was proposed
3855	3884	architecture and wedded them to an existing microarchitecture simulator. To estimate the cache bank delay, we used Cacti 3.0 which accounts for capacity, organization, and fabrication technology . Since Cacti produces timing estimates in nanoseconds, we converted these cache delays to processor cycles by assuming an aggressive clock of 8 FO4 inverter delays per cyclesPhase L2 load Phase L2
3855	3887	Valero  examined a cache organization that could adaptively avoid caching data with low locality and a locality detection scheme to split the cache into temporal and spatial caches. Tyson et al.  also proposed a scheme to bypass the cache with low-locality data. 7 Summary and Conclusions Non-uniform accesses are just starting to appear in highperformance cache designs . In this paper,
3855	3889	a tremendous area penalty. Popular cache modeling tools, such as Cacti, enable fast exploration of the cache design spaces by automatically optimizing for sub-bank count, size, and orientation . However, as cache capacities grow, existing cache architectures and the tools used to model them break down. First, large caches are much more sensitive to wire delays and great care must be taken
3890	3892	given call of aSubject.detach(anObserver), and before any subsequent call of aSubject.notify() or of aSubject.attach(anObserver), the set of observers known by aSubject must not contain anObserver.  A given call of aSubject.notify() must be followed by calls of observers.update(), and all these calls must precede any subsequent call of aSubject. attach(anObserver), of
3890	3892	erroneous orders. A form of temporal logic would provide the right level of abstraction to express the behavioral properties expected of all pattern occurrences. Some recent research efforts  have begun to investigate the integration of temporal operators within OCL, reusing the current part of OCL for atoms of temporal logic formulas. Although this work is very valuable and necessary,
3890	3894	work 4.1 Related work PatternWizard is one of the most extensive projects of design pattern specification, and has influenced our research work in several points. PatternWizard proposes LePUS  a declarative, higher order language, designed to represent the generic solution, or leitmotif, indicated by design patterns. Our work differs from PatternWizard in two aspects. First, we use UML
3890	3895	details (e.g. associations, methods) and concentrate on more important tasks. We can also foresee tool support for design patterns in UML as a help to designers in overcoming some adversities . More precisely, a tool can ensure that pattern constraints are respected, relieve the designer of some implementation burdens, and even recognize pattern occurrences within source code,
3890	3896	PatternWizard works at the code level and is not integrated to any design model. An approach to the validation of design patterns through their precise representation is proposed by Görel Hedin . She uses attribute grammars to precisely model a pattern and explicit markers in a program to distinguish a pattern occurrence and validate it. Patterns are represented as a set of class,
3890	3903	in a collaboration, nor is it possible to describe behavioral constraints (e.g. operation A should call operation B). These limitations were extensively discussed in previous work by the authors . In this paper, we propose some solutions to overcome these problems. A misunderstanding with the term role might be a possible source of the present inadequacy of collaborations to model design
3911	3912	the geometry of the extracted isosurface. Static data decompositions that guarantee good external memory performance and parallel load balancing for large rectilinear grids were first introduced in  and successively improved with random data distribution  and view-dependent data selection . The coarse-to-fine adaptive refinement used in  avoids loading fine resolution data for
3911	3913	The common idea is to find an on-disk arrangement of the volume grid that allows selective loading of cells, and avoids loading those that clearly do not intersect the isosurface. Chiang et al.  adapt the idea of treating isosurface extraction as an interval stabbing problem  to an outof-core setting. They rearrange the on-disk layout of the volume grid into clusters of cells or meta
3911	3914	selective loading of cells, and avoids loading those that clearly do not intersect the isosurface. Chiang et al.  adapt the idea of treating isosurface extraction as an interval stabbing problem  to an outof-core setting. They rearrange the on-disk layout of the volume grid into clusters of cells or meta cells that are referenced by an interval tree. This allows them to querysonly those
3911	3915	some generalizations to compress irregular volume meshes . Efficient encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire volume grid , others compress only a single isosurface for a particular isovalue  by encoding its occupancy grid plus information to refine the vertex placement. In either case the respective grid is
3911	3916	rectilinear grids were first introduced in  and successively improved with random data distribution  and view-dependent data selection . The coarse-to-fine adaptive refinement used in  avoids loading fine resolution data for isosurfaces that are further from the viewpoint. Recent developments  deal with accurate estimate of the isosurface extraction cost to fine-tune the
3911	3917	data has motivated research on geometric compression. Most works have focused on irregular triangulated surface meshes , with some generalizations to compress irregular volume meshes . Efficient encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire volume grid , others compress only a single isosurface for a
3911	3918	would otherwise be implicit. The need for compact representations of geometry data has motivated research on geometric compression. Most works have focused on irregular triangulated surface meshes , with some generalizations to compress irregular volume meshes . Efficient encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire
3911	3919	some generalizations to compress irregular volume meshes . Efficient encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire volume grid , others compress only a single isosurface for a particular isovalue  by encoding its occupancy grid plus information to refine the vertex placement. In either case the respective grid is
3911	3920	data has motivated research on geometric compression. Most works have focused on irregular triangulated surface meshes , with some generalizations to compress irregular volume meshes . Efficient encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire volume grid , others compress only a single isosurface for a
3911	3920	points of the cells visited during range propagation are compressed using predictive coding. Our approach is similar to the compression technique for hexahedral volume meshes by Isenburg and Alliez . Each iteration of their encoding algorithm compresses a hexahedron that is face-adjacent to one or more previously compressed hexahedra. This involves specifying local connectivity around the
3911	3920	it could not be decided from the previously decoded data alone. A processed cell is in one of nine configurations faceadjacent to previously visited cells. These configurations are detailed in  and we provide them for convenience in Figure 1. The grid points whose scalar values are not yet encoded are called free points, and exist only in the hut, step, and corner configurations. We
3911	3920	Figure 1: Nine different configurations in which a cell (blue) can be face-adjacent to processed cells (green). Free grid points are drawn as unfilled circles. We use a prediction scheme similar to  to compress the scalar values at the free points. We predict the value at a grid point based on values from previously encoded grid points, compute the difference between the predicted and the
3911	3921	data. Stream algorithms that seamlessly operate on large data sets using relative little memory have been developed for separate tasks, such as simplification, compression, and mesh generation . But stream processing also creates the possibility of pipelining these algorithms together to work concurrently on the same dataset. In order to stream-process geometric data it must be kept in a
3911	3921	while it is being extracted. It also allows us to directly compress the isosurface as we write it to disk, or for transmission to a remote display site. Related prior work. Isenburg and Lindstrom  present the underlying theory of creating and working with a streaming representation for polygonal meshes. They define coherent and compatible orderings of mesh vertices and triangles and present
3911	3921	the final mesh will have, it does know when vertices will no longer be used for subsequent triangles. It comes therefore quite natural to output the extracted mesh using a streaming format . A streaming mesh format can be as simple as the ASCII example in Figure 3. It interleaves vertices and the triangles that use them, and in addition specifies when vertices are referenced for the
3911	3921	would mean that the mesh is no longer produced in a single pass. When producing large meshes, there is a big payoff in arranging vertices and triangles in a coherent manner. Isenburg and Lindstrom  describe metrics that measure, and diagrams that visualize the coherency in a mesh layout (e.g. in the ordering of vertices and triangles) and advocate to produce streaming meshes that are low in
3911	3921	mesh is determined by the traversal of the volumetric cells, which is breadth-first. It was shown that a breadth-first traversal results in sufficiently low width for most practical applications , and furthermore assures that the “lifetime” of each vertex is proportional to the width, which translates into low span. It should be noted that although we traverse the volume grid component by
3911	3923	a particular mesh layout is. They also mention a straight-forward, but I/Oinefficient, method for creating streaming isosurface output. Their implementation for marching-cubes isosurface extraction  loads the volume grid layer by layer, outputs all vertices of one volume layer, followed by a set of triangles, and always finalizes the vertices from the previous layer before moving on to the
3911	3924	encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire volume grid , others compress only a single isosurface for a particular isovalue  by encoding its occupancy grid plus information to refine the vertex placement. In either case the respective grid is compressed with a complete layer by layer traversal. In contrast, our scheme
3911	3925	would otherwise be implicit. The need for compact representations of geometry data has motivated research on geometric compression. Most works have focused on irregular triangulated surface meshes , with some generalizations to compress irregular volume meshes . Efficient encodings for regular volume grids has been investigated in two contexts. Some approaches compress the entire
3911	3928	= f ?1 (k), for k ? R. The set of cells Ak that intersect the isosurface Ik are called active cells; these are organized into face-connected components. Isosurfaces can be extracted by continuation : starting from a seed cell in an isosurface component, the entire component can be extracted by breadth-first traversal through the cell adjacency graph. A seed set is a set of cells that contains
3911	3930	that guarantee good external memory performance and parallel load balancing for large rectilinear grids were first introduced in  and successively improved with random data distribution  and view-dependent data selection . The coarse-to-fine adaptive refinement used in  avoids loading fine resolution data for isosurfaces that are further from the viewpoint. Recent
3911	3931	performance and parallel load balancing for large rectilinear grids were first introduced in  and successively improved with random data distribution  and view-dependent data selection . The coarse-to-fine adaptive refinement used in  avoids loading fine resolution data for isosurfaces that are further from the viewpoint. Recent developments  deal with accurate estimate of
3932	3934	unique characteristics of these interfaces with regard to their evaluation, which is usually done by applying methods that have been shown to be effective for the evaluation of WIMP user interfaces . MODEL-BASED DESIGN FOR NEW USER INTERFACES Even though the models and formalisms that have been cited in the previous section can be thought of being simple and easy to apply, they can give us the
3932	3935	are usually applied at two different levels: • The first level make use of specification practices such as task hierarchical analysis (HTA) , annotated scene graph  or UML use cases . The level of abstraction at this point is, therefore, high, and pursues the description of the user’s tasks and the interaction processes that relates the user and the application objects. • A
3932	3937	interaction objects and techniques (usually called widgets 3D ) that can be claimed to be shared by most of the different applications, although there is an ongoing research effort in this topic . A similar discussion could be made on the current role of usability engineering on the development of these new user interfaces, although this matter is outside the scope of this paper. Anyhow, it
3932	3938	the development of new user interfaces are reviewed in this paper. Keywords Model-based design, new user interfaces INTRODUCTION The new user interfaces that are usually known as non- or post-WIMP  are those that aim to free us from the tyranny of the PC desktop and its windows-based interface, offering a style of interaction much closer to the human being that is not only based on the
3932	3938	interaction processes that relates the user and the application objects. • A second level is based on formalisms such as augmented transition nets (ATNs) , Petri nets , data-flow diagrams  and state charts . This formalisms are used to describe the fine-grained detail of the system, such as the interaction techniques that the user will use in the application or the different
3932	3938	has been done on new formalisms that describe both the continuous and discrete flows of information that characterizes these new interfaces, a kind of formalisms that are known as hybrid notations . The connection between the two previous levels is usually established through the refinement of the hierarchy of tasks, until the low-level subtasks are identified, which correspond to the
3932	3942	the user’s tasks and the interaction processes that relates the user and the application objects. • A second level is based on formalisms such as augmented transition nets (ATNs) , Petri nets , data-flow diagrams  and state charts . This formalisms are used to describe the fine-grained detail of the system, such as the interaction techniques that the user will use in the
3932	3942	been done on new formalisms that describe both the continuous and discrete flows of information that characterizes these new interfaces, a kind of formalisms that are known as hybrid notations . The connection between the two previous levels is usually established through the refinement of the hierarchy of tasks, until the low-level subtasks are identified, which correspond to the basic
3932	3945	development of new user interfaces are reviewed in this paper. Keywords Model-based design, new user interfaces INTRODUCTION The new user interfaces that are usually known as non- or post-WIMP  are those that aim to free us from the tyranny of the PC desktop and its windows-based interface, offering a style of interaction much closer to the human being that is not only based on the
10547806	3969	concerning the schools' professional culture (Staessens, 1990). For both these phases a multiple case design was set up in order to be able to generalize the &quot;ndings on a legitimate basis (Firestone, 1993; Yin, 1989). In the second phase a case study was made of two schools, as the attention was focused on the relations between the concepts that were explored in the &quot;rst phase and on the patterns
4041	4044	is not necessarily calibrated, e.g., the Naive Bayes classifier.Procedures for calibrating classifiers have been proposed in different contexts: In weather prediction tasks , in game theory , and more recently in the context of pattern classification . Zadrozny and Elkan were also the first to notice the need of calibrating classifiers when used as decision making aids. Our own
4041	4045	for calibrating classifiers have been proposed in different contexts: In weather prediction tasks , in game theory , and more recently in the context of pattern classification . Zadrozny and Elkan were also the first to notice the need of calibrating classifiers when used as decision making aids. Our own incentive to study calibration came from applying probabilistic
4041	4045	used for scheduling purposes, we also need to accompany each forecast withsan accurate estimate of the probability of the forecast. We applied a variant of the calibration procedure suggested in  and noticed that in addition to producing more accurate estimates, the classification accuracy of our induced classifiers increased. While these empirical results agree with those of Zadrozny and
4041	4045	process of calibrating may be seen as the process of bringing ˆp(C|X) closer to the real density. Calibrating a classifier is a mapping from ˆp(c|x) to p(c|t). In fact the procedures proposed in  essentially implement this mapping. Thus, under certain conditions we outline below the optimal threshold ? ? of the original classifier is one where in the calibration mapping p(c|? ? ) = 0.5. To
4041	4045	on t for which p(C = 1|t) = c10 c10+c01 .s5 Calibration with finite data With finite data sets, we want to estimate p(C = 1|ˆp(C = 1|x)) reliably. A procedure for this estimation was provided in , where ˆp(C = 1|x) is binned on the interval  and the calibration map is estimated by counting the number of samples that fall into each bin. The procedure was originally suggested as a
4041	4046	for calibrating classifiers have been proposed in different contexts: In weather prediction tasks , in game theory , and more recently in the context of pattern classification . Zadrozny and Elkan were also the first to notice the need of calibrating classifiers when used as decision making aids. Our own incentive to study calibration came from applying probabilistic
4041	4046	noticed that in addition to producing more accurate estimates, the classification accuracy of our induced classifiers increased. While these empirical results agree with those of Zadrozny and Elkan , a theoretical guarantee that calibration cannot degrade classification performance was still missing. Our investigation of the calibration produced the following results which we prove in Sections
4041	4046	process of calibrating may be seen as the process of bringing ˆp(C|X) closer to the real density. Calibrating a classifier is a mapping from ˆp(c|x) to p(c|t). In fact the procedures proposed in  essentially implement this mapping. Thus, under certain conditions we outline below the optimal threshold ? ? of the original classifier is one where in the calibration mapping p(c|? ? ) = 0.5. To
4041	4046	on t for which p(C = 1|t) = c10 c10+c01 .s5 Calibration with finite data With finite data sets, we want to estimate p(C = 1|ˆp(C = 1|x)) reliably. A procedure for this estimation was provided in , where ˆp(C = 1|x) is binned on the interval  and the calibration map is estimated by counting the number of samples that fall into each bin. The procedure was originally suggested as a
4041	4046	with small training sets. Reducing the possibility of overfitting can be done by smoothing of the calibration map or estimating a smooth function (such as the sigmoid) as the calibration map . As a note, the number of thresholds on the decision function would depend on the smoothing function used, e.g., with a sigmoid, only one 10 5sthreshold can be found, which might not be always
4041	4047	at which the forecaster predicted C = 1 with probability t on a set of N samples, with N ? ?. As such, given the probability density of the features, p(x), ?(t) can be expressed as: ?(t) = ? p(x) . (6) x?Rt Let p(C = 1|t) be the probability that C = 1 given that the forecaster predicts C = 1 with probability t. The Brier score can be rewritten as (see  for derivation): BS = ? ?(t)(t ? p(c|t))
4041	4048	minimize the classification error. We show that when a single threshold is derived from the calibration procedure, the result is equivalent to finding the point of minimum error in an ROC curve . However, when calibration produces multiple thresholds on the decision rule, the error achieved with those is lower than that of any single threshold derived from the ROC based methods. Thus, in
4041	4048	= 1) thus by varying the threshold ?, we can generate the entire ROC curve using the calibration map. At this point it is clear that methods that find the threshold of minimum error from ROC curves  produce the exact same result as the calibration procedure, when the calibration map does not cross 1/2 more than once. However, the calibration procedure generalizes more than what can be achieved
4041	4048	especially with small sample sizes. Extending the method beyond binary classification problems is another research direction; similar to methods extending ROC curves beyond binary classification . We are also exploring the use of calibration in semi-supervised learning, helping eliminate the possibility of performance degradation when using unlabeled data to learning classifiers, a
4041	4051	calibrating Naive Bayes classifiers, but is applicable to any classifier that outputs probabilities, or a distance measure that can be converted to probabilities (e.g., Tree-augmented Naive Bayes , Logistic regression, mixture models, and SVMs). The empirical success of calibration on various (typically large sized data sets) has been shown in previous works – in this section we aim at
4041	4052	the possibility of performance degradation when using unlabeled data to learning classifiers, a phenomenon that occurs with biased models that output uncalibrated a-posteriori probabilities . Acknowledgments We thank Terence Kelly both for his help and suggestions and his work on the I/O response time prediction. We also thank Kim Keeton for providing the I/O data, Tom Fawcett for his
3260	4063	The performance of the algorithm is tunable by choosing the value of x. A larger x results in a larger probability of finding a feasible path and a larger overhead. Awerbuch et al. algorithm : Awerbuch et al. proposed a throughput-competitive routing algorithm for bandwidthconstrained connections. The algorithm tries to maximize the amortized (average) throughput of the network over
3260	4064	achieved by the best off-line algorithm that is assumed to know all of the connection requests in advance. ( c ) The competitive routing for connections with unknown duration was studied in . A survey for the competitive routing algorithms was done by Plotkin . Summary: All the above algorithms require a global state to be maintained at every node. Most algorithms transform the
3260	4065	topology aggregation, which abstracts a group by a single logical node. There are other types of aggregation using different simple topologies to replace a group. Their performance was studied in . Each physical node maintains an aggregated network image. The image maintained at node A.a.1 is shown in Figure 3 (e). It stores different portions of the network in different details. More
3260	4065	an alternative path. 5.3 Strengths and weaknesses of hierarchical routing The hierarchical routing has long been used to cope with the scalability problem of source routing in large internetworks . The PNNI (Private Network-Network Interface)  standard for routing in ATM networks is also hierarchical. The hierarchical routing scales well because each node only maintains a partial global
3260	4067	an alternative path. 5.3 Strengths and weaknesses of hierarchical routing The hierarchical routing has long been used to cope with the scalability problem of source routing in large internetworks . The PNNI (Private Network-Network Interface)  standard for routing in ATM networks is also hierarchical. The hierarchical routing scales well because each node only maintains a partial global
3260	4069	is feasible if and only if it satisfies the delay constraint. Ma-Steenkiste algorithm : Ma and Steenkiste showed that when a class of WFQ-like (Weighted Fair Queueing) scheduling algorithms  are used, the end-to-end delay, delay-jitter, and buffer space bounds are not independent. They are functions of the reserved bandwidth, the selected path and the traffic characteristics.
3260	4070	which may cause the overall computational overhead excessively high. Path precomputation and caching were studied to make a tradeoff between the processing overhead and the routing performance . 6.2 Distributed routing algorithms Wang-Crowcroft algorithm : Wang and Crowcroft proposed a hop-by-hop distributed routing scheme. Every node pre-computes a forwarding entry for every possible
3260	4071	the multicast tree, a feasible extension of the tree is found. The worst-case message complexity of the above receiver-initiated probing is O(e) for a single receiver. Carlberg-Crowcroft algorithm : The spanning-joins approach was proposed by Carlberg and Crowcroft for the construction of multicast trees across different domains . A new group member broadcasts a join-request message. When
3260	4071	best path according to the QoS information carried by the received reply messages. Reverse path multicasting, time to live field, and directed spanning joins are used to reduce the message overhead . An excellent recent work was done by Faloutsos et al. . It improves the performance of spanningjoins by the help of a Manager router. Based on the topology information, the Manager router
3260	4072	or unbounded integer numbers. If all metrics except one take bounded integer values, then the problems are solvable in polynomial time by running an extended Dijkstra’s (or Bellman-Ford) algorithm . 3 If all metrics are dependent on a common metric, then the problems may also be solvable in polynomial time. For example, the worst-case delay and delay jitter are functions of bandwidth in
3260	4072	(1) Source O(kve) (1) Global Zero Guerin-Orda  Bandwidth-constrained r. Source O(vlogv + e) Imprecise global Zero Delay-constrained r. Source Polynomial (2) Imprecise global Zero Chen-Nahrstedt  Bandwidth-cost-constrained r. Source O(xve) (3) Global Zero Wang-Crowcroft  Bandwidth-optimization r. Distributed O(ve) Global O(v) Salama et al.  Delay-constrained least-cost r.
3260	4072	by 8.0 and the cost is bounded by 5. A feasible path is s ? u ? v ? t, which, as expected, is also a feasible path for the original problem. (c) the cost-mapping table. Chen-Nahrstedt algorithm : Chen and Nahrstedt proposed a heuristic algorithm for the NP-complete multi-pathconstrained routing problem. We have already known in Section 4.1 that if all metrics except one take bounded
3260	4072	Figure 7 for an example. The new problem with the link cost bounded by x+1 can be solved in polynomial time by an extended Dijkstra’s algorithm (EDSP) or an extended Bellman-Ford algorithm (EBF) . It was proved that a feasible path of the new problem must also be a feasible path of the original problem. The performance of the algorithm is tunable by choosing the value of x. A larger x
3260	4073	is made on a hop-by-hop basis. Some floodingbased algorithms do not require any global state to be maintained. The routing decision and optimization is done entirely based on the local states . The distributed routing algorithms which depend on the global state share more or less the same problems of the source routing algorithms. The distributed algorithms which do not need any global
3260	4073	least-cost r. Distributed O(v) Global O(v) Cidon et al.  Generic r. (5) Distributed O(e) Global O(e) (6) Shin-Chou  Delay-constrained r. Distributed O(e) Local O(e) Chen-Nahrstedt  Generic r. (5) Distributed O(e) Local O(e) PNNI  Generic r. (5) Hierarchical Polynomial (7) Aggregated O(v) • v is the number of nodes and e is the number of edges. • After a source routing
3260	4073	Zero Rouskas-Baldine  Delay-constrained least-cost r. Source O(klgv 4 ) (3) Global Zero Kompella et al.  Delay-constrained least-cost r. Distributed O(v 3 ) Global O(v 3 ) Chen-Nahrstedt  Generic r. Distributed O(ge) local O(ge) • v is the number of nodes, e is the number of edges, and g is the number of destinations. • After a source routing algorithm constructs a multicast tree, a
3260	4073	one message sent along every link. Another flooding-based routing algorithm was proposed by Hou . It routes virtual circuits with delay requirements in ATM networks. Chen-Nahrstedt algorithm : 1) selective probing  Chen and Nahrstedt proposed a distributed routing framework based on selective probing. After a connection request arrives, probes are flooded selectively along those
3260	4073	continues until every destination is included in the tree. The above algorithm requires intensive multi-pass message exchange. The worst-case message complexity is O(v 3 ). Chen-Nahrstedt algorithm : Chen and Nahrstedt extended their distributed unicast routing algorithms  (Section 6.2) for multicast routing. Probes (routing messages) are flooded from the source toward the destinations of
3260	4074	environment. The imprecision directly affects the routing performance. Therefore, the design of routing algorithms for large networks should take the information imprecision into consideration . Distributed and Hierarchical Routing: Source routing based on the complete global state is generally not scalable because of the following reasons. The communication overhead to maintain the
3260	4074	In particular, the distributed algorithm based on selective probing  uses only local states, and no shortest-path computation is conducted at a single node. The ticket-based probing algorithm  works with imprecise state information, which allows relatively infrequent state updates. The hierarchical routing provides a clean solution to the scalability problem. It maintains an aggregate
3260	4075	decoupled from resource reservation in most existing schemes, some recent proposals combine routing and resource reservation in a single multi-path message pass from the source to the destination . QoS routing and admission control: The task of admission control is to determine whether a connection request should be accepted or rejected. Once a request is accepted, the required resources
3260	4075	O(v) Salama et al.  Delay-constrained least-cost r. Distributed O(v 3 ) Global O(v 3 ) (4) Sun-Landgendorfer  Delay-constrained least-cost r. Distributed O(v) Global O(v) Cidon et al.  Generic r. (5) Distributed O(e) Global O(e) (6) Shin-Chou  Delay-constrained r. Distributed O(e) Local O(e) Chen-Nahrstedt  Generic r. (5) Distributed O(e) Local O(e) PNNI  Generic r.
3260	4075	The least-cost (least-delay) path computed based on such inconsistent information may contain a loop, which makes the control message not able to reach the destination. Cidon et al. algorithm : The distributed multi-path routing algorithms proposed by Cidon et al. combine the process of routing and resource reservation. Every node maintains the topology of the network and the cost of
3260	4075	and resumes with an alternative path. This approach works well with network dynamics. The disadvantage is longer routing time. The parallel multipath routing was proposed to overcome this problem . Routing messages are sent along multiple paths in parallel and reserve resources along the way. If more than one message arrive at the destination, the best path is selected and resources reserved
3260	4076	summarizing comparison can be found in Table 3. 7.1 Source routing algorithms MOSPF : MOSPF is a multicast extension of the unicast link-state protocol OSFP . It was based on Deering’s work . In addition to a global state, the protocol maintains at every node the membership information of every multicast group in the routing domain. The group membership change in a subnetwork is
3260	4077	is feasible if and only if it satisfies the delay constraint. Ma-Steenkiste algorithm : Ma and Steenkiste showed that when a class of WFQ-like (Weighted Fair Queueing) scheduling algorithms  are used, the end-to-end delay, delay-jitter, and buffer space bounds are not independent. They are functions of the reserved bandwidth, the selected path and the traffic characteristics.
3260	4078	routing, which is to find a path whose bottleneck bandwidth is above a required value. The link-optimization routing problem can be solved by a slightly modified Dijkstra’s algorithm  or Bellman-Ford algorithm . The linkconstrained routing problem can be easily reduced to the linkoptimization problem. For other QoS metrics such as delay, delay jitter and cost, the state of a
3260	4079	reply messages. Reverse path multicasting, time to live field, and directed spanning joins are used to reduce the message overhead . An excellent recent work was done by Faloutsos et al. . It improves the performance of spanningjoins by the help of a Manager router. Based on the topology information, the Manager router selects a subset of the on-tree nodes to send the reply messages
3260	4080	the fairness, overall throughput and average response time are the essential issues for the traditional routing. QoS routing and resource reservation: The QoS routing and the resource reservation  are two important, closely related network components. In order to provide the guaranteed services, the required resources (CPU time, buffer, bandwidth, etc.) must be reserved when a QoS connection
3260	4082	is feasible if and only if it satisfies the delay constraint. Ma-Steenkiste algorithm : Ma and Steenkiste showed that when a class of WFQ-like (Weighted Fair Queueing) scheduling algorithms  are used, the end-to-end delay, delay-jitter, and buffer space bounds are not independent. They are functions of the reserved bandwidth, the selected path and the traffic characteristics.
3260	4083	to achieve scalability is to reduce the size of the global state by aggregating information according to the hierarchical structure of large networks. Figure 3 shows the hierarchical model used by . In Figure 3 (a), nodes are clustered into the firstlevel groups. The nodes with at least one link crossing two groups are called border nodes. In Figure 3 (b), each group is represented by a
3260	4083	In summary, the source routing has the scalability problem. It is impractical for any single node to have access to the detailed 8 state information about all nodes and all links in a large network . 5.2 Strengths and weaknesses of distributed routing In distributed routing, the path computation is distributed among the intermediate nodes between the source and the destination. Hence, the
3260	4083	because the routing computation is shared by many nodes. However, as the network state is aggregated, additional imprecision is introduced, which has a significant negative impact on QoS routing . Recall that a logical node in an aggregated network image may represent a large subnet with complex internal structure and a logical link may be the abstraction of multiple physical links.
3260	4083	time by taking those functional relationships into consideration. A much further study of the QoS routing in rate-based scheduling networks was done recently by Orda . Guerin-Orda algorithm  4 : Guerin and Orda studied the bandwidth-constrained routing problem and the delayconstrained routing problem with imprecise network states. The model of imprecision is based on the probability
3260	4083	of link l having a delay of d units, where d ranges from zero to the maximum possible value. It is NP-hard to find the path that has the highest probability of satisfying a given delay constraint . But various special cases (e.g., symmetric networks and tight constraints) can be solved in polynomial time. Heuristic algorithms were proposed for the NP-hard problem. The idea is to transforming
3260	4088	MOSPF  Least-delay r. Source O(vlogv) Global Zero Kou et al.  Least-delay r. Source O(gv 2 ) Global Zero Takahashi-Matsuyama  Least-delay r. Source O(gv 2 ) Global Zero Kompella et al.  Delay-constrained least-cost r. Source O(v 3 ?) (1) Global Zero Sun-Landgendorfer  Delay-constrained least-cost r. Source O(vlogv + e) Global Zero Widyono  Delay-constrained least-cost r.
3260	4088	until all destinations are included in the tree. Constrained Steiner tree problem The problem of finding a delay-bounded least-cost multicast tree, called a constrained Steiner tree, is NP-complete . Heuristic source routing algorithms were proposed for this problem . A C.1 14 C.3 C C.1 C.2 C.2 A B C C.1 C.3 C C.2 C.1 C.2 A B C Figure 9: An example of PNNI routing A.1 A.2 A B A B
3260	4088	at B.1 C.1 C.3 C C.2 ( d ) aggregated topology at C.1 performance evaluation of these algorithms was done by Salama et al. through the extensive simulation . 1) Kompella et al. algorithm : A source routing heuristic was proposed by Kompella et al. to construct a constrained Steiner tree. The first step is to create a complete graph, where the nodes represent the source and the
3260	4089	Zero Zhu et al.  Delay-constrained least-cost r. Source O(kv 3 logv) (3) Global Zero Rouskas-Baldine  Delay-constrained least-cost r. Source O(klgv 4 ) (3) Global Zero Kompella et al.  Delay-constrained least-cost r. Distributed O(v 3 ) Global O(v 3 ) Chen-Nahrstedt  Generic r. Distributed O(ge) local O(ge) • v is the number of nodes, e is the number of edges, and g is the
3260	4089	the multicast tree. Dashed links are not in the tree. Path s ? k ? n in (a) is replaced by path s ? n in (b). The cost is reduced by 1. 7.2 Distributed routing algorithms Kompella et al. algorithm : Kompella et al. proposed a distributed heuristic algorithm for constructing the constrained Steiner tree. The algorithm requires every node to maintain a distance vector storing the minimum delay
3260	4092	routing based on the aggregated network state of the hierarchical model (Section 3). A further study of QoS routing with imprecise state based on the probability model was done by Lorenz and Orda . 4 Guerin-Orda algorithm was designed to be used in the hierarchical routing, though we present it as an independent source routing algorithm in this paper.sTable 2: Unicast routing algorithms
3260	4092	environment. The imprecision directly affects the routing performance. Therefore, the design of routing algorithms for large networks should take the information imprecision into consideration . Distributed and Hierarchical Routing: Source routing based on the complete global state is generally not scalable because of the following reasons. The communication overhead to maintain the
3260	4095	route the connection through every group. Table 3: Multicast routing algorithms Algorithm Solving problem Routing Time complexity Communication complexity strategy Maintaining state routing MOSPF  Least-delay r. Source O(vlogv) Global Zero Kou et al.  Least-delay r. Source O(gv 2 ) Global Zero Takahashi-Matsuyama  Least-delay r. Source O(gv 2 ) Global Zero Kompella et al.
3260	4095	and (4) the delay-delayjitter-constrained multicast routing. We describe the algorithms in this section. A summarizing comparison can be found in Table 3. 7.1 Source routing algorithms MOSPF : MOSPF is a multicast extension of the unicast link-state protocol OSFP . It was based on Deering’s work . In addition to a global state, the protocol maintains at every node the membership
3260	4098	connections are established and existing connections are torn down upon completion, the network state changes locally and globally, which makes the routes of the remaining connections less optimal . Routes with light (heavy) traffic at the beginning may become congested (lightly loaded) later. Shorter paths for some existing connections may become available. Re-routing helps to balance the
3260	4100	of the connection requests in advance. ( c ) The competitive routing for connections with unknown duration was studied in . A survey for the competitive routing algorithms was done by Plotkin . Summary: All the above algorithms require a global state to be maintained at every node. Most algorithms transform the routing problem to a shortest path problem and then solve it by Dijkstra’s or
3260	4102	of a single one for a connection. When there does not exist a feasible path with sufficient resources, the algorithm tries to find multiple paths whose combined resources satisfy the requirement . Transmitting contiguous data (audio and video) along multiple paths arises the problem of synchronization. In addition, it demands more buffer space at the receiving end to absorb the delay jitter
3260	4103	or a constrained Steiner tree is NPcomplete . The delay-delayjitter-constrained multicast routing problem belongs to the multi-tree-constrained routing problem class. It is also NP-complete , under the assumptions that (1) the metrics under constraints are independent and (2) they are allowed to take real numbers or unbounded integer numbers. However, this problem (or any other
3260	4103	+ e) Global Zero Widyono  Delay-constrained least-cost r. Source Exponential (2) Global Zero Zhu et al.  Delay-constrained least-cost r. Source O(kv 3 logv) (3) Global Zero Rouskas-Baldine  Delay-constrained least-cost r. Source O(klgv 4 ) (3) Global Zero Kompella et al.  Delay-constrained least-cost r. Distributed O(v 3 ) Global O(v 3 ) Chen-Nahrstedt  Generic r. Distributed
3260	4103	for finding such a replacement. The algorithm always finds a delay-constrained tree (probably not least-cost), if one exists, because it starts with a shortest path tree. Rouskas-Baldine algorithm : Rouskas and Baldine proposed a heuristic for constructing a delay-delayjitterconstrained multicast tree. The tree must have (1) bounded delay along the paths from the source to the destinations
3260	4104	and the algorithm is more scalable. Searching multiple paths in parallel for a feasible one is made possible, which increases the chance of success. Most existing distributed routing algorithms  require each node to maintain a global network state (distance vectors), based on which the routing decision is made on a hop-by-hop basis. Some floodingbased algorithms do not require any global
3260	4104	(2) Imprecise global Zero Chen-Nahrstedt  Bandwidth-cost-constrained r. Source O(xve) (3) Global Zero Wang-Crowcroft  Bandwidth-optimization r. Distributed O(ve) Global O(v) Salama et al.  Delay-constrained least-cost r. Distributed O(v 3 ) Global O(v 3 ) (4) Sun-Landgendorfer  Delay-constrained least-cost r. Distributed O(v) Global O(v) Cidon et al.  Generic r. (5)
3260	4104	if the state information at all nodes is consistent. However, in a dynamic network, the path may have a loop due to the contradicting state information at different nodes. Salama et al. algorithm : Salama et al. proposed a distributed heuristic algorithm for the NP-complete delayconstrained least-cost routing problem. A cost vector and a delay vector are maintained at every node by a
3260	4107	which may cause the overall computational overhead excessively high. Path precomputation and caching were studied to make a tradeoff between the processing overhead and the routing performance . 6.2 Distributed routing algorithms Wang-Crowcroft algorithm : Wang and Crowcroft proposed a hop-by-hop distributed routing scheme. Every node pre-computes a forwarding entry for every possible
3260	4109	of other resources. Global state: The combination of the local states of all nodes is called a global state. Every node is able to maintain the global state by either a link-state protocol  or a distance-vector protocol , which exchanges the local states among the nodes periodically. The link-state protocols broadcast the local state of every node to every other node so that
3260	4113	2 ) Global Zero Kompella et al.  Delay-constrained least-cost r. Source O(v 3 ?) (1) Global Zero Sun-Landgendorfer  Delay-constrained least-cost r. Source O(vlogv + e) Global Zero Widyono  Delay-constrained least-cost r. Source Exponential (2) Global Zero Zhu et al.  Delay-constrained least-cost r. Source O(kv 3 logv) (3) Global Zero Rouskas-Baldine  Delay-constrained
3260	4113	problem The problem of finding a delay-bounded least-cost multicast tree, called a constrained Steiner tree, is NP-complete . Heuristic source routing algorithms were proposed for this problem . A C.1 14 C.3 C C.1 C.2 C.2 A B C C.1 C.3 C C.2 C.1 C.2 A B C Figure 9: An example of PNNI routing A.1 A.2 A B A B C ( b ) aggregated topology at A.1 B.3 B.1 B.2 A B C ( c ) aggregated topology at
3260	4113	path is used to replace the minimum-cost path. The advantage of the algorithm is its low time complexity, O(v log v), which is the same complexity of Dijkstra’s algorithm.s3) Widyono algorithm : Widyono proposed several heuristic algorithms for the constrained Steiner tree problem. The one with the best performance is called the constrained adaptive ordering heuristic. At each step, a
3260	4114	is feasible if and only if it satisfies the delay constraint. Ma-Steenkiste algorithm : Ma and Steenkiste showed that when a class of WFQ-like (Weighted Fair Queueing) scheduling algorithms  are used, the end-to-end delay, delay-jitter, and buffer space bounds are not independent. They are functions of the reserved bandwidth, the selected path and the traffic characteristics.
3260	4115	the fairness, overall throughput and average response time are the essential issues for the traditional routing. QoS routing and resource reservation: The QoS routing and the resource reservation  are two important, closely related network components. In order to provide the guaranteed services, the required resources (CPU time, buffer, bandwidth, etc.) must be reserved when a QoS connection
3260	4116	?) (1) Global Zero Sun-Landgendorfer  Delay-constrained least-cost r. Source O(vlogv + e) Global Zero Widyono  Delay-constrained least-cost r. Source Exponential (2) Global Zero Zhu et al.  Delay-constrained least-cost r. Source O(kv 3 logv) (3) Global Zero Rouskas-Baldine  Delay-constrained least-cost r. Source O(klgv 4 ) (3) Global Zero Kompella et al.  Delay-constrained
3260	4116	problem The problem of finding a delay-bounded least-cost multicast tree, called a constrained Steiner tree, is NP-complete . Heuristic source routing algorithms were proposed for this problem . A C.1 14 C.3 C C.1 C.2 C.2 A B C C.1 C.3 C C.2 C.1 C.2 A B C Figure 9: An example of PNNI routing A.1 A.2 A B A B C ( b ) aggregated topology at A.1 B.3 B.1 B.2 A B C ( c ) aggregated topology at
3260	4116	as well as the destination is then inserted into the tree. The cost of links in the tree is set to zero. The above process repeats until the tree covers all destinations. 4) Zhu et al. algorithm : Zhu et al. proposed a source routing heuristic to construct the constrained Steiner tree. The algorithm allows variable delay bounds on destinations. A shortest path tree in terms of delay is
3799519	4118	the demand for personalized information is increasing rapidly and will by the end of year 2000 be Òthe most dramatic trend in corporate publishingÓ.  With an increasing information supply , the use of personalization gives a consumer promises of timesaving and better matching of given service or product to desire. Businesses see the opportunity to increase the consumerÕs satisfaction
8919158	4132	on a helipad. The Robotics Institute at Carnegie Mellon University has developed a ”visual odometer” which can visually lock-on to ground objects and sense relative helicopter position in real time , but they have not integrated vision-based sensing with autonomous landing. The problem of autonomous landing is particularly difficult because the inherent instability of the helicopter near the
8919158	4133	they have not integrated vision-based sensing with autonomous landing. The problem of autonomous landing is particularly difficult because the inherent instability of the helicopter near the ground .  have demonstrated tracking of a landing pad based on vision but have not shown landing as such. Their landing pad had a unique shape which made the problem of identification of the landing pad
8919158	4136	relative to the helicopter. These state estimates are sent to the helicopter controller. 3.3 Control Strategy The helicopter is controlled using a hierarchical behavior-based control architecture . Briefly, a behavior-based controller partitions the control problem into a set of loosely coupled behaviors. Each behavior is responsible for a particular task. The behaviors act in parallel to
8919158	4137	tests. We plan to obtain these values analytically and tune them in the future. 4 Experimental Results A total of fourteen landings were performed to validate the algorithm. We have previously  presented a detailed analysis of the results obtained in the first nine landings, which dealt with the nominal case, i.e. the helipad was stationary and always visible to the helicopter. In this
265533	4160	Box 210030, Cincinnati, Ohio 45221–0030, U.S.A. 1563 event simulation (PDES). Most simulationists prescribe to one of the two widely known distributed synchronization techniques, namely optimistic (Jefferson 1985; Steinman 1991) and conservative (Bryant 1979; Misra 1986). While optimizations to these techniques have produced aremarkable improvement in performance, researchers have constantly been faced with
265533	4160	aset of concurrently executing LPs. The LPs communicate by exchanging time-stamped messages. In order to maintain causality, LPs must process messages in strictly non-decreasing time-stamp order (Jefferson 1985; Lamport 1978). There are two basic synchronization protocols used to ensure that this condition is not violated: (i) conservative and (ii) optimistic. Conservative protocols (Bryant 1979) strictly
265533	4161	in the following section. 5 ANALYSIS Unsynchronized simulation can be applied to observe lumped properties of anumber of discrete-time Markov chains which are easy to conceptualize and understand (Kleinrock 1975). A set of random variables, Xn forms aMarkov chain if the probability that the next value (or state) is Xn+1 depends only upon the current 1 In a Time Warp based simulation kernel, the input queue
265533	4161	example being Little’s Law. We exploit this property of the queueing systems in our unsynchronized simulations. 5.1 The Queueing Model Thequeueingmodelweconsiderisaverygeneralqueueing G/G/m system (Kleinrock 1975). This is asystem whose inter-arrival time distribution A(t) is completely arbitrary and whose service time distribution B(x) is also arbitrary (all inter-arrival times are assumed to be
265533	4162	executing LPs. The LPs communicate by exchanging time-stamped messages. In order to maintain causality, LPs must process messages in strictly non-decreasing time-stamp order (Jefferson 1985; Lamport 1978). There are two basic synchronization protocols used to ensure that this condition is not violated: (i) conservative and (ii) optimistic. Conservative protocols (Bryant 1979) strictly avoid
265533	4165	(PDES). Most simulationists prescribe to one of the two widely known distributed synchronization techniques, namely optimistic (Jefferson 1985; Steinman 1991) and conservative (Bryant 1979; Misra 1986). While optimizations to these techniques have produced aremarkable improvement in performance, researchers have constantly been faced with the problem of reducing overheads in the simulation to
265533	4165	situations if no appropriate precautions are taken. Several studies on conservative mechanisms and optimizations to conservative protocols have been presented in the literature (Bryant 1979; Misra 1986). In a Time Warp simulator, each LP operates as a distinct discrete event simulator, maintaining input and output event lists, astate queue, and alocal simulation
265533	4166	the error can be recalculated with the help of statistical methods. 4 UNSYNCHRONIZED SIMULATION Is synchronization overrated? This is precisely the question we seek to investigate and study. Nicol (Nicol and Liu 1997) reminds us of the dangers of allowing “risk” when synchronizing aparallel discrete event simulation. While Nicol (Nicol and Liu 1997) reports how this problem may occur and the damage it may
265533	4169	strict causality requirement in distributed simulation. Some preliminary insight into this relaxation possibility can be found by reviewing the Time Warp optimization called rollback relaxation (Wilsey and Palaniswamy 1994). Rollback relaxation proposes the use of arelaxed recovery mechanism when memoryless logical process receive straggler events. The memoryless property can be detected using standard optimizing
265533	4169	in the input queue when rollback occurs. Similarly, logical processes representing stateless functions mapping asingle input to asingle output can be completely implemented without synchronization (Wilsey and Palaniswamy 1994). In this paper, we investigate the notion of ignoring causality violations (and thereby avoiding synchronization altogether) and study the consequences of such astep. In particular,
4171	4174	, so that this process is also known as fractional diffusion. A linear advection-fractional dispersion equation has recently been developed  which combines fractional diffusion with linear advection. The subordination model considered here is governed by a fractional partial differential
4171	4174	to an ergodic limit theorem, whereas the stream tube model varies the velocities in space according to given soil properties. Another related model is the fractional advection-dispersion equation . In the case of symmetric plumes their equation is equivalent to subordination of pure diffusion, together with a moving coordinate system to handle the
4171	4192	Saichev and Zaslavsky, 1997; Chaves, 1998], so that this process is also known as fractional diffusion. A linear advection-fractional dispersion equation has recently been developed  which combines fractional diffusion with linear advection. The subordination model considered here is governed by a fractional partial differential equation that includes
4097972	4230	completely different solution, there is no information added to the system when a good solution is found. A method that has been used lately to overcome this problem is called path relinking (PR) (Aiex et al., 2003; Resende and Ribeiro, 2003). In PR, a subset of the best solutions found is kept in a separate memory, called the elite set. At each iteration, one of the solutions s will be selected, and a
4097972	4232	are restricted to planar, unit disk graphs (Baker, 1994). Following the increased interest in wireless ad hoc networks, many approaches have been proposed for the MCDS problem in the recent years (Alzoubi et al., 2002; Butenko et al., 2002; Das and Bharghavan, 1997; Stojmenovic et al., 2001). Most of the heuristics are based on the idea of creating a dominating set incrementally, using some greedy technique.
4097972	4232	links lead to vertices which cannot be a part of the CDS. We assume, as usual, that there is a starting vertex, found by means of some leader election algorithm (Malpani et al., 2000). It is known (Alzoubi et al., 2002) that thisscan be done in O(n log n) time. We also assume that the leader vertex vl is a vertex with the smallest number of neighbors. This feature is not difficult to add to the original leader
4097972	4232	described in the previous section. For the distributed algorithm, we used the additional requirement of k-connectedness with k = 20. The algorithms used for comparison are the ones proposed in (Alzoubi et al., 2002) and (Butenko et al., 2002). They are referred to in the results (Tables 7–1 and 7–2) as AWF and BCDP, respectively. The results show that the non-distributed version of Algorithm 19 consistently
4097972	4234	breadth first search (BFS), which is known tosrun in O(D log 3 n), where D is the diameter (length of the maximum shortest path) of the network, and sends at most O(m + n log 3 n) messages (Awerbuch and Peleg, 1990). Thus, each step of our distributed algorithm has the same time complexity. 123 To speed up the process, we can change the requirements of the algorithm by asking the resulting graph to be
4097972	4236	example of use of this idea occurs in the CBT (core-based tree) algorithm (Ballardie et al., 1993). A recent method proposed for distributing data in multicast groups is called ring based routing (Baldi et al., 1997; Ofek and Yener, 1997). The idea 14sis to have a ring linking nodes in a group, to minimize costs and improve reliability. Note for example that trees can be broken by just one link failure; on the
4097972	4240	group. The set of required nodes is defined as the union of source and destinations. This technique is one of the most studied for multicast tree construction, with many algorithms available (Bauer and Varma, 1995; Chow, 1991; Chen et al., 1993; Kompella et al., 1992, 1993b,a; Hong et al., 1998; Kompella et al., 1996; Ramanathan, 1996). In the remaining of this and the next sections we discuss the versions
4097972	4241	all nodes in R. The nodes in V \ R can be used if needed, and are called “Steiner” points. This is a classical N P-hard problem (Garey and Johnson, 1979), and has a vast literature on its own (Bauer and Varma, 1997; Du et al., 2001; Du and Pardalos, 1993b; Hwang and Richards, 1992; Hwang et al., 1992; Kou et al., 1981; Takahashi and Matsuyama, 1980; Winter, 1987; Winter and Smith, 1992). Thus, in this
4097972	4247	challenging. However, in some cases the problem of computing an acceptable virtual backbone can be reduced 115sto the well known minimum connected dominating set problem in unit-disk graphs (Butenko et al., 2002). 116 Given a simple undirected graph G = (V, E) with the set of vertices V and the set of edges E, a dominating set (DS) is a set D ? V such that each vertex in V \ D is adjacent to at least one
4097972	4247	unit disk graphs (Baker, 1994). Following the increased interest in wireless ad hoc networks, many approaches have been proposed for the MCDS problem in the recent years (Alzoubi et al., 2002; Butenko et al., 2002; Das and Bharghavan, 1997; Stojmenovic et al., 2001). Most of the heuristics are based on the idea of creating a dominating set incrementally, using some greedy technique. Some approaches try to
4097972	4247	section. For the distributed algorithm, we used the additional requirement of k-connectedness with k = 20. The algorithms used for comparison are the ones proposed in (Alzoubi et al., 2002) and (Butenko et al., 2002). They are referred to in the results (Tables 7–1 and 7–2) as AWF and BCDP, respectively. The results show that the non-distributed version of Algorithm 19 consistently gives results which are not
4097972	4250	constraints are modeled in what is called the multicast packing problem in networks. This problem has attracted some attention in the past few years (Wang et al., 2002; Priwan et al., 1995; Chen et al., 1998). The congestion ?e on edge e is given by the sum of all load imposed by the groups using e. The maximum congestion ? is then defined as the maximum of all congestion ?e, over edges e ? E. If we
4097972	4254	data. Common examples of such applications are multimedia distribution systems (Pasquale et al., 1998), video-conferencing (Eriksson, 1994), software delivery (Han and Shahmehri, 2000), group-ware (Chockler et al., 1996), and game communities (Park and Park, 1997). Multicast is a technique used to facilitate this type of information exchange, by routing data from one or more sources to a potentially large number
4097972	4254	have become increasingly important for many organizations due to the large number of applications of multicasting, which include data distribution, video-conferencing (Eriksson, 1994), groupware (Chockler et al., 1996), and automatic software updates (Han and Shahmehri, 2000). Due to the lack of multicast support in existing networks, there is an arising need for updating unicast oriented networks. Thus, there
4097972	4255	required nodes is defined as the union of source and destinations. This technique is one of the most studied for multicast tree construction, with many algorithms available (Bauer and Varma, 1995; Chow, 1991; Chen et al., 1993; Kompella et al., 1992, 1993b,a; Hong et al., 1998; Kompella et al., 1996; Ramanathan, 1996). In the remaining of this and the next sections we discuss the versions of this
4097972	4255	cost. Using this technique, the source and destinations are the required nodes, the remaining ones being the Steiner nodes. Many heuristic algorithms have been proposed for this kind of problem (Chow, 1991; Feng and Yum, 1999; Kompella et al., 1993b,a; Kumar et al., 1999; Salama et al., 1997b; Sriram et al., 1998). The problems above, however, consider that all nodes support a multicast protocol.
4097972	4259	(Baker, 1994). Following the increased interest in wireless ad hoc networks, many approaches have been proposed for the MCDS problem in the recent years (Alzoubi et al., 2002; Butenko et al., 2002; Das and Bharghavan, 1997; Stojmenovic et al., 2001). Most of the heuristics are based on the idea of creating a dominating set incrementally, using some greedy technique. Some approaches try to construct a MCDS by finding
4097972	4076	(Park and Park, 1997). Multicast is a technique used to facilitate this type of information exchange, by routing data from one or more sources to a potentially large number of destinations (Deering and Cheriton, 1990). This is done in such a way that overall utilization of resources in the underlying network is minimized in some sense. To handle multicast routing, many proposals of multicast technologies have
4097972	4076	(1988), Eriksson (1994), and Wall (1980). Some examples of multicast protocols are PIM – Protocol Independent Multicast (Deering et al., 1996), DVMRP – Distance-Vector Multicast Routing Protocol (Deering and Cheriton, 1990; Waitzman et al., 1988), MOSPF – Multicast OSPF (Moy, 1994a), and CBT – Core Based Trees (Ballardie et al., 1993). See Levine and Garcia-Luna-Aceves (1998) for a detailed comparison of diverse
4097972	4261	not exactly the topological center, but which can be thought of as a good approximation. Along these lines we have algorithms using core points (Ballardie et al., 1993) and also rendez-vous points (Deering et al., 1994). 17sIt is interesting to note that, for simplicity, most of the papers which try to create routing trees using center-based techniques simply disregard the N Pcomplete problem and try to find
4097972	4261	minimized in some sense. To handle multicast routing, many proposals of multicast technologies have been done in the last decade. Examples are the MBONE (Eriksson, 1994), MOSPF (Moy, 1994a), PIM (Deering et al., 1996), core-based trees (Ballardie 4set al., 1993) and shared tree technologies (Chiang et al., 1998; Wei and Estrin, 1994). Each proposed technology requires the solution of (usually hard)
4097972	4261	made to create technology supporting multicast routing, such as 6sby Deering (1988), Eriksson (1994), and Wall (1980). Some examples of multicast protocols are PIM – Protocol Independent Multicast (Deering et al., 1996), DVMRP – Distance-Vector Multicast Routing Protocol (Deering and Cheriton, 1990; Waitzman et al., 1988), MOSPF – Multicast OSPF (Moy, 1994a), and CBT – Core Based Trees (Ballardie et al., 1993).
4097972	4261	efficient way, avoiding duplication of transmissions, and therefore saving bandwidth. With this aim, special purpose multicast protocols have being devised in the literature. Examples are the PIM (Deering et al., 1996) and core-based (Ballardie et al., 1993) distribution protocols. The basic operation in these routing protocols is to send data for a subset of nodes, duplicating the information only when
4097972	4078	inside a subnetwork. In OSPF, each router in the network is responsible for maintaining a table of paths for reachable destinations. This table can be created using the Dijkstra’s algorithm (Dijkstra, 1959) to calculate shortest paths from the current node to all other destinations in the current sub-network. This process can be done deterministically in polynomial time, using at most O(n 3 )
4097972	4078	note that this problem is solvable in polynomial time, since the paths from source to destination are considered separately. Shortest path algorithms such as, for example, the Dijkstra’s algorithm (Dijkstra, 1959), can be used to achieve this objective. A second objective is to minimize the total cost of the routing tree. This is again an additive metric, where we look for the minimum sum of costs for edges
4097972	4078	s to t with minimum cost. The solution of shortest path problems is required in most implementations of routing algorithms. This problem can be solved in polynomial time using standard algorithms (Dijkstra, 1959; Bellman, 1958; Ford, 1956). However, other versions of the shortest problem are harder, and cannot be solved exactly in polynomial time. An example of this occurs when we add delay constraints to
4097972	4264	nodes in V \ R can be used if needed, and are called “Steiner” points. This is a classical N P-hard problem (Garey and Johnson, 1979), and has a vast literature on its own (Bauer and Varma, 1997; Du et al., 2001; Du and Pardalos, 1993b; Hwang and Richards, 1992; Hwang et al., 1992; Kou et al., 1981; Takahashi and Matsuyama, 1980; Winter, 1987; Winter and Smith, 1992). Thus, in this subsection we give only
4097972	4264	the objective is to send data from sources to destinations with minimum cost. In the case in which there are no additional constraints, this reduces to the Steiner tree problem on graphs (Du et al., 2001). In other words, it is required to find a tree linking all destinations to the source, with minimum cost. Using this technique, the source and destinations are the required nodes, the remaining
4097972	4266	targeting larger audiences. This phenomenon became more important due to the development of new technologies such as virtual conference (Sabri and Prasada, 1985), video on demand, group-ware (Ellis et al., 1991), etc. This series of developments gave momentum for the creation of multicast routing protocols. In multicast routing, data can be sent from one or more source nodes to a set of destination nodes
4097972	4268	the Set Cover problem. Set Cover: Given a ground set T = t1, . . . , tn, with subsets S1, . . . , Sm ? T , find the minimum cardinality set C ? {1, . . . , m} such that ? i?C Si = T . It is known (Feige, 1998) that Set Cover does not have approximation algorithms for any guarantee better than O(log n). Thus, if we find a transformation from Set Cover to FSCPP that preserves approximation, we can
4097972	4268	equivalent optimal solutions. ? Corollary 12 Given an instance I of SC, and the transformation ? described above, then we have OP T (I) = OP T (?(I)). s 70sThe following theorem, proved by Feige (Feige, 1998), will be useful for our main result. Theorem 13 (Feige (Feige, 1998)) If there is some ? > 0 such that a polynomial time algorithm can approximate set cover within (1 ? ?) log n, then N P? T IME(n
4097972	4268	? reduces any gap of log n to log log k. Thus, with such an algorithm one can differentiate between instances I with a gap of log n. But this is not possible in polynomial time, according to (Feige, 1998, Theorem 10) unless N P? T IME(n O(log log n) ). ? 72s4.4 Concluding Remarks The SCPP is a difficult combinatorial optimization problem occurring in multicast networks. We have shown that the SCPP
4097972	4269	this technique, the source and destinations are the required nodes, the remaining ones being the Steiner nodes. Many heuristic algorithms have been proposed for this kind of problem (Chow, 1991; Feng and Yum, 1999; Kompella et al., 1993b,a; Kumar et al., 1999; Salama et al., 1997b; Sriram et al., 1998). The problems above, however, consider that all nodes support a multicast protocol. This is not a realistic
4097972	4080	such as multimedia delivery, are sensitive to transmission delays and require that the total time between delivery and arrival of a data package be restricted to some particular maximum value (Ferrari and Verma, 1990). The delay function d(i, j) is used to model this kind of constraint. The delay d(i, j) represents the time needed to transmit information between nodes i and j. As a typical example,
4097972	4276	Each entry reported in this table represents the averaged results over 30 instances of the stated size. Each instance was created using a random generator for connected graphs, first described in (Gomes et al., 1998). This generator creates graphs with random distances and capacities, but guarantees that the resulting instance is connected. Destinations were also defined randomly, with the number of
4097972	4277	algorithm, and subsequently improved, using some local search method. GRASP has been very successful in a number of applications such as QAP (Oliveira et al., 2003b), frequency assignment (Gomes et al., 2001), and satisfiability. The steps of GRASP are summarized in Algorithm 12. The GRASP algorithm is known to be a multi-start method, where at each iteration a new solution is constructed, and
4097972	4285	That is the reason why the most well studied version of the Steiner tree problem applied to multicast routing is the delay constrained version (Im et al., 1997; Kompella et al., 1992, 1993b,a; Jia, 1998; Sriram et al., 1998). We give in this section some examples of methods used to give approximate solutions to this problem. One of the strategies used to solve the delay constrained Steiner tree
4097972	4285	algorithms which can use this computational power in order to reduce their time complexity. A number of papers have focused on distributed strategies for delay constrained minimum spanning tree (Jia, 1998; Chen et al., 1993). A good example is the algorithm presented in Chen et al. (1993). The authors propose a heuristic that is similar to the general technique used in the KMB heuristic for Steiner
4097972	4289	union of source and destinations. This technique is one of the most studied for multicast tree construction, with many algorithms available (Bauer and Varma, 1995; Chow, 1991; Chen et al., 1993; Kompella et al., 1992, 1993b,a; Hong et al., 1998; Kompella et al., 1996; Ramanathan, 1996). In the remaining of this and the next sections we discuss the versions of this problem which are most useful, as well as
4097972	4289	for delivering of the information. That is the reason why the most well studied version of the Steiner tree problem applied to multicast routing is the delay constrained version (Im et al., 1997; Kompella et al., 1992, 1993b,a; Jia, 1998; Sriram et al., 1998). We give in this section some examples of methods used to give approximate solutions to this problem. One of the strategies used to solve the delay
4097972	4291	is one of the most studied for multicast tree construction, with many algorithms available (Bauer and Varma, 1995; Chow, 1991; Chen et al., 1993; Kompella et al., 1992, 1993b,a; Hong et al., 1998; Kompella et al., 1996; Ramanathan, 1996). In the remaining of this and the next sections we discuss the versions of this problem which are most useful, as well as algorithms proposed for them. In one of the first uses
4097972	4291	the complexity of solving exactly the routing problem, a large number of heuristic algorithms have been proposed to find good, non-optimal solutions (Ballardie et al., 1993; Hong et al., 1998; Kompella et al., 1996; Salama et al., 1997b; Sriram et al., 1999; Zhu et al., 1995). Such methods, however, lack any guarantee of local optimality. This turns out to be an important shortcoming, specially for instances
4097972	4088	source and destinations are the required nodes, the remaining ones being the Steiner nodes. Many heuristic algorithms have been proposed for this kind of problem (Chow, 1991; Feng and Yum, 1999; Kompella et al., 1993b,a; Kumar et al., 1999; Salama et al., 1997b; Sriram et al., 1998). The problems above, however, consider that all nodes support a multicast protocol. This is not a realistic assumption on existing
4097972	4296	active links only, since all other links lead to vertices which cannot be a part of the CDS. We assume, as usual, that there is a starting vertex, found by means of some leader election algorithm (Malpani et al., 2000). It is known (Alzoubi et al., 2002) that thisscan be done in O(n log n) time. We also assume that the leader vertex vl is a vertex with the smallest number of neighbors. This feature is not
4097972	4304	idea occurs in the CBT (core-based tree) algorithm (Ballardie et al., 1993). A recent method proposed for distributing data in multicast groups is called ring based routing (Baldi et al., 1997; Ofek and Yener, 1997). The idea 14sis to have a ring linking nodes in a group, to minimize costs and improve reliability. Note for example that trees can be broken by just one link failure; on the other hand, rings are
4308	4310	and scalability, are still not satisfactory. Besides, these schemes still need TTP; compromising the TTP compromises all the keys it issues. On the other hand, contributory key agreement protocols , , , , , , in which each node contributes an input to establish a common secret (which is a function of all nodes’ inputs) through successive pairwise message exchanges among the
4308	4310	presented the first key agreement protocol for 2 parties, which is the well-known Diffie-Hellman key exchange. A number of natural extensions to their scheme have been proposed for n parties , , , , , . All of these schemes are based on successively running the 2-party Diffie-Hellman key exchange on different message exchange topologies and sequences to achieve a
4308	4311	scalability, are still not satisfactory. Besides, these schemes still need TTP; compromising the TTP compromises all the keys it issues. On the other hand, contributory key agreement protocols , , , , , , in which each node contributes an input to establish a common secret (which is a function of all nodes’ inputs) through successive pairwise message exchanges among the nodes
4308	4311	presented the first key agreement protocol for 2 parties, which is the well-known Diffie-Hellman key exchange. A number of natural extensions to their scheme have been proposed for n parties , , , , , . All of these schemes are based on successively running the 2-party Diffie-Hellman key exchange on different message exchange topologies and sequences to achieve a common
4308	4311	different tree structures for group key agreement which are efficient with respect to a number of group operations such as member add, member delete, group merge and group partition. Becker et. al. demonstrated 2 protocols, namely Hypercube and Octopus, which achieve the lower bounds on the round complexity and the number of exchanged messages respectively. These key agreement schemes are
4308	4314	scheme (KPS), independently introduced by Blom  and Matsumoto et. al. , offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived
4308	4314	a common key with any other user, say j. In other words any two users can compute a common key without interaction. The scheme is against a collusion of at most w other users. Blundo et. al.  generalized Blom’s scheme to allow any t users (out of n users) to compute a common key non-interactively. They also showed that the total size of the secret keys stored at each user is optimal.
4308	4314	from the universal set (which is publicly known) to form its key ring using a certain procedure to ensure that the exclusion property is satisfied. As the majority of the existing KPS schemes , , , , ,  can be considered as a special case of the cover-free family (CFF) , we will formulate B={p 1 ,p 2 ,p 3 , ...., p k-1 , p k } P 1 P 2 .... P k P 3 P 5 P 4 P k-1 .... P
4308	4316	by Blom  and Matsumoto et. al. , offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of
4308	4316	the universal set (which is publicly known) to form its key ring using a certain procedure to ensure that the exclusion property is satisfied. As the majority of the existing KPS schemes , , , , ,  can be considered as a special case of the cover-free family (CFF) , we will formulate B={p 1 ,p 2 ,p 3 , ...., p k-1 , p k } P 1 P 2 .... P k P 3 P 5 P 4 P k-1 .... P B'={p 1
4308	4317	offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of nodes (Si ?S) later on can find or compute
4308	4317	kBS and the secret key of each node is generated from the node’s identity and kBS. Eschenauer and Gligor  applied probabilistic key predistribution to distributed sensor networks. Chan et. al.  further improved the security of this work  by using multiple paths for Eschenauer’s indirect key establishment. These two pieces of work also suggested that key pre-distribution is the only
4308	4318	to establish a common secret (which is a function of all nodes’ inputs) through successive pairwise message exchanges among the nodes in a secure manner using the 2-party Diffie-Hellman exchange , are not practical to ad hoc networks either. These protocols are fully distributed and self-organized without needing any TTP, but they are not robust to changing topology or intermittent links
4308	4318	,  by replicating the KDC in an arbitrary or hierarchical arrangement. The reliance on online TTP renders these schemes impractical for ad hoc network key management. Diffie and Hellman  presented the first key agreement protocol for 2 parties, which is the well-known Diffie-Hellman key exchange. A number of natural extensions to their scheme have been proposed for n parties ,
4308	4319	to ensure that the exclusion property is satisfied. As the majority of the existing KPS schemes , , , , ,  can be considered as a special case of the cover-free family (CFF) , we will formulate B={p 1 ,p 2 ,p 3 , ...., p k-1 , p k } P 1 P 2 .... P k P 3 P 5 P 4 P k-1 .... P B'={p 1 ',p 2 ',p 3 ', ...., p k-1 ', p k '} Fig. 1. DKS based on probabilistic CFF construction
4308	4320	practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of nodes (Si ?S) later on can find or compute
4308	4320	to Needham’s idea  except that the base station just needs to store a single root key kBS and the secret key of each node is generated from the node’s identity and kBS. Eschenauer and Gligor  applied probabilistic key predistribution to distributed sensor networks. Chan et. al.  further improved the security of this work  by using multiple paths for Eschenauer’s indirect key
4308	4321	by Blom  and Matsumoto et. al. , offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of nodes
4308	4321	any t users (out of n users) to compute a common key non-interactively. They also showed that the total size of the secret keys stored at each user is optimal. Broadcast Encryption by Fiat and Naor  is another model of key pre-distribution. But this model assumes that a single sender broadcasts to multiple receivers and the sender knows all the keys of each receiver. Thus, it is not applicable
4308	4321	universal set (which is publicly known) to form its key ring using a certain procedure to ensure that the exclusion property is satisfied. As the majority of the existing KPS schemes , , , , ,  can be considered as a special case of the cover-free family (CFF) , we will formulate B={p 1 ,p 2 ,p 3 , ...., p k-1 , p k } P 1 P 2 .... P k P 3 P 5 P 4 P k-1 .... P B'={p 1 ',p 2
4308	4323	Blom  and Matsumoto et. al. , offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of nodes (Si
4308	4323	for key allocation in  to make the scheme secure against collusion of any size. This is the Subset Difference method. Applying a hierarchical tree to Naor’s work , Halvey and Shamir  reduced the key storage requirement of each receiver by almost a square root factor without significantly increasing other parameters (computation complexity and message overhead are of the same
4308	4325	still not satisfactory. Besides, these schemes still need TTP; compromising the TTP compromises all the keys it issues. On the other hand, contributory key agreement protocols , , , , , , in which each node contributes an input to establish a common secret (which is a function of all nodes’ inputs) through successive pairwise message exchanges among the nodes in a secure
4308	4325	first key agreement protocol for 2 parties, which is the well-known Diffie-Hellman key exchange. A number of natural extensions to their scheme have been proposed for n parties , , , , , . All of these schemes are based on successively running the 2-party Diffie-Hellman key exchange on different message exchange topologies and sequences to achieve a common key. The earliest
4308	4325	protocols based on star, tree, broadcast and cyclic topologies. McGrew et. al.  used a one-way function tree (OFT), which is a binary tree, for the exchange of key information. Kim et. al.  investigated a number of different tree structures for group key agreement which are efficient with respect to a number of group operations such as member add, member delete, group merge and group
4308	4326	and Matsumoto et. al. , offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of nodes (Si ?S)
4308	4326	set (which is publicly known) to form its key ring using a certain procedure to ensure that the exclusion property is satisfied. As the majority of the existing KPS schemes , , , , ,  can be considered as a special case of the cover-free family (CFF) , we will formulate B={p 1 ,p 2 ,p 3 , ...., p k-1 , p k } P 1 P 2 .... P k P 3 P 5 P 4 P k-1 .... P B'={p 1 ',p 2 ',p 3
4308	4328	not satisfactory. Besides, these schemes still need TTP; compromising the TTP compromises all the keys it issues. On the other hand, contributory key agreement protocols , , , , , , in which each node contributes an input to establish a common secret (which is a function of all nodes’ inputs) through successive pairwise message exchanges among the nodes in a secure manner
4308	4328	key agreement protocol for 2 parties, which is the well-known Diffie-Hellman key exchange. A number of natural extensions to their scheme have been proposed for n parties , , , , , . All of these schemes are based on successively running the 2-party Diffie-Hellman key exchange on different message exchange topologies and sequences to achieve a common key. The earliest
4308	4328	n ? 1. Burmester et. al. introduced the first n-party key agreement protocol (BD) based on a tree . They described protocols based on star, tree, broadcast and cyclic topologies. McGrew et. al.  used a one-way function tree (OFT), which is a binary tree, for the exchange of key information. Kim et. al.  investigated a number of different tree structures for group key agreement which
4308	4329	Matsumoto et. al. , offers practical and efficient solutions to the key management problem in a variety of models including conferencing , , broadcast/multicast , , , , , and sensor networks , . In KPS, an offline TTP pre-initializes each node in a set S with some secret information (a set of long-lived keys) with which any subset of nodes (Si ?S) later on
4308	4329	Thus, it is not applicable for our purpose. This scheme is secure against a collusion of at most r receivers. Naor et. al. further developed this work using a tree structure for key allocation in  to make the scheme secure against collusion of any size. This is the Subset Difference method. Applying a hierarchical tree to Naor’s work , Halvey and Shamir  reduced the key storage
4308	4331	to both ciphertextonly and known-plaintext attacks, but it can only support multiplications, not additions which are needed for SSD (the first property). There also exist other schemes , , , but they are either impractical , too computationally inefficient , or over-restrictive on the number of additions that can be done . Regardless of all these, the major problem is that
4335	4338	capture noise-induced logic failures in on-chip buses . At-speed testing of crosstalk in chip interconnects and testing interconnect crosstalk defects using an on-chip processor are reported in , respectively. A BIST (BuiltIn Self-Test) based architecture to test long interconnects for signal integrity , and the use of boundary scan and IDDT for testing buses  are other proposed
4335	4341	Work Signal Integrity Modeling and Analysis: Maximum aggressor (MA) fault model  is one of the fault models proposed for crosstalk. Various approaches to analyze the crosstalk are described in   . Interconnect design for GHz+ integrated circuits is discussed in . The author observed that chips failed, when a specific test pattern (not included in the MA model) is applied to the
4335	4348	effects for multiple signal lines may not be included in the MA fault model. Several researchers have worked on test pattern generation for crosstalk noise/delay and signal integrity   . Test Methodologies: There is a long list of possible design and fabrication solutions to reduce signal integrity problems on the interconnect. None guarantees to resolve the issue perfectly. A
4335	4348	the MA model may not reflect the worst case, and presented other ways (pseudorandom, weighted pseudorandom or deterministic) to generate test patterns to create maximal integrity loss   . As reported in , a chip fails when the nearest aggressor lines change in one direction and the other aggressors in the opposite direction. This and many similar carefully chosen scenarios are
4335	4348	m increases the number of test patterns increases exponentially. Simulations show that in an interconnect system the lines which are far away from the victim cannot affect the victim line much . Therefore, the number of lines (aggressor) before and after the victim line can be limited. We define k as the locality factor that is empirically determined showing how far the effect of
4335	4349	problems on the interconnect. None guarantees to resolve the issue perfectly. A double sampling data checking (DSDC) technique is used to capture noise-induced logic failures in on-chip buses . At-speed testing of crosstalk in chip interconnects and testing interconnect crosstalk defects using an on-chip processor are reported in , respectively. A BIST (BuiltIn Self-Test) based
4335	4349	for various delay thresholds and technologies. A double sampling technique is applied by on-line error detector circuit to test multiple-source noise-induced errors on the interconnects and buses . Modified Boundary Scan and IEEE Standards: BIST-based test pattern generators for board level interconnect and delay testing are proposed in  and , respectively. A test methodology
4335	4335	noise detector (ND) and skew detector (SD) cells, based on a modified cross-coupled PMOS differential sense amplifier. To detect delay violation, an integrity loss sensor (ILS) has been designed in  which is flexible and tunable for various delay thresholds and technologies. A double sampling technique is applied by on-line error detector circuit to test multiple-source noise-induced errors on
4335	4335	more expensive and less flexible in adopting other types of noise detectors/sensors. Various issues on the extended JTAG architecture to test SoC interconnects for signal integrity are reported in   and  using the Maximum Aggressor (MA) and Multiple Transition (MT) fault model, respectively. C. Contribution and Paper Organization Our main contribution is an on-chip mechanism to extend
4335	4335	explored for various tradeoffs. For example, a simple yet efficient compression technique for the enhanced boundary scan architecture using the PGBSC cells to minimize delivery time is discussed in . 2 two rotations (clocks) are enough to set victim/aggressor nets and B. Observation BSC (OBSC) We propose a new BSC at the receiving side of interconnects which can employ any integrity loss
4335	4355	errors on the interconnects and buses . Modified Boundary Scan and IEEE Standards: BIST-based test pattern generators for board level interconnect and delay testing are proposed in  and , respectively. A test methodology targeting defects on bus structures using IDDT and boundary scan has been presented in . P1500 proposes standardization of Core Test Wrapper and Core Test
8919177	4369	representation and appearance factorization for facial expression synthesis and recognition. 2. ACTIVE FACIAL APPEARANCE MODELS We choose to represent faces using the active appearance model (AAM)  which is a powerful tool allowing to extract from any unknown target face, a set of appearance parameters coding a synthetic face similar to the target in terms of minimum texture error. AAM uses
8919177	4369	is the texture difference between the synthesized face and the corresponding mask of the image it covers. The optimization scheme used here is based on the first order Taylor expansion described in  and returns parameters cop and p op . The appearance model is constructed using the CMU expressive face database . Each sequence of this database contains ten to twenty images, beginning with a
8919177	4370	scheme used here is based on the first order Taylor expansion described in  and returns parameters cop and p op . The appearance model is constructed using the CMU expressive face database . Each sequence of this database contains ten to twenty images, beginning with a neutral expression and ending with a high magnitude expression. We select 338 frontal still face images composed of
8919177	4370	matrix: Y = USV T . Then W is given by the first J column of US and B is given by the first J rows of V T .s4.3. Experimental setup To build the bilinear model we extract from the CMU database  a training set containing 70 frontal face images of 10 different persons (contents) showing each of the seven basic facial expressions (styles). The observation matrix is built by stacking the
8919177	4372	linear in either factors when the other is held constant. They provide rich factor interactions by allowing factors to modulate each other’s contributions multiplicatively. Tenenbaum and Freeman  model the interaction between face illumination and pose using bilinear models in order to perform face synthesis under novel illuminations as well as face pose recognition. Similarly Chuang et al.
8919177	4372	content vectors. I is chosen to be equal to the number of styles shown in the training set (I = S) whereas it is recommended to choose J by looking for an elbow in the singular value spectrum of Y . Least squares optimal values of A and B are iteratively estimated using singular value decomposition as follows: 1. Y = USV T and initialize B as the first J rows of V T . 2.  VT = USV T
8919177	4373	model the interaction between face illumination and pose using bilinear models in order to perform face synthesis under novel illuminations as well as face pose recognition. Similarly Chuang et al.  use bilinear models to separate video data into expressive features and underlying content in order to perform facial expression synthesis on speaking faces. In a more general approach Vasilescu
8919177	4374	models to separate video data into expressive features and underlying content in order to perform facial expression synthesis on speaking faces. In a more general approach Vasilescu and Terzopoulos  propose multilinear analysis of faces to separate factors such as identity, viewpoint, illumination and expression from pixel grey level values. This representation is then used to perform face
4376	4377	identity is accepted). Otherwise the person is classified as an «impostor» and the access to the required resource is denied. So far many algorithms to match two fingerprints have been proposed . Some of these algorithms are based on different representations of fingerprint (e.g., representations based on minutiae-points or fingerprint texture). It is reasonable to hypothesise that
4376	4377	two fingerprint verification algorithms. Given the input fingerprint image associated to the claimed identity i: - For each algorithm, compute the matching score (a real value on the interval ) between the given fingerprint and the «template» fingerprint stored in the database and associated to the identity i. Let sm and st be the matching scores provided by the two individual
4376	4377	if diverse and complementary algorithms are used , for our experiments, we selected one algorithm for each type. The selected minutiae-based algorithm is commonly referred as «String» algorithm . The ridge bifurcations and endings, usually called «minutiae», are extracted from the input fingerprint image. Such «minutiae» set is compared with that of the template fingerprint. Such
4376	4377	the individual fingerprint matchers for any couple of FMR and FNMR values, that is, for any «operational» point. 4 Conclusions So far, many algorithms to match two fingerprints have been proposed , but few works investigated the possibility of fusing them in order to improve verification performances . In this paper, various methods for fusing two different fingerprint verification
4376	4378	identity is accepted). Otherwise the person is classified as an «impostor» and the access to the required resource is denied. So far many algorithms to match two fingerprints have been proposed . Some of these algorithms are based on different representations of fingerprint (e.g., representations based on minutiae-points or fingerprint texture). It is reasonable to hypothesise that
4376	4378	issworth noting that other minutiae-based algorithms have been proposed , but «String» shown the best performances . The selected texture-based algorithm is also known as «Filter» algorithm . The input fingerprint image is «partitioned» around its «centre» (the so called «core» point) by a tessellation . A feature vector (called «finger-code») is computed by evaluating the outcomes
4376	4378	values of EER, FMR, and FNMR.. The fifth column of Table 1 shows the CSS values between the genuine and impostor distributions of the individual and combined algorithms. The CSS takes values in . The maximum value indicates that the distributions are completely separated, while the minimum value indicates the total distributions overlapping. It is worth noting that the degree of
4376	4378	the individual fingerprint matchers for any couple of FMR and FNMR values, that is, for any «operational» point. 4 Conclusions So far, many algorithms to match two fingerprints have been proposed , but few works investigated the possibility of fusing them in order to improve verification performances . In this paper, various methods for fusing two different fingerprint verification
4376	4382	image. Other texture-based algorithms have been proposed, but their use is often limited to the fingerprint classification task , for which the finger-code approach shown the best performances . In general, the characteristics of these approaches are also the main limitation, because the fingerprint description is less detailed with respect to the minutiae-based one. Consequently, the
4376	4384	. 3 Experimental results 3.1 The Data Set For our experiments, we used the FVC-DB1 data base that was recently introduced as a benchmark data set for fingerprint verification algorithms . This data set is made up of 800 fingerprint images acquired from a low-cost optical sensor. The image size is 300x300 pixels and the image resolution is 500 dpi. The number of identities is 100,
4396	4399	a huge number of customer queries asking for their best itineraries. The most frequently encountered applications of the above scenario involve route planning systems for cars, bikes and hikers  or scheduled vehicles like trains and buses . Similar, query intensive applications include spatial databases  and web searching . Users of such systems continuously enter their
4396	4399	and hence only linear in the number of nodes space requirements are acceptable. The application of shortest path computations in travel networks is widely covered in the literature; see e.g., . One of the features of travel planning is the fact that the network does not change ? This work was partially supported by the Human Potential Programme of the European Union under contract no.
4396	4400	planning systems for cars, bikes and hikers  or scheduled vehicles like trains and buses . Similar, query intensive applications include spatial databases  and web searching . Users of such systems continuously enter their requests for finding their “best connections” and the main goal is to reduce the (average) response time for answering a query. The algorithmic core
4396	4401	employ several techniques to maintain a common code base that is at the same time small, flexible and efficient. We use a blend of the design pattern template method , parameterized inheritance , and template meta-programming . Adding functionality to graph algorithms can be achieved by the design pattern template method  or an extension of the design pattern visitor, the
4396	4401	programming in C++, an extension of the C++ language is proposed in . However, such an extension can be avoided through the use of parameterized inheritance  (also known as mix-in classes ) and template meta-programming , which provides the base to a solution with standard C++ compilers. The rest of the paper is organized as follows. The next section contains – after some
4396	4403	for the dynamic single source shortest path problem are usually analyzed. We would also like to mention that existing approaches for the dynamic all-pairs shortest paths problem (see e.g., , and  for a recent overview) are not applicable to maintain geometric containers, because of their inherent quadratic space requirements. The last contribution of this paper concerns
4396	4406	necessary parameters are given as arguments to the constructor. We will now discuss the question how additional parameters that are needed by aspects can be provided. We use a technique inspired by , but omit the creation of a repository. Consider again Dijkstra’s algorithm. The parameters that are given to the constructor are the graph and the edge lengths in this case. A speed-up technique
4396	4406	B is a little bit tricky (see Chapter 2.7 in  for more details). template<typename T, template<typename L> class Aspect> struct Provides { private: class Yes { char a; }; class No { char a; }; static No ProvidesTest( ... ); template<typename S> static Yes ProvidesTest( Aspect<S> const* ); public: static bool const RET = (sizeof(ProvidesTest(static_cast<T*>(0))) == sizeof(Yes)); };
4396	4408	constant space (two points and the radius). Smallest Enclosing Disk. The smallest enclosing disk is the unique disk with smallest area that includes all points. We use the implementation in CGAL  of Welzl’s algorithm  with expected linear running time. The algorithm works off-line and storage requirement is at most three points. Smallest Enclosing Ellipse. The smallest enclosing ellipse
4396	4408	Fibonacci heaps and the convex hull algorithm provided. I/O was done by the LEDA extension package for GraphML with Xerces 2.1. For the minimal disks, ellipses and parallelograms, we used CGAL 2.4 . In order to perform efficient containment tests for minimal disks, we converted the result from arbitrary precision to built-in doubles. To overcome numerical inaccuracies, the radius was
4396	4410	are perhaps the first results towards an efficient algorithm for the dynamic single source shortest path problem without using the output complexity model – introduced in  and extended in  ??? under which algorithms for the dynamic single source shortest path problem are usually analyzed. We would also like to mention that existing approaches for the dynamic all-pairs shortest paths
4396	4411	are perhaps the first results towards an efficient algorithm for the dynamic single source shortest path problem without using the output complexity model – introduced in  and extended in  ??? under which algorithms for the dynamic single source shortest path problem are usually analyzed. We would also like to mention that existing approaches for the dynamic all-pairs shortest paths
4396	4412	arbitrary non-negative edge lengths, taking O(m + n log n) worst-case time. For special cases (e.g., undirected graphs, integral or uniformly distributed edge weights) better algorithms are known . 3 Geometric Pruning 3.1 Shortest Paths Containers In this section, we introduce the concept of containers which helps to reduce the search space of Dijkstra’s algorithm. Containers are used to
4396	4412	a lot of cases. The presented technique can easily be combined with other methods: 19s• The geometric pruning is independent of the priority queue. Algorithms using a special priority queue such as  can easily be combined with it. The decrease of the search space is in fact the same (but the actual running time would be different). • Goal-directed search  or A ? has been shown in
4396	4413	employ several techniques to maintain a common code base that is at the same time small, flexible and efficient. We use a blend of the design pattern template method , parameterized inheritance , and template meta-programming . Adding functionality to graph algorithms can be achieved by the design pattern template method  or an extension of the design pattern visitor, the
4396	4413	aspects. To support aspect oriented programming in C++, an extension of the C++ language is proposed in . However, such an extension can be avoided through the use of parameterized inheritance  (also known as mix-in classes ) and template meta-programming , which provides the base to a solution with standard C++ compilers. The rest of the paper is organized as follows. The next
4396	4414	can easily be combined with it. The decrease of the search space is in fact the same (but the actual running time would be different). • Goal-directed search  or A ? has been shown in  to be very useful for transportation networks. As it simply modifies the edge weights, a combination of geometric pruning and A ? can be realized in a straightforward manner. • Bidirectional search
4396	4416	for the dynamic single source shortest path problem are usually analyzed. We would also like to mention that existing approaches for the dynamic all-pairs shortest paths problem (see e.g., , and  for a recent overview) are not applicable to maintain geometric containers, because of their inherent quadratic space requirements. The last contribution of this paper concerns
4396	4417	container, the larger the average number of wrong nodes should be. 3.4 Experimental Setup We implemented the algorithm in C++ using g++ 2.95.3. We used the graph data structure from LEDA 4.3 (see ) as well as the Fibonacci heaps and the convex hull algorithm provided. I/O was done by the LEDA extension package for GraphML with Xerces 2.1. For the minimal disks, ellipses and parallelograms,
4396	4417	Due to numeric instabilities, our implementation does not guarantee to find the minimal container, but asserts that all points are inside the container. The convex hulls was computed with LEDA . The experiments were performed on an Intel Xeon with 2.4 GHz on the Linux 2.4 platform. It is crucial to this problem to do the statistics with data that stem from real applications. We are using
4396	4418	arbitrary non-negative edge lengths, taking O(m + n log n) worst-case time. For special cases (e.g., undirected graphs, integral or uniformly distributed edge weights) better algorithms are known . 3 Geometric Pruning 3.1 Shortest Paths Containers In this section, we introduce the concept of containers which helps to reduce the search space of Dijkstra’s algorithm. Containers are used to
4396	4418	a lot of cases. The presented technique can easily be combined with other methods: 19s• The geometric pruning is independent of the priority queue. Algorithms using a special priority queue such as  can easily be combined with it. The decrease of the search space is in fact the same (but the actual running time would be different). • Goal-directed search  or A ? has been shown in
4396	4420	arbitrary non-negative edge lengths, taking O(m + n log n) worst-case time. For special cases (e.g., undirected graphs, integral or uniformly distributed edge weights) better algorithms are known . 3 Geometric Pruning 3.1 Shortest Paths Containers In this section, we introduce the concept of containers which helps to reduce the search space of Dijkstra’s algorithm. Containers are used to
4396	4423	Our dynamic algorithms are perhaps the first results towards an efficient algorithm for the dynamic single source shortest path problem without using the output complexity model – introduced in  and extended in  – under which algorithms for the dynamic single source shortest path problem are usually analyzed. We would also like to mention that existing approaches for the dynamic
4396	4424	Our dynamic algorithms are perhaps the first results towards an efficient algorithm for the dynamic single source shortest path problem without using the output complexity model – introduced in  and extended in  – under which algorithms for the dynamic single source shortest path problem are usually analyzed. We would also like to mention that existing approaches for the dynamic
4396	4426	using bounding boxes (right). for a certain period of time while there are many queries for shortest paths. This justifies a heavy preprocessing of the network to speed up the queries (see e.g., ). Although pre-computing and storing the shortest paths for all pairs of nodes would give us “constant-time” shortest-path queries, the quadratic space requirement for traffic networks with more
4396	4426	the layout. In fact, for some of our experimental data this is not even the case. We would like to mention that a particular type of geometric objects, the angular sectors, has been introduced in  for the special case of a time table information system. Our results, however, are more general in two respects: (a) we examine the impact of various different geometric objects; and (b) we
4396	4426	in line 1 for each run of Dijkstra’s algorithm can be omitted by introducing a global integer variable “time” and replacing the test dist(v) = ? by checking a time stamp for every node. See e.g.,  for a detailed description. 4s1 for all nodes u ? V set dist(u) := ? 2 initialize priority queue Q with source s and set dist(s) := 0 3 while priority queue Q is not empty 3a if u = t return 4 get
4396	4426	radius needed to cover S(u, v). It suffices to remember the radius, which can be found on-line similarly as in the disk case. Angular Sector. Angular sectors are the objects that were used in . For each edge (u, v) a node p left of (u, v) and a node q right of (u, v) are determined such that all nodes in S(u, v) lie within the angular sector ?(p, u, q). The nodes p and q are chosen in a
4396	4426	pruning for disks around the tail is by far not as good as the other methods. Note however, that the average search space is still reduced to about 10%. The only type of objects studied previously , the angular sectors, result in a reduction to about 6%, but, if both are intersected, we get only 3.5%. Surprisingly the result for bounding boxes is about the same as for the better tailored
4396	4430	scenario involve route planning systems for cars, bikes and hikers  or scheduled vehicles like trains and buses . Similar, query intensive applications include spatial databases  and web searching . Users of such systems continuously enter their requests for finding their “best connections” and the main goal is to reduce the (average) response time for answering a query.
4396	4432	To give a concrete example, goal-directed search as well as geometric pruning depend both on a layout of the graph. Template meta-programming can also be used to check dependencies of such concepts . In our case, concepts coincide most of the time with the use of a mix-in class (a derived class where the base class is a template parameter). This enables us to actually add the mix-in class in
4396	4433	aspect-oriented programming. Aspect-oriented programming tries to provide a modular way to overcome the single dimension of functional decomposition by the design pattern template method (see e.g., ). More precisely, it is necessary to change the inheritance hierarchy to create arbitrary combinations of aspects. To support aspect oriented programming in C++, an extension of the C++ language is
4396	4433	As a concrete example, the shortest path computation with Dijkstra’s algorithm can be improved by goal-directed search or geometric pruning. Apart from such refinements, other so-called aspects  can be added to the algorithm: operation counting, time measurement or debugging output. 5.1 Adding Aspects by Parameterized Inheritance For complex algorithms, it is favorable to keep the code of
4396	4434	arbitrary non-negative edge lengths, taking O(m + n log n) worst-case time. For special cases (e.g., undirected graphs, integral or uniformly distributed edge weights) better algorithms are known . 3 Geometric Pruning 3.1 Shortest Paths Containers In this section, we introduce the concept of containers which helps to reduce the search space of Dijkstra’s algorithm. Containers are used to
4396	4435	the intersection can be as well. Smallest Enclosing Rectangle. We allow the rectangle to be oriented in any direction and search for one with smallest area containing all points. The algorithm from  finds such a rectangle in linear time. However, due to numerical inconsistencies we had to incorporate additional tests to assure that all points are in fact inside the rectangle. As for the
4396	4436	algorithms in a type-safe manner and automatically resolving dependencies between different variations of our algorithm. We conclude in Section 6. Preliminary portions of this work appeared in . 2 Definitions and Problem Description 2.1 Graphs A directed simple graph G is a pair (V, E), where V is a finite set and E ? V ×V . The elements of V are the nodes and the elements of E are the
4396	4437	algorithms in a type-safe manner and automatically resolving dependencies between different variations of our algorithm. We conclude in Section 6. Preliminary portions of this work appeared in . 2 Definitions and Problem Description 2.1 Graphs A directed simple graph G is a pair (V, E), where V is a finite set and E ? V ×V . The elements of V are the nodes and the elements of E are the
5288078	4492	have been proposed (Ouzounis et al., 2003). Its authors point to the need for a meta-classification as a foundation upon which more refined classifications – and ontologies – can be built. As (Liu & Rost, 2001) have argued, a wide range of additional factors needs to be taken into account for a complete description of a protein’s function. These factors include not only cellular roles but also molecular
4504	4722	the first, but can also be the only step in the “see–measure–control” sequence in semiconductor manufacturing. Therefore, the need for rapid nondestructive imaging-based techniques is growing , . According to the ITRS , there is also a clear requirement for a paradigm shift in the role of metrology from off-line sampling to in-line control, as real-time integrated metrology is essential
4504	4512	part of the wafer map is shown in Fig. 1(a). Various approaches have been applied to vision-based localization. For an extensive survey on the developments in this area, we refer the reader to . In our case, in order to handle the enormous amounts of geometric structures contained in wafer images, we choose to address the positioning problem using a highly efficient technique from the
4504	4516	changes. Typical wafer images contain rectangular structures; therefore, the objects being of particular concern are corners, straight line segments, and rectangles. We used a Harris detector  to find corner features. As expected, using such simple features results in a high number of entries being stored in the hash table. Typical model images with which we worked contained more than
4504	4518	III-C. In Fig. 16(b), the results of the same test as before are shown, while using a quadtree with depth two for efficient access to the hash table entries (quadtree implementation is taken from ). The discrimination power of the algorithm is improved by a factor of four as expected, since only one-fourth of the models actually participate in navigation. It can be seen that the position of
4504	4519	a basis in order to create an eye-point invariant representation. The choice of two points, used to form a basis, greatly influences the algorithm’s performance. This phenomenon is discussed in ???, among others. For example, a large separation of the basis points results in an invariant model description that is less sensitive to noise; i.e., the computed invariant coordinates will have
4504	4521	basis in order to create an eye-point invariant representation. The choice of two points, used to form a basis, greatly influences the algorithm’s performance. This phenomenon is discussed in –, among others. For example, a large separation of the basis points results in an invariant model description that is less sensitive to noise; i.e., the computed invariant coordinates will have
8919190	4531	matrix. This model has been formulated and utilised previously for both language modelling — as a class-based bigram model ,  — known as aggregate Markov model and for bibliometric analysis , as a probabilistic version of the HITS algorithm, known under the name of probabilistic HITS (PHITS). In this model, clustering of states is achieved by estimating a first order discrete Markov
8919190	4531	value obtained.sAlthough these multiplicative updates are simple and easy to implement, the popular solution that can be found in the literature for estimating aggregate Markov models , ,  is the Expectation-Maximisation (EM). This is the following. For obtaining an EM solution, an auxiliary function is first created for (6) in the standard way, by employing Jensen’s inequality,
8919190	4531	? k Qij(k) = 1. Now solving the stationary equations with respect to all parameters Qij(k), P (i|k) and P (k|j) and taking into account all constraints, we arrive at the EM algorithm given in , , . • E step: • M step: Qij(k) = P (k|i, j) ? P (i|k)P (k|j) (12) P (i|k) ? ? nijP (k|i, j) (13) j P (k|j) ? ? nijP (k|i, j) (14) Because Qij(k) is the exact posterior P (k|i, j), than the lower
8919190	4531	memberships, we could either threshold the parameters P (k|j) directly — this is the approach taken in  — or we can use Bayes theorem, which involves both parameters P (i|k) and P (k|j) — as in . As can be expected, these two approaches provide slightly different results. We have opted for the latter for the following reason: Note that the parameter P (k|j) is in fact concerned with
8919190	4531	loss when thresholding. Apart from cluster membership values, probabilistic quantities indicating analogous notions to hubs and authorities can also be computed from the model, similarly to . Not surprisingly, the most authoritative user across all groups has turned out to be the chat moderator (userID=’cic-cnn’). We also show the split-up of the whole time-course of the chat session
8919190	4535	value obtained.sAlthough these multiplicative updates are simple and easy to implement, the popular solution that can be found in the literature for estimating aggregate Markov models , ,  is the Expectation-Maximisation (EM). This is the following. For obtaining an EM solution, an auxiliary function is first created for (6) in the standard way, by employing Jensen’s inequality,
8919190	4535	Qij(k) = 1. Now solving the stationary equations with respect to all parameters Qij(k), P (i|k) and P (k|j) and taking into account all constraints, we arrive at the EM algorithm given in , , . • E step: • M step: Qij(k) = P (k|i, j) ? P (i|k)P (k|j) (12) P (i|k) ? ? nijP (k|i, j) (13) j P (k|j) ? ? nijP (k|i, j) (14) Because Qij(k) is the exact posterior P (k|i, j), than the lower bound
8919190	4535	should now be addressed. That is, selecting the optimal number of clusters. It should be noted that in spite of the wide popularity of the Aggregate Markov and related models , , , ,  in a number of areas, to our knowledge — except the work log likelihood ?4.6 ?4.8 ?5 ?5.2 ?5.4 ?5.6 ?5.8 ?6 2 4 6 8 10 Iterations 12 14 16 18 20 Fig. 7. Convergence speed of the gradient (solid
8919190	4538	dynamics analysed. I. INTRODUCTION With the increase of Internet-based on-line communication, such as Internet chat, the need for organising and structuring such processes has arisen. Previous work , ,  has looked exclusively at analysing the text streams produced, in order to reveal the evolution of topics that underlie such discussion streams and possibly to provide a topographically
8919190	4541	by computing the partial derivatives with respect to the two parameters of the model, P (i|k) and P (k|j) and equating them to zero. These equations can be solved by fixed point iterations, as in  leading to the following alternating iterative algorithm: P (i|k) ? P (i|k) P (k|j) ? P (k|j) S? nij ?K k ? =1 P (i|k? )P (k ? P (k|j) (8) |j) S? nij ?K k ? =1 P (i|k? )P (k ? P (i|k) (9) |j) j=1
24862388	4551	instead. A framework for investigating different aspects of faces in user interfaces and a broad review of research on the use of faces in user interfaces is found in Catrambone, Stasko, and Xiao (Catrambone, et al., 2002). While much of this research about the face relates to a humanoid representation of a computer software agent, it is nonetheless relevant to consider for the purpose of creating humanoid
24862388	4553	how individuals are influenced by others’ appearance and behavior. A number of researchers have discussed how identity and exchange of social cues impact online communication (Turkle, 1995)(Donath, 1998). Yet few researchers have empirically studied how visual representations of users impact the communication that takes place in online spaces. This thesis contributes an initial understanding of
24862388	4553	can quickly discern the most valued members of the group. There is a risk, however that users will attempt to manipulate a system that attempts to show the most valuable members. As documented in (Donath, 1998) and (Turkle, 1995), users of online systems have often deceived others by posing as someone they are not and have even gone so far as to impersonate particular individuals to defame their image.
24862388	4555	(Whittaker, et al., 1998)(Smith, 1999)(Fiore, et al., 2002). Past attempts to visualize historical information about users have represented them with abstract graphical forms such as simple shapes (Donath, 2002). These abstract visualizations are intriguing but do not convey the kind of social cues we experience in face-to-face interaction. As human beings, we internalize many social rules that influence
24862388	4557	spaces. A variety of data can be mined from these messages. Several scholars have researched which data may be most feasible and useful to analyze and present (Whittaker, et al., 1998)(Smith, 1999)(Fiore, et al., 2002). Past attempts to visualize historical information about users have represented them with abstract graphical forms such as simple shapes (Donath, 2002). These abstract visualizations are
24862388	4557	additional conversation. Initial studies by Smith and Andrew Fiore show that users’ opinions about other newsgroup participants correlate to some extent with aspects of the participants’ behavior (Fiore, et al., 2002). For instance, the more groups an author posts to, the less likely other individuals are to be interested in the author’s messages. This is likely because authors who post to many groups may be
24862388	4558	from this communication. By looking for patterns in this data, it is often possible to uncover “implicit, previously unknown, and potentially useful information” in a “data mining” process (Frawley et al., 1992). In the last several years, scholars have applied data mining techniques to large online social spaces, analyzing messages in Usenet newsgroups, for instance. “Social data mining” is used to make
24862388	4566	and generates suggested emotional expressions and gestures and places the characters in a comic book scene. Users can also override the expressions and gestures with 26stheir own selection (Kurlander, et al., 1996). While an entertaining way to view messages, the stylized nature of the representations of the users and unlikely environments they inhabit seem to make it less likely that they refer to the
24862388	4569	different rankings of appropriateness for the graphic elements. He also suggests that the most important attribute should be represented with the most effective technique for representing the data (Mackinlay, 1986). See Figure 7 for his ranking of the appropriateness of “perceptual tasks” with certain visual elements for particular types of data. Figure 7. Mackinlay’s ranking of visual attributes’
24862388	4569	on the average emotional tone of the message. Color hue is identified by Mackinlay as an appropriate attribute for representing nominal values such as which emotion is dominant in the message (Mackinlay, 1986). For the Anthropomorphs, I used a spectrum of hues from red to blue, where red was mapped to the negative emotion anger, and blue was mapped to the negative emotion happiness. Yellow was mapped to
24862388	4569	of data elements, the visualization must also include only elements that are themselves comprehensible. Mackinlay has identified visual attributes that are appropriate for particular types of data (Mackinlay, 1986). As discussed in Hearst’s Information Visualization tutorial (Hearst, 2003), certain of these attributes are assumed to have intrinsic meaning. For instance, larger objects are assumed to
24862388	4574	data about an individual are mapped to different parts of a representation of their “body”, resulting in an abstract yet humanoid depiction of a person (called an “Anthropomorph”)  (Perry and Donath, 2004). My goal was to incorporate elements of participants’ history into an information visualization with the following properties: • Information about participants’ past behavior is legible. • The
24862388	4583	1996). • Ellen Spertus’s SMOKEY project looked for common words and phrases used in “flames” or abusive messages, and was able to correctly categorize 64% of the flames and 98% of the nonflames (Spertus, 1997). ??? Janyce Wiebe’s group studies how to use to natural language processing to identify patterns that indicate how subjective a text is, that is whether it includes statements of opinion. (Riloff
24862388	4589	avatars with these cues led to better conversational quality than equivalent conversations using text chat. Users reported that the interaction felt more like a face-to-face conversation (Vilhjalmsson, 2003). This thesis also investigates whether users also have a different experience in a conversation with anthropomorphic figures compared to a text-based discussion. 2.3.3 Graphical Abstract Spaces
24862388	4593	– the archives of messages from these spaces. A variety of data can be mined from these messages. Several scholars have researched which data may be most feasible and useful to analyze and present (Whittaker, et al., 1998)(Smith, 1999)(Fiore, et al., 2002). Past attempts to visualize historical information about users have represented them with abstract graphical forms such as simple shapes (Donath, 2002). These
24862388	4593	threads, indicating frequent extended conversations.” Groups that were larger in size with fewer posters contributing more than one message were somewhat less likely to have extended conversations (Whittaker, et al., 1998). There is a disparity between the posts that do not receive responses (perhaps indicating a lack of social capital on the part of the poster) and the messages that spawn more extended
24862388	4595	application displayed a set of users from a particular group as plants in a garden, with various pieces of information about each participant represented as a different element of the plant (Xiong and Donath, 1999). 27sFigure 4. PeopleGarden visualization of two different newsgroups with each author represented as a plant. Loom is a series of visualizations of Usenet newsgroups in which entire groups and
8919200	4597	it is a ’bid and offer set’, but for ease of presentation we refer to it as the bid set and refer to all bids/offers as bids. To each auction a ? A is associated a price distribution Pa : Z ?  representing the belief that, with probability Pa(p), auction a will close at price p. We set Fa(p) = ? p ? ?p Pa(p? ): the agent’s believed probability that auction a will close at or above price
8919200	4597	In addition to work on participation in a single many-to-many double auction, researchers have developed algorithms able to participate in many auctions simultaneously (, , , , , , ). These algorithms differ from the work presented here in that they focus on the purchase of one or more homogenous goods from multiple auctions, whereas we present an algorithm for the
8919200	4601	of aspects of a business process to third parties. This is primarily treated as a one-to-one negotiation problem, and various heuristic algorithms for negotiation in this context are discussed in . Other work in one-to-one negotiation includes the game-theoretic approach of  and the logic-based argumentation approach of . Work has also focussed on effective algorithms for use in
8919200	4602	the environment. In addition to work on participation in a single many-to-many double auction, researchers have developed algorithms able to participate in many auctions simultaneously (, , , , , , ). These algorithms differ from the work presented here in that they focus on the purchase of one or more homogenous goods from multiple auctions, whereas we present
8919200	4603	and the logic-based argumentation approach of . Work has also focussed on effective algorithms for use in manyto-many negotiation environments such as a double auction. Gjerstad and Dickhaut  use a belief-based modelling approach to generate appropriate bids and offers. Their work is close in spirit to ours, in that it combines belief-based learning of individual agents’ bidding
8919200	4605	have directly improved this approach  or worked on alternative algorithms in a similar environment. These include heuristic approaches (,), stochastic analysis (,), fuzzy logic  and dynamic programming . Work has also been carried out to distribute  and generalise  the environment. In addition to work on participation in a single many-to-many double auction,
8919200	4606	In addition to work on participation in a single many-to-many double auction, researchers have developed algorithms able to participate in many auctions simultaneously (, , , , , , ). These algorithms differ from the work presented here in that they focus on the purchase of one or more homogenous goods from multiple auctions, whereas we present an algorithm for
8919200	4607	conducted by automated auctioneer software. Agent technology has been proposed as a means of automating some of the more sophisticated negotiations which businesses are involved in (e.g. ). In this paper we look at a specific class of business process that will become increasingly important in the virtual economy - service composition. We focus specifically on algorithms that
8919200	4607	of contracts and subcontracts to suppliers. It uses a form of distributed request-for-proposals. However, it does not discuss algorithms for determining what price to ask in a proposal.  use a more sophisticated negotiation protocol to allow the subcontracting of aspects of a business process to third parties. This is primarily treated as a one-to-one negotiation problem, and
8919200	4610	pioneering work, others have directly improved this approach  or worked on alternative algorithms in a similar environment. These include heuristic approaches (,), stochastic analysis (,), fuzzy logic  and dynamic programming . Work has also been carried out to distribute  and generalise  the environment. In addition to work on participation in a single
8919200	4611	work, others have directly improved this approach  or worked on alternative algorithms in a similar environment. These include heuristic approaches (,), stochastic analysis (,), fuzzy logic  and dynamic programming . Work has also been carried out to distribute  and generalise  the environment. In addition to work on participation in a single many-to-many
8919200	4612	algorithms for negotiation in this context are discussed in . Other work in one-to-one negotiation includes the game-theoretic approach of  and the logic-based argumentation approach of . Work has also focussed on effective algorithms for use in manyto-many negotiation environments such as a double auction. Gjerstad and Dickhaut  use a belief-based modelling approach to generate
8919200	4613	the environment. In addition to work on participation in a single many-to-many double auction, researchers have developed algorithms able to participate in many auctions simultaneously (, , , , , , ). These algorithms differ from the work presented here in that they focus on the purchase of one or more homogenous goods from multiple auctions, whereas we present an
8919200	4614	the environment. In addition to work on participation in a single many-to-many double auction, researchers have developed algorithms able to participate in many auctions simultaneously (, , , , , , ). These algorithms differ from the work presented here in that they focus on the purchase of one or more homogenous goods from multiple auctions, whereas we present an
8919200	4615	A service composer, therefore, may be simultaneously interacting with many potential customers and many potential suppliers. This affects its behavior throughout the business lifecycle (see  for more details). In this paper, we focus particularly on the decision problem it faces during negotiation. To operate effectively, the service composer will be involved in many interlinked
8919200	4615	the buyers is not known. Unlike the TAC, both buyers and sellers must be simultaneously negotiated with through forward and reverse auctions. The algorithm we present is a generalization of that in , which focused only on forward auctions. An alternative approach is to attempt to provide the right market mechanism in the first place, providing a centralized point of contact for all buyers and
8919200	4619	auctions. An alternative approach is to attempt to provide the right market mechanism in the first place, providing a centralized point of contact for all buyers and sellers to trade. Sandholm  proposes a sophisticated marketplace able to handle combinatorial bidding, and able to provide guidance to buyers and sellers as to which market mechanism to adopt for a particular negotiation. In
8919200	4620	from multiple auctions, whereas we present an algorithm for the simultaneous purchase of component services and sale of one or more composite services in an auction environment. Schillo et. al  analyse task assignment in contract nets. They consider the problem of potential overcommitment by a supplier to several contractors caused by the delay between a supplier making an offer and the
8919200	4622	() presents a problem where an agent must participate in several simultaneous auctions to purchase flights, accommodation and entertainment. Successful agents in this competition include ATTac (,) and SouthamptonTAC . The work presented in this paper differs from these in that it tackles a more generic version of the problem, where the utility of the buyers is not known. Unlike the
8919200	4623	presents a problem where an agent must participate in several simultaneous auctions to purchase flights, accommodation and entertainment. Successful agents in this competition include ATTac (,) and SouthamptonTAC . The work presented in this paper differs from these in that it tackles a more generic version of the problem, where the utility of the buyers is not known. Unlike the TAC,
8919200	4624	approach  or worked on alternative algorithms in a similar environment. These include heuristic approaches (,), stochastic analysis (,), fuzzy logic  and dynamic programming . Work has also been carried out to distribute  and generalise  the environment. In addition to work on participation in a single many-to-many double auction, researchers have developed
8919200	4625	it is applied to a single double auction marketplace, and does not allow agents to bid in a variety of auctions. Subsequent to this pioneering work, others have directly improved this approach  or worked on alternative algorithms in a similar environment. These include heuristic approaches (,), stochastic analysis (,), fuzzy logic  and dynamic programming . Work has
8919200	4626	problem of simultaneous purchase of heterogeneous component services in an auction environment in response to a request for a composite service. Most notably, the Trading Agent Competition (TAC) () presents a problem where an agent must participate in several simultaneous auctions to purchase flights, accommodation and entertainment. Successful agents in this competition include ATTac
8919205	4633	on addressing. Contrary to IPng proposals, a number of other ideas build on the reuse of the existing 32-bit address space. The separation of private and public address space was first proposed in  then in . Address translation and NATs emerged as a way to connect private networks to the global Internet. Realm Specific IP  starts from private address realms and NATs. It provides an
8919205	4858	“has never been fully developed, although is fully compatible with end-to-end addressing.” The IETF also investigated the use of IP options for address extension, as suggested by Brian Carpenter . The idea is quite similar to 4+4, but beacause it is based on IP options it requires changes to ARP, DNS, SNMP and routers within a site. This idea has been abandoned and never implemented. The
8919205	4635	is added to the packet. addressing has already been considered during the design of IP . The primary reason is for address extensions, but the final 32-bit address seemed large enough. See  for a detailed discussion on addressing. Contrary to IPng proposals, a number of other ideas build on the reuse of the existing 32-bit address space. The separation of private and public address
8919205	4875	2960 Broadway New York, NY ¡ 10027 @comet.columbia.edu zoltan,andras,campbell¢ Internet Protocol. At that time, however, NATs were not yet in widespread use. The ngtrans Working Group of IETF  developed several transition mechanisms that would allow the temporary co-existence of IPv4 and IPv6 and the communication in mixed environments. However, despite availability of IPv6 and
8919205	4875	transition can be gradually started, gradually blending to the new architecture. There is no need for temporary transition mechanisms (such as tunneling, tunnel brokers, 6to4, 6over4 or DSTM; see ), all new mechanisms are final. There is no need for a new addressing plan, dual routing, new network management tools, new 7 Of course, if a router has not been upgraded, its control plane cannot
8919205	4876	node has no level 1 address, the DNS is queried for the hostname (see later) to obtain the level 1 address parts. The kernel module operates using the Netfilter architecture of the Linux 2.4 kernel . Each locally terminated and originated packet is captured (on the LOCAL IN and LOCAL OUT hooks). If the protocol field of an incoming packet is 233 (the value indicating 4+4 encapsulation), the
8919206	4677	advantage will invade and spread to fixation. If this view were correct, one would expect that the population should either continue to switch between alternative states (e.g., Liberman, 1991; Stadler, 1996; Godelle & Reboud, 1997), or converge to a monomorphic ESS population. For instance, Godelle & Reboud (1997) arrived at this conclusion when they applied the gene substitution approach in the
8919206	4683	factors into account (van Boven & Weissing, 1998a). Our study sheds some light on the current controversy on the relevance of evolutionary stability in the context of long-term evolution (e.g., Weissing, 1996; Marrow et al., 1996). In essence, the ESS approach towards adaptive evolution considers evolution as a series of gene substitution events. According to this view, a monomorphic wildtype population
8919219	4925	will “intelligently” cooperate with their environment in order to support features not possible so far. Large industrial consortia, most importantly the 3 rd Generation Partnership Project (3GPP)  (with its Open Service Access (OSA) APIs) as well as the Parlay group , are working in a merging direction towards the specification of open interfaces for services that can operate across
8919219	4927	and extending Parlay with required interactions. Following this, we present a thorough evaluation of our proposed mobile agent-based performance monitoring system (based on IKV++’ s Grasshopper  mobile agent platform), compared with three other systems of similar functionality based on the Common Object Request Broker Architecture (CORBA), Sun Microsystems’ s Java Remote Method Invocation
8919219	4930	the use of a distributed object framework as proposed in the mid90s. Management based on distributed object frameworks allowed “ decentralized” and “ static” systems, as exemplified by CORBA  and Java-RMI . Decentralization issachieved by placing required management logic in network nodes and by creating instances of management objects specific to interested clients. Although
8919219	4697	for network management systems. The lesson learnt from the deployment of management systems based on early standards was that they were highly complex and suffered from long standardization cycles . The latter means that network administrators had to wait several years before a standardization cycle was completed and the required management functionality was embedded in network nodes. In
8919219	4701	to a remote node, where object instantiation and stand-alone execution takes place. The REV paradigm evolved further into the ‘Constrained’ mobility model involving mobile software agents . The model was termed constrained mobility since the software agent, upon its creation at a client site, performs a single migration to a remote server where its execution is confined. An important
8919219	4701	software agent (e.g. choosing its migration node, intelligently collaborating with other agents to achieve its task, etc). Earlier studies on constrained mobility of agents (e.g. assessments in  and ) have shown that the model fits well typical network management requirements in systems that involve long-term management tasks for which programmability of the distributed management
8919219	4705	mobile and static objects in both directions. Although some previous work considered the architectural aspects involved in the integration of mobile agents with static CORBA objects (e.g. , ), important issues of system design are yet to be thoroughly investigated. This is the direction of our current research work and an issue that standardization bodies such as the OMG should attempt
4715	4718	all the demands. Note that this result implies a (5, 5)-approximation algorithm for MinCon in the case of a single-source unsplittable flow problem. For the single commodity case, Baier, et.al.  considered the maximum routable demand when there is a bound on the total number of paths. They have constant factor approximations for that problem. They also considered the multicommodity case
4715	4720	to the above definition. A. Complexity Issues and Related Work Both MinCon and MinBan problems are NP-complete. The MinCon problem is a generalization of the unsplittable flow problem , . The Unsplittable Flow Problem (UFP) has been considered in several prior works. Klienberg  provides a comprehensive background on these problems. In fact, we state a stronger theorem, and show
4715	4720	algorithm. In the case of single-source UFP where all demands are from a single source, constant factor approximations are known for all three of the above-mentioned problems. Dinitz, et.al.,  have shown how to change a fractional flow to an unsplittable flow by violating each link by at most the maximum demand. Based on this algorithm, they have a 5-approximation for minimizing
4715	4720	LimBan problem is related to the problem of minimizing the number of rounds . There is no known constant factor approximation for the multi-commodity case but exists in the single source case . Therefore we make the natural assumption that dmax ? Cmin . Now we can show that this new problem LimBan can be reduced to the MinCon problem by splitting the edges of G to form a new graph G
4715	4720	the case of large number of demands. It would be interesting to get constant factor approximations for the MinCon problem for small number of demands. As mentioned in Section II, a 5approximation  is known for the single-source unsplittable case. Getting constant factor for the multicommodity case is of theoretical interest. The problem of decomposing a given flow with the minimum number of
4715	4722	0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sof a transport network. Our work assumes that a reasonable estimate of the traffic demand matrix is known in the network planning phase . Traffic demands are assumed to be aggregated and point to point. In the SimPol algorithm we use an LP-formulation of the multi-commodity flow problem to minimize congestion. The optimal path
4715	4723	multiplexing gain decreases. In some special cases such as in MPLS networks there may even be restrictions (label space) that may limit the number of paths that can be provisioned in the network . Furthermore, it is essential to have an estimate of the number of paths that must be monitored. This knowledge helps to optimize the management infrastructure that is required to manage customer
4715	4724	fact, using Chernoff bound they proved that if we round the fractional flow to an unsplittable flow, we lose a factor of O(log n) in expectation. For maximizing routable demand, Guraswami et. al.  proved that UFP is hard to approximate within a factor of m 1/2?? for any ? > 0 in directed networks. Kolman and Scheideler  provide a O( ? m) approximation via a greedy algorithm. In the case
4715	4725	the problem of optimizing the restoration capacity of a network. Multi-commodity flow primitives have also been used to design minimum interference routing paths for provisioning dynamic demands . The objective function in our work differs significantly from previous work. We minimize two objectives (viz) congestion and the number of paths, through the use of a polynomial time algorithm. A.
4715	4725	over the links with maximum congestion. Similarly, network operators could establish new paths to handle short-lived traffic using the unreserved bandwidth in the network. An algorithm like MIRA  can be used to set up paths using the unreserved bandwidth over the network. VI. SIMULATION RESULTS The performance of the SimPol algorithm was studied on several different simulated topologies.
4715	4725	for comparing with shortest path. A. SimPol on Known Topologies SimPol was run on 4 different known topologies: (1) a regular topology that has a lattice like structure , (2) the KL topology , (3) NSF-Net map topology  and (4) a US map topology . Table I shows the results for the known topologies. First we would like to point out that, as expected, from Theorem 3.1 the total
4715	4727	are NP-complete. The MinCon problem is a generalization of the unsplittable flow problem , . The Unsplittable Flow Problem (UFP) has been considered in several prior works. Klienberg  provides a comprehensive background on these problems. In fact, we state a stronger theorem, and show the tightness of our approximation factor. Theorem 2.1: Given an undirected graph G =(V,E) and
4715	4729	n) in expectation. For maximizing routable demand, Guraswami et. al.  proved that UFP is hard to approximate within a factor of m 1/2?? for any ? > 0 in directed networks. Kolman and Scheideler  provide a O( ? m) approximation via a greedy algorithm. In the case of single-source UFP where all demands are from a single source, constant factor approximations are known for all three of the
4715	4730	of the multi-commodity flow problem to minimize congestion. The optimal path layout scheme obtained from SimPol can be used to design network capacity as well as a good network management system , . The results of our algorithm have been tested on several different simulated topologies. The performance criteria used to evaluate the algorithm are (1) the average number of paths obtained
4715	3026	that have been used in previous work. Most of these topologies are relatively small and provide an intuitive understanding of our framework. We also generated large topologies using BRITE . The BRITE topology generator uses a Waxman model to generate flat topologies . The model parameters used were alpha = 0.15 and beta =0.20, where alpha captures the relationship between short
4715	4731	networks such as MPLS and ATM. I. INTRODUCTION In all transport networks such as Frame Relay, MPLS, ATM, etc., there is a considerable amount of effort spent on network planning , , . Planning is necessary to determine the required network capacity as well as to design efficient network management systems. Typically, service providers perform network planning on an annual or
4715	4731	solving different network optimization problems. Each of these problems differ in terms of their objective function. For example, a deterministic, multirate, multi-commodity flow problem is used in   to maximize network revenue in ATM networks. In , the authors use a multi-commodity flow formulation to solve the problem of optimizing the restoration capacity of a network.
4715	4731	path. A. SimPol on Known Topologies SimPol was run on 4 different known topologies: (1) a regular topology that has a lattice like structure , (2) the KL topology , (3) NSF-Net map topology  and (4) a US map topology . Table I shows the results for the known topologies. First we would like to point out that, as expected, from Theorem 3.1 the total number of paths obtained using the
4715	4732	different network optimization problems. Each of these problems differ in terms of their objective function. For example, a deterministic, multirate, multi-commodity flow problem is used in   to maximize network revenue in ATM networks. In , the authors use a multi-commodity flow formulation to solve the problem of optimizing the restoration capacity of a network. Multi-commodity
4715	4500	there is more than one path for a given endpoint pair, it may be necessary to perform unequal traffic splitting at the network ingress to do load balancing between these paths. As pointed to in , unequal splitting can be provided by manually setting up rules that relate the routing prefixes to particular paths. This process requires traffic measurements at the level of routing prefixes,
4715	4500	prefixes. This result is also useful even if the network operator prefers to adapt the IGP metrics to reflect the path layout in an IP network without actually setting up the paths as described in . ??? Incorporate policies in path selection: Limited bandwidth assignment framework, introduced in Section IV, prevents a single path from dominating the total bandwidth of a link. The factor ? aims
4715	4736	value it is possible to obtain one or almost one path per demand. Policy Assignment Schemes: LimBan is an example of using the SimPol framework to incorporate path/link based policy constraints . One of the advantages of using SimPol is that we can add new optimization functions through the use of integer linear program formulations. SimPol provides a small number of candidate Fig. 2.
4715	4742	for transport networks such as MPLS and ATM. I. INTRODUCTION In all transport networks such as Frame Relay, MPLS, ATM, etc., there is a considerable amount of effort spent on network planning , , . Planning is necessary to determine the required network capacity as well as to design efficient network management systems. Typically, service providers perform network planning on an annual
4744	4745	Concrete examples include nearest- or k-nearest-neighbor classifier , Bayes classifier , polynomial classifier , neural network , and support vector machine . Also structural classifiers which use string or graph representations of the characters to be classified have been proposed, see , Chapters 12 and 13 in , and . It has been
4744	4747	exclusively synthetic training data . A recent review on document image degradation models and their use in synthetic data generation of machine printed character recognition can be found in . A system for machine printed Arabic OCR that was trained on synthetic data only is described in . Recently similar ideas were proposed in the field of handwriting recognition. In  the
4744	4750	been proposed in the field of machine learning. They are characterized by the fact that they produce several classifiers out of one base classifier automatically. Prominent examples are Bagging , Adaboost  and random subspace method . For a summary of these methods see . Applications to the recognition of cursive words are described in . Once a number of classifiers
4744	4753	need to be trained. As a rule of thumb, the larger the training set, the better is the recognition performance of the system. This empirical finding has been confirmed in a number of experiments . However, the acquisition of training data is a tedious and expensive process with clear limitations. In the area of machine printed character recognition it was proposed to use synthetic data for
4744	4753	found in . A system for machine printed Arabic OCR that was trained on synthetic data only is described in . Recently similar ideas were proposed in the field of handwriting recognition. In  the synthetic generation of isolated characters has been described. The generation of synthetic handwritten words and sentences has been described in . The basic idea is to use image templates
4744	4754	or parts of two adjacent characters, into the same constituent. A large number of heuristics for achieving such kind of segmentation have been reported in the literature. For surveys see .sOnce the given input word has been transformed, through segmentation, into a sequences of graphemes, (g1,g2,...,gn), all possible combinations of adjacent graphemes, up to a maximum number M,
4744	4756	although they share some common subtasks with handwriting recognition, for example, preprocessing, feature extraction and classification methods. Recent work in this field has been reported in . An earlier survey can be found in . In this paper we focus on the recognition of cursive Roman script only. The problem of Asian script recognition is addressed in . It has to be noted,
4744	4759	a one-dimensional signal while handwriting is intrinsically two-dimensional. There has been surprisingly little work on developing two-dimensional HMMs or twodimensional HMM-like stochastic models . A major obstacle in developing such methods is surely their complexity. However with an steadily increasing power of modern computers and the potential of synthetic training data generation,
4744	1818	Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE packages including all necessary modules for training and decoding have become available . When using HMMs for a classification problem an individual HMM is usually constructed for each pattern class. For each sequence of feature vectors extracted from the input pattern, the likelihood
4744	4761	are . Finally we want to point out that there is another group of successful approaches to word recognition, which are inspired by human perception. Some representative references are . 2.4 Cursive Word Sequence Recognition In its most general form, cursive handwriting recognition requires the transcription of some handwritten text that consists of a sequence of words, for
4744	4762	that they produce several classifiers out of one base classifier automatically. Prominent examples are Bagging , Adaboost  and random subspace method . For a summary of these methods see . Applications to the recognition of cursive words are described in . Once a number of classifiers have been generated, an appropriate procedure has to be defined to combine their outputs in
4744	4764	is captured by means of a scanner and becomes available in form of an image without any temporal information. Also the use of cameras for capturing handwriting is becoming increasingly popular . Because of the lack of temporal information, off-line recognition is considered the more difficult problem. In the current paper we will focus our attention on off-line recognition. However, it
4744	4767	are . Finally we want to point out that there is another group of successful approaches to word recognition, which are inspired by human perception. Some representative references are . 2.4 Cursive Word Sequence Recognition In its most general form, cursive handwriting recognition requires the transcription of some handwritten text that consists of a sequence of words, for
4744	4770	be used in a postprocessing phase. The procedures sketched in the previous paragraph provide just a generic framework. Many instances of this generic procedure have been reported in the literature . An advantage of segmentation based word recognition schemes as discussed above is that the problem is reduced to isolated character recognition - a problem for which a number of quite mature
4744	4771	with real images but augmented with synthetic variations . Based on the same degradation model, a full-ASCII, 100-typeface classifier was developed using exclusively synthetic training data . A recent review on document image degradation models and their use in synthetic data generation of machine printed character recognition can be found in . A system for machine printed Arabic
4744	4780	field of handwriting recognition. In  the synthetic generation of isolated characters has been described. The generation of synthetic handwritten words and sentences has been described in . The basic idea is to use image templates consisting of n-tuples of characters (with n =1, 2, 3) and to concatenate them to generate words and word sequences from a given ASCII text. A similar
4744	4781	the direct comparison of different recognition algorithms. A survey of existing databases for handwriting recognition research covering the state of the art until about 1996 has been provided in . Databases included in this survey are CEDAR , NIST  and CENPARMI . Moreover there are databases for the online domain  and for Asian characters . A database that contains
4744	654	based on the confidence values returned by the classifier, the best sequence of characters matching the input word image. Typically, dynamic programming  or some A*-type search algorithm , is used. The search procedure is often run under the control of a dictionary of legal words. As an alternative, the dictionary may be used in a postprocessing phase. The procedures sketched in the
4744	654	constraints, methods for the recognition of digit strings are similar to those used for cursive word recognition. Examples of systems specifically developed for digit string recognition are . Finally we want to point out that there is another group of successful approaches to word recognition, which are inspired by human perception. Some representative references are . 2.4
4744	4786	They are characterized by the fact that they produce several classifiers out of one base classifier automatically. Prominent examples are Bagging , Adaboost  and random subspace method . For a summary of these methods see . Applications to the recognition of cursive words are described in . Once a number of classifiers have been generated, an appropriate procedure has
4744	4787	look at dependencies between classifiers in the so-called behavior-knowledge space . If the classifiers’ output is a ranked list of classes, Borda count or related methods can be applied . In the most general case, a classifier outputs a score value for each class. Then the sum, product, maximum, minimum, or the median of the scores of all classifiers can be computed for each class
4744	4788	the language model on possible sequences of words become weaker. Another approach to integrating linguistic knowledge into the recognition process is based on language syntax and syntactic parsing . Here the output of the recognizer is constrained to those sequences of words that constitute syntactically correct sentences of the underlying language. Similar ideas have been reported for the
4744	4788	we are still at a stage that is comparable to c). Very few attempts have been reported in the literature to integrate methods from natural language parsing and text understanding into a recognizer . However there are such methods available from natural language understanding. Natural language processing techniques, and machine tanslation  are very promising to improve the recognition
4744	4792	the language model on possible sequences of words become weaker. Another approach to integrating linguistic knowledge into the recognition process is based on language syntax and syntactic parsing . Here the output of the recognizer is constrained to those sequences of words that constitute syntactically correct sentences of the underlying language. Similar ideas have been reported for the
4744	4792	we are still at a stage that is comparable to c). Very few attempts have been reported in the literature to integrate methods from natural language parsing and text understanding into a recognizer . However there are such methods available from natural language understanding. Natural language processing techniques, and machine tanslation  are very promising to improve the recognition
4744	4794	Another task related to handwriting recognition is automatic signature verification. Signature is regarded an important biometric feature that is very useful to establish the identity of a person . We will not review writer identification and signature verification in greater detail in this paper, although they share some common subtasks with handwriting recognition, for example,
4744	1957	sum, product, maximum, minimum, or the median of the scores of all classifiers can be computed for each class and the class with the highest value is output as the ensembles’ classification result . It is also possible to weight each classifier according to its recognition performance and then apply a combination rule. This strategy has been adopted in Adaboost . More sophisticated
4744	4803	recognizers can be found in . As an extension of pure HMM-based recognition, some authors have proposed to use HMMs together with neural networks in hybrid systems  A special case of cursive words is digit string. The recognition of digit strings is needed in address reading (zip code) and bank check processing (courtesy amount). This problem is simpler than
4744	4807	two consecutive connected components is to be assigned to the class ’within-word’ or ’between-words’. For more details of the segmentation and the corresponding word recognition procedures see. The problem of segmenting a line of text into words is usually easier than the problem of segmenting a word into its constituent characters. Nevertheless, there are cases where complicated
4744	4810	Concrete examples include nearest- or k-nearest-neighbor classifier , Bayes classifier , polynomial classifier , neural network , and support vector machine . Also structural classifiers which use string or graph representations of the characters to be classified have been proposed, see , Chapters 12 and 13 in , and . It has been
4744	4818	properties, such as ascenders, descenders, loops or cusps, or they are derived from the greylevel distribution of the pixels in the window. Examples of HMM-based word recognizers can be found in . As an extension of pure HMM-based recognition, some authors have proposed to use HMMs together with neural networks in hybrid systems  A special case of cursive words is digit string.
4744	4818	two consecutive connected components is to be assigned to the class ’within-word’ or ’between-words’. For more details of the segmentation and the corresponding word recognition procedures see. The problem of segmenting a line of text into words is usually easier than the problem of segmenting a word into its constituent characters. Nevertheless, there are cases where complicated
4744	4822	found in . A system for machine printed Arabic OCR that was trained on synthetic data only is described in . Recently similar ideas were proposed in the field of handwriting recognition. In  the synthetic generation of isolated characters has been described. The generation of synthetic handwritten words and sentences has been described in . The basic idea is to use image templates
4744	1069	2.3.3 HMM Based Recognition The third main approach to cursive handwritten word recognition is based on hidden Markov models (HMMs). For all technical details of HMMs we refer the reader to . For a survey of HMMs in cursive handwriting recognition see . HMMs qualify as a suitable tool for cursive script recognition for a number of reasons. First they are stochastic models able to
4744	1069	free parameters of an HMM, i.e. the transition probabilities and the parameters of the feature probability distributions, BaumWelch training, a special version of the EM-algorithm is oftenused. Features are normally extracted in a left-to-right scan over the word to be recognized. Often a sliding window is used. The features describe structural properties, such as ascenders, descenders,
4744	4834	linguistic knowledge into a recognizer Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE is word N-grams . A word N-gram is a sequence of words of length N with an associated probability of occurrence. N-gram probabilities are usually estimated from natural language corpora. They are utilized in the
4744	4834	training data available for robust estimation of N-gram probabilities if N>3(itmaybeeven a problem for N = 3 or N = 2). However in this case back-off modeling and similar techniques can be applied . An alternative is to use N-grams at the level of syntactic word classes . Here each word gets assigned a syntactic tag, such as noun, verb, adverb etc., and N-grams relate to the syntactic
4744	4834	and text understanding into a recognizer . However there are such methods available from natural language understanding. Natural language processing techniques, and machine tanslation  are very promising to improve the recognition performance of today’s handwriting recognition procedures. In addition they would naturally lead to tools for content based search and retrieval in the
4744	4836	although they share some common subtasks with handwriting recognition, for example, preprocessing, feature extraction and classification methods. Recent work in this field has been reported in . An earlier survey can be found in . In this paper we focus on the recognition of cursive Roman script only. The problem of Asian script recognition is addressed in . It has to be noted,
4744	4853	for example, the transcription of personal notes, faxes, and letters, or the electronic conversion of historical handwritten archives in the context of the creation of digital libraries . The field of handwriting recognition can be divided into on-line and off-line recognition. In on-line recognition the writer is physically connected to a computer via a mouse, an electronic pen,
4744	4854	explored. Under the most elaborate model, the system trained exclusively on synthetic data reached a recognition rate comparable to that of the same system trained with natural handwriting only. In  a geometrical distortion model for complete lines of handwritten text was proposed. The model is 1 Please contact the author or Matthias Zimmermann at zimmerma@iam.unibe.ch. See also:
4744	4857	of the dilemma is to use data from a general pool of writers in a first training phase and to adjust the system in a second phase, using some carefully chosen samples provided by the future user . However, the question of writer-dependent vs. writer-independent performance as well as adjusting an existing system to some particular writing style has been rarely addressed in the literature,
4744	4858	sequence recognition it is common to approximate the handwriting’s contour by straight line segments and use the average or median direction of these straight segments as an estimate of the slant . Other methods have been reported in .s2.2 Recognition of Isolated Characters The task of isolated character recognition is usually cast as a pattern classification problem, where
4744	4858	process, like the segmentation of a word into its constituent characters is delivered as a by-product of HMM-based word recognition. This technique has been successfully used in some systems . In cursive word recognition, a lexicon of legal words is usually assumed. This lexicon significantly decreases the number of possible character sequences to be taken into account by the
4744	4858	© 2003 IEEE The IAM database was instrumental in the development of a number of handwriting recognition systems at the University of Bern. However it has also been used by other research groups  The database is still being enlarged and freely available upon request. 1 3.2 Synthetic Training Data All methods for handwritten character, word or sentence recognition need to be trained. As a
4744	4860	used in the recognition phase, making a classifier insensitive to perturbations that naturally occur in handwriting . Further methods for synthetic handwriting generation have been reported in . 3.3 Multiple Classifier Systems Recently it has been shown that systems incorporating multiple classifiers have the potential of improved classification accuracy over single classifiers in
4744	4861	programming techniques in order to optimally align the individual classifiers’ outputs. However this topic is still under research and only a few solutions have been reported in the literature . 4 Outlook and Conclusions The focus of attention in handwriting recognition research has been gradually shifting from isolated character recognition to more complex tasks, such as recognition of
4744	4863	properties, such as ascenders, descenders, loops or cusps, or they are derived from the greylevel distribution of the pixels in the window. Examples of HMM-based word recognizers can be found in . As an extension of pure HMM-based recognition, some authors have proposed to use HMMs together with neural networks in hybrid systems  A special case of cursive words is digit string.
4744	4866	the language model on possible sequences of words become weaker. Another approach to integrating linguistic knowledge into the recognition process is based on language syntax and syntactic parsing . Here the output of the recognizer is constrained to those sequences of words that constitute syntactically correct sentences of the underlying language. Similar ideas have been reported for the
4744	4866	we are still at a stage that is comparable to c). Very few attempts have been reported in the literature to integrate methods from natural language parsing and text understanding into a recognizer . However there are such methods available from natural language understanding. Natural language processing techniques, and machine tanslation  are very promising to improve the recognition
377185	4874	1: Dartmouth Re-configurable Wireless Sensor PlatformsAccording to a survey on sensor networks , currently most research efforts focus on sensor hardware design such as the Smart Dust mote , protocols and algorithms design for different network layers, which include the physical layer, the data link layer, the network layer, the transport layer and the application layer. The positions
377185	4875	routing algorithms to setup effective communication networks. Many ad-hoc routing algorithms and power-saving methods are proposed to address such issues, such as the direct diffusion algorithm . At the application layer, some projects such as Cornell’s COUGAR project  build systems to query and merge sensor’s data. However, high-level sensor information systems are needed to integrate
377185	4876	so no special handling is required to distinguish these different resource types. This system supports searching and matching of both non-functional and functional properties. See details in . Figure 4: Composer In the sensor network environment, there are two types of services: sensor web services and fuselet services. While sensor web services provide sensor data, fuselet services are
377185	4878	of a sequence of related events along the time. Data association algorithms are used to cluster events for different hypotheses. Multiple target tracking algorithms such as Reid’s algorithm  recursively calculate the probability on how likely the new observations are associated with every existing hypotheses. Then the new observation is added into the hypothesis with the maximum
4879	4884	the governing equation of motion . When waiting times have heavy tails of order 1 <?? 2, meaning that the probability of waiting longer than t falls off like t ?? , a different model is needed . In this case, convergence of the waiting time process requires centering to the mean waiting time w, which is not necessary when 0 <?<1. Accounting for this leads to a waiting time process W (t)
4879	4884	so it is proper to use Max(t) =sup{W (u) :0? u ? t} to represent the particle jump times. This substitution is explained as a particle that makes more than one jump without intervening rest periods . Then the inverse or first passage time process H(t) = inf{x : Max(x) ? t} counts the number of particle jumps. The first passage time process H(t) serves as the subordinator for (and when ?<1 is
4879	4884	process A(E(t)). The density h(x, t) of this process solves a fractional Cauchy problem (d/dt) ? h = Lh where 0 <?<1 . The next theorem extends this result to 1 <?? 2. Theorem 4.1 in  shows that the long-time CTRW limit process in this case is A(H(t)), and Corollary 4.2 in  shows that this limit process has density ? ? 0 p(x, s) ds(H(t, s)). The next theorem shows that this
4879	4192	density function tails that fall off like |x| ?1??  with some index 0 <?<2), the limit process A(t) is an ?-stable Lévy motion, and the governing equation becomes ?p/?t = ??? ? p/?|x| ? . When waiting times between the jumps are introduced, the limiting process is altered via subordination . ? Corresponding Author Email addresses: bbaeumer@maths.otago.ac.nz (B. Baeumer),
8919231	4916	to encode their approach in our formalism, and analyse their assumptions, which are quite strong. This is left for future work. Our approach has many similarities with the Acta framework . Acta provides a set of logical primitives over execution histories, including presence of an event, implication, and causal dependence and ordering between events. Acta makes assumptions specific
8919231	4917	of our CSMP property. Sousa et al.  generalise Lamport’s state-machine approach to the commitment of partially replicated databases. Much formal work on consistency focuses on serialisability . CSMP constitutes a generalisation of serialisability. As seen earlier, we can encode a linearisable protocol  with a transition rule. The X-Ability theory  allows an action to appear several
8919231	4917	to encode their approach in our formalism, and analyse their assumptions, which are quite strong. This is left for future work. Our approach has many similarities with the Acta framework . Acta provides a set of logical primitives over execution histories, including presence of an event, implication, and causal dependence and ordering between events. Acta makes assumptions specific
8919231	4918	of the Acta dependencies into our language. Acta takes serialisability as the definition of consistency, and does not deal with partial replication. Constraints ? and ? were first proposed by Fages  for general reconciliation problems in optimistic replication systems. 6 Conclusions and future work We presented a formalism for describing replication protocols and consistency. Our significant
8919231	4919	hence vacuously acyclic, hence multilogs are sound. Every action is guaranteed, hence decided, and is stable immediately when submitted. This explains why consensus is not needed. The ESDS protocol  assumes that ? is acyclic and that every action is guaranteed. However, actions do not commute, and ESDS requires a distributed consensus for serialisation. Many systems centralise consensus at a
8919231	4920	focuses on serialisability . CSMP constitutes a generalisation of serialisability. As seen earlier, we can encode a linearisable protocol  with a transition rule. The X-Ability theory  allows an action to appear several times in the same schedule if it is idempotent; for instance, retrying a failed action is allowed. Schedules are tested for equivalence after filtering out such
8919231	4921	possibility for S1(3) is init.?.?. At this point, it would be unsound to add ? ? ?. A specific protocol may have additional transition rules. As an example, let us encode a linearisable protocol , i.e., where an action takes effect at some instant in time, and actions execute in taking-effect order. Let us identify the time an action is submitted with when it takes effect. The following
8919231	4921	replicated databases. Much formal work on consistency focuses on serialisability . CSMP constitutes a generalisation of serialisability. As seen earlier, we can encode a linearisable protocol  with a transition rule. The X-Ability theory  allows an action to appear several times in the same schedule if it is idempotent; for instance, retrying a failed action is allowed. Schedules are
8919231	4162	might fail. ED implies that every action eventually becomes stable . 3.3 Common Monotonic Strong Prefix (CMSP) Intuitively, if all sites execute the same schedule then the system is consistent . But this does not work for optimistic protocols where Si(t) is not nec5sessarily a prefix of Si(t + 1). However, even in optimistic systems, we expect all schedules to have a same (up to
8919231	4162	this analysis. The primitives are common to all protocols, as are the significant events of actions becoming guaranteed, dead, serialised, decided and stable. Lamport’s state-machine replication  broadcasts actions to all sites and ensures consistency because each site executes exactly the same schedule. This is a special case of our CSMP property. Sousa et al.  generalise Lamport’s
8919231	4922	constraints language is surprisingly expressive. We have used it to express the semantics of applications as diverse as a shared calendar, a travel reservation system and a replicated file system . For instance if ? creates a directory and ? a file in that same directory, the file system submits ? ? ? ? ? ? ? (causal dependence) along with ?. Constraints are also the language for enforcing
8919231	4922	is allowed. Such local decisions may be sub-optimal. To ensure optimality, viz., that the smallest possible number of actions is made dead, it is necessary to consider the whole graph as in IceCube . 4 Partial replication Up to now we assumed that all data is replicated at every site. Let us now consider partial replication: shared data is partitioned into n disjoint databases D 1 , . . . , D
8919231	4923	motivated us to understand the commonalities and differences between protocols. The relations between consistency and ordering have been well studied in the context the causal dependence relation . Our simpler and modular primitives clarify and generalise this analysis. The primitives are common to all protocols, as are the significant events of actions becoming guaranteed, dead, serialised,
8919231	4924	motivated us to understand the commonalities and differences between protocols. The relations between consistency and ordering have been well studied in the context the causal dependence relation . Our simpler and modular primitives clarify and generalise this analysis. The primitives are common to all protocols, as are the significant events of actions becoming guaranteed, dead, serialised,
8919231	4925	availability at the cost of maintaining consistency, since each site’s view may be partial or stale. Although a number of protocols have been proposed to achieve various degrees of consistency , we lack a common framework for understanding and comparing them. This paper presents such a framework. In our model, a replicated system consists of a database, replicated at several sites, and a
8919231	4925	they access independent variables. (ii) Overwriting: in some systems an out-of-order write has no effect; then writes effectively commute. For instance in timestamped replication (Last Writer Wins) , writing a variable tests whether the write timestamp is greater than the object’s; if so the write takes effect, otherwise it is a no-op . (iii) Reconciliation: for instance in Operational
8919231	4925	occasional cycles of ?, which require a localised consensus. 4 An action unrelated to any other by ? is considered a chain of length 0. 10s5 Related work Our survey of optimistic replication  motivated us to understand the commonalities and differences between protocols. The relations between consistency and ordering have been well studied in the context the causal dependence relation
8919231	4928	replication  broadcasts actions to all sites and ensures consistency because each site executes exactly the same schedule. This is a special case of our CSMP property. Sousa et al.  generalise Lamport’s state-machine approach to the commitment of partially replicated databases. Much formal work on consistency focuses on serialisability . CSMP constitutes a generalisation
8919231	4930	. 3.4 Eventual consistency Another classical property is Eventual Consistency, which has been used to argue informally about the correctness of optimistic systems such as Grapevine  or Bayou . This will be our second definition of consistency. Property 3 A system is Eventually Consistent if, if every client stops submitting, and submitted actions are decided, then eventually every site
8919231	4930	acyclic and that every action is guaranteed. However, actions do not commute, and ESDS requires a distributed consensus for serialisation. Many systems centralise consensus at a primary site. Bayou  partitions 8 ? ? ? A 3 ? A 2 ? ? ? ? ? ? ? ? ? A 2sthe data into independent databases, each with its own primary site. Primaries implement consensus for their own actions. Between databases, Bayou
8919231	4931	variable tests whether the write timestamp is greater than the object’s; if so the write takes effect, otherwise it is a no-op . (iii) Reconciliation: for instance in Operational Transformation , two actions submitted concurrently execute in arbitrary order. The second one to execute is transformed to ignore the effect of the first, in effect rendering them commutative. (iv) Failure or
4937	5172	each user, two structures are exported: 1) SEM bundle, which includes the SEM’s half-key dSEM i , and 2) user bundle, which includes du i and the entire server bundle. The server bundle is in PKCS#7 format, which is basically a RSA envelope signed by the CA and encrypted with the SEM’s public key. The client bundle is in PKCS#12 format, which is a shared-key envelope also signed by the CA
4937	4938	discussion.) 2 There is also a very similar multiplicative mRSA (*mRSA) first proposed by Ganesan .s6 Xuhua Ding and Gene Tsudik Our analysis is mainly derived from the results in , ( has similar results) where it was shown that a public-key encryption system in a multi-user setting is semantically secure against certain types of attacks if and only if the same system in a
4937	4939	the relevant discussion.) 2 There is also a very similar multiplicative mRSA (*mRSA) first proposed by Ganesan .s6 Xuhua Ding and Gene Tsudik Our analysis is mainly derived from the results in , ( has similar results) where it was shown that a public-key encryption system in a multi-user setting is semantically secure against certain types of attacks if and only if the same system in a
4937	4939	we begin by asserting that IB-mRSA in single-user mode is equivalent to the standard RSA/OAEP, which is proven secure against CCA-2 under the random oracle . Next, we apply the theorems in  with the condition that all users are honest. To remove this condition, we analyze the distribution of views of the system from users and outside adversaries. Furthermore we introduce an additional
4937	4939	Thus, we claim that they are equally secure. For the multi-user setting, we cannot claim that IB-mRSA with n users is semantically secure by directly applying the security reduction theorem in . The reason is that our system is not a typical case referred in . Sharing a common RSA modulus among many users results in their respective trapdoors not being independent; consequently, there
4937	4939	underlying protocol so that they can bypass the mandatory security control of the SEM. However, assuming for the moment, that all users are honest, we can obtain the following lemma derived from . Lemma 2. IB-mRSA/OAEP system with n users is semantically secure if all n are honest. More precisely,swhere t1 = tn + O(log(qen)) Simple Identity-Based Cryptography with Mediated RSA 7 Succ IB n
4937	4939	there are clearly no attacks. Thus, IB-mRSA with multi-user can be considered as an example of encryption system in  where each user has an independent trapdoor. We adapt the original proof in  in order to claim security against CCA-2 since no user actually knows its own trapdoor in IB-mRSA. See Appendix A.3 for details. Unfortunately, in a real application, all users cannot be assumed to
4937	4941	and communicate public keys. In this paper, we present IB-mRSA, a simple variant of mRSA that combines identity-based and mediated cryptography. Under the random oracle model, IB-mRSA with OAEP is shown as secure (against adaptive chosen ciphertext attack) as standard RSA with OAEP. Furthermore, IB-mRSA is simple, practical, and compatible with current public key infrastructures.
4937	4941	PKIs. At the same time, IB-mRSA offers security comparable to that of RSA, provided that a SEM is not compromised. Specifically, it can be shown that, in the random oracle model, IB-mRSA with OAEP is as secure – against adaptive chosen ciphertext attacks – as RSA with OAEP. The rest of the paper is organized as follows. The next section gives a detailed description of IB-mRSA. The security
4990	4991	and to conduct Hoare-style verification using programmer-supplied loop invariants in the PALE system . A decidable logic called Lr (for “logic of reachability expressions”) was defined in . Lr is rich enough to express the shape descriptors studied in  and the path matrices introduced in . Also, in contrast to WS2S, MSO-E and ??(DTC) , Lr can describe certain families of
4990	4992	composition, conditional statements, and loop invariants is expressed by similar extensions to the standard Hoare-style approach. 3.2 Abstract Interpretation The abstract-interpretation technique  allows conservative automatic verification of partial correctness to be conducted by identifying sound over-approximations to loop invariants. An iterative computation is carried out to determine
4990	4993	Work 4.1 Decidable Logics for Expressing Data-Structure Properties Two other decidable logics have been successfully used to define properties of linked data structures: WS2S has been used in  to define properties of heap-allocated data structures, and to conduct Hoare-style verification using programmer-supplied loop invariants in the PALE system . A decidable logic called Lr (for
4990	4994	A decidable logic called Lr (for “logic of reachability expressions”) was defined in . Lr is rich enough to express the shape descriptors studied in  and the path matrices introduced in . Also, in contrast to WS2S, MSO-E and ??(DTC) , Lr can describe certain families of arbitrary stores—not just trees or structures with one binary predicate. However, the expressive power of
4990	4995	procedure for MSO-E, we have implemented a translation of MSO-E formulas into WS2S (using the simulation method, as described in the full paper), and used an existing decision procedure MONA  for WS2S logic. Remark. If the decidable logic D places a syntactic restriction on formulas, as is the case for ??(DTC) , then condition (ii) of Corollary 1 may not be satisfied. In such
4990	5000	Work 4.1 Decidable Logics for Expressing Data-Structure Properties Two other decidable logics have been successfully used to define properties of linked data structures: WS2S has been used in  to define properties of heap-allocated data structures, and to conduct Hoare-style verification using programmer-supplied loop invariants in the PALE system . A decidable logic called Lr (for
4990	5000	obvious advantage of specifying the formulas directly, without the need for translation, whereas our method requires translation, which may not be defined for some formulas and some logics. PALE  uses a hard-coded mapping of linked data-structures into WS2S, and uses MONA decision procedures. The simulation technique can be used to extend the applicability of WS2S to more general sets of
4990	5002	brute-force way (by generating a validity query for all elements in the domain). For more elaborate algorithms that handle finite-height lattices of infinite cardinality, the reader is referred to . 3.3 Summary We now summarize the steps that a user of our approach has to go through to carry out a verification. The user must provide the inputs listed below. The first two inputs are standard
4990	5003	loop invariants in the PALE system . A decidable logic called Lr (for “logic of reachability expressions”) was defined in . Lr is rich enough to express the shape descriptors studied in  and the path matrices introduced in . Also, in contrast to WS2S, MSO-E and ??(DTC) , Lr can describe certain families of arbitrary stores—not just trees or structures with one binary
4990	5004	technique by showing its applicability to semi-automatic Hoare-style verification. The technique can also be applied to improve the precision of operations used in abstract interpretation . Hoare-style verification: Recall that in Hoare-style verification, a programmer expresses partial-correctness requirements of the form {pre}st{post}, where pre and post are logical formulas that
4990	5004	the need to provide loop invariants, but may only establish weaker safety properties than those proved by Hoare-style verification. This fills in the missing piece of the algorithm presented in , which requires a decidable logic to compute the best abstract transformer automatically. This allows us to apply the algorithm from  to imperative programs that perform destructive updates. We
4990	5004	brute-force way (by generating a validity query for all elements in the domain). For more elaborate algorithms that handle finite-height lattices of infinite cardinality, the reader is referred to . 3.3 Summary We now summarize the steps that a user of our approach has to go through to carry out a verification. The user must provide the inputs listed below. The first two inputs are standard
8908374	5007	antigen (Anderton et al., 2001) and those that respond to immunodominant epitopes appear to be more susceptible to exhaustion than those that respond to subdominant epitopes (Aichele et al., 1997; Zajac et al., 1998; Slifka et al., 2003). It has also been found that death of effector T cells by antigenic stimulation in vitro is dose-dependent (Iezzi et al., 1998). To add CTL exhaustion to the model, I
5009	5010	use of energy resources is a must in sensor networks. Various solutions have been proposed to reduce the sensors energy expenditure. For instance, energy-efficient MAC layer schemes can be found in , . Traffic routing and connectivity issues in sensor networks are addressed in , , , while energy-aware strategies for data dissemination and data collection appear in , , .
5009	5010	From the energy consumption viewpoint, an effective technique is to place sensors in sleep mode during idle periods . The benefits of using sleep modes at the MAC layer are presented in , where the authors describe the so-called PAMAS scheme that allows a node to turn off its RF apparatus when it overhears a packet that is not destined for it. The work in , ,  propose
5009	4875	to reduce the sensors energy expenditure. For instance, energy-efficient MAC layer schemes can be found in , . Traffic routing and connectivity issues in sensor networks are addressed in , , , while energy-aware strategies for data dissemination and data collection appear in , , . This work was supported by the Italian Ministry of University and Research through the
5009	5013	the sensors energy expenditure. For instance, energy-efficient MAC layer schemes can be found in , . Traffic routing and connectivity issues in sensor networks are addressed in , , , while energy-aware strategies for data dissemination and data collection appear in , , . This work was supported by the Italian Ministry of University and Research through the VICOM and
5009	5013	quality of service provisioning. Relevant to our work are also the numerous network layer schemes that address the problem of data routing in the case where some network nodes may be sleeping , . With regard to analytical studies, results on the capacity of large stationary ad hoc networks are presented in  (note that sensor networks can be viewed as large ad hoc networks). In  two
5009	5014	be found in , . Traffic routing and connectivity issues in sensor networks are addressed in , , , while energy-aware strategies for data dissemination and data collection appear in , , . This work was supported by the Italian Ministry of University and Research through the VICOM and the PRIMO projects From the energy saving viewpoint, a widely employed technique is to
5009	5014	we show an important phenomenon that is observed when the network load G is close to 1. Multipointto-point communications suffer from the well known problem of data implosion at the destination . Solving this problem was not the scope of this work; thus, in Section III we simply adopted an architectural solution that allows nodes to adapt to traffic conditions avoiding network instability
5009	5015	found in , . Traffic routing and connectivity issues in sensor networks are addressed in , , , while energy-aware strategies for data dissemination and data collection appear in , , . This work was supported by the Italian Ministry of University and Research through the VICOM and the PRIMO projects From the energy saving viewpoint, a widely employed technique is to place
5009	5016	in , . Traffic routing and connectivity issues in sensor networks are addressed in , , , while energy-aware strategies for data dissemination and data collection appear in , , . This work was supported by the Italian Ministry of University and Research through the VICOM and the PRIMO projects From the energy saving viewpoint, a widely employed technique is to place nodes
5009	5016	are studied: one including arbitrarily located nodes and traffic patterns, the other one with randomly located nodes and traffic patterns. The case of tree-like sensor networks is studied in , where the authors present optimal strategies for data distribution and data collection, and analytically evaluate the time performance of their solution. An analytical approach to coverage and
5009	5017	the VICOM and the PRIMO projects From the energy saving viewpoint, a widely employed technique is to place nodes in a low-power operational mode, the so-called sleep mode, during idle periods . In fact, in idle state sensors do not actually receive or transmit, nevertheless they consume a significant amount of power. In sleep mode, instead, some parts of the sensor circuitry (e.g.,
5009	5017	algorithms for traffic routing, topology management and channel access control. From the energy consumption viewpoint, an effective technique is to place sensors in sleep mode during idle periods . The benefits of using sleep modes at the MAC layer are presented in , where the authors describe the so-called PAMAS scheme that allows a node to turn off its RF apparatus when it overhears a
5009	5017	maintained ready to operate. Notice also that an energy cost E t is associated with each transition from sleep to active mode, while the cost of passing from active to sleep mode can be neglected . We assume that E t is twice the energy consumption per time slot in idle mode 2 . buffer not empty R N R buffer empty S A S A S Fig. 2. Temporal evolution of the sensor state Based on the above
5009	5018	are presented in , where the authors describe the so-called PAMAS scheme that allows a node to turn off its RF apparatus when it overhears a packet that is not destined for it. The work in , ,  propose wake-up scheduling schemes at the MAC layer which wake up sleeping nodes when they need to transmit/receive, thus avoiding a degradation in network connectivity or quality of service
5009	3551	of data routing in the case where some network nodes may be sleeping , . With regard to analytical studies, results on the capacity of large stationary ad hoc networks are presented in  (note that sensor networks can be viewed as large ad hoc networks). In  two network scenarios are studied: one including arbitrarily located nodes and traffic patterns, the other one with
5009	3551	Consider a transmission over one hop and let nodes i and j (1 ? i ? N, and 0 ? j ? N with 0 indicating the sink) be the transmitter and the receiver, respectively. The transmission is successful if : 1) the distance between i and j is not greater than r, (2) di,j ? r (3) 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sR 0 R R R 1 2 3 S N N N 0 1 2 3 Fig. 3. Markov chain describing the
5009	5020	located sensors sending data to a unique destination and operating in sleep or active mode. Finally, relevant to our work is the Markov model of the sensor sleep/active dynamics developed in . The model predicts the sensor energy consumption; by acquiring this information for each sensor, a central controller constructs the network energy map representing the energy reserves available
5009	2722	the system. Therefore sensors have knowledge of their neighboring nodes, as well as of the possible routes to the sink. (for instance through a routing algorithm such as the one proposed in ). Since we consider a network of stationary nodes performing, for instance, environmental monitoring and surveillance, the routes and their conditions can be assumed to be either static or slowly
5009	5023	node, l, simultaneously transmitting, dl,j >r. (5) To avoid unsuccessful transmissions, we assume that sensors employ a CSMA/CA mechanism with handshaking, as in the MACA and MACAW schemes ,  (although, other MAC protocols could be considered as well), and that the radio range of handshaking messages transmission is equal to r. Ifi wants to transmit to j and senses the channel as idle,
7451576	5031	into several groups: • Methods that use a monolingual dictionary (, …), • Methods that use WordNet (,  …), • Methods that use WordNet and use some analyses on a textual corpus (, ,  …), • Methods that use a thesaurus ( …). Methods that use WordNet or a thesaurus may give very good results, because WordNet and thesauri are manually created and the relations
7451576	5273	lexical proximity between words opens ways for the semantic processing of text. The main existing methods can be categorized into several groups: • Methods that use a monolingual dictionary (, …), • Methods that use WordNet (,  …), • Methods that use WordNet and use some analyses on a textual corpus (, ,  …), • Methods that use a thesaurus ( …). Methods that use
7451576	5034	several groups: • Methods that use a monolingual dictionary (, …), • Methods that use WordNet (,  …), • Methods that use WordNet and use some analyses on a textual corpus (, ,  …), • Methods that use a thesaurus ( …). Methods that use WordNet or a thesaurus may give very good results, because WordNet and thesauri are manually created and the relations between words in
7451576	5037	into several groups: • Methods that use a monolingual dictionary (, …), • Methods that use WordNet (,  …), • Methods that use WordNet and use some analyses on a textual corpus (, ,  …), • Methods that use a thesaurus ( …). Methods that use WordNet or a thesaurus may give very good results, because WordNet and thesauri are manually created and the relations between
7451576	5282	to experiment this method on the English language, we will have to create a network of English words on which we can compute the similarity. To do so, we have used the US Webster 1913 dictionary , which is freely available online thanks to the Gutenberg project: www.gutenberg.net. The application of our method to more structured and better created resources like WordNet and Roget is also
8919262	5058	the role of interpersonal relationships in human interaction (Coleman 1988; Granovetter 1985), the concept of social capital has been little used in economics (Fafchamps and Minten 1999; Barr 1997; Narayan and Pritchett 1996). There are two possible meanings of social capital. The first definition sees social capital as a “stock” of trust and an emotional attachment to a group or society that facilitates the provision
8919262	5309	Historically, institutions have emerged in various contexts to facilitate anonymous trade. Historical institutional analysis of premodern trade in medieval Europe by Milgrom, North, and Weingast (1990) shows that an institution known as the Law Merchant enabled impersonal exchange to occur in 12th- and 13th-century Champagne fairs. The Law Merchant enabled trade through a reputation mechanism
8919262	5092	by the transaction costs of search and by social capital. Recursive econometric models have been used to explain gift exchange (Ravallion and Dearden 1988) and technology adoption (Kumar 1994; Zeller et al. 1996). A two-step Tobit estimation avoids the inconsistent estimates of brokerage use due to the simultaneity bias that arises because trader-specific variables influence both the use of brokers and the
8919263	5342	on some aspects of flows. The most useful of these for application breakdowns are the source and destination port numbers, and the IP protocol number. The protocol numbers used are well documented , with TCP being protocol 6, and UDP being 17. TCP, and UDP traffic also define (16 bit) source and destination port numbers intented (in part) to for use by different applications. The port numbers
4208	5096	on the problem’s value function, exploiting the dynamic programming (DP) equations, by induction on the finite horizon. Infinite-horizon models inherit such properties. See, e.g.,  and . Related yet distinct qualitative properties are suggested by the indexability analysis of RB models, given in terms of the monotonicity of the index on the state. Consider a queueing admission
4208	5096	admission to a birth-death queue This section discusses a model for the optimal control of admission to a birth-death queue, a fundamental problem which has drawn extensive research attention. See . We shall use the model to motivate our approach, by introducing a novel analysis grounded on an intuitive index characterization of optimal threshold policies. 2.1. Model description Consider the
4208	5096	the action a(t) ?{0, 1} to take at each time t. Policies are chosen from the class U of stationary policies, basing action choice on the state. Given policy u and state j, we denote by u(j) ?  the probability of taking action a = 1(shut the entry gate), so that 1 ? u(j) is the probability of action a = 0(open it). We shall refer to a = 1astheactive action, and to a = 0 as the passive
4208	5096	research attention, aiming to establish the optimality of threshold policies (which shut the entry gate iff L(t) lies above a critical threshold), and to compute and optimal threshold. See . 2.2. Optimal index-based threshold policy In contrast with previous analyses, we introduce next a novel solution approach grounded on the following observation: one would expect that, under
4208	5096	actions to take: active (engage the project; a(t) = 1) or passive (let it rest; a(t) = 0). Denote by U the class of state-dependent, or stationary policies. A policy u ? U is thus a mapping u : N ? , where u(i) (resp. 1 ? u(i)) is the probability that action a = 1 (resp. a = 0) is taken in state i. Taking action a in state i has two effects: first, cost ha i is incurred in the current period,
4208	5098	to offer in the field of stochastic optimization. We highlight two avenues for further research, which are the subject of ongoing work: test empirically the proposed heuristic index policies, as in ; and provide approximate and asymptotic analyses of their performance, as in . A. Discrete-time reformulation We reformulate the model of concern into discrete time by deploying the standard
4208	5103	of the optimality of Gittins’ index rule for the MBP and extensions. Approximate GCLs were deployed in  to establish the near-optimality of Klimov’s rule in the parallel-server case. See  for an overview of such achievable region approach. The theory of conservation laws was extended in , through the notion of partial conservation laws (PCLs), which were brought to bear on the
4208	5109	into the generalized conservation laws (GCLs) framework in , giving a polyhedral account of the optimality of Gittins’ index rule for the MBP and extensions. Approximate GCLs were deployed in  to establish the near-optimality of Klimov’s rule in the parallel-server case. See  for an overview of such achievable region approach. The theory of conservation laws was extended in ,
4208	5109	research, which are the subject of ongoing work: test empirically the proposed heuristic index policies, as in ; and provide approximate and asymptotic analyses of their performance, as in . A. Discrete-time reformulation We reformulate the model of concern into discrete time by deploying the standard uniformization technique (cf. ), which proceeds in two steps: (i) the original
4208	4206	for indexability hindered the application of such index in the 1990s. An alternative index, free from such scope limitation, was proposed in . Regarding indexability, we first presented in  a tractable set of sufficient conditions, based on the notion of partial conservation laws, along with a one-pass index algorithm. Monotone optimal policies in MDPs In MDP applications intuition
4208	4206	in  to establish the near-optimality of Klimov’s rule in the parallel-server case. See  for an overview of such achievable region approach. The theory of conservation laws was extended in , through the notion of partial conservation laws (PCLs), which were brought to bear on the analysis of Whittle’s RB index. PCLs imply the optimality of index policies with a postulated structure
4208	4206	same finite optimal value v LP . 4.3. Adaptive-greedy algorithm and allocation indices This section discusses the adaptive-greedy algorithm AG1(·|F), described in Figure 4.1, which we introduced in . It defines a tractable domain of admissible cost vectors, under which it constructs an optimal index-based solution to dual LP (4.6). The algorithm is fed with input cost vector c, and produces as
4208	4206	the coefficients of corresponding marginal cost terms yields the stated identity. ?? 5. Partial conservation laws This section reviews the partial conservation laws (PCLs) framework introduced in , emphasizing its grounding on F-extended polymatroid theory. Consider a scheduling model involving a finite set J of n job classes. Effort is allocated to competing jobs through a scheduling policy
4208	4206	of the decomposition property in Section 4.4. We shall apply a special case of the result below in Section 6. Decomposition results have been previously established in  (under GCLs), and in  (under PCLs). The following is a refined version of the latter. Consider a finite collection of m ? 2 projects, with project k ? K ={1,... ,m} evolving through finite state space Nk. Effort is
4208	5120	rate, starting at each state.sDynamic allocation indices for restless projects 363 As for the general RBP, its increased modeling power comes at the expense of tractability: it is P-SPACE HARD . The research focus must hence shift to the design of well-grounded, tractable heuristic index policies. Whittle  first proposed such a policy, which recovers Gittins’in the MBP case, and
5128	5129	although many of the results herein do not require it. Therefore, a faulty sorter or disturbance may unsort the list, but the assumption requires that the list may not change as a set. A list L = , ..., L] drawn from the set {1, 2, ..., m} is a sequence of n ordered and distinct elements. We further assume that m = n: the set of all lists of length n is then the symmetric group Sn of all
5128	5129	the range of the pseudo-energy functions are {0, 1, ..., ? ? n 2 } and {0, 1, ..., n ? 1}, respectively. Thus, VTI gives a finer resolution over Sn than VAI . For example, consider the lists L1 =  and L2 = . The pseudo-energies are VT I(L1) = 2, VT I(L2) = 4 and VAI(L1) = VAI(L2) = 1, so the two lists are equally sorted in the sense of adjacency while L2 is less sorted according
5128	5129	1} n has n components  T , where the L th component is defined by n? ?k ? L?. q o L (L) = j=k 6 2sIn words, q o i is the number of elements less than i, located in {L, ..., L}, to the right of i. The definition for q o is based on  and references therein, which discuss the construction of a (total) inversion list from a given permutation. The reason for
5128	5129	definitions that VTI (L) = d(L, L ? ). Henceforth, in referencing the metric d above we will utilize the ?·?1 norm, e.g. on R 2 we have ?(x, y)?1 = |x| + |y|. From the example above, q o (L1) T = , q o (L2) T =  ? d(L1, L2) = 2. Spearman’s footrule distance F is given by F (L1, L2) ? ? n i=1 |L1 ? L2|, which is itself a ? · ?1-like norm. Over Sn, the metric F clearly has
5128	5129	the progress of a sorter in the sense of adjacency than the metric d, as K actually gives the number of adjacent sort operations between lists. To see this by example, consider the permutations ? = , ? = , ? = . The values for the Kendall distances are K(?, ?) = 3, K(?, ?) = 1 and K(?, ?) = 2 while the metric d (define with q o ) returns d(?, ?) = 1, d(?, ?) = 1 and
5128	5131	process as input for the next set of iterations. 3 Related Work The research we report on in this paper is is closely related to the theory of self-checking programs as described in, for example, . In this theory, checkers C are constructed that check the accuracy of programs P that (supposedly) compute some function f. The checker is supposed to fundamentally differ from the program (e.g.
5128	5131	example, one might check the result of a program P that computes y = f(x) = ? x by multiplying y by itself to see if it equals x. The problem of sorting has also been considered in this community , albeit in a different manner. In particular, in this paper we suppose that a software process may be monitored as it is executing to see if it is converging 4sto a correct answer or diverging.
5128	5131	the range of the pseudo-energy functions are {0, 1, ..., ? ? n 2 } and {0, 1, ..., n ? 1}, respectively. Thus, VTI gives a finer resolution over Sn than VAI . For example, consider the lists L1 =  and L2 = . The pseudo-energies are VT I(L1) = 2, VT I(L2) = 4 and VAI(L1) = VAI(L2) = 1, so the two lists are equally sorted in the sense of adjacency while L2 is less sorted according
5128	5131	the progress of a sorter in the sense of adjacency than the metric d, as K actually gives the number of adjacent sort operations between lists. To see this by example, consider the permutations ? = , ? = , ? = . The values for the Kendall distances are K(?, ?) = 3, K(?, ?) = 1 and K(?, ?) = 2 while the metric d (define with q o ) returns d(?, ?) = 1, d(?, ?) = 1 and
5128	5135	violates the behavior of the observer (because it has a bug, for instance), a warning flag is raised and the program can be shut down. Similar, if less formal, ideas appear in operating systems  where various quantities can be measured to determine of a system is operating normally, or if it has ben “infected” or otherwise compromised. We expect such an approach to be useful in control
5128	5136	related to checking a program as it executes is the design of synchronous observers . In  finite state machine abstractions of programs written in the data-flow programming language LUSTRE  are used to verify safety properties. The abstractions specify more behaviors than the actual code (i.e. they contain all safe behaviors) and are easier to verify than the original programs.
5128	5137	we are extremely interested in the relation between self-testing and feedback control theory. Somewhat related to checking a program as it executes is the design of synchronous observers . In  finite state machine abstractions of programs written in the data-flow programming language LUSTRE  are used to verify safety properties. The abstractions specify more behaviors than the actual
5128	5138	however, we are extremely interested in the relation between self-testing and feedback control theory. Somewhat related to checking a program as it executes is the design of synchronous observers . In  finite state machine abstractions of programs written in the data-flow programming language LUSTRE  are used to verify safety properties. The abstractions specify more behaviors than
5128	5141	and computes some function of that data. Robustness and control in our networked systems is thus similar to problems found in for example internet congestion control  and coupled oscillators . Also similar is the idea of self-stabilizing protocols  wherein a network of processors executing a self-stabilizing protocol can be shown to recover from disturbances and arbitrary initial
5128	5142	data from adjacent nodes and computes some function of that data. Robustness and control in our networked systems is thus similar to problems found in for example internet congestion control  and coupled oscillators . Also similar is the idea of self-stabilizing protocols  wherein a network of processors executing a self-stabilizing protocol can be shown to recover from
5128	5144	violates the behavior of the observer (because it has a bug, for instance), a warning flag is raised and the program can be shut down. Similar, if less formal, ideas appear in operating systems  where various quantities can be measured to determine of a system is operating normally, or if it has ben “infected” or otherwise compromised. We expect such an approach to be useful in control
5128	5145	on separate processors. A voting mechanism is used to determine which of the outputs to use. The goal is that because the implementations are software engineered separately, they will not have bugs  on the same inputs – safety is thereby acheived through redundancy. We are interested in the logical extension of this idea to large networks of software processes (such as networked computers on
5128	5396	in Section 2.5. In this context, a generic control algorithm would incorporate a sensor that sub-samples state information according to some randomized algorithm. In control theory, an estimator  uses a model of the process dynamics along with the system inputs and outputs to generate an estimate of the system state in the presence of noisy and/or incomplete data. This concept can be
5128	5147	process as input for the next set of iterations. 3 Related Work The research we report on in this paper is is closely related to the theory of self-checking programs as described in, for example, . In this theory, checkers C are constructed that check the accuracy of programs P that (supposedly) compute some function f. The checker is supposed to fundamentally differ from the program (e.g.
8919267	5153	query-response and broadcast, and other iPA peers. Servers could include standard database and web servers, as well as sensor networks. Broadcast servers would ideally consist of two main channels. The first channel is a push-based channel that continuously broadcasts a data stream of XML documents. The other channel is a pull-based channel where the user or iPA can submit queries to the
8919267	5159	of these workflows is similar to the integration of different businesses in the establishment of a virtual enterprise. In our previous work, we developed a scheme to automate this process. We will show that we can extend this framework to also support emergency service integration. One key modification is the introduction of our mobile Intelligent Personal Assistants, or iPAs, to
8919267	5159	Resource Typing system. • Significance, which indicates whether the task is vital to the workflow and therefore must be executed, or whether it is non-vital, and need only be executed if feasible. Workflow Properties In addition to specifying the representation, our workflow model includes three properties of workflows which are important for scheduling and integration. These are:
8919267	5163	name, site identification, credentials, etc.. While we are not concerned with issues of trust in this paper, the profile and request could be enhanced with schemes like Trust-X and TrustBuilder. When an iPA receives the broadcast view request, it must determine if its user is capable of contributing to its execution. It must consider the specified goals of the request and its constraints
5164	5430	, I 1 , u 1 , d 1 , f 1 ), (l 2 , v 2 , I 2 , d 2 )) (R1) l 1 = l 2 ; (R2) for any x ? Var, |v 2 (x) ? (T 1 ? r 1 (x) + u 1 )| ? ?L + ?P (R3) for any ? ? Labin, I 1 (?) = I 2 (?); (R4) d 1 = d 2 ; (R5) there exists (l 3 , v 3 , I 3 , d 3 ) such that: ((l 2 , v 2 , I 2 , d 2 ), ?L ? u 1 , (l 3 , v 3 , I 3 , d 3 )) ?? 2 . We have now to prove that R is a simulation relation in the precise sense of
5164	5432	Proof. Let  Prg the relation R ? S1 × S2 that contains the pairs: such that the following conditions hold: (s 1 , s 2 ) = ((l 1 , r 1 , T 1 , I 1 , u 1 , d 1 , f 1 ), (l 2 , v 2 , I 2 , d 2 )) (R1) l 1 = l 2 ; (R2) for any x ? Var, |v 2 (x) ? (T 1 ? r 1 (x) + u 1 )| ? ?L + ?P (R3) for any ? ? Labin, I 1 (?) = I 2 (?); (R4) d 1 = d 2 ; (R5) there exists (l 3 , v 3 , I 3 , d 3 ) such that: ((l
5164	5434	in ? ?1 ? , we say that T 1 is simulable by T 2 and as receptive as T 2 , noted T 1 ? r T 2 , if there exists a relation R ? S 1 × S 2 (called a simulation relation) such that: (S1) (?1 , ?2 ) ? R (S2) for any (s1 1 , s21 ) ??? R, we have that: (S21) for any ? ? ? ? R?0 , for any s1 2 such that (s11 , ?, s12 ) ??1 , there exists s2 2 ? S2 such that (s2 1, ?, s2 2) ??2 and (s1 2, s2 2) ? R; (S22)
5164	5435	2 and as receptive as T 2 , noted T 1 ? r T 2 , if there exists a relation R ? S 1 × S 2 (called a simulation relation) such that: (S1) (?1 , ?2 ) ? R (S2) for any (s1 1 , s21 ) ? R, we have that: (S21) for any ? ? ? ? R?0 , for any s1 2 such that (s11 , ?, s12 ) ??1 , there exists s2 2 ? S2 such that (s2 1, ?, s2 2) ??2 and (s1 2, s2 2) ? R; (S22) RefT 1(s1 1 ) = RefT 2(s21 ). The notion of
5164	5447	capture the events emitted by the scanner and to give orders to the arm respecting a strict timing in order to properly remove suspicious luggages. An example of such a controller is given in Fig. 1(a). An observer modeled by the timed automaton of Fig. 1(e) ensures that whenever the event Lug is emitted then the events AtP2 and Hit are at most 2 time units apart. As the luggage has some
5164	5447	given in definition 11 is problematic for the controller part if our goal is to transfer the properties verified on the model to an implementation. Consider the controller for the arm in Fig. 1(a). Below, we illustrate the properties of the classical semantics that makes impossible to both implement the controller and ensure formally that the properties of the model are preserved: – First,
5164	5447	8. Clearly, no hardware can guarantee that the transition will always proceed when t = 8. 1 defined as usual st ? 8 1 Lug t := 0 2 Stop t ? 4 4 t ? 4 Bwd t ? 6 t := 0 t ? 8 t := 0 3 t ? 6 (a) Elastic controller for an arm. x ? 20, Lug, x := 0 1 x ? 20 (c) Timed automatongenerating Lug. Stop Fwd 1 Lug z := 0 z ? 9 AtP2 z ? 10 (b) Timed automaton modeling the moving of the luggage. 1 Fwd
5164	5447	5 on the Lego MindstormsTM platform, using a slightly modified version of the open-source operating system brickOSTM . We then ran this implementation using the Elastic controller of figure 1(a). The platform allows ?L to be as low as 6ms and offers a digital clock with ?P = 1ms which is thus ample enough. Stop (1, ?) (4, ?) d ? ? ? t ? 4 + ? t ? 4 ? ? d := 0 Lug d ? ?? t ? 4 + ?? yLug ? ?
5164	5447	? ? t ? 6 ? ? t := 0 d := 0 Bwd t ? 8 ? ? t := 0 d := 0 Fwd Lug t := 0 d := 0 (2, ?) yLug := 0 d ? ?? t ? 8 + ?? yLug ? ? Lug yLug := 0 Fig. 2. Semantic timed automaton for the Elastic of figure 1(a). 7 Related and future works Lug t ? 8 ? ? t := 0 d := 0 d ? ? ? t ? 8 + ? (2, ?) Fwd (3, ?) d ? ? ? t ? 6 + ? In this section, we compare our work with some recent related works. We also point out
5164	5458	? = ? 1 out ? ? 1 in ? ?1 ? , we say that T 1 is simulable by T 2 and as receptive as T 2 , noted T 1 ? r T 2 , if there exists a relation R ? S 1 × S 2 (called a simulation relation) such that: (S1) (?1 , ?2 ) ? R (S2) for any (s1 1 , s21 ) ? R, we have that: (S21) for any ? ? ? ? R?0 , for any s1 2 such that (s11 , ?, s12 ) ??1 , there exists s2 2 ? S2 such that (s2 1, ?, s2 2) ??2 and (s1 2,
5164	5459	in ? ?1 ? , we say that T 1 is simulable by T 2 and as receptive as T 2 , noted T 1 ? r T 2 , if there exists a relation R ? S 1 × S 2 (called a simulation relation) such that: (S1) (?1 , ?2 ) ? R (S2) for any (s1 1 , s21 ) ??? R, we have that: (S21) for any ? ? ? ? R?0 , for any s1 2 such that (s11 , ?, s12 ) ??1 , there exists s2 2 ? S2 such that (s2 1, ?, s2 2) ??2 and (s1 2, s2 2) ? R; (S22)
5164	5465	• for the continuous transitions, ((l, v), t, (l ? , v ? )) ?? iff l = l ? and for each variable x ? Var there exists a differentiable function fx :  ? (x) such that (i) fx(0) = v(x), (ii) fx(t) = v ? (x) and (iii) ?0 < t ? < t : f(t ? ? ) ? (x). In the sequel, we need to refer to the subclass of timed automata. Definition 12  A timed automaton is a
5164	5465	Bad. The following additional physical constraints are modeled by the product of automata: (i) any luggage takes between 9 and 10 time units to go from the scanner to the center of the arm, (ii) the arm takes between 1 and 2 time units to reach the center of the moving belt when it is launched forward from its idle position, (iii) it takes at most 4 times units to come back to its idle
5164	5465	options to get out of this situation: (i) we ask the designer to give up the synchrony hypothesis and ask the designer to model the platform on which the control strategy will be implemented, or (ii) we let the designer go on with the synchrony hypothesis at the modeling level but relax the ASAP semantics during the verification phase in order to formally validate the synchrony hypothesis. We
5164	5465	transitions; – the next round is started. All we require from the hardware is to respect the following two requirements: (i) the clock register of the CPU is incremented every ?P time units and (ii) the time spent in one loop is bounded by a certain fixed value ?L. We choose this semantics for its simplicity and also becausesit is obviously implementable. There are more efficient ways to
6619651	5216	biased by human-made abstraction causing excessive simplifications of modelled world. “The abstraction reduces the input data so that the program experiences the same perceptual world as humans” . As a result a robot operating in a block or “toy world” after moving it into the real one, faces discrepancies which cannot be removed afterwards by simple means and the model used usually needs
6619651	5216	, responsible for avoiding obstacles, wandering, exploring etc. Such systems or “the Creatures” are “collections of competing behaviours” and has no explicit representation even on the local level . If the system is complex enough, “out of the local chaos of their interactions there emerges, in the eye of an observer, a coherent pattern of behaviour” Brooks Figure 2.1: The traditional
6619651	5216	which roughly illustrates the new design of the dog system. 3sFigure 2.2: The subsumption architecture proposed by Brooks  decomposes a system horizontally into task achieving behaviours. claims , and furthermore the complexity of the behaviour of the Creature reflects complexity of the environment rather then the Creature itself. I do not know if the complexity of behaviour, especially in
6619651	5217	be clearly defined goal 1 , what is in contrast with behaviour-based systems where “behavioural responses are explicitly designed into the system but there are not any explicitly represented goals” . Having already a concept of a plan, one can escape most of the mentioned troubles, although, as usual, there is no rose without a thorn. 1 Unfortunately, the authors I cite in the report, use
6619651	201	it maximises the distance to the surrounding obstacles. The main weakness of the Voronoi graph method is difficulty with its calculation in high-dimensional spaces. The probabilistic roadmap (see ) technique creates nodes of the graph in the free space with the use of random generator. The method is more general then all the previous ones, it deals better with open spaces, and more
5225	5239	indexes (e.g., ), temporal storage structures (e.g., ), and temporal join (e.g., ) and coalescing algorithms . Further, it seems that there has been an implicit assumption (e.g., in ) that the performance of temporal DBMSs should be similar to that of conventional DBMSs, even when a temporal DBMS manages multiple versions of data and a conventional DBMS manages only one
5225	5239	in the query language PostQuel. The TempIS Temporal DBMS supports both valid and transaction time  and extends academic Ingres . This system implements the TQuel temporal query language . (The implementation of the TempIS Temporal DBMS is discontinued.) The TimeMultiCal is another temporally enhanced DBMS built from scratch . It supports multiple calendars, but neither valid
5225	5242	a temporal DBMS prototype, called TimeDB, which supports both valid time and transaction time . It is built on top of the Oracle DBMS and supports the ATSQL2 temporal query language , a descendent of the TSQL2  temporal query language. The Tiger prototype  is a close relative of TimeDB. It implements ATSQL  and can be tested online. A mixture of an integrated and a
5225	5243	It supports transaction time and so-called time travel in the query language PostQuel. The TempIS Temporal DBMS supports both valid and transaction time  and extends academic Ingres . This system implements the TQuel temporal query language . (The implementation of the TempIS Temporal DBMS is discontinued.) The TimeMultiCal is another temporally enhanced DBMS built from
5225	5244	to a stratum approach to building a temporal DBMS is the integrated architecture where a DBMS is built from scratch and the implementation incorporates temporal support. The Postgres DBMS  is the most wellknown example of such an architecture. It supports transaction time and so-called time travel in the query language PostQuel. The TempIS Temporal DBMS supports both valid and
5225	5245	to a stratum approach to building a temporal DBMS is the integrated architecture where a DBMS is built from scratch and the implementation incorporates temporal support. The Postgres DBMS  is the most wellknown example of such an architecture. It supports transaction time and so-called time travel in the query language PostQuel. The TempIS Temporal DBMS supports both valid and
5249	5250	communities. The work reported in this paper is a continuation of that on the use of knowledge acquisition and representation tools to model the knowledge structures of scholarly communities . These studies involved the use of the visual language  that allowed knowledge structures to be expressed as semantic networks with well-defined semantics that were automatically translated into
5249	5251	of that on the use of knowledge acquisition and representation tools to model the knowledge structures of scholarly communities . These studies involved the use of the visual language  that allowed knowledge structures to be expressed as semantic networks with well-defined semantics that were automatically translated into data structures in KRS , an implementation of a
5249	5251	RACER . It is simple to change the language to conform to existing practices, user preferences, or changing notions of what is required. The examples given follow the conventions described in  and are similar to those of other graphical interfaces for description logics such as RICE . Concepts are represented by the concept name in an oval. Concepts are defined through the property
5249	5253	allowed knowledge structures to be expressed as semantic networks with well-defined semantics that were automatically translated into data structures in KRS , an implementation of a CLASSIC-like  description logic. Inferences in KRS were graphed automatically as additions to the semantic network so that users could visualize both the inputs and outputs without translation into logical
5249	5254	logic. Inferences in KRS were graphed automatically as additions to the semantic network so that users could visualize both the inputs and outputs without translation into logical formulae . In recent years there have been major advances in description logic research that make it realistic to use richer representations incorporating negation, disjunction and some aspects of recursion
5249	5258	language should correspond to the natural graph-theoretic representation of description logics that is commonly used in describing operations on them and in implementing computational inference . 4 As many as possible of the syntactic and inferential transformations of expressions in the visual language should be formally specifiable as graph-theoretic operations. 5 The visual language
5249	5259	output of logical inferences. 6 The visual language should support modularity in the specification of ontologies such that definitions/assertions may be specified in one document and used in others . 7 Subject to these requirements, the visual language should be similar to existing languages for semantic networks. 2.2 Overview of the KNet visual language KNet, the visual language used in this
5249	5260	for node types and connecting lines and a user scriptable interface for translation to and from semantic networks in the visual language enabling integration with web services such as KRS and RACER . It is simple to change the language to conform to existing practices, user preferences, or changing notions of what is required. The examples given follow the conventions described in  and are
5249	5261	or changing notions of what is required. The examples given follow the conventions described in  and are similar to those of other graphical interfaces for description logics such as RICE . Concepts are represented by the concept name in an oval. Concepts are defined through the property they encode  in the graph derived by tracing outgoing arrows from the concept, terminating at
5249	5265	standard semantics for negation, including De Morgan’s laws linking conjunction, disjunction and negation. A negation arrow from a concept to a set may be used to represent a rule with exceptions . An existential constraint is specified through a set with an arrow to a concept applying to the individuals included in it. If a graph contains a conjunction/disjunction of two identical graphs
5249	5269	whereby users can adjust what parts of a graph are shown by moving a slider to change a threshold. It would be interesting to take output from an inference engine in a proof markup language  and have a slider that moved through a linear representation of the proof steps while showing the resulting inferences being graphed in the semantic network. Figure 6 provides a simple example of
5273	5274	a small number of samples. Typically, these representations are achieved via invertible and nonredundant transforms. Currently, the most popular choices for this purpose are the wavelet transform ??? and the discrete cosine transform . The success of wavelets is mainly due to the good performance for piecewise smooth functions in one dimension. Unfortunately, such is not the case in two
5273	5274	treestructured filter bank with levels, where and are low and high pass synthesis filters, respectively. Then the family of functions is the orthogonal basis of the discrete-time wavelet series . Here, denotes the equivalent synthesis filters at level ,or, more specifically The basis functions from are called the scaling functions, while all the others functions in the wavelet basis are
5273	5279	ON IMAGE PROCESSING, VOL. 12, NO. 1, JANUARY 2003 The Finite Ridgelet Transform for Image Representation Minh N. Do, Member, IEEE, and Martin Vetterli, Fellow, IEEE Abstract—The ridgelet transform  was introduced as a sparse expansion for functions on continuous spaces that are smooth away from discontinuities along lines. In this paper, we propose an orthonormal version of the ridgelet
5273	5279	1057-7149/03$17.00 © 2003 IEEE the discontinuity across an edge, but will not see the smoothness along the edge. To overcome the weakness of wavelets in higher dimensions, Candès and Donoho ,  recently pioneered a new system of representations named ridgelets which deal effectively with line singularities in 2-D. The idea is to map a line singularity into a point singularity using the
5273	5279	ridgelet transform and showing its connections with other transforms in the continuous domain. Given an integrable bivariate function , its continuous ridgelet transform (CRT) in is defined by ,  where the ridgelets in 2-D are defined from a wavelet-type function in 1-D as Fig. 1 shows an example ridgelet function, which is oriented at an angle and is constant along the lines . For
5273	5279	analysis. VI. NUMERICAL EXPERIMENTS A. Nonlinear Approximation Following the study of the efficiency of the ridgelet transform in the continuous domain using the truncated Gaussian functions , we first perform numerical comparison on a 256 256 image of the function: , using four 2-D transforms: DCT, DWT, FRAT, and FRIT. The comparison is evaluated in terms of the
5273	5281	on discretization of continuous formulae would require interpolation in polar coordinates, and thus result in transforms that would be either redundant or cannot be perfectly reconstructed. In ???, the authors take the redundant approach in defining discrete Radon transforms that can lead to invertible discrete ridgelet transforms with some appealing properties. For example, a recent
5273	5283	on discretization of continuous formulae would require interpolation in polar coordinates, and thus result in transforms that would be either redundant or cannot be perfectly reconstructed. In ???, the authors take the redundant approach in defining discrete Radon transforms that can lead to invertible discrete ridgelet transforms with some appealing properties. For example, a recent
5273	5285	problem where there exist other approaches that explore the geometrical regularity of edges, for example by chaining adjacent wavelet coefficients and then thresholding them over those contours . However, the discrete ridgelet transform approach, with its “built-in” linear geometrical structure, provide a more direct way—by simply thresholding significant ridgelet coefficients— in
5273	5296	1 and obtain a tight frame. Consequently, this makes the reconstruction more robust against noise on the FRAT coefficients due to thresholding and/or quantization. This follows from the result in  that with the additive white noise model for the coefficients, the tight frame is optimal among normalized frames in minimizing mean-squared error.s22 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.
5300	5303	and fixed-width format. Previously proposed user-level temporal relations may be mapped to this format , and the user-level data model and query language may be point-based or intervalbased . More generally, the algebra is independent of the specific user-level temporal relational query language and data model employed, and it provides support for the two main classes of temporal
5300	1755	is denoted by , due to the requirement of minimality (see Section 2.2) and our relations being list based. The coalescing of Böhlen et al. merges value-equivalent tuples with adjacent or overlapping time periods;
5300	1755	of operations, coalescing their arguments and results is equivalent to coalescing their results only (rules C5–C7). Our list of coalescing transformations extends the list provided by Böhlen et al. . Due to the differences in coalescing definitions (see Section 2.4) and because  allows duplicates in snapshots of temporal relations, but not regular duplicates, the following three
5300	5309	(2) statements that explicitly manipulate values of (new) temporal abstract data types with convenient operations and predicates defined on them. The temporal aspect considered here is valid time , which captures when data was, is, or will be true in the modeled reality; the approach can be extended to also handle transaction time, either alone or in combination with valid time. In the
5300	5309	of these and other list operations in the algebra may be explored. In addition, the algebra may be extended to support modifications, NOW-relative values , and both valid and transaction time . Acknowledgments This research was supported in part by the Danish Technical Research Council through grant 9700780, by the U.S. National Science Foundation through grant IIS-9817798, by the
5300	5310	time periods, which are useful for implementation because of their granularity independence and fixed-width format. Previously proposed user-level temporal relations may be mapped to this format , and the user-level data model and query language may be point-based or intervalbased . More generally, the algebra is independent of the specific user-level temporal relational query language
5300	5300	2.4. Fundamental algebra operations We describe briefly all the fundamental algebra operations. We then consider temporal duplicate elimination in detail. Other operations are defined elsewhere . Table 1 lists all operations. Selection ( ), projection ( ), union ALL (t), Cartesian product ( ), difference (n), aggregation ( ), and duplicate elimination (rdup) derive from the conventional
5300	5300	be equivalent as multisets) and snapshot set equivalence (at each point in time, the relations should be equivalent as sets). Formal definitions may be found in the associated technical report . We can exemplify the different types of equivalences using the relations in Figure 3. Relations R1 and R2 are not equivalent as lists or as multisets because the tuple for Anna with times 2 and 6
5300	5300	r2 be relations. Then the following implications hold. (Implications pointing downward apply only to temporal relations.) r1 L r2 ) r1 M r2 ) r1 S r2 r1 + + + S L r2 ) r1 S M r2 ) r1 S S r2 Proof:  2 The different types of equivalences can be exploited in query optimization. Transformation rules (to be discussed in Section 4) can be divided into six categories, one for each type of
5300	5300	duplicate elimination, coalescing, sorting, and transfer operations in turn, listing central rules. The full rule set, which extends all existing rule sets known to the authors, can be found in . The transformation rules are given as equivalences that express that two algebraic expressions are equivalent according to one of the six equivalence types from Section 3; we always give the
5300	5300	Cartesian product, and temporal difference destroy coalescing. The projection in the second rule is necessary because the temporal Cartesian product retains the timestamps of its argument relations . The first two transformations can be modified to have type L if we require that the arguments do not have duplicates in snapshots (rules C8–C9). Adding the same requirement, the third rule can be
5300	5225	available. This has led to the consideration of a layered, or stratum, approach where a layer, implementing temporal support, is interposed between the applications and a conventional DBMS . The layer maps temporal SQL statements to regular SQL statements and passes them to the DBMS, which is not altered. With this approach, it is feasible to support a temporal SQL that strictly
5317	5319	focuses on HTML table analysis (for offers of products, namely, cars),  deals with free text 5 (obituaries); the nature of ‘extraction ontology’ however remains the same. In the Armadillo  project, an (inductively learnt) presentation ontology allows to reuse a surface-logical structure from one resource to another, e.g. accross multiple bibliography resources from the same domain,
5317	5320	of conceptual abstraction upto an arbitrary level of taxonomy. 4 http://www.unspsc.orgsFig. 4. Fragment of empirical taxonomy for bicycle ‘categories’ 3 Ontologies in Other WIE Projects Embley  uses the notion of ‘extraction ontology’ for conceptual schema with data frames hand-crafted by domain expert (i.e. presentation ontology). While  focuses on HTML table analysis (for offers of
5317	5321	of conceptual abstraction upto an arbitrary level of taxonomy. 4 http://www.unspsc.orgsFig. 4. Fragment of empirical taxonomy for bicycle ‘categories’ 3 Ontologies in Other WIE Projects Embley  uses the notion of ‘extraction ontology’ for conceptual schema with data frames hand-crafted by domain expert (i.e. presentation ontology). While  focuses on HTML table analysis (for offers of
5317	5322	e.g. accross multiple bibliography resources from the same domain, containing data about overlapping sets of publications. A sort of presentation ontology is also used in the OntoBuilder project  aiming at ‘deep web’ information extraction. It defines layout rules for HTML forms used as input to online databases. On the other hand, the Crossmarc project  is limited to terminological
5317	5323	be designed e.g. for company profile (pages) or contact info (pages); the former would presumably be more linguistic-oriented as the profile information is typically expressed by free text, cf. .sFig. 3. Bicycle offer presentation ontology to its properties. Note (in contrast to the domain ontology above) the direct link between product offer information and information about bike parts.
5317	5324	to online databases. On the other hand, the Crossmarc project  is limited to terminological level (term sets mapped on semantic classes) in its usage of ontologies 6 .IntheAeroDAML approach , a terminological ontology (WordNet) is used for annotation and a knowledge ontology (expressed in DAML) is populated by extraction results. Since the extraction method is named-entity recognition
5317	5325	rather than structural IE, consistency-constraints are only applied at the level of target domain ontology rather than within a dedicated presentation ontology. Similarly, Maedche et al.  used an ontology engine (OntoBroker) to verify ‘conceptual bridges’ between terms extracted via shallow syntactic analysis. 4 Conclusions and Future Work We discussed the types and roles of
5317	5326	used in the OntoBuilder project  aiming at ‘deep web’ information extraction. It defines layout rules for HTML forms used as input to online databases. On the other hand, the Crossmarc project  is limited to terminological level (term sets mapped on semantic classes) in its usage of ontologies 6 .IntheAeroDAML approach , a terminological ontology (WordNet) is used for annotation and a
5317	5328	contains one ‘true class’, that of Bike Offer; the remaining concepts are shrunk 2 It is available at http://rainbow.vse.cz:8000/sesame. Details on the RDF query technology used can be found in  3 Similar presentation ontologies could be designed e.g. for company profile (pages) or contact info (pages); the former would presumably be more linguistic-oriented as the profile information is
5317	5328	we have not used a lexical taxonomy in the primary annotation of bike names, prices, component names and the like. The annotation was carried out by means of statistical (Hidden Markov) models; see  for details. However, a collection of more than 60 bicycle categories (in various sense) arose as side product of annotation, and was later arranged into a hierarchy (see part of it at Fig. 4). We
8919285	5342	protein complexes, side chains of Arg, Lys, Asp, Glu and Met present the highest frequency and amplitude of movements between the structures of free and co-crystallized proteins. Norel et al.  approach the unbound protein docking problem in a similar way, the conclusion withdrawn in all these studies being that the geometrical shape criteria captures the most important features of
8919285	8919290	section of conformational space. In recent years our group has been involved in the development of an overall system for the assessment of protein-protein interaction that we have called MIAX , the main features of the system being the search for binding sites on the associating proteins, the potential function to score candidate complex conformations, and the module to refine them by
5351	5352	delivery model in the Internet—each packet was efficiently multicast to all receivers simultaneously using a multicast-capable portion of the Internet known as the Multicast Backbone or MBone . Although bandwidth efficient, this style of multipoint transmission—where a packet stream is transmitted to all receivers at a uniform rate—is undesirable because receivers are usually connected
5351	5359	an improvement in reconstruction quality. By combining this approach of layered source coding with a layered transmission system, we can solve the multicast heterogeneity problem ,  , , . In this architecture, the multicast source produces a layered stream where each layer is transmitted on a different network channel, as illustrated in Fig. 3 for the case of the UCB seminar. In
5351	5359	of the previous work leaves this problem as an implementation detail, a novel and practical scheme was proposed by Deering  and was further described and/or independently cited in , –, , and . In this approach, the layers that comprise the hierarchical signal are striped across distinct IP MulticastsMCCANNE et al.: LOW-COMPLEXITY VIDEO CODING FOR RLM 985 groups, thereby
5351	5359	multicast or RLM . A number of research activities have laid the groundwork both for layered video compression , ,  and for layered transmission systems , , , , , . However, these research efforts are each polarized: they either solve the networking half of the problem (i.e., the transmission system) or they solve the compression half of the problem.
5351	5362	work leaves this problem as an implementation detail, a novel and practical scheme was proposed by Deering  and was further described and/or independently cited in , –, , and . In this approach, the layers that comprise the hierarchical signal are striped across distinct IP MulticastsMCCANNE et al.: LOW-COMPLEXITY VIDEO CODING FOR RLM 985 groups, thereby allowing
5351	5363	and specific adaptation protocols that employ the architecture had not been developed. In recent work, we filled this void with a specific protocol we call receiver-driven layered multicast or RLM . A number of research activities have laid the groundwork both for layered video compression , ,  and for layered transmission systems , , , , , . However, these
5351	5363	this section, we give a high-level sketch of our receiverdriven layered multicast scheme to establish design constraints on and motivation for a new layered codec. Details of RLM are presented in  and . RLM operates within the traditional Internet Protocol architecture, and relies upon the delivery efficiency of IP Multicast . It does not require real-time traffic guarantees, and
5351	5363	Instead, RLM augments its adaptation scheme with “shared learning,” where receivers learn from other receivers’ failed join experiments. Details of the shared learning algorithm are described in . Although RLM receivers adapt locally to network capacity, the target operating point is not globally optimized. If multiple, simultaneous transmissions are sharing a single network, RLM apportions
5351	5363	across distinct RTP sessions. An effort is currently underway—based in part on the work presented in this paper—to modify RTP to allow a single session to span multiple underlying network channels , . Our proposed change is an extension to RTP that allows a participant to use one Source ID consistently across the logically distinct RTP sessions comprising the hierarchy. Accordingly, we
5351	5363	C++ object in the Tcl/Tk-based  multimedia toolkit used to build vic. We implemented the RLM protocol in our network simulation testbed , and carried out a simulation study reported in , . Even with RLM fully integrated into vic, the current framework is still experimental. We are just beginning to understand the interaction between RLM and other adaptive congestion control
5351	5365	receiver-driven layered multicast or RLM . A number of research activities have laid the groundwork both for layered video compression , ,  and for layered transmission systems , , , , , . However, these research efforts are each polarized: they either solve the networking half of the problem (i.e., the transmission system) or they solve the compression half of
5351	5368	we give a high-level sketch of our receiverdriven layered multicast scheme to establish design constraints on and motivation for a new layered codec. Details of RLM are presented in  and . RLM operates within the traditional Internet Protocol architecture, and relies upon the delivery efficiency of IP Multicast . It does not require real-time traffic guarantees, and assumes only
5351	5368	for the simplicity and practical advantages of conditional replenishment. In short, our compression algorithm exploits temporal redundancy only through conditional replenishment. Reference  presents evidence that for certain signals and packet loss rates, conditional replenishment outperforms traditional codecs based on temporal prediction. We now describe the major components of our
5351	5368	is less than that in higher rate layers. For example, layer 1 alone never has a redundant block update, while the full hierarchy contains the maximum number of redundant updates. Reference  contains a detailed analysis of this overhead. B. Spatial Compression After the conditional replenishment stage selects blocks for transmission, they are compressed spatially. In this section, we
5351	5368	object in the Tcl/Tk-based  multimedia toolkit used to build vic. We implemented the RLM protocol in our network simulation testbed , and carried out a simulation study reported in , . Even with RLM fully integrated into vic, the current framework is still experimental. We are just beginning to understand the interaction between RLM and other adaptive congestion control schemes,
5351	4076	for a new layered codec. Details of RLM are presented in  and . RLM operates within the traditional Internet Protocol architecture, and relies upon the delivery efficiency of IP Multicast . It does not require real-time traffic guarantees, and assumes only best effort, multipoint packet delivery. A key feature of IP Multicast is the level of indirection provided by its host group
5351	2243	Under this scheme, a receiver searches for the optimal level of subscription much as a TCP source searches for the bottleneck transmission rate with the slow-start congestion avoidance algorithm . The receiver adds layers until congestion occurs, and backs off to an operating point below this bottleneck. Although a receiver can easily detect that the network is congested by noting gaps in
5351	3259	in an ad hoc fashion. In general, it is not possible to achieve a “fair” allocation of bandwidth without some additional machinery in the network, even if all of the end nodes cooperate . Even if the bandwidth allocation were fair, the aggregate system performance, as measured by the sum of distortions at each receiver, would not be optimal. As shown in , minimization of the
5351	5372	the future. Given that no current algorithm satisfied all of our design constraints, we designed a new layered compression scheme based on our experiences adapting H.261 for Internet transmission . To meet our goal of low complexity, the algorithm is relatively simple and admits an efficient software implementation. Moreover, the software-based approach provides an easy route for
5351	5372	eliminate redundancies by exploiting statistical correlations within a given frame. Our algorithm employs a very simple model for temporal compression known as block-based conditional replenishment , , and uses a hybrid DCT/subband transform coding scheme for spatial compression. In the next section, we describe the conditional replenishment algorithm, and in the subsequent section, we
5351	5372	small blocks (e.g., 8 8or16 16 pixels), and only the blocks that change in each new frame are encoded and transmitted. Several existing Internet video tools use this approach (e.g., our tool vic , the Xerox PARC Network Video nv , and Cornell’s CU-SeeMe ), and some commercial H.261 codecs send “block skip codes” for static blocks. Fig. 6. Temporal compression models. A conditional
5351	5372	coefficients to encode. Although this codec outperforms several existing Internet video coding schemes, its compression performance is somewhat inferior to the commonly used Intra-H.261 format . To carry out ongoing, large-scale experiments within the MBone user community, we rely on active use of the applications, protocols, and compression formats. Our experience is that a few isolated
5351	5372	interval) all into a single pass. We implemented PVH and these optimizations in our videoconferencing application vic, and compared its performance with the widely used Intra-H.261 codec . As a simple quantitative assessment, we measured the run-time performance of both codecs within vic on an SGI Indy (200 MHz MIPS R4400) using the built-in VINO video device. To measure the maximum
5351	5374	and only the blocks that change in each new frame are encoded and transmitted. Several existing Internet video tools use this approach (e.g., our tool vic , the Xerox PARC Network Video nv , and Cornell’s CU-SeeMe ), and some commercial H.261 codecs send “block skip codes” for static blocks. Fig. 6. Temporal compression models. A conditional replenishment system encodes and
5351	5374	replenishment algorithm: block selection, block aging, and temporal layering. Our scheme is derived in part from the conditional replenishment algorithm used by the Xerox PARC Network Video tool nv . 1) Block Selection: To decide whether or not to encode and transmit a block, the conditional replenishment algorithm computes a distance between the reference block and the current block. As is
5351	5377	be lost in the end system when the decoder cannot keep up with a high-rate incoming bit stream. In this case, the decoder should gracefully adapt by trading off reconstruction quality to shed work , . However, such adaptation is difficult under the temporal prediction model because the decoder must fully decode all differential updates to maintain a consistent prediction state. In
5351	5378	in the end system when the decoder cannot keep up with a high-rate incoming bit stream. In this case, the decoder should gracefully adapt by trading off reconstruction quality to shed work , . However, such adaptation is difficult under the temporal prediction model because the decoder must fully decode all differential updates to maintain a consistent prediction state. In contrast,
5351	5274	described above was to use short analysis filters to increase the coherence between the subband and pixel representations. We used the following biorthogonal filters for the first-stage analysis : with the following synthesis: 2 and Haar filters for the remaining three stages. Because a fourtap filter induces only one pixel of overlap, and because the Haar basis vectors induce no additional
5351	1629	and framing. About the same time that ALF emerged, we and others developed a number of tools to explore the problem of interactive audio and video transport across packet-switched networks ???. After several iterations of protocols and experimentation with audio and several different video compression formats, it became clear that a “one size fits all” protocol was inadequate , .
5351	5394	–. After several iterations of protocols and experimentation with audio and several different video compression formats, it became clear that a “one size fits all” protocol was inadequate , . Instead, a framework based on ALF emerged where a “thin” base protocol defines the core mechanisms and profile extensions define applicationspecific semantics. The Audio/Video Transport
5351	5395	applicationspecific semantics. The Audio/Video Transport Working Group of the Internet Engineering Task Force (IETF) standardized this base protocol in the “real-time transport protocol” or RTP , and developed a profile for audio- and videoconferences with minimal control , along with a number of payload format standards for specific applications like H.261, JPEG, MPEG, etc. A. The
5351	5396	Engineering Task Force (IETF) standardized this base protocol in the “real-time transport protocol” or RTP , and developed a profile for audio- and videoconferences with minimal control , along with a number of payload format standards for specific applications like H.261, JPEG, MPEG, etc. A. The Real-Time Transport Protocol RTP defines much of the protocol architecture necessary
5351	5400	scheme are all implemented in an experimental version of our videoconferencing application vic. The PVH codec and framing protocol are implemented as a modular C++ object in the Tcl/Tk-based  multimedia toolkit used to build vic. We implemented the RLM protocol in our network simulation testbed , and carried out a simulation study reported in , . Even with RLM fully
5351	4115	to the well-connected MBone, 256 kbit/s across our campus network, and 1 Mbit/s throughout the department network. PVH can also be used in tandem with the resource reservation protocol (RSVP) , , which supports the notion of layered reservations. In this approach, receivers negotiate explicitly with the network for bandwidth by adjusting their reservation to the maximum number of
5351	3029	to the well-connected MBone, 256 kbit/s across our campus network, and 1 Mbit/s throughout the department network. PVH can also be used in tandem with the resource reservation protocol (RSVP) , , which supports the notion of layered reservations. In this approach, receivers negotiate explicitly with the network for bandwidth by adjusting their reservation to the maximum number of layers
184154	5403	as a Markovmodulated fluid (MMF). MMF models are commonly used to model high-priority QoS-sensitive (quality of service sensitive)sINFOCOM 2001 2 traffic, mostly comprising voice and video , , , . In our setting, the service rate is the total link bandwidth reduced by the portion consumed by the high-priority traffic. Hence, the service rate can also be characterized using
184154	5403	performance improvements over the PD controller when small time-scale bandwidth variations are present. Although MMF models have been extensively employed in network performance analysis (e.g., , ), our work is the first to exploit such models for rate-based congestion control. In , , the authors model cross traffic by an auto-regressive moving-average process corrupted by a
184154	5403	companies have begun to carry voice connections, which require real-time guaranteed service, over packet-switched networks. The dynamics of voice connections are well captured by MMF models , . We model a single voice connection by a two-state ON-OFF MMF model, with the expected ON and OFF periods being 400 and 600 ms, respectively. Since a standard voice connection consumes 64
184154	5406	maximize the average net reward over a finite horizon by choosing proper transmission rates for the controlled sources. We then extend our previously proposed Hindsight Optimization (HO) technique  to provide a heuristic solution to the MDP problem. The HO technique has never previously been used to address a problem with an infinite control action space. The main contribution of this work is
184154	5406	HINDSIGHT OPTIMIZATION A. The Hindsight Optimization Technique In this subsection, we outline our solution approach, which extends a technique called hindsight optimization, first described in . The overall control architecture is illustrated insINFOCOM 2001 5 ? ¡ ? ¥ ??s? ? ?s? ??sState Observer ??? ¡ ? ¥s? ? ? ??sNetwork ? £ £ ? ? £ ??? ??? ¥ ??????? ? ? ? ??? ? ? ? ??????? ? ? ??? ¥ ¥
184154	5406	to different stochastic futures—in contrast the “max” in ? ¥ ??s?s? ? ¡ ¥ ??s? ? ? ? occurs outside the expectation, requiring a single policy to be selected for all futures. As discussed in , the upper bound on? ¥ ??s?s? ? can be arbitrarily loose. However, these estimates are only used to rank competing candidate actions, and thus it only matters whether or not these estimates
184154	5408	Edwin K. P. Chongs, and Robert Givan School of Electrical and Computer Engineering Purdue University West Lafayette, IN 47907, U.S.A. ¡ wu15, echong, givan¢ @ecn.purdue.edu volves binary feedback  and proportional controllers  for ATM (Asynchronous Transfer Mode) networks and linearincrease/exponential-decrease controllers for TCP/IP (Transmission Control Protocol/Internet Protocol)
184154	5410	(see  for a recent version). Recent rate-based approaches attempt to achieve better performance by incorporating control-theoretic techniques, including proportional-derivative (PD) controllers , , , , and those using optimal control and dynamic game techniques such as linear quadratic (LQ) team, , and noncooperative game controllers , , . £¥¤ We motivate our work
184154	5410	by the product of the number of source pairs (3 in our case) and the ? ¤ sum of , pairwise arrival difference per unit arrival. In most previous papers on rate-based congestion control, e.g., , , the test metric is the controller's ability to maintain a target queue size. However, by design the HO controller ¨ ? £?¥ ¡ ¥ ? . In order words, our fairness metric is the the average does
184154	5410	but still keep the same mean rate. We investigate how the variance impacts the performance of the HO and the PD controllers. PD-type congestion controllers have been shown to be generally effective , , , . PD controllers adjust the transmission rate based on the deviation of the queue length from a target value. We tested the PD controller with different target queue sizes. The
184154	5410	same rate command to every controlled source at every decision-making epoch. We chose the values of the parameters (gains) of the PD controller by first following the design procedure provided in  and then by fine tuning manually to obtain the best response possible for a constant cross traffic with rate of 22 Mbps, which is the mean rate of the cross traffic with which we will be carrying
184154	5412	(PD) controllers , , , , and those using optimal control and dynamic game techniques such as linear quadratic (LQ) team, , and noncooperative game controllers , , . £¥¤ We motivate our work by noticing that most of the above control schemes are designed for constant or slowly-varying service rates (with the exception of the LQ team and the £¦¤ controllers,
184154	5414	(PD) controllers , , , , and those using optimal control and dynamic game techniques such as linear quadratic (LQ) team, , and noncooperative game controllers , , . £¥¤ We motivate our work by noticing that most of the above control schemes are designed for constant or slowly-varying service rates (with the exception of the LQ team and the £¦¤
184154	5414	Although MMF models have been extensively employed in network performance analysis (e.g., , ), our work is the first to exploit such models for rate-based congestion control. In , , the authors model cross traffic by an auto-regressive moving-average process corrupted by a sequence of independent and identically distributed random numbers with zero mean and finite variance.
184154	5415	Traffic. High-priority cross traffic represents, for example, CBR/VBR traffic in ATM networks, or traffic in IP networks receiving high-priority service via the CBQ (class-based queuing) scheme . This cross traffic determines the service rate that controlled traffic experiences at the bottleneck node. We assume that the cross-traffic process can change at any (discrete) time. For
184154	5416	controllers  for ATM (Asynchronous Transfer Mode) networks and linearincrease/exponential-decrease controllers for TCP/IP (Transmission Control Protocol/Internet Protocol) networks (see  for a recent version). Recent rate-based approaches attempt to achieve better performance by incorporating control-theoretic techniques, including proportional-derivative (PD) controllers ,
184154	5416	random numbers with zero mean and finite variance. Compared with , , MMF models have more structure, and better performance is therefore expected when such models are available. In , the authors incorporate a long-range dependent model into the design of a linear-increase/exponential-decrease controller. Our MMF model yields to a decision-theoretic analysis, as mentioned
184154	5418	a recent version). Recent rate-based approaches attempt to achieve better performance by incorporating control-theoretic techniques, including proportional-derivative (PD) controllers , , , , and those using optimal control and dynamic game techniques such as linear quadratic (LQ) team, , and noncooperative game controllers , , . £¥¤ We motivate our work by noticing
184154	5418	the same mean rate. We investigate how the variance impacts the performance of the HO and the PD controllers. PD-type congestion controllers have been shown to be generally effective , , , . PD controllers adjust the transmission rate based on the deviation of the queue length from a target value. We tested the PD controller with different target queue sizes. The target queue
184154	5430	where the trace-relative hindsight-optimal value is not differentiable. The use of gradient ascent methods with functions that are not everywhere differentiable has been studied before (e.g., ). In practice, we have found that the non-differentiable points in our objective function do not impact the efficacy of the gradient ascent algorithm. In fact, in our empirical study the gradient
184154	5432	analysis, as mentioned above, resulting in a controller that is not constrained to be linear-increase/exponential-decrease. Previous work on congestion control using MDP formulations include , , . Our work differs from , ,  in several ways: 1) Our action space is continuous; 2) our reward structure is more general; 3) we develop an online sampling-based approach to cope
184154	5434	is continuous; 2) our reward structure is more general; 3) we develop an online sampling-based approach to cope with the continuous action and state spaces. Recent work on rollout algorithms (e.g., ) provide a means of using simulation to select “good” control actions heuristically, but requires starting with a good heuristic policy. The remainder of the paper is organized as follows. In
5435	5443	building of the object representation. There exist approaches to map building that apply more sophisticated geometric method to scan data, e.g., Forsberg et al.  and Jensfelt and Christensen . However, they focus on extraction of linear structures only, why we not only consider extraction of polygonal structures but also on similarity of polygonal structures. The similarity of polygonal
5435	5445	(and noise) retrieved by the sensor. To make the representation more compact and to cancel out noise, we employ a technique called Discrete Curve Evolution (DCE) introduced by Latecki & Lakämper  which achieves these goals without losing valuable shape information. DCE is a context-sensitive process that proceeds iteratively. Though the process is context-sensitive, it is based on a local
5435	5446	(and noise) retrieved by the sensor. To make the representation more compact and to cancel out noise, we employ a technique called Discrete Curve Evolution (DCE) introduced by Latecki & Lakämper  which achieves these goals without losing valuable shape information. DCE is a context-sensitive process that proceeds iteratively. Though the process is context-sensitive, it is based on a local
5435	5446	that so far has not received much attention. In the presented approach we adopt a shape matching originated from computer vision that has proven successful in the context of shape retrieval . It may easily be adapted to our needs. The property of invariance to change of scale as often desired in computer vision approaches is not adequate in our domain and must be excluded. To compute
5435	5446	of the corresponding visual parts is as defined below. Using dynamic programming, the similarity between corresponding parts is computed and aggregated. The computation is described extensively in . The similarity induced from the optimal correspondence of polylines ? and? will be denoted ¡ §??¢¥??? . Basic similarity of arcs is defined in tangent space, a multi-valued step function
5435	5446	¥ ? ? ¤ ? §??? §?? ? ©???? §?? ???????????? ¥ ? ? ¡?? denotes the arc length of? , and the whole integral is over arc length. The constant? ???? is chosen to minimize the integral (cp. where??§??? ). Obviously, the similarity measure is a rather a dissimilarity measure as the identical yield? curves , the lowest possible measure. This measure differs from the original work in that it is
5435	5447	Matching scans against the global map within the context of a shape based representation is naturally based on shape matching. This somehow revives a notion in Lu and Milos’ fundamental work  ”scan matching is similar to model-based shape matching” that so far has not received much attention. In the presented approach we adopt a shape matching originated from computer vision that has
5435	5448	of Bremen, Bremen, Germany email: dwolter@informatik.unibremen.de and Xinyu Sun ¢ and Diedrich Wolter £ or corner points are extracted (Cox ; Gutmann and Schlegel ; Gutmann ; Röfer ). Although robot mapping and localization techniques are very sophisticated they do not yield the desired performance. We observe that these systems use only a very primitive geometric
5435	5449	out simultaneously, this technique is called SLAM (Simultaneous Localization and Mapping). To attack the problem of mapping and/or localization, mainly statistical techniques are used (Thrun , Dissanayake et al. ), e.g., the extended Kalman filter, a linear recursive estimator for systems described by non-linear process models and/or observation models, are the basis for most current
5435	5449	of the local environment. The robot’s internal spatial representation is referred to as a map, in the case of a feature based spatial representation it is commonly referred to as a feature map . A key challenge in map building is to match a local sensor reading against the global map. Multiple problems occur, e.g., the noise of the data perceived must be filtered in a way to obtain the
5457	5458	practical detection/estimation algorithms as message passing in the graph. In this paper, we outline three examples of ongoing work of this type. For an introduction to factor graphs, we refer to  and . We will use the notation of . 2. EMG SIGNAL DECOMPOSITION All muscular activity in human bodies is accompanied by electrical signals inside the muscle fibers. Such signals can be
5457	5458	filters) are deterministic functions of Si,j,k, which is represented by the boxes labeled 2 ? . The nodes labeled ? 3 represent Gaussian distributions. By iterative sum-product message passing (cf. , ), we obtain a practical algorithm to estimate Xi,k (simultaneously for all i and k) with a computational complexity that is roughly linear in the number of sources Nsrc, in the number of
5457	5459	detection/estimation algorithms as message passing in the graph. In this paper, we outline three examples of ongoing work of this type. For an introduction to factor graphs, we refer to  and . We will use the notation of . 2. EMG SIGNAL DECOMPOSITION All muscular activity in human bodies is accompanied by electrical signals inside the muscle fibers. Such signals can be measured by
5457	5459	? N ? = ? S2,k ? = ? 1? ? = ? 4 ? 2 ? + ? W1,k ? + 3 ? ?Y1,k ? Z1,2,k ? 2 ? N ? ? 4 ? + ? W2,k ? + 3 ? S2,k+1 ? 2 ?Y2,k ? . . . Z2,2,k Fig. 2. The time-k section of a factor graph (in the style of ) corresponding to Fig. 1. The maximum-likelihood (or MAP) estimate of Xi,k appears to be computationally untractable. Linear estimators will not work well: the individual FIR filters are highly
5457	5459	are deterministic functions of Si,j,k, which is represented by the boxes labeled 2 ? . The nodes labeled ? 3 represent Gaussian distributions. By iterative sum-product message passing (cf. , ), we obtain a practical algorithm to estimate Xi,k (simultaneously for all i and k) with a computational complexity that is roughly linear in the number of sources Nsrc, in the number of electrodes
5457	5459	T Sk + Wk = (Xk, . . . , Xk?M+1) T A ? ? ? T a = I 0 (9) (10) (11) (12) b ? = c ? = (1, 0, . . . , 0) T . (13) The factor graph corresponding to (9)–(10) is shown in Fig. 5. Using the recipes from , we obtain message passing algorithms for the simultaneous estimation of Sk, a, ?2 U , and ?2 W . The estimation of Sk amounts to Kalman filtering (and smoothing), whichsuses a hard-decision
5457	5459	model parameters should be straightforward. 5. DISCUSSION We have outlined three examples of ongoing work in signal processing with factor graphs. Using the general recipes described in  and , we have obtained practical algorithms for complex detection/estimation problems; these algorithms either outperform previously published algorithms or are actually the first working
5457	5461	model parameters should be straightforward. 5. DISCUSSION We have outlined three examples of ongoing work in signal processing with factor graphs. Using the general recipes described in  and , we have obtained practical algorithms for complex detection/estimation problems; these algorithms either outperform previously published algorithms or are actually the first working estimators for
5457	5465	Kalman filters (or a Kalman filter coupled with an LMS-type algorithm) coupled with two particle filters. A more detailed description of the algorithms and some simulation results are given in  (and a full report is in preparation). The extension of the message passing algorithms to time-varying model parameters should be straightforward. 5. DISCUSSION We have outlined three examples of
8919298	5491	Perceptions 1. Introduction Much of the current research agenda on teaching encompasses craft knowledge, practical knowledge, personal practical knowledge, and pedagogical content knowledge (cf. Fenstermacher, 1994; Hoyle & John, 1995). All these types of knowledge refer to teacher knowledge expressed in practice which is, above all, experiential and implicit (Eraut, 1988). Despite a lack of consensus about
8919298	5491	practice teachers are continually confronted with this (Fenstermacher, 1992; Oser, 1992). In general, moral and ethical dimensions are more present in teaching than in many other professions (cf. Fenstermacher, 1994). In our postmodern societies, teachers increasingly face moral, social, and emotional dilemmas, such as: How cans752 D. Beijaard et al. / Teaching and Teacher Education 16 (2000) 749}764 we
8919314	5540	feedback is used. The former can be built easily with an opamp, where “opamp” can be a voltage opamp or any of the other eight types of opamps, e.g., a current opamp or a current-feedback opamp . The most straightforward way to build an amplifier without gain-setting feedback is to use current mirrors, which results in a current-mode filter. The amplifier gain is comparably precise in both
8919314	5544	(including the charge pump), whereas, e.g., Filter 4 (an LC ladder simulation) uses 0.25 mm 2 per pole, and Filter 7 uses 0.12 mm 2 per pole, but uses less power per pole and frequency. Filter 9  is the best single biquad we could build with the MOSFET–C SAB technique. It has a pole Q of three. With its high SNR, its low power per pole and frequency, its tuning range of 26–36 MHz, and its
8919314	5545	output resistance also introduces a pair of complex zeros that causes the transfer function to rise to a certain level for higher frequency and thus limits the achievable stopband attenuation . This behaviour is shown using idealised transfer functions in Fig. 2 for different sets of input capacitances and output resistances, and for the basic band-pass and high-pass filters as well as
5552	5553	alignment method, which does not add significantly to the processing cost , is essential for a stable and useful result. We use an inverse compositional method for its efficiency , although the fact that Wi changes in each frame means that some of its efficiency advantage over other methods is lost. The warps Wi and Wn ? 1 are a function of the relative position of the
5552	5554	but generally using controlled robot platforms with non-visual sensors. In this paper we build on a recent step forward demonstrating real-time SLAM using only a single camera, freely moving in 3D  — bringing the SLAM methodology into the “pure vision” arena. 1.1 Single Camera SLAM and and Persistent Landmarks In the single camera SLAM approach of , estimates of the locations of a
5552	5554	and it is therefore critical that the small set of features used act as long-term, high-quality landmarks — this is what will prevent the drift of motion estimates with time. In the system of , feature matching is on the basis of simple 2D image templates, taking no account of the change in feature appearance as the viewpoint changes — the result is a severely limited range of camera
5552	5554	matching of features without updating their templates. The key assumption is to assert that each salient image patch detected as a feature candidate (using the Shi and Tomasi operator as in ) corresponds to an observation of a locally planar surface in the 3D scene, rather than a 2D image entity. This approximation of flatness is relative to the size of camera motion over which the
5552	5555	of image gradient, which limits the system to slow camera motions, and the method is not described as working in real time. Our approach is also related to the stereo work of Hattori et al , which performs stereo matching undersan affine warp parameterised in terms of feature depth and surface orientation. 2.1 The Transformation of a Surface between Two Views Camera 0 y z x t xp n ? R
5552	5556	normal estimation is used only as an aid to robust feature matching and it is the image positions of the matches which are used to update the SLAM filter. Related work includes that of Jin et al , whose sequential structure from motion algorithm estimates camera pose and the position and orientation of locally planar features directly by comparison of the current images to that implied by
5552	5558	z ? In? Wn ? 1? x?¢? ? IT ? x? ? ?IT ?Wi ?p ?p ? n ? where the n term represents zero mean measurement noise. Calculation of a posterior estimate of p is discussed in detail in our previous work . If the prior estimate of p is ? pn 1 with ?p? covariance ? n 1, and the measurement noise n has variance ? 2 z , the posterior distribution of p has mean pn and ?p? covariance n given by: pn ? ?p?
5552	5558	In this work we set the variance ? of n (i.e. ? 2 z ) to a constant value as (7) (8) (9)swe have previously found that more sophisticated noise models do not give a noticeably improved result . In practice the value of p is calculated by iteratively applying Equations 5 and 9, and maintaining the prior pn ? 1 to represent the value at the start of iteration, while keeping the covariance
5552	5558	below) and are trying to maintain an estimate of the surface normal with first order uncertainty, use of a probabilistic alignment method, which does not add significantly to the processing cost , is essential for a stable and useful result. We use an inverse compositional method for its efficiency , although the fact that Wi changes in each frame means that some of its efficiency
5552	5558	z ? In? Wn ? 1? x?¢? ? IT ? x? ? ?IT ?Wi ?p ?p ? n ? where the n term represents zero mean measurement noise. Calculation of a posterior estimate of p is discussed in detail in our previous work . If the prior estimate of p is ? pn 1 with ?p? covariance ? n 1, and the measurement noise n has variance ? 2 z , the posterior distribution of p has mean pn and ?p? covariance n given by: pn ? ?p?
5552	5558	below) and are trying to maintain an estimate of the surface normal with first order uncertainty, use of a probabilistic alignment method, which does not add significantly to the processing cost , is essential for a stable and useful result. We use an inverse compositional method for its efficiency , although the fact that Wi changes in each frame means that some of its efficiency
8919322	5564	(SVMs) are a good example of the way in which current machine learning research combines ideas from different research areas. SVMs were introduced in the early nineties by Vapnik and co-workers  – bringing together ideas that had been around since the 1960s – and the topic has developed into a very active research area. Support Vector Machines combine two key ideas. The first is the
8919322	5566	combines aspects of a textbook (the excellent tutorials), a ‘Readings in graphical models’ (the journal-quality papers I mentioned), and a con8 A version of this chapter has been published as . 18sference proceedings. On the one hand, this leads to a certain imbalance in form and content. On the other hand, it is obvious that a large audience will find at least part of the book useful.
8919323	5577	the use of overcomplete wavelet decomposition to overcome the aliasing problem. Several works have been recently proposed for motion estimation and compensation in the over-complete wavelet domain -. Using an embedded coder  , the residues are compressed to produce a bitstream. The embedded bitstream can be partitioned into layers. The base layer also includes the motion vectors. One
8919323	5577	EZWEncoder Motion Estimation DWT- Discrete Wavelet Transform EZW-Embedded Zerotree Wavelet ODWT-Over-complete Wavelet Transform Fig.1. Proposed encoder are estimated using the low band shift method . The prediction error is encoded using the embedded zerotree wavelet coder (EZW) . The bitstream is partitioned into chunks of data such that there is one chunk per layer per frame. A portion of
8919323	5577	value is very important. Here, threshold ET and BT are chosen based on simple heuristic methods. 4. EXPERIMENTAL RESULTS A wavelet based video coder is implemented using low band shift method  for our simulation purpose. A Daubechies (9,7) filter with a three level decomposition is used to compute wavelet coefficients. The motion estimation is performed in the over complete domain using
8919323	5586	has shown that drift need not be completely eliminated, but it can be controlled   . Another way to completely eliminate drift is to use three dimensional (3-D) subband coding methods . ME/MC introduces a predictive feedback loop, which hinders in achieving a high degree of scalability. This is due to the fact that scalable coding introduces drift in the predictive coders. In 3-D
4884	4177	we obtain an ?-stable Lévy motion {A(t)} as the stochastic solution of the fractional diffusion equation (5.2) ?p(x, t) ?t = qD?? p(x, t) ?(?x) ? +(1?q)D ??p(x, t) ?x? where D > 0 and 0 ? q ? 1 . Using the Fourier transform ˆp(k, t) = ? e ?ikx p(x, t)dx, so that ˆp(?k, t) is the characteristic function of A(t), the fractional space derivative ? ? p(x, t)/?(±x) ? is defined as the inverse
4884	5598	of Lévy motion. If {A(t)} is an operator Lévy motion on R d and if ? t is the probability distribution of A(t) then the linear operators Ttf(x) = ? f(x ? y)? t (dy) form a convolution semigroup  with generator L = limt?0 t ?1 (Tt ? T0). Then q(x, t) = Ttf(x) solves the abstract Cauchy problem ?q(x, t)/?t = Lq(x, t); q(x, 0) = f(x) for any initial condition f(x) in the domain of the
4884	5602	be new even in the one dimensional case. Kotulski  also considers a coupled CTRW model in which the jump times and lengths are dependent. Coupled CTRW models occur in many physical applications . The authors are currently working to extend the results of this section to coupled models. In the one dimensional situation d = 1 Corollary 4.4 implies that M(t) d = (t/D) ?/? A where D is the
4884	4192	Cauchy problem whose Green’s function solution p(x, t) is the Lebesgue density of A(t). If {A(t)} is an ?-stable Lévy motion on R d , then L is a multidimensional fractional derivative of order ? . If {A(t)} is an operator Lévy motion then L represents a generalized fractional derivative on R d whose order of differentiation can vary with coordinate . Zaslavsky  proposed a fractional
4884	5617	as c ?? in D( does not apply, so we cannot prove convergence in the J1topology. Instead we use Theorem 13.2.4 in Whitt  which applies as long as x = E(t) is (almost surely) strictly increasing whenever A(x)
8919343	5667	research interest from the database community. A number of Data Stream Management Systems (DSMSs) are being developed for the purpose of information/knowledge extraction (querying) from stream data . The main challenge of handling data streams is imposed by the fact that the volume of data is unbounded as they are generated in a continuous manner. As a result, most queries against stream data
8919343	5668	results, queries in a DSMS are generally required to be delivered in a timely fashion. However, we may accept query results of di#erent levels of accuracy in a DSMS due to resource constraints . The set of parameters that describes the temporal/spatial requirements of applications are usually called Quality of Service (QoS) 1 . Similar to those in multimedia applications, the QoS
8919343	5671	requirements imposed by multimedia applications. Most of these e#orts emphasize system and network level resource management. QoS guarantees are provided as a service of the operating system  or middleware . The system maps QoS requirements of applications to resource use (system QoS) and QoS control is accomplished by regulating resource allocation to individual applications. For
8919343	5672	imposed by multimedia applications. Most of these e#orts emphasize system and network level resource management. QoS guarantees are provided as a service of the operating system  or middleware . The system maps QoS requirements of applications to resource use (system QoS) and QoS control is accomplished by regulating resource allocation to individual applications. For DSMSs, however,
8919343	5673	needs via a real-time operating system or middleware is a thread 2 . We cannot run each data stream as a thread as their number is in the thousands and no known system can support this many threads . Secondly, a real-time operating system lacks the ability to capture the application-level semantics of QoS. For example, when looking at the same streams of bank transaction data, a security
8919343	5673	means to adjust the load in our framework. Instead, our system adapts to the desired state by changing the QoS levels of streams. The QoS change follows a user-friendly roadmap. In another paper  from the Aurora group, various operator scheduling algorithms with di#erent optimization goals are presented. A scheduling algorithm that minimizes runtime memory consumption is given in  as
8919343	5674	di#cult as the estimation of future resource 2 Some only provide services on the process level. consumption could easily fail. As a result, traditional reservation-based QoS control methods  are not applicable to DSMSs. What we need is an qualityoriented adaptation framework that smartly adjusts QoS levels of applications in response to fluctuations of system and input status.
8919343	5676	delays and jitters are e#ectively controlled. We understand network delay dominates in other situations and we leave it to future research. Various system resources including CPU , memory , and network bandwidth  could be the bottleneck in DSMS query processing. With the previous assumption of su#cient memory and network throughput, we only need to consider the management of CPU
8919343	5676	In practice, as tuples are discarded in case of system overloading, the queue size will be kept small. Therefore, insertion into the queue only takes a few memory accesses. As discussed in STREAM , context switch in our DSMS model is accomplished by performing procedure calls, which bears extremely low CPU costs. The weight of our scheduler is even lower as we always schedule the operator
8919343	5676	)(2.2, s 1 )(4, 0) Single-stream, join with stored relation (0.4, s 1 )(1.3, s 2 )(1.5, s 1 )(4, 0) Multi-stream slide-window join (4, s 1 )(1.5, s 2 )(1.3, 0) in a similar way as in Babcock et al . We consider 4 http://ita.ee.lbl.gov/index.html 0 .2 .3 .4 .5 .6 .7 .8 .9 1.0 .1 0 20 40 60 80 100 120 140 160 180 200 Time (s) U:control U:static DM:static DM:control U:desired Figure 4:
8919343	5676	paper  from the Aurora group, various operator scheduling algorithms with di#erent optimization goals are presented. A scheduling algorithm that minimizes runtime memory consumption is given in  as part of the STREAM project. In , load shedding strategy that minimizes the loss of accuracy of aggregation queries in DSMS is discussed. The use of feedback control is inspired by the work
8919343	5677	e#ectively controlled. We understand network delay dominates in other situations and we leave it to future research. Various system resources including CPU , memory , and network bandwidth  could be the bottleneck in DSMS query processing. With the previous assumption of su#cient memory and network throughput, we only need to consider the management of CPU cycles. 3 The Control-Based
8919343	1496	processing within bounded delays, a real-time CPU scheduler is deployed. There are two main categories of real-time CPU scheduling policies: dynamic priority policies and fixed priority policies . The first class is represented by the Earliest Deadline First (EDF) algorithm while the second class by the Relative Deadline Monotonic (RDM) algorithm. Both algorithms are straightforward: EDF
8919343	1496	t be its user-specified processing delay, EDF sorts the jobs by r + t while RDM by t. We choose EDF to implement our scheduler as it is shown to have a utilization bound 3 of 1.0 for both periodic  and sporadic tasks . We tested the utilization bound of our scheduler by injecting a series of randomly generated loads into a simulated CPU queue (Fig 2, experimental setup in Section 4). As
8919343	5679	processing delay, EDF sorts the jobs by r + t while RDM by t. We choose EDF to implement our scheduler as it is shown to have a utilization bound 3 of 1.0 for both periodic  and sporadic tasks . We tested the utilization bound of our scheduler by injecting a series of randomly generated loads into a simulated CPU queue (Fig 2, experimental setup in Section 4). As we can see, very few DM
8919343	5680	the incoming load fluctuates wildly. AC can smooth the aggregated tra#c to make the fluctuations less dramatic. 3.5 QoS Adaptor Load shedding is frequently used to protect DSMSs from overloading . In order to e#ectively relieve system load and yet preserve utility, the load shedding algorithm needs to know when, where to shed load and how much load to shed. When we utilize a load shedder
8919343	5680	operator scheduling algorithms with di#erent optimization goals are presented. A scheduling algorithm that minimizes runtime memory consumption is given in  as part of the STREAM project. In , load shedding strategy that minimizes the loss of accuracy of aggregation queries in DSMS is discussed. The use of feedback control is inspired by the work of Lu et al . The major di#erence
8919343	5682	and # values are set to 0.5 ans 0.15, respectively. We use a time window of width 4 for the integral part of the controller. The PID controllers are chosen based on stability conditions derived in . We do not consider the overhead for context switch and operator scheduling as they are relatively small and we assume they are uniformly absorbed by the 0.1 free CPU utilization. We test our QoS
8919343	2243	We choose a method that updates the shed factor by increments of a base factors(0.1 in this experiment), similar to the approach in the Aurora project . The rate control mechanism in TCP  is of the same flavor: a connection exponentially decreases its sending rate when congestion is detected. Figure 4 shows the results of both shedding methods using the same set of data streams. The
8919343	5685	Shedding Figure 8: Utility gain by smart QoS adaptation vs. statistical shedding. 5 Comparison to Related Work Current work on DSMSs has concentrated on system architecture , query processing , query optimization , and stream monitoring . Relatively less attention has been paid to the development of a unified framework to support application-specific QoS requirements. Research
8919343	5686	Shedding Figure 8: Utility gain by smart QoS adaptation vs. statistical shedding. 5 Comparison to Related Work Current work on DSMSs has concentrated on system architecture , query processing , query optimization , and stream monitoring . Relatively less attention has been paid to the development of a unified framework to support application-specific QoS requirements. Research
8919343	5687	by smart QoS adaptation vs. statistical shedding. 5 Comparison to Related Work Current work on DSMSs has concentrated on system architecture , query processing , query optimization , and stream monitoring . Relatively less attention has been paid to the development of a unified framework to support application-specific QoS requirements. Research that is most closely
8919343	5688	statistical shedding. 5 Comparison to Related Work Current work on DSMSs has concentrated on system architecture , query processing , query optimization , and stream monitoring . Relatively less attention has been paid to the development of a unified framework to support application-specific QoS requirements. Research that is most closely related to our work is the
8919343	5689	the STREAM project. In , load shedding strategy that minimizes the loss of accuracy of aggregation queries in DSMS is discussed. The use of feedback control is inspired by the work of Lu et al . The major di#erence between our work and  is that we use feedback control to solve different problems (general real-time scheduling vs. QoS adaptation in DSMSs). As a result, there are also
8919343	3015	while we aim at more dramatic fluctuations of job arrivals. Therefore, a P controller is good for their purposes while we utilize the PID controller to massage the e#ects of the bursty tra#c. In , feedback control is used to assist QoS adaptation of concurrent video tracking tasks. Research on CPU scheduling in real-time databases is closely related to our e#orts in guaranteeing QoS. Abott
8919343	5691	databases is closely related to our e#orts in guaranteeing QoS. Abott and Garcia-Molina  compared the performance of major scheduling algorithms under di#erent load situations. Haritsa et al.  presented a modification to EDF that randomly decreases the priority of some of the jobs under high system contentions. 6 Conclusions and Future Research We describe a QoS adaptation framework for
5703	5705	frequently used to refer to the concepts in a specific domain. Typically, automatic terminology extraction (TE, Term Extraction) (ATR, Automatic Terminology Recognition) is divided in three steps (Bourigault, 1992) (Frantzi et al., 1999): 1. Term extraction via morphological analysis, part of speech tagging and shallow parsing. 2. Term weighting with statistical information. The weight is a measure of the
8919351	5710	is an application layer tool which provides the secure, robust, reliable and flexible transfer of data in a high performance computing environment. A detailed description of GridFTP can be found in . Background on the Grid Environment can be found in . 3.2 SABUL SABUL stands for Simple Available Bandwidth Utilization Library. It is designed for applications that require very high bandwidths
8919351	2981	striping . For example, using multiple TCP connections improves application bandwidth as mentioned in the first section. Perhaps the most widely deployed striped TCP implementation is GridFTP . It is relatively easy to develop an application that uses multiple TCP streams. No kernel level work is required. But as the number of the streams used grows, this method becomes complicated and
8919351	2981	has been redesigned as a web service. 5s3 Network and Data Protocols Studied In this section, we will describe the network and data protocols used in our experimental studies. 3.1 GridFTP GridFTP  is a high-performance data transfer protocol that is optimized for high-bandwidth, wide-area networks. It provides a superset of the features of the standard FTP protocol. It has become a standard
8919351	5714	robust, reliable and flexible transfer of data in a high performance computing environment. A detailed description of GridFTP can be found in . Background on the Grid Environment can be found in . 3.2 SABUL SABUL stands for Simple Available Bandwidth Utilization Library. It is designed for applications that require very high bandwidths over networks with high bandwidth delay products. SABUL
8919351	2993	grid could set up, monitor, and tear down their lambdas, assuring them of the bandwidth they require . For any approach it is possible to improve performance through the use of network striping . For example, using multiple TCP connections improves application bandwidth as mentioned in the first section. Perhaps the most widely deployed striped TCP implementation is GridFTP . It is
8919351	5720	types of protocols are built on top of TCP and UDP, no changes are required on the kernel level. There are several competing implementations of this approach include , TSUNAMI, and RBUDP. Although the basic mechanisms behind these solutions are very similar, they have different rate control and congestion control algorithms, resulting in different properties and performances. Both
8919351	2997	and used throughout the session. At the end of each block, 4sthe receiver feeds back a list of lost packets to be retransmitted. The third approach is to develop new network infrastructures. XCP , which is an open loop protocol operating at the network level, is an example of this approach. As another example, some researchers are advocating replacing the ISO Level 3 routed infrastructure
8919351	5723	applications, such as scientific computing and distributed data mining. The relation between RTT and maximal bandwidth for single TCP flows is, in practice, well estimated by the Mathis Equation : BW < C ? MSS RTT ? ? p . Here BW is the number of bytes transmitted per second, MSS denotes the maximum segment size, p is the packet loss ratio, which is the number of packets that are
5730	5731	specialized terms and corpora from the World Wide Web Marco Baroni SSLMIT, University of Bologna Corso della Repubblica 136 47100 Forlì, Italy baroni@sslmit.unibo.it Abstract The BootCaT toolkit (Baroni and Bernardini, 2004) is a suite of perl programs implementing a procedure to bootstrap specialized corpora and terms from the web using minimal knowledge sources. In this paper, we report ongoing work in which we
5730	6051	queries originates from Ghani et al. (2001), who applied it to the creation of minority language corpora. Our corpus-comparisonbased term extraction methodology was inspired by Rayson and Garside (2000). There is, of course, a large body of work on Japanese terminology, some of it involving web mining. For example, Fujii and Ishikawa (2000) use the web to search for definitions of pre-selected
5734	5736	in Section VIII. II. RELATED WORK Extensive research efforts have studied the problem of energy-efficient task allocation and scheduling with DVS in uni-processor real-time systems, including , , , . Recently, research interests have been shifted to multi-processor systems. A list-scheduling based heuristic is proposed in , to dynamically recalculate the priority of
5734	5737	methodology. This can lead to inefficient utilization of the system. The main motivation of our efforts is to develop techniques for systematic and rapid design and deployment of WSN applications , , . We focus on the development of energy-efficient collaborative algorithms for WSNs based on high-level computation models of WSNs. Such high-level models allow designers to make
5734	5738	upon the arrival of a task instance. Because at most one switch is needed for executing a task instance, the associated time overhead is assumed to be included in the workload of the task. From , the power consumption for executing a task follows a monotonically increasing and strictly convex function of the computation speed, ? ? ¡ , which can be represented as a polynomial function of at
5734	5741	a 10% (or better) precision is achieved. A. Synthetic Application Graphs Experimental Procedure: The structure of the application graph was generated using a method similar to the one described in . The only difference is that we enforce multiple source tasks in the generation of the DAG. According to Rockwell’s WINS node , the power consumption of an Intel StrongARM 1100 processor with
5734	5742	end of the epoch. Such a requirement is usually called the latency constraint, We use the term period to indicate the length of each epoch. Also, we assume that time-synchronization schemes (e.g., ) are available within the cluster. We consider the exclusive access constraint. Specifically, a non-preemptive scheduling policy is employed by each sensor node and each wireless channel. Also, at
5734	1061	networks (WSNs) are being developed for a wide range of civil and military applications, such as target tracking, infrastructure monitoring, habitat sensing, and battlefield surveillance , . WSNs usually contain a number of networked sensor nodes with each sensor node consisting of computation, communication, and sensing devices. These sensor nodes collaborate with each other to
5734	5743	various applications for energy saving in computation activities, techniques for exploring the energy-latency tradeoffs of communication activities are gaining interest. An important observation  is that in many channel coding schemes, the transmission energy can be significantly reduced by lowering the transmission power and increasing the duration of the transmission. Techniques such as
5734	5743	been proposed for implementing such tradeoffs. Recently, algorithms for applying such techniques in the context of packet transmissions or data gathering in wireless networks have been studied in , , . Our approaches can be extended to incorporate the above tradeoffs. In the following, we discuss through the example of modulation scaling that explores the tradeoffs by adapting the
5734	5744	DVS in uni-processor real-time systems, including , , , . Recently, research interests have been shifted to multi-processor systems. A list-scheduling based heuristic is proposed in , to dynamically recalculate the priority of communicating tasks. In , static and dynamic variable voltage scheduling heuristics for real-time heterogeneous embedded systems are proposed. An
5734	5744	processor voltage adjustment mechanism for a homogeneous multi-processor environment is discussed. However, the time and energy costs for communication activities are not addressed in any of , , and . 3 DRAFTsThe goal of all the above works is to minimize the overall energy dissipation of the system. While such a goal is reasonable for tightly coupled systems, it does not
5734	5746	over a long period of time require energy-aware design and operation at all levels of abstraction, from the physical layer to the application layer. However, while many hardware techniques , , network protocols , , and data processing algorithms ,  have been proposed for energy-aware design, systematic mechanisms for designing energy-aware collaborative processing
5734	5747	in Section VIII. II. RELATED WORK Extensive research efforts have studied the problem of energy-efficient task allocation and scheduling with DVS in uni-processor real-time systems, including , , , . Recently, research interests have been shifted to multi-processor systems. A list-scheduling based heuristic is proposed in , to dynamically recalculate the priority of
5734	4875	require energy-aware design and operation at all levels of abstraction, from the physical layer to the application layer. However, while many hardware techniques , , network protocols , , and data processing algorithms ,  have been proposed for energy-aware design, systematic mechanisms for designing energy-aware collaborative processing between sensor nodes still need to
5734	5748	Recently, research interests have been shifted to multi-processor systems. A list-scheduling based heuristic is proposed in , to dynamically recalculate the priority of communicating tasks. In , static and dynamic variable voltage scheduling heuristics for real-time heterogeneous embedded systems are proposed. An approach based on critical-path is used for selecting the voltage settings
5734	5749	at all levels of abstraction, from the physical layer to the application layer. However, while many hardware techniques , , network protocols , , and data processing algorithms ,  have been proposed for energy-aware design, systematic mechanisms for designing energy-aware collaborative processing between sensor nodes still need to be addressed. The state of the art in
5734	5749	application consists of a set of communicating tasks. Throughout the paper, the term activity refers to either a computation task or a communication request. We consider an epoch-based scenario , where an instance of the application is executed during the beginning of each epoch and must be completed before the end of the epoch. Such a requirement is usually called the latency constraint,
5734	5749	beyond the scope of this paper. We also assume that computation and communication activities can be parallelly executed on any sensor node. 4 DRAFTsB. Application Model An epoch-based application  consisting of a set of communicating tasks is considered. Let È denote the period of the application, which is the length of each epoch. An instance of the application is activated at time ?È, and
5734	5751	? and a standard deviation of ? . The power function of task Ì?, ?? ËÈ , was of the form ?? ¡ ËÈ ? ?? , where ?? and ?? were random variables with uniform distribution between 2 and 10, and 2 and 3 , respectively. For example, suppose ? ? ? ?? ? . Then, to execute a task of ¢ ? instructions costs 2 mSec and 4 mJ in the highest speed, and 6.7 mSec and 1 mJ in the lowest speed. The time and
5734	5756	This can lead to inefficient utilization of the system. The main motivation of our efforts is to develop techniques for systematic and rapid design and deployment of WSN applications , , . We focus on the development of energy-efficient collaborative algorithms for WSNs based on high-level computation models of WSNs. Such high-level models allow designers to make informed
5734	5759	Task Allocation Problem: We consider a single-hop cluster of homogeneous sensor nodes connected through multiple wireless channels. Each sensor node is equipped with dynamic voltage scaling (DVS)  The target application consists of a set of communicating tasks. Throughout the paper, the term activity refers to either a computation task or a communication request. We consider an epoch-based
5734	5759	VIII. II. RELATED WORK Extensive research efforts have studied the problem of energy-efficient task allocation and scheduling with DVS in uni-processor real-time systems, including , , , . Recently, research interests have been shifted to multi-processor systems. A list-scheduling based heuristic is proposed in , to dynamically recalculate the priority of communicating tasks. In
5734	5760	for implementing such tradeoffs. Recently, algorithms for applying such techniques in the context of packet transmissions or data gathering in wireless networks have been studied in , , . Our approaches can be extended to incorporate the above tradeoffs. In the following, we discuss through the example of modulation scaling that explores the tradeoffs by adapting the modulation
5734	5761	1 DRAFTsFor instance, in a target tracking application, up to thousands of sensor nodes are dispersed over a specific area of interest. The sensor nodes are usually organized into clusters ,  with each cluster consisting of tens of sensor nodes. Distributed signal detection and collaborative data processing are performed within each cluster for detecting, identifying, and tracking
5734	5761	This can lead to inefficient utilization of the system. The main motivation of our efforts is to develop techniques for systematic and rapid design and deployment of WSN applications , , . We focus on the development of energy-efficient collaborative algorithms for WSNs based on high-level computation models of WSNs. Such high-level models allow designers to make informed decisions
5734	5762	is used for selecting the voltage settings of tasks. However, both  and  assume that the task assignment is given. A similar problem to the one studied in this paper is investigated in . A two-phase framework is presented to first determine the allocation of tasks onto processors and then the voltage settings of tasks using convex programming. In , a dynamic processor voltage
5734	5763	this paper is investigated in . A two-phase framework is presented to first determine the allocation of tasks onto processors and then the voltage settings of tasks using convex programming. In , a dynamic processor voltage adjustment mechanism for a homogeneous multi-processor environment is discussed. However, the time and energy costs for communication activities are not addressed in
5764	5765	the reusability of existing tools and platforms. A key enabler in recent software successes with small- to medium-scale DRE systems (such as avionics mission computing systems) has been middleware , which is software that provides platform-independent execution semantics and reusable services that coordinate how application components are composed and interoperate. To address the many
5764	5767	to all the challenges of large-scale DRE system development. For example: • Our R&D on model-driven configuration and deployment of component-based DRE systems has resulted in the CoSMIC toolsuite , which is an open-source MDD toolsuite with an integrated collection of modeling, analysis, and synthesis tools that address key lifecycle challenges of DRE middleware and applications. The CoSMIC
5764	5767	The CoSMIC toolsuite and the associated Cadena and OTIF integration translators are available atwww.dre.vanderbilt.edu/cosmic. 3 that combines the MDD paradigm with QoS-enabled component middleware . We also provide an overview of the Cadena model checking tool developed at Kansas State University . 2.1 The CoSMIC Deployment and Configuration Modeling Environment As shown in Figure 1,
5764	5768	for analyzing and validating the functional correctness and QoS properties of DRE systems. • Conversely, various MDD tools exist that perform model checking component-based systems (such as Cadena ) or real-time schedulability analysis (such as AIRES  and VEST ). Model checking is useful for detecting errors early in the development stage, instead of sporadically and at runtime. In
5764	5768	3 that combines the MDD paradigm with QoS-enabled component middleware . We also provide an overview of the Cadena model checking tool developed at Kansas State University . 2.1 The CoSMIC Deployment and Configuration Modeling Environment As shown in Figure 1, CoSMIC consists of an integrated collection of modeling, analysis, and synthesis tools that address key
5764	5768	predictability, safety, schedulability, and security) specified via models. Model interpreters  translate the information specified by models into the input format expected by model checking  and analysis tools . These tools can check whether the requested behavior and properties are feasible given the specified application and resource constraints. Tool-specific model analyzers
5764	5768	which enables run-time reconfiguration and resource management to maintain end-to-end QoS. The CoSMIC toolsuite also provides the capability to interwork with model checking tools, such as Cadena  (described in Section 2.2), and aspect model weavers, such as C-SAW . The integration of CoSMIC with Cadena is the focus of Section 4. The CoSMIC toolsuite provides a number of modeling
5764	5768	components in the path of a signal, and detect signal feedbacks that can bring instability to a DRE system. To augment CoSMIC with these model checking capabilities, we integrated it with Cadena  shown in Figure 4, which is an open-source MDD environment for modeling and model checking CCM-based DRE systems. Cadena provides analysis capabilities that enable developers to navigate
5764	5769	QoS requirements. Other related work on model-driven analysis and development is the Virginia Embedded System Toolkit (VEST)  and Automatic Integration of Reusable Embedded Systems (AIRES) . VEST is an embedded system composition tool based on GME  that (1) enables the composition of reliable and configurable systems from COTS component libraries and (2) checks whether certain
5764	6093	and MIC technologies are aligning  to add the QoS capabilities necessary to support DRE systems in domains ranging from aerospace  to telecommunications  and industrial process control . This section provides an overview of our Component Synthesis using Model Integrated Computing (CoSMIC) toolsuite 1 The CoSMIC toolsuite and the associated Cadena and OTIF integration translators
5764	5773	enhancements to the CORBA Component Model (CCM) . CIAO abstracts component QoS requirements into metadata that can be specified in a component assembly after a component has been implemented . Decoupling the specification of QoS requirements from component implementations greatly simplifies the conversion and validation of an application model with multiple QoS requirements into CCM
5764	5776	schedulability, and security) specified via models. Model interpreters  translate the information specified by models into the input format expected by model checking  and analysis tools . These tools can check whether the requested behavior and properties are feasible given the specified application and resource constraints. Tool-specific model analyzers  can also analyze
5764	5788	could be used for the other files used by Cadena. To achieve this behavior requires support by the communication model for seamless data interchange. OTIF accepts a Unified Data Model (UDM)  interface to the data for the backplane. UDM provides a development process and set of supporting tools that generate C++ programmatic interfaces from UML class diagrams of data structures. These
5764	5791	import and is implemented inside the GReAT-based PICML tool adapter and semantic translator. 5 Related Work Our work on mode-based software extends earlier work on model-integrated computing (MIC)  that focused on modeling and synthesizing embedded software. Examples of MIC technology used today include GME  and Ptolemy  (used primarily in the real-time and embedded domain) and MDA
5764	5792	import and is implemented inside the GReAT-based PICML tool adapter and semantic translator. 5 Related Work Our work on mode-based software extends earlier work on model-integrated computing (MIC)  that focused on modeling and synthesizing embedded software. Examples of MIC technology used today include GME  and Ptolemy  (used primarily in the real-time and embedded domain) and MDA
5764	5794	earlier work on model-integrated computing (MIC)  that focused on modeling and synthesizing embedded software. Examples of MIC technology used today include GME  and Ptolemy  (used primarily in the real-time and embedded domain) and MDA  based on UML  and XML  (which have been used primarily in the business domain). Previous efforts using MIC technologies for
5764	6119	modeling and synthesizing embedded software. Examples of MIC technology used today include GME  and Ptolemy  (used primarily in the real-time and embedded domain) and MDA  based on UML  and XML  (which have been used primarily in the business domain). Previous efforts using MIC technologies for QoS adaptation have been applied to embedded systems comprising digital signal
8919357	5798	(the variances of their implicit Gaussian distribution). This decomposition can be used to reconstruct a new or existing face through the linear combination of “eigenhead” basis functions . An inspection of the PCA eigenvalue spectrum and the resulting shape reconstructions indicated that the first 60 eigenheads were quite sufficient for capturing most of the salient facial features
8919357	5799	on the face through dense 2D correspondence on the images . Other methods leverage off parameterized 3D face models and search for optimal parameters which best describe the input images . In either case, the number and viewpoint of input images is an important parameter for high quality 3D reconstruction. By intuition, the more input images taken from different viewpoints, the
8919357	5801	recover 3D face shape from 2D images or projections. Some of these are based on a direct approach which obtains 3D location of points on the face through dense 2D correspondence on the images . Other methods leverage off parameterized 3D face models and search for optimal parameters which best describe the input images . In either case, the number and viewpoint of input images is
8919357	5802	on the face through dense 2D correspondence on the images . Other methods leverage off parameterized 3D face models and search for optimal parameters which best describe the input images . In either case, the number and viewpoint of input images is an important parameter for high quality 3D reconstruction. By intuition, the more input images taken from different viewpoints, the
8919357	5802	for the shape recovery. We compare two silhouette-based methods in our (near) exhaustive optimization search. One is a model-based approach using a boundary-weighted silhouette contour technique . The other is a data-driven visual hull construction method based on a volume carving algorithm . Due to the large number of potential views, an aggressive pruning of the view-sphere is needed.
8919357	5802	“centroids” of each cluster as our aspect views. Any combinatorial subset of these aspect views constitutes a candidate multi-view configuration. 2. Multi-View 3D Face Modeling In previous work , we introduced a model-based shape-from-silhouette method for capturing 3D face models using multiple calibrated cameras. Figure 1 shows one such 3D model obtained from a novel subject inside a
8919357	5802	incorporate existing physical constraints into the optimal view selection problem. In this section we briefly review our 3D modeling methodology (for further details the reader is referred to ). For model building we used the USF “HumanID” dataset  of 3D Cyberware scans of 97 male and 41 female adult faces of various races and ages. The number of points in each face mesh varies from
8919357	5802	5, although there is some experimental evidence that beyond this one may encounter diminishing returns. The default reconstruction method is our modelbased (eigenhead) 3D face shape recovery method . By way of comparison, we also examined a purely data-driven method using visual hull construction. It should be noted that visual hulls by themselves are not at all capable of accurate
8919357	1112	recover 3D face shape from 2D images or projections. Some of these are based on a direct approach which obtains 3D location of points on the face through dense 2D correspondence on the images . Other methods leverage off parameterized 3D face models and search for optimal parameters which best describe the input images . In either case, the number and viewpoint of input images is
8919357	5804	recover 3D face shape from 2D images or projections. Some of these are based on a direct approach which obtains 3D location of points on the face through dense 2D correspondence on the images . Other methods leverage off parameterized 3D face models and search for optimal parameters which best describe the input images . In either case, the number and viewpoint of input images is
5806	6136	a Noun. This order will be inverted to give importance to the Noun. For Example: ‘pumping machinery’ will be rendered as ‘machinery, pumping’. Controlled vocabulary is used in forming the concepts . AsWordNet Has No ‘Recycle Bin’ 315 controlled vocabulary contains a unique term for each meaning. Also this may not hold well in all compound words. This study shows that there is a great
5817	5818	of the systems and their interrelationships as entities flow through the system (an example is shown in the following section). Developing a generic model from scratch can be overwhelming (Brown and Powers 2000) and so it can be beneficial to start with a representative system that is subsequently broadened to the domain level. Once a system specific conceptual diagram is constructed, careful
3153	5851	are orthogonal, of equal dimensions and intersect only at the origin, one can define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques . Once the clustering of the data has been found, a basis for each subspace is estimated using standard PCA. Unfortunately, this method is sensitive to noise in the data, as pointed out in ,
3153	5851	we are now interested in estimating a basis for each subspace. In our 2 example, let P2(x) =  and consider the derivatives of P2(x) at two points in each of the subspaces y1 = T ?S1 and y2 = T ?S2: ? ? ? x3 0 DP2(x)= ? 0 x3? ? DP2(y1)= ? x1 x2 10 ? ? 01?,DP2(y2)= ? 00 00 ? 00?. 11 Then the columns of DP2(y 1) span S ? 1 and the columns of DP2(y 2) span S ? 2
3153	5851	2 = p21(x) 2 + p22(x) 2 = (x 2 1 + x 2 2)x 2 3, and then choose a point in the data set that minimizes this distance. Say we pick y 2 ? S2 as such point. We can then compute the normal vector b2 = T to S2 from DP(y 2) as above. How do we now pick a point in S1 but not in S2? As it turns out, this can be done by polynomial division. We can just divide the original polynomials of degree n =2by
3153	5854	subspace (see Figure 1). For example, the segmentation of dynamic scenes involves the estimation of multiple 2-D or 3-D motion models from optical flow or point correspondences in two or more views . Subspace clustering is a challenging problem that is usually regarded as “chicken-and-egg.” If the segmentation of the data was known, one could easily fit a single subspace to each group of
3153	5854	in computer vision In this section, we apply GPCA to problems in computer vision such as vanishing point detection, face clustering, and news video segmentation. We refer the reader to ,  and  for applications in 2-D and 3-D motion segmentation from two, three and multiple views, respectively. Detection of vanishing points. Given a collection of parallel lines in 3-D, it is well
3153	5855	. Once the clustering of the data has been found, a basis for each subspace is estimated using standard PCA. Unfortunately, this method is sensitive to noise in the data, as pointed out in , where various improvements are proposed. When the subspaces are of co-dimension one, i.e. hyperplanes, the above method does not apply because the subspaces need not be orthogonal and their
3153	5855	?P : x ? R K ?? x ? = ?P(x) ?P, 2 This is because Mn is of the order n K . 3 For example, in 3-D motion segmentation from affine cameras, it is known that the subspaces have dimension at most four . ,sthe dimension of each original subspace Si is preserved, 4 and there is a one-to-one correspondence between Si and its projection – no reduction in the number of subspaces n. 5 Although all
3153	5857	3 S2 b11 b12 Figure 1: Data samples drawn from a mixture of one plane and one line (through the origin o)inR 3 . Arrows are normal vectors. y 1 o Expectation Maximization (EM) for mixtures of PCA’s . However, the performance of iterative approaches to subspace clustering is in general very sensitive to initialization. In fact, multiple starts are needed in order to obtain a good solution,
3153	3154	not apply because the subspaces need not be orthogonal and their intersection is nontrivial. A purely algebraic solution to this problem, called Generalized PCA (GPCA), was recently proposed in . It was shown that one can model the hyperplanes with a homogeneous polynomial that can be estimated linearly from data. The clustering problem is shown to be equivalent to factorizing this
3153	3154	by applying standard PCA to the set of derivatives (normal vectors) at those points. Our experiments on low-dimensional data show that our algorithm gives about half the error of the PFA of , and improves the performance of iterative techniques, such as K-subspace and EM, by about 50% with respect to random initialization. The best performance is achieved by using our algorithm to
3153	3154	can be written as a linear combination of a vector of coefficients cn ? RMn as , (1) pn(x) =c T n ?n(x) = ? cn1,n2,...,nK xn1 1 xn2 2 ···xnK K where ?n : RK ?RMn is the Veronese map of degree n , also known as the polynomial embedding in machine learning, defined as ?n : T ??  T with I chosen in the degree-lexicographic order.s3.1. Representing subspaces with
3153	3154	cn can be computed as the unique vector in the null space of Ln. Given cn, computing the normal vectors {bi} is equivalent to factorizing pn(x) into a product of linear forms as in (2). In , we solved this problem using a polynomial factorization algorithm (PFA) that computes the roots of a univariate polynomial of degree n and solves K ? 2 linear systems. Unfortunately, one may not
3153	5858	subspace (see Figure 1). For example, the segmentation of dynamic scenes involves the estimation of multiple 2-D or 3-D motion models from optical flow or point correspondences in two or more views . Subspace clustering is a challenging problem that is usually regarded as “chicken-and-egg.” If the segmentation of the data was known, one could easily fit a single subspace to each group of
3153	5858	Applications in computer vision In this section, we apply GPCA to problems in computer vision such as vanishing point detection, face clustering, and news video segmentation. We refer the reader to ,  and  for applications in 2-D and 3-D motion segmentation from two, three and multiple views, respectively. Detection of vanishing points. Given a collection of parallel lines in 3-D, it is
3153	5859	subspace (see Figure 1). For example, the segmentation of dynamic scenes involves the estimation of multiple 2-D or 3-D motion models from optical flow or point correspondences in two or more views . Subspace clustering is a challenging problem that is usually regarded as “chicken-and-egg.” If the segmentation of the data was known, one could easily fit a single subspace to each group of
3153	5859	?P : x ? R K ?? x ? = ?P(x) ?P, 2 This is because Mn is of the order n K . 3 For example, in 3-D motion segmentation from affine cameras, it is known that the subspaces have dimension at most four . ,sthe dimension of each original subspace Si is preserved, 4 and there is a one-to-one correspondence between Si and its projection – no reduction in the number of subspaces n. 5 Although all
3153	5859	in computer vision In this section, we apply GPCA to problems in computer vision such as vanishing point detection, face clustering, and news video segmentation. We refer the reader to ,  and  for applications in 2-D and 3-D motion segmentation from two, three and multiple views, respectively. Detection of vanishing points. Given a collection of parallel lines in 3-D, it is well know
3153	5860	4.3. Combining PDA with spectral clustering Although subspace clustering can be viewed as a special clustering problem, many of the classical clustering algorithms such as spectral clustering  cannot be directly applied. This is because, in order for spectral clustering algorithms to work well, one would need to define a distance between any pair of points in the subspaces that depends
8919370	5865	algebra formalism PEPA. 1 Introduction In this paper we investigate the interplay of two emerging technologies: active networks and software agents to support electronic commerce. Active networks are a compelling new initiative in networking. An active network extends a conventional one with the ability for network switches to process data as it is being transmitted. The processing which is
8919370	5867	by a cooperation project funded by the CNRS and The Royal Society. 1sSome recent work has focused on the performance gains that active network technology may bring to distributed applications . Examples include active reliable multi-cast  and cache routing . In  on-line auctions are suggested as applications which might benefit from processing at active nodes within the network.
8919370	5867	behaviour on the part of the bidder relies on a rapid response to submitted bids. However this may be jeopardised by network latency and/or server overload. As suggested, but not investigated in , the in-network processing capabilities provided by an active network appears to provide a solution to this problem. As far as we are aware no thorough performance analysis of such a scenario has
8919370	5869	1sSome recent work has focused on the performance gains that active network technology may bring to distributed applications . Examples include active reliable multi-cast  and cache routing . In  on-line auctions are suggested as applications which might benefit from processing at active nodes within the network. The Internet offers exciting new prospects for electronic commerce,
8919370	5871	on-line auction systems have been developed experimentally, such as the AuctionBot system (auction.eecs.umich.edu) from the University of Michigan, the Fishmarket Project (www.fishmarket.com)  or the eAuctionHouse (ecommerce.cs.wustl.edu) which supports combinatorial auctions and which is from the University of Washington. In such systems, competitive behaviour on the part of the bidder
8919370	5872	should result in a significant benefit in terms of both system throughput and system latency. The resulting system is then analysed using the stochastic process algebra modelling formalism PEPA . The paper is organised as follows. In Section 2, we describe the on-line auction system we investigate, and the motivation for the approach that we take. Then, in Section 3, after a brief
8919370	5872	from classical process algebra: prefix, choice, parallel composition and abstraction. We explain each of the combinators informally below. A formal operational semantics for PEPA is available in . Prefix: The prefix combinator “.” is used to designate the first action in the behaviour of a component, e.g. (?, r).P will carry out an action of type ? with an average duration of 1/r
8919370	5873	a continuous-time Markov process directly from the PEPA model which faithfully encodes the behavioural and temporal aspects of the modelled system. Details of this mapping can be found elsewhere . Other high-level notations for Markov processes, such as GSPN  or SAN  could equally have been used but the compositional structure of PEPA seemed well-suited to the structure of the
8919370	5873	of the system. The PEPA Workbench is a suite of tools which perform the well-formedness checking of PEPA models as well as the generation and solution of the corresponding Markov process . It detects faults such as deadlocks and cooperations which do not involve active participants. In the most recent version, it includes support for a modal logic, allowing behavioural requirements
8919370	5876	not involve active participants. In the most recent version, it includes support for a modal logic, allowing behavioural requirements of a model to be formally expressed and automatically checked . In essence, the translation process which occurs within the PEPA Workbench accepts a PEPA model as input and produces a matrix containing the Markov process encoding of the model given. In the
8919370	5877	with each equivalence class of states, where two states are considered equivalent if they generate the same observable behaviour. More details of this automatic aggregation can be found in . Performance measures are derived via the steady state probability distribution of the Markov process. A variety of linear algebra techniques may be employed to obtain this vector and the PEPA
5878	5879	storage devices over a network for scalability and performance . Several studies have focused in the performance of the iSCSI protocol from the perspective of on data path overheads and latency. With the exception of , which compares iSCSI to SMB, most of these efforts focus solely on iSCSI performance. Our focus is different in that we examine the suitability of block- and file-level
5878	5880	on the amount of meta-data sharing across client machines. Hence, we determine the characteristics of metadata sharing in NFS by analyzing two real-world NFS workload traces from Harvard University . We randomly choose one day (09/20/2001) trace from the EECS traces (which represents a research, software development, and course-based workload) and the home02 trace from the Campus traces (which
5878	5882	allow NFS clients to have comparable performance with respect to iSCSI clients even for metadata update intensive benchmarks. Directory delegation can be implemented using leases and callbacks . The effectiveness of strongly-consistent read-only meta-data cache as well as directory delegation depends on the amount of meta-data sharing across client machines. Hence, we determine the
5878	5882	is beyond the scope of this paper and is the subject of future research. 8 Related Work Numerous studies have focused on the performance and cache consistency of network file-access protocols . In particular, the benefits of meta-data caching in a distributed file system for a decade old workload were evaluated in . The VISA architecture was notable for using the conNormalized Num
5878	5884	By One Client Read By Multiple Client Written By Multiple Client 0 0 200 400 600 800 1000 1200 Interval T (b) Campus Trace Figure 7: Sharing Characteristics of Directories cept of SCSI over IP. Around the same time, a parallel effort from CMU also proposed two innovative architectures for exposing block storage devices over a network for scalability and performance . Several studies
5878	5887	broadly defined to be any storage technology that permits access to remote data over IP. The traditional method for networking storage over IP is to simply employ a network file system such as NFS . In this approach, the server makes a subset of its local namespace available to clients; clients access meta-data and files on the server using a RPC-based protocol (see Figure 1(a)). In contrast
5878	5887	is beyond the scope of this paper and is the subject of future research. 8 Related Work Numerous studies have focused on the performance and cache consistency of network file-access protocols . In particular, the benefits of meta-data caching in a distributed file system for a decade old workload were evaluated in . The VISA architecture was notable for using the conNormalized Num
5878	5888	protocol (see Figure 1(a)). In contrast to this widely used approach, an alternate approach for accessing remote data is to use an IPbased storage area networking (SAN) protocol such as iSCSI . In this approach, a remote disk exports a portion of its storage space to a client. The client handles £This research was supported in part by NSF grants CCR-9984030, EIA-0080119 and a gift from
5878	5888	the I/O operations are carried out over a network using a block access protocol (see Figure1(b)). In case of iSCSI, remote blocks are accessed by encapsulating SCSI commands into TCP/IP packets . The two techniques for accessing remote data employ fundamentally different abstractions. Whereas a network file system accesses remote data at the granularity of files, SAN protocols access
5878	5888	(IPSEC)—these features are negotiated at sessionstartup time; and (iv) it supports advanced error recovery using explicit retransmission requests, markers and connection allegiance switching . applicationss2.3 Differences Between NFS and iSCSI NFS and iSCSI provide fundamentally different data sharing semantics. NFS is inherently suitable for data sharing, since it enable files to be
5878	5888	storage devices over a network for scalability and performance . Several studies have focused in the performance of the iSCSI protocol from the perspective of on data path overheads and latency. With the exception of , which compares iSCSI to SMB, most of these efforts focus solely on iSCSI performance. Our focus is different in that we examine the suitability of block- and file-level
5878	5889	explores enhancements that eliminate these overheads. The consistency check related messages can be eliminated by using a strongly-consistent read-only name and attribute cache as proposed in . In such a cache, meta-data read requests are served out of the local cache. However, all update requests are forwarded to the server. On an update of an object, the server invalidates the caches
5878	5889	is beyond the scope of this paper and is the subject of future research. 8 Related Work Numerous studies have focused on the performance and cache consistency of network file-access protocols . In particular, the benefits of meta-data caching in a distributed file system for a decade old workload were evaluated in . The VISA architecture was notable for using the conNormalized Num
8919371	5897	are in conflict: they do not produce the same decision: one favors Si, whereas the other puts more emphasis on Si. Next the N mass functions are combined. Finally the pignistic transformation  produces a mass on each Si, ?, and ?, allowing an immediate decision considering the segment that has the maximum pignistic probability. This two-stage combination method is practical, but can lead
5898	5899	limits in the inequality d h+1 /(d ? 1) ? n ? 1, because d h+1 /(d ? 1) is an upper bound for the number of nodes within h hops for a given maximum node degree d. In fact, it can be seen in  that for O(1) node degree, expected path lengths can be, at best, O(log n), while for O(log n) node degree, expected path lengths cannot be shorter than O(log n/ log log n). When the distribution
5898	5899	expected performance of several peer-to-peer systems P2P system Node degree Path lengths Small-worlds  O(1) O(log 2 n) Chord , Pastry , Tapestry , eCAN  O(log n) O(log n) Koorde  (config. 1), D2B , Viceroy  O(1) O(log n) Koorde  (config. 2) O(log n) O(log n/ log log n) CAN  O(d) O(dn 1/d ) may opt to cover the entire identification space with LRCs, thus coupling
5898	5899	In most of them, separation between nearby and long-range contacts is only implicit and in some cases non-existent as there is no underlying lattice. For instance, Chord , Viceroy  and Koorde  use a ring as the underlying lattice. They only differ in the way they organize their LRCs. In Pastry , the existence of an underlying lattice is not so explicit, but we could consider the
5898	5899	(ii) balanced distribution of nodes, vs. unbalanced distribution. To unbalance node distribution we used a truncated Gaussian bivariate distribution with standard deviations of 1.0, 0.1and0.01 in a ?? square 3 . To route the messages we used the greedy routing algorithm, as it has good performance and requires no extension to use LRCs. Furthermore, it agrees to the conditions of Section
5898	5899	that our scheme is nearly location-independent. 3 To do this, we used the Box-Muller transformation and a translation to transform a two-dimensional continuous uniform distribution in the square  ??  to a Gaussian bivariate distribution centered at (0.5, 0.5) with the standard deviations mentioned. Truncation came from elimination of all points outside the  ×  square.
5898	5900	1sTable 1: Comparison between expected performance of several peer-to-peer systems P2P system Node degree Path lengths Small-worlds  O(1) O(log 2 n) Chord , Pastry , Tapestry , eCAN  O(log n) O(log n) Koorde  (config. 1), D2B , Viceroy  O(1) O(log n) Koorde  (config. 2) O(log n) O(log n/ log log n) CAN  O(d) O(dn 1/d ) may opt to cover the entire identification
5898	5900	greater than the number of nodes (e.g., Chord  1 ) or they may use a sparser cover of the identification space (e.g., typical configurations of expressways Content-Addressable Networks, eCAN ), which will not ensure logarithmic path lengths if distribution of nodes is not sparse and homogeneous. This paper is motivated by the observation that there are circumstances where a homogeneous
5898	5900	lattice to route messages. Contrary to the previous examples, CAN  has no LRCs but only short range contacts. For this reason, it has longer path lengths. To overcome this problem, Xu and Zhang  proposed a mechanism called “expressway CAN” that augments basic CAN with LRCs. Although their work is fairly complex and includes many subtleties to account for topological information in the
5898	5900	sharing a common domain name, like “someexistingname.com”. Perhaps the works that are closer in spirit to this paper are those of Kleinberg , Barrière et al.  and of Xu and Zhang  as they all address an explicit separation between an underlying lattice (often bi-dimensional) and a set of LRCs created to ensure short path lengths. The work of Kleinberg  models the small
5898	5900	lengths. 5.2 eCAN-like Mechanism To offer some comparative measurement, we run our scheme against a benchmark mechanism called “eCAN-like”. This benchmark results from an adaptation of the eCAN  logarithmic/logarithmic node degree/path length mechanism (whose applications most closely resemble those of our own algorithm). We must emphasize that the resulting mechanism, “eCAN-like”, is a
5898	5902	path lengths. Designers 1sTable 1: Comparison between expected performance of several peer-to-peer systems P2P system Node degree Path lengths Small-worlds  O(1) O(log 2 n) Chord , Pastry , Tapestry , eCAN  O(log n) O(log n) Koorde  (config. 1), D2B , Viceroy  O(1) O(log n) Koorde  (config. 2) O(log n) O(log n/ log log n) CAN  O(d) O(dn 1/d ) may opt to cover the
5898	5902	as there is no underlying lattice. For instance, Chord , Viceroy  and Koorde  use a ring as the underlying lattice. They only differ in the way they organize their LRCs. In Pastry , the existence of an underlying lattice is not so explicit, but we could consider the entire set of individual node’s leaf sets as a lattice between nearby nodes. Unlike these overlay networks,
5898	5903	Designers 1sTable 1: Comparison between expected performance of several peer-to-peer systems P2P system Node degree Path lengths Small-worlds  O(1) O(log 2 n) Chord , Pastry , Tapestry , eCAN  O(log n) O(log n) Koorde  (config. 1), D2B , Viceroy  O(1) O(log n) Koorde  (config. 2) O(log n) O(log n/ log log n) CAN  O(d) O(dn 1/d ) may opt to cover the entire
5898	5903	of an underlying lattice is not so explicit, but we could consider the entire set of individual node’s leaf sets as a lattice between nearby nodes. Unlike these overlay networks, neither Tapestry , nor D2B  use an underlying lattice to route messages. Contrary to the previous examples, CAN  has no LRCs but only short range contacts. For this reason, it has longer path lengths. To
5898	5904	of several peer-to-peer systems P2P system Node degree Path lengths Small-worlds  O(1) O(log 2 n) Chord , Pastry , Tapestry , eCAN  O(log n) O(log n) Koorde  (config. 1), D2B , Viceroy  O(1) O(log n) Koorde  (config. 2) O(log n) O(log n/ log log n) CAN  O(d) O(dn 1/d ) may opt to cover the entire identification space with LRCs, thus coupling the number of LRCs
5898	5904	lattice is not so explicit, but we could consider the entire set of individual node’s leaf sets as a lattice between nearby nodes. Unlike these overlay networks, neither Tapestry , nor D2B  use an underlying lattice to route messages. Contrary to the previous examples, CAN  has no LRCs but only short range contacts. For this reason, it has longer path lengths. To overcome this
5898	5905	peer-to-peer systems P2P system Node degree Path lengths Small-worlds  O(1) O(log 2 n) Chord , Pastry , Tapestry , eCAN  O(log n) O(log n) Koorde  (config. 1), D2B , Viceroy  O(1) O(log n) Koorde  (config. 2) O(log n) O(log n/ log log n) CAN  O(d) O(dn 1/d ) may opt to cover the entire identification space with LRCs, thus coupling the number of LRCs and/or
5898	5905	networks. In most of them, separation between nearby and long-range contacts is only implicit and in some cases non-existent as there is no underlying lattice. For instance, Chord , Viceroy  and Koorde  use a ring as the underlying lattice. They only differ in the way they organize their LRCs. In Pastry , the existence of an underlying lattice is not so explicit, but we could
5898	5906	achieves similar results in balanced scenarios and offers better performance with unbalanced distributions. To experimentally evaluate our solution we have used a location-aware peer-to-peer system . The remainder of the paper is organized as follows: Section 2 states the problem we are solving. Section 3 overviews previous work. Our long range contact mechanism is described and evaluated,
5898	5906	is ensured by some kind of bi-dimensional lattice that uses short range contacts to connect nodes with close identifications. Meshes formed by a bi-dimensional CAN, a Delaunay triangulation , or a structure made of squares as in  serve as examples of lattices. However, we emphasize that nodes are not required to be evenly spread in space. Moreover, although we are considering a
5898	5906	In this section we experimentally evaluate hop level with b = 2 and eCAN-like through simulation, using the Delaunay triangulation built by a location-aware peer-to-peer system called GeoPeer  as the underlying network. To do this we send a large number of messages between arbitrary pairs of nodes, under the following variables conditions: (i) 100, 500, 1000, 5000, 10000 and 50000
5898	5907	but at the cost of trying to populate the entire node identification space with LRCs. Table 1 resumes expected performance of the overlay networks mentioned here. Unlike most other work, SkpiNet  was built from scratch to cope with the unbalanced use of identification space. However, identification space of a SkipNet is unidimensional and extension to the more complex two-dimensional case,
5898	5908	within scope of some organization sharing a common domain name, like “someexistingname.com”. Perhaps the works that are closer in spirit to this paper are those of Kleinberg , Barrière et al.  and of Xu and Zhang  as they all address an explicit separation between an underlying lattice (often bi-dimensional) and a set of LRCs created to ensure short path lengths. The work of Kleinberg
11501563	5945	processes are added to a behavior-based control architecture  we are incrementally developing, making use of SORGIN, the software framework specially designed for those kind of systems . All behaviors are combined and, using a finite state automathon (FSA) that connects the different corridors with non-corridors locations, identify at each step the landmarks needed along a
11501563	5948	points of view. Many approaches combine environmental landmarks with odometric information to reduce the aliasing, to deambiguate different locations with the same sensory perception to the robot .  present an application to landmark-based navigation that builds a robust map that allows the robot to plan alternative maps to succed in the assigned task.  uses Genetic Algorithms (GA)
11501563	5951	builds a robust map that allows the robot to plan alternative maps to succed in the assigned task.  uses Genetic Algorithms (GA) to correct robot’s position and orientation from sonar readings.  also use GAs to learn office nameplates and to read the text on them.  presents a method for the equalization of landmark description using uniquely visual information for robot homing, and
11501563	5953	The reason is that a 1% loose in accuracy is compensated with the substantially lower number of calculations needed to classify a single pattern. 5 Corridor identification We do agree with Nehmzow  when affirming that nominal paths are better suited for landmark identification than just wandering behaviors, arguing that robust robot navigation is feasible if the environment has cues that can
11501563	5955	of view. Many approaches combine environmental landmarks with odometric information to reduce the aliasing, to deambiguate different locations with the same sensory perception to the robot .  present an application to landmark-based navigation that builds a robust map that allows the robot to plan alternative maps to succed in the assigned task.  uses Genetic Algorithms (GA) to
11501563	5959	points of view. Many approaches combine environmental landmarks with odometric information to reduce the aliasing, to deambiguate different locations with the same sensory perception to the robot .  present an application to landmark-based navigation that builds a robust map that allows the robot to plan alternative maps to succed in the assigned task.  uses Genetic Algorithms (GA)
10708	5967	+ 92]), NIAM (), IFO () and the generalized object role data modelling technique PSM (, ), action modelling techniques such as Task Structures (, ), DFD () and ExSpect (), and furthermore object oriented modelling techniques () adhering to the OO typing mechanism as described in . In , the application of the theory presented
10708	5620	provided. The central part of this theory will make weak assumptions on the underlying modelling technique, making it therefore applicable for a wide range of data modelling techniques such as ER (, ), EER 1 Currently at: Department of Computer Science, University of Queensland, Queensland 4072, Australia 1s(), NIAM (), IFO () and the generalized object role data
10708	5620	technique, we consider ER in this section. As stated before, a fully elaborated and formalised application of the theory to an objectrole modelling technique can be found in . For Chen’s () ER model (extended with subtyping), the information structure universe will be: Label Types The set of label types L in ER corresponds to the printable attribute types. Note that in some ER
10708	5969	and F a set of functions over T . For the moment, F is assumed to contain the one-step increment operator ?, and the comparison operator ?. Several ways of defining a time axis exist, see e.g. ,  or . The time axis is the axis along which the application model evolves. With this time axis, an application model history is a (partial) mapping T ? AME. 2 AMH is the set of all
10708	5974	specification of dynamic aspects). In case of snapshot databases, structure modifications will lead to costly data conversions and reprogramming. The intention of an evolving information system (, ) is to be able to handle updates of all components of the so-called application model, containing the information structure, the constraints on this structure, the population conforming to
10708	5974	combination of information structure, constraints and population. A conceptual specification of a universe of discourse, containing both the action and world model, is called an application model (, ). The resulting hierarchy of models is depicted in figure 2. The part of an (evolving) information system that is allowed to change over time, will be referred to as corpus evolutionis. In
10708	5974	provided in section 6. 1 In this paper, the difference between recording and event time , and the ability to correct stored information are not taken into consideration. For more details, see  or . 8s2. Time, essential to evolution, is incorporated into the theory through the algebraic structure Ts = ?T , F ?, where T is a (discrete, totally ordered) time axis, and F a set of
10708	5975	Evolving Information Systems, Temporal Information Systems, Schema Evolution, Data Modelling, Type Relatedness, Predicator Set Model, ER Model 1 Introduction As has been argued in  and , there is a growing demand for information systems, not only allowing for changes of their information base, but also for modifications in their underlying structure (conceptual schema and
10708	5975	section 6. 1 In this paper, the difference between recording and event time , and the ability to correct stored information are not taken into consideration. For more details, see  or . 8s2. Time, essential to evolution, is incorporated into the theory through the algebraic structure Ts = ?T , F ?, where T is a (discrete, totally ordered) time axis, and F a set of functions over
10708	5976	Australia 1s(), NIAM (), IFO () and the generalized object role data modelling technique PSM (, ), action modelling techniques such as Task Structures (, ), DFD () and ExSpect (), and furthermore object oriented modelling techniques () adhering to the OO typing mechanism as described in . In , the application of the
10708	5976	it must be applied to some modelling techniques. In the mean time the theory has been applied to PSM, resulting in EVORM (, ), and the conceptual transaction modelling technique Hydra (,), leading to Hydrae (). Furthermore, based on the notion of evolution as laid down in the axioms of the general theory, a query and manipulation language has been defined supporting
10708	5978	of Computer Science, University of Queensland, Queensland 4072, Australia 1s(), NIAM (), IFO () and the generalized object role data modelling technique PSM (, ), action modelling techniques such as Task Structures (, ), DFD () and ExSpect (), and furthermore object oriented modelling techniques () adhering to the OO typing
8919391	5980	Abstract. Recent work in Bayesian classifiers has shown that a better and more flexible representation of domain knowledge results in better classification accuracy. In previous work , we have introduced a new type of Bayesian classifier called Case-Based Bayesian Network (CBBN) classifiers. We have shown that CBBNs can capture finer levels of semantics than possible in
8919391	5980	than traditional BN classifiers (i.e., TAN and BAN), experiments have shown that BMN classifiers perform as well as BN classifiers and that neither approach clearly dominates . In previous work , we have introduced a new type of Bayesian classifier called Case-Based Bayesian Network (CBBN) classifiers. The basic idea behind CBBN classifiers is to intelligently partition the training data
8919391	5980	assignments. Finally, to produce the index of each cluster, we simply discard any “don’t care” attributes in each cluster’s assignment. The algorithms for the above procedure can be found in . Learning: We apply a BN learning algorithm to learn a BN classifier from the data objects in each indexed cluster. This local classifier, Bi where i ? {1, 2, ..., k}, is defined over a subset of
8919391	5981	classification node as an ordinary node and identifies a relevant attribute subset around it determined by its Markov blanket. These classifiers have been shown to outperform naive-Bayes classifier . Bayesian Multi-net (BMN) classifiers are a generalized form of the augmented naive structure (i.e., TAN and BAN) in the sense that they allow different relationships among attributes for different
8919391	4051	classification node as an ordinary node and identifies a relevant attribute subset around it determined by its Markov blanket. These classifiers have been shown to outperform naive-Bayes classifier . Bayesian Multi-net (BMN) classifiers are a generalized form of the augmented naive structure (i.e., TAN and BAN) in the sense that they allow different relationships among attributes for different
8919391	4051	of the prior probability distribution of the class variable C andsa set of local networks, each corresponding to a value of C. TwoformsofBMN classifiers have been examined. Tree Multi-net (BMN-TAN) , a generalization of TAN, where the structure of a local network is restricted to tree-like structure and Bayesian Multi-net (BMN-BAN) , a generalization of BAN, where a local network can
8919391	4051	more expressive than traditional BN classifiers (i.e., TAN and BAN), experiments have shown that BMN classifiers perform as well as BN classifiers and that neither approach clearly dominates . In previous work , we have introduced a new type of Bayesian classifier called Case-Based Bayesian Network (CBBN) classifiers. The basic idea behind CBBN classifiers is to intelligently
8919391	5982	classifiers have been examined. Tree Multi-net (BMN-TAN) , a generalization of TAN, where the structure of a local network is restricted to tree-like structure and Bayesian Multi-net (BMN-BAN) , a generalization of BAN, where a local network can have unrestricted structure. Although the structures of these classifiers are strictly more expressive than traditional BN classifiers (i.e., TAN
8919391	5983	In order to compare BMN classifiers to our CBBN classifiers, we have learned three types of BMN classifiers: BMN-TAN, BMN-BAN and BMN-BAN* based on three different learning algorithms: Chow-Liu , MDL score  and CBL2  respectively 1 . We compare the classification accuracy of these classifiers to their corresponding CBBN classifiers (i.e., CBBN-TAN, CBBN-BAN and CBBN-BAN*). These
8919391	5984	compare BMN classifiers to our CBBN classifiers, we have learned three types of BMN classifiers: BMN-TAN, BMN-BAN and BMN-BAN* based on three different learning algorithms: Chow-Liu , MDL score  and CBL2  respectively 1 . We compare the classification accuracy of these classifiers to their corresponding CBBN classifiers (i.e., CBBN-TAN, CBBN-BAN and CBBN-BAN*). These classifiers have
8919391	5985	classifiers to our CBBN classifiers, we have learned three types of BMN classifiers: BMN-TAN, BMN-BAN and BMN-BAN* based on three different learning algorithms: Chow-Liu , MDL score  and CBL2  respectively 1 . We compare the classification accuracy of these classifiers to their corresponding CBBN classifiers (i.e., CBBN-TAN, CBBN-BAN and CBBN-BAN*). These classifiers have been learned
8919391	5986	In all data sets, objects with missing attribute values have been removed and numerical attributes have been categorized. For data clustering in CBBNs, we used the clustering algorithm, k-modes . This algorithm requires that the user specify the number of clusters k. In this work, we have determined an acceptable range of k for each data set. More specifically, k can take integer values
5987	5988	PoP. One could detect volume anomalies by collecting IP flow-level traffic summaries on all input links at all PoPs, and applying temporal decomposition methods to each OD flow in the manner of . In general, this is impractical, for a number of reasons. First, there can be hundreds of customer links in a network. Monitoring all input links to collect and aggregate flow level data is
5987	5988	are ARIMA-based BoxJenkins forecasting models . A second class of methods process data in batches and are based on signal analysis techniques, such as the wavelet analysis scheme used in . Such schemes model the timeseries mean by isolating low-frequency components; anomalies are then flagged at those points in time that deviate significantly from the modeled behavior of the mean.
5987	5988	across links) in network measurements. The same general strategy of seeking a useful alternate basis set for data representation lies behind previous approaches to anomaly detection as well . However, these previous approaches have made use of correlation in the temporal domain – for example, using the wavelet transform, or using exponential smoothing. However it is reasonable to ask
5987	5988	such a method can allow the detection of anomalies at all timescales. 8. RELATED WORK A number of techniques have been proposed to detect anomalies in traffic volume. Some examples include . As discussed in Section 7, all these schemes operate on single-timeseries traffic, measured for example from a network link, and independent of traffic on other links in a network. Thus, these
5987	5991	Sun 5. DIAGNOSING VOLUME ANOMALIES The methods we use for detecting and identifying volume anomalies draw from theory developed for subspace-based fault detection in multivariate process control . Our notation in the following subsections follows . 5.1 Detection Detecting volume anomalies in link traffic relies on the separation of link traffic y at any timestep into normal and anomalous
5987	5991	anomaly ?i and the normal subspace can make anomalies of a given size in one direction harder to detect than in other directions. A sufficient condition for detectability in our context is given in . Specializing their results to the case of one-dimensional anomalies, we can guarantee detectability of anomaly Fi if: fi > 2?? ? ˜ C?i? If a single-flow anomaly Fi consists of bi additional or
5987	5991	when an anomaly involves multiple OD flows, when it arises from routing changes, or when it arises from network abuse (e.g, DDoS attacks). To handle this case, we may proceed as follows (see also ). We replace ?i with a matrix ?i having as many columns as there are flows that participate in the anomaly; each column of ?i consists of the (normalized) column of A corresponding to a
5987	5991	no evaluation is given. Finally, as has been noted earlier in the paper, our methodology draws strongly on techniques from subspace-based fault diagnosis, such as those used in chemical engineering . Those methods exploit correlation patterns among state variables in an industrial process control setting, whereas we focus on covariance patterns across link traffic timeseries. 9. CONCLUSIONS In
5987	4722	PoPs of each flow. The ingress PoP can be identified because we collect flows from each ingress link in both networks. For egress PoP resolution, we use BGP and ISIS routing tables as detailed in . For Sprint, we supplemented routing tables with router configuration files to resolve customer IP address spaces. Also, Abilene anonymizes the last 11 bits of the destination IP address. This is
5987	5994	is that it does not require detailed modeling assumptions about the underlying normal traffic behavior. Previous methods have principally relied on timeseries models to approximate normal traffic  or have relied on building models of the data during a training period . For example, the commonly used Holt-Winters algorithm builds a model for normal traffic based on parameter settings and
5987	5994	in networks (which is a more general problem than traffic anomaly detection) has also attracted some attention insthe past. A review of research in this area is ; noteworthy examples include  and . The approach of  relies on an autoregressive (AR) model of signals, coupled with Bayes nets for detecting faults, but no significant validation is given. The methods of  rely on
5987	5998	PoP. One could detect volume anomalies by collecting IP flow-level traffic summaries on all input links at all PoPs, and applying temporal decomposition methods to each OD flow in the manner of . In general, this is impractical, for a number of reasons. First, there can be hundreds of customer links in a network. Monitoring all input links to collect and aggregate flow level data is
5987	5998	based on gross deviations from forecasted behavior. Simple instances of such a strategy are the exponential weighted moving average (EWMA) and Holt-Winters forecasting algorithms, both used in ; more sophisticated examples are ARIMA-based BoxJenkins forecasting models . A second class of methods process data in batches and are based on signal analysis techniques, such as the wavelet
5987	5998	as: ˆzt+1 = ?zt +(1? ?)ˆzt where 0 ? ? ? 1 is a parameter that controls the relative weight placed on past values. We selected values for ? based on applying a multi-grid parameter search (as in ) performed on sample training data, and found that values of 0.2 ? ? ? 0.3 isolated the spikes in our data well. Anomalies can then be quantified by taking the difference between the forecasted and
5987	5998	across links) in network measurements. The same general strategy of seeking a useful alternate basis set for data representation lies behind previous approaches to anomaly detection as well . However, these previous approaches have made use of correlation in the temporal domain – for example, using the wavelet transform, or using exponential smoothing. However it is reasonable to ask
5987	5998	such a method can allow the detection of anomalies at all timescales. 8. RELATED WORK A number of techniques have been proposed to detect anomalies in traffic volume. Some examples include . As discussed in Section 7, all these schemes operate on single-timeseries traffic, measured for example from a network link, and independent of traffic on other links in a network. Thus, these
5987	6000	by 3 or 4 principal components. This low effective dimensionality of link timeseries is consistent with the finding that the underlying OD flows themselves have low intrinisic dimensionality . In fact, the low effective dimensionality of link traffic forms the basis for the success of the subspace methods we describe in the following sections. 4.3 Subspace construction via PCA Once the
5987	6000	each arrival of new traffic measurements using the matrix PP T , which is derived from the SVD. Previous work has shown that (for OD flows) this matrix can be reasonably stable from week to week . Thus one need only compute the SVD occasionally, rather than at each timestep. Finally, it is conceivable that the straightforward SVD procedure could become a bottleneck if applied to data with a
5987	6002	all underlying OD flow intensities from link data, which is a much harder problem. As a result, most traffic matrix estimation work to date has concentrated on estimating mean values of large flows . In contrast, our work addresses estimation of anomalous values occurring in any flow. The authors in  study a problem similar to the volume anomaly diagnosis problem and propose a solution
5987	6004	such a method can allow the detection of anomalies at all timescales. 8. RELATED WORK A number of techniques have been proposed to detect anomalies in traffic volume. Some examples include . As discussed in Section 7, all these schemes operate on single-timeseries traffic, measured for example from a network link, and independent of traffic on other links in a network. Thus, these
5987	6007	traffic behavior. Previous methods have principally relied on timeseries models to approximate normal traffic  or have relied on building models of the data during a training period . For example, the commonly used Holt-Winters algorithm builds a model for normal traffic based on parameter settings and a priori knowledge of the periodic structure in traffic. In contrast, our
5987	6008	on link data, so in order to validate against true OD flows we must obtain a set of link traffic counts consistent with the sampled OD flow data collected. To obtain this, we follow the method of  and construct link counts from OD flow counts using a routing table taken from the network in operation. 4. SUBSPACE ANALYSIS OF LINK TRAFFIC Effective diagnosis of anomalies in traffic requires
5987	6008	accurate across all our datasets. Average quantification estimates for all but one dataset are within 21% of the true size of the anomaly, which is sufficiently accurate for operational settings . To summarize, our results here demonstrate that regardless of the dataset or the validation method employed, the subspace diagnosis procedure shows high detection rates and low false alarm rates.
5987	6008	all underlying OD flow intensities from link data, which is a much harder problem. As a result, most traffic matrix estimation work to date has concentrated on estimating mean values of large flows . In contrast, our work addresses estimation of anomalous values occurring in any flow. The authors in  study a problem similar to the volume anomaly diagnosis problem and propose a solution
8919397	3551	improve the reuse characteristics. Recently, extensive research efforts have been devoted to the performance evaluation of ad hoc networks, most of which focus on the capacity analysis . To the best of our knowledge, the influence of interference range on the network performance was first explored in  which examines the Fengji Ye, Su Yi and Biplab Sikdar ECSE Department,
8919397	3551	it does not take a fixed value. To obtain the value of Ê?, we need to introduce the model of signal to interference ratio (SIR), which directly follows the Physical Model in Gupta and Kumar’s work . Suppose that node ? is receiving packets from node ?, which is at an onehop distance of ?× meters, and concurrently another node ?, ?? meters away from ?, is sending packets to a fourth node ?. We
8919397	6017	improve the reuse characteristics. Recently, extensive research efforts have been devoted to the performance evaluation of ad hoc networks, most of which focus on the capacity analysis . To the best of our knowledge, the influence of interference range on the network performance was first explored in  which examines the Fengji Ye, Su Yi and Biplab Sikdar ECSE Department,
8533973	6029	- Each file is written in 64KB blocks; the last block may be smaller. Each of these blocks is encrypted with a fast encryption algorithm. This analysis uses the RC5 encryption algorithm  with a 128-bit key. This ensures the data can not be read without the accompanying RC5 key. This key is used to encrypt each block in a file or group of files. The key is generated by the user upon
8919404	6041	have been widely used in graph partitioning, data clustering, linear labeling of a graph, and load balancing. The optimality of the spectral order in many applications is discussed in . Figure 2 gives the pseudo code of the Spectral LPM. An example of applying the Spectral LPM to a set of two-dimensional points in a ?¡s? grid is given in Figure 3. Notice that Figures 3b and 3c
8919404	6044	are nearby in the multidimensional space are nearby in the one-dimensional space. Fractal space-filling curves (e.g., the Hilbert and Peano) have long been used as a locality-preserving mapping  for multi-dimensional similarity search queries, spatial join, R-tree packing, declustering, spatial access methods, and GIS applications. In this paper, we go beyond the idea This work was
265230	6048	RI &RPSXWHU 6FLHQFH 8QLYHUVLW\ RI :DLNDWR 3ULYDWH %DJ +DPLOWRQ 1HZ =HDODQG ^VWHYHM SD\QWHU`#FV ZDLNDWR DF Q] documents , search and browsing interfaces , retrieval engines  and thesaurus construction . Keyphrases are often chosen manually, usually by the author of a document, and sometimes by professional indexers. Unfortunately not all documents contain
265230	6049	phrases from text by using stopword delimiters, and then consider phrases to be meaningful if they occur in the document collection more than some fixed number of times. Barker and Cornacchia  identify noun phrases using dictionary lookup, and then consider the frequency of a given noun as a phrase head within a document, discarding those that fall below a given threshold. Tolle and Chen
265230	6049	&RPELQHG /LVW 5 49 6 58 6 47 6 58 10 51 6 66 8 54 6 68 5 51 6 57 7 55 6 67 There are several problems with evaluations based purely on authorchosen keyphrases. Barker and Cornacchia identify four . First, author keyphrases do not always appear in the text of the document to which they belong. Second, authors choose keyphrases for purposes other than document description—to increase the
265230	6049	are available for a limited number and type of documents. A second approach is to gather subjective keyphrase assessments from human readers. Previous studies involving human phrase assessment  follow essentially the same methodology. Subjects are provided with a document and a phrase list and asked to assess in some way the relevance of the individual phrases (or of sets of phrases) to
265230	6049	of their significance shows that the inter-assessor agreement was significant at the 0.01 level in all cases. We found substantially greater agreement between subjects than Barker and Cornacchia  observed in a study of keyphrase produced by Extractor and their system B&C. They reported that “on average, the judges agree only about as much as can be expected by chance”. In all cases, our
265230	6049	is that assessor agreement is so low that little can be determined from the data. For example, both Chen  (“Inter-indexer inconsistency is obvious in our experiment”) and Barker and Cornacchia  (???Kappa values are spectacularly low”) experienced this difficulty. However, we have established that the subjects in our experiment achieved a significant level of agreement. We attribute this to
265230	6051	RI &RPSXWHU 6FLHQFH 8QLYHUVLW\ RI :DLNDWR 3ULYDWH %DJ +DPLOWRQ 1HZ =HDODQG ^VWHYHM SD\QWHU`#FV ZDLNDWR DF Q] documents , search and browsing interfaces , retrieval engines  and thesaurus construction . Keyphrases are often chosen manually, usually by the author of a document, and sometimes by professional indexers. Unfortunately not all documents contain
265230	2459	that it first occurs, and how specific it is to the document (its TF•IDF value). The attribute values of every phrase in every training document are used to construct a Naive Bayes classifier  that predicts whether or not a phrase is an author keyphrase based on its other attributes. A range of options allows control over the model building process, and consequently the characteristics
265230	6052	and keyphrase extraction. In keyphrase assignment (also known as text categorization) an analysis of a document leads to selection of keyphrases for that document from a controlled vocabulary . It has two main advantages: the controlled vocabulary ensures that similar documents are classified consistently, and documents can be associated with concepts that are not explicitly mentioned in
265230	6053	expertise, and can give inconsistent results, so automatic methods benefit both the developers and the users of large document collections. In this paper we describe a human evaluation of Kea , an automatic keyphrase extraction algorithm developed by members of the New Zealand Digital Library Project . Kea uses machine learning techniques to build a model that characterises document
265230	6053	New Zealand Digital Library Project. The algorithm is substantially simpler, and therefore less computationally intensive, than many previous approaches. Kea has been described in detail elsewhere , and its operation is summarised below. Kea uses a model to identify the phrases in a document that are most likely to be good keyphrases. This model must be learned from a set of training
265230	6053	Web pages and computer science technical reports. Previous work shows that models built for specific collections are more likely to account for the idiosyncrasies of that collection’s keyphrases . 5. EVALUATING KEYPHRASES There are two basic approaches to evaluating automatically generated keyphrases. The first adopts the standard Information Retrieval metrics of precision and recall to
265230	6053	match phrases which are considered to be ‘relevant.’ Author phrases are usually used as the set of relevant phrases, or the ‘Gold Standard.’ This approach was adopted in previous evaluations of Kea .sTable 1: Profile of phrases associated with each document 3DSHU $XWKRU 0HUJHG .HD 1XPEHU RI NH\SKUDVHV )RRG &RPELQHG /LVW 5 49 6 58 6 47 6 58 10 51 6 66 8 54 6 68 5 51 6 57 7 55 6 67 There are
265230	6053	first, aliweb, was trained on a set of typical web pages found by Turney . The second, cstr, is derived from a collection of computer science technical reports as described by Frank et al.. The third, cstr-kf, was trained on the same documents as cstr, but uses a further attribute which reflects how frequently a phrase occurs as a specified keyphrase in a set of training documents.
265230	6055	-RQHV *RUGRQ : 3D\QWHU 'HSDUWPHQW RI &RPSXWHU 6FLHQFH 8QLYHUVLW\ RI :DLNDWR 3ULYDWH %DJ +DPLOWRQ 1HZ =HDODQG ^VWHYHM SD\QWHU`#FV ZDLNDWR DF Q] documents , search and browsing interfaces , retrieval engines  and thesaurus construction . Keyphrases are often chosen manually, usually by the author of a document, and sometimes by professional indexers. Unfortunately
265230	6055	into the web browser. Figure 2: The Phrasier user interface. Kniles is a simplified, Web-based version of Phrasier, a program that supports authors and readers who work within a digital library . In Phrasier, browsing and querying activities are seamlessly integrated with document authoring and reading tasks (see Figure 2). Keyphrases are used to dynamically insert hypertext link anchors
265230	6057	-RQHV *RUGRQ : 3D\QWHU 'HSDUWPHQW RI &RPSXWHU 6FLHQFH 8QLYHUVLW\ RI :DLNDWR 3ULYDWH %DJ +DPLOWRQ 1HZ =HDODQG ^VWHYHM SD\QWHU`#FV ZDLNDWR DF Q] documents , search and browsing interfaces , retrieval engines  and thesaurus construction . Keyphrases are often chosen manually, usually by the author of a document, and sometimes by professional indexers. Unfortunately
265230	6057	forth between result lists and document content. Another system, called Kniles, eliminates this extraneous navigation by embedding the browsing interface directly into documents as they are viewed . Kniles uses keyphrases to automatically construct browsable hypertexts from plain text documents that are displayed in a conventional Web browser. Link anchors are inserted into the text wherever
265230	6057	into the web browser. Figure 2: The Phrasier user interface. Kniles is a simplified, Web-based version of Phrasier, a program that supports authors and readers who work within a digital library . In Phrasier, browsing and querying activities are seamlessly integrated with document authoring and reading tasks (see Figure 2). Keyphrases are used to dynamically insert hypertext link anchors
265230	6062	:DLNDWR 3ULYDWH %DJ +DPLOWRQ 1HZ =HDODQG ^VWHYHM SD\QWHU`#FV ZDLNDWR DF Q] documents , search and browsing interfaces , retrieval engines  and thesaurus construction . Keyphrases are often chosen manually, usually by the author of a document, and sometimes by professional indexers. Unfortunately not all documents contain author- or indexer-assigned keyphrases.
265230	6063	our use of keyphrases in user interfaces for searching and browsing. We have built a number of novel systems that use keyphrases to support new styles of interaction with digital libraries. Phind  adds a browsable topic-oriented structure to collections of documents where no structure existed before—a structure that cannot be uncovered through conventional keyword queries. Users interact
265230	6064	for searching a database of patent information. Within the system phrases are used to suggest query expansions to users based on the search terms that have been specified. Similarly, Pedersen et al  use phrases to support query reformulation in their Snippet Search system. Krulwich and Burkey  exploit heuristically extracted phrases to inform InfoFinder, ans'intelligent' agent that learns
265230	6066	Anick and Vaithyanathan  carry out part of speech tagging and identify noun compounds—word sequences of two or more adjectives and nouns terminating in a head noun. Smeaton and Kelledy  identify 2 or 3 word candidate phrases from text by using stopword delimiters, and then consider phrases to be meaningful if they occur in the document collection more than some fixed number of
265230	6068	each document are less consistent, and it is not easy to identify the “most appropriate” words and phrases. A wide range of techniques has been applied to the problem of phrase extraction. Turney  uses a set of heuristics that are fine tuned using a genetic algorithm. Chen  uses statistical measures exploiting importance, frequency, co-occurrence and distance attributes of word pairs.
265230	6068	keyphrases specified in the paper, and unrelated control phrases. Three Kea models were used to extract keyphrases. The first, aliweb, was trained on a set of typical web pages found by Turney . The second, cstr, is derived from a collection of computer science technical reports as described by Frank et al.. The third, cstr-kf, was trained on the same documents as cstr, but uses a
265230	6069	enhance user interaction. The Journal of Artificial Intelligence Research (http://extractor.iit.nrc.ca/jair/keyphrases/) can be accessed through an interface based on phrases produced by Extractor . Larkey  describes a system for searching a database of patent information. Within the system phrases are used to suggest query expansions to users based on the search terms that have been
265230	6069	each document are less consistent, and it is not easy to identify the “most appropriate” words and phrases. A wide range of techniques has been applied to the problem of phrase extraction. Turney  uses a set of heuristics that are fine tuned using a genetic algorithm. Chen  uses statistical measures exploiting importance, frequency, co-occurrence and distance attributes of word pairs.
265230	6069	are available for a limited number and type of documents. A second approach is to gather subjective keyphrase assessments from human readers. Previous studies involving human phrase assessment  follow essentially the same methodology. Subjects are provided with a document and a phrase list and asked to assess in some way the relevance of the individual phrases (or of sets of phrases) to
265230	6069	keyphrases specified in the paper, and unrelated control phrases. Three Kea models were used to extract keyphrases. The first, aliweb, was trained on a set of typical web pages found by Turney . The second, cstr, is derived from a collection of computer science technical reports as described by Frank et al.. The third, cstr-kf, was trained on the same documents as cstr, but uses a
265230	6069	university study in the English language—where adequate English language skills are a necessity. 7.5 Related Human Evaluations Turney carried out a simple Web-based subjective evaluation  of the keyphrases produced by Extractor. Self-selecting subjects were requested to gauge keyphrases as good or bad with respect to a document that they themselves submitted, and provide feedback
265230	6071	expertise, and can give inconsistent results, so automatic methods benefit both the developers and the users of large document collections. In this paper we describe a human evaluation of Kea , an automatic keyphrase extraction algorithm developed by members of the New Zealand Digital Library Project . Kea uses machine learning techniques to build a model that characterises document
265230	6071	New Zealand Digital Library Project. The algorithm is substantially simpler, and therefore less computationally intensive, than many previous approaches. Kea has been described in detail elsewhere , and its operation is summarised below. Kea uses a model to identify the phrases in a document that are most likely to be good keyphrases. This model must be learned from a set of training
265230	6071	match phrases which are considered to be ‘relevant.’ Author phrases are usually used as the set of relevant phrases, or the ‘Gold Standard.’ This approach was adopted in previous evaluations of Kea .sTable 1: Profile of phrases associated with each document 3DSHU $XWKRU 0HUJHG .HD 1XPEHU RI NH\SKUDVHV )RRG &RPELQHG /LVW 5 49 6 58 6 47 6 58 10 51 6 66 8 54 6 68 5 51 6 57 7 55 6 67 There are
265230	6072	2001 ACM 1-58113-345-6/01/0006…$5.00. 6WHYH -RQHV *RUGRQ : 3D\QWHU 'HSDUWPHQW RI &RPSXWHU 6FLHQFH 8QLYHUVLW\ RI :DLNDWR 3ULYDWH %DJ +DPLOWRQ 1HZ =HDODQG ^VWHYHM SD\QWHU`#FV ZDLNDWR DF Q] documents , search and browsing interfaces , retrieval engines  and thesaurus construction . Keyphrases are often chosen manually, usually by the author of a document, and
8919407	6074	and a search problem. We have separate ranked lists for each individual scan, and the goal is to merge them into a single ranked list. There are many approaches to aggregating ranked lists (eg,  These solutions mainly involve merging ranked lists from independent sources to ensure adequate performance. Since our problem involves merging ranked lists from highly dependant sources, and we
8919407	6075	and a search problem. We have separate ranked lists for each individual scan, and the goal is to merge them into a single ranked list. There are many approaches to aggregating ranked lists (eg,  These solutions mainly involve merging ranked lists from independent sources to ensure adequate performance. Since our problem involves merging ranked lists from highly dependant sources, and we
8919407	6078	that focuses on this kind of task. The challenge of linking paper and digital media has received substantial attention in the ubiquitous computing and human computer interaction literature (eg, ), but none of this work appears to be directly relevant to our task. For instance, in the efforts to link physical and Web artifacts , it is assumed that an explicit link between a
8919407	6081	that focuses on this kind of task. The challenge of linking paper and digital media has received substantial attention in the ubiquitous computing and human computer interaction literature (eg, ), but none of this work appears to be directly relevant to our task. For instance, in the efforts to link physical and Web artifacts , it is assumed that an explicit link between a
6082	6084	i.e. either the source or the destination of the traffic is located inside their network. These ASes are called stubs. As more than half of the stub ASes have several connections to the Internet , these stubs may want to evenly distribute the load of the traffic among their Internet links. As stub ASes must pay for their Internet connection, the economical cost of these connections can
6082	6085	problem is thus intrinsicallysa multiple-objectives optimization problem. For such problems, evolutionary algorithms are a well-known technique capable to find a non-dominated front in a single run . Recall that a front is a set of solutions and that a solution is said non-dominated if no other solution of the set is better in terms of all the considered objectives at the same time.
6082	6087	step is to check for non-domination on this population of improved individuals to obtain a non-dominated front. For that purpose, we rely on the fast non-domination check procedure introduced in . This procedure has time complexity 0(MN 2 ) where M is the number of objectives and N the size of the population. We do not describe this procedure in details but refer to  for the original
6082	6088	(ISP) rely on traffic engineering to optimize the flow of the traffic inside their network . While ISPs know their internal topology and techniques exist to tune the intradomain routing , most of them rely on manual tuning to do it. At the interdomain level, traffic engineering is even more challenging . Operators change their routing policies and the attributes of their BGP
6082	6088	ASes on the other hand do not have to pay providers but need to carefully distribute the load of their traffic inside their network. For that purpose, one way is to tune their intradomain routing . However, tuning the intradomain routing not only changes the distribution of the flow of the traffic inside the AS, but also their traffic demand, i.e. how traffic enters and leaves the network
6082	6090	is important in practice for ISPs to be able to automatically engineer the flow of their traffic with neighboring ASes. Having to do it manually often may lead to router misconfigurations  that exacerbate the stability of interdomain routing. In this paper, we present a multiple-objectives evolutionary algorithm especially designed to deal with interdomain traffic engineering with
6082	6091	their internal topology and techniques exist to tune the intradomain routing , most of them rely on manual tuning to do it. At the interdomain level, traffic engineering is even more challenging . Operators change their routing policies and the attributes of their BGP routes on a manual basis, without a proper understanding of the implications of such changes on the flow of the traffic.
6082	6093	of the traffic inside the AS. The first objective concerns interdomain routing. Given that there are many remote networks with which an AS exchanges traffic on timescales of hours to days , an interdomain traffic engineering technique should ideally minimize the number of reachable networks that need to be influenced. As the number of influenced networks corresponds to the number of
6094	6095	Ontologies; 2. it has a simple, clean and well defined first-order semantics; 3. automated reasoning support (e.g. class consistency and subsumption checking) can be provided. The FaCT system , a DL reasoner developed at the University of Manchester, can be (and has been) used to this end . It is envisaged that this core language will be extended in the future with sets of additional
6094	1023	other KR formalisms. 1. INTRODUCTION Currently, computers are changing from single isolated devices into entry points into a worldwide network of information exchange and business transactions (cf. ). Support in data, information, and knowledge exchange is becoming the key issue in current computer technology. Ontologies will play a major role in supporting information exchange processes in
6094	6098	of designing ontology-representation languages that are Webenabled only date from recent years. The most prominent (or even: the only) efforts in this area have been SHOE , Ontobroker , OIL and DAML-ONT 9 , and more recently, as a replacement for DAML-ONT, DAML+OIL 10 . Of these, only the last three have been defined on top of RDF(S). Since DAML+OIL is essentially a merger
6094	6099	RDF Schema specification ). We will show how a formal knowledge representation language can be used as the third, logical, layer. We will illustrate this by defining the ontology language OIL  as an extension of RDF Schema. OIL (Ontology Inference Layer), a major spin-off from the IST project On-To-Knowledge 1 , is a Web-based representation and 1 On-To-Knowledge: Content-driven
6094	6100	layer. We will illustrate this by defining the ontology language OIL  as an extension of RDF Schema. OIL (Ontology Inference Layer), a major spin-off from the IST project On-To-Knowledge 1 , is a Web-based representation and 1 On-To-Knowledge: Content-driven Knowledge-Management Tools through Evolving Ontologies (IST-1999-10132). http://www.ontoknowledge.org/sinference layer for
6094	6101	various areas. Many definitions of ontologies have been given in the last decade, but one that, in our opinion, best characterizes the essence of an ontology is based on the related definitions by : An onCopyright is held by the author/owner. WWW10, May 1-5, 2001, Hong Kong. ACM 1-58113-348-0/01/0005. † Aidministrator Nederland bv, Holland jeen.broekstra@aidministrator.nl ‡ Vrije Universiteit
6094	6102	of AI. However, efforts of designing ontology-representation languages that are Webenabled only date from recent years. The most prominent (or even: the only) efforts in this area have been SHOE , Ontobroker , OIL and DAML-ONT 9 , and more recently, as a replacement for DAML-ONT, DAML+OIL 10 . Of these, only the last three have been defined on top of RDF(S). Since DAML+OIL is essentially
6094	6104	of the class expressions in the list are equivalent (i.e. they have the same instances). The syntax of OIL is oriented towards XML and RDF.  defines a DTD and a XML schema definition for OIL.  derives an XML Schema for writing down instances of an OIL ontology. In this paper, we will derive the RDFS syntax of OIL. disjoint carnivore herbivore class-def mammal subclass-of animal class-def
6094	1028	the Semantic Web . At the lowest level of the Semantic Web a generic mechanism for expressing machine readable semantics of data is required. The Resource Description Framework (RDF)  is this foundation for processing metadata, providing a simple data model and a standardized syntax for metadata. Basically, it provides the language for writing down factual statements. The next
6094	1028	review some of their design decisions. 2.1 Introduction to RDF A prerequisite for the Semantic Web is machine-processable semantics of the information. The Resource Description Framework (RDF)  is a foundation for processing metadata; it provides interoperability between applications that exchange machineunderstandable information on the Web. Basically, RDF defines a data model for
6094	6105	of AI. However, efforts of designing ontology-representation languages that are Webenabled only date from recent years. The most prominent (or even: the only) efforts in this area have been SHOE , Ontobroker , OIL and DAML-ONT 9 , and more recently, as a replacement for DAML-ONT, DAML+OIL 10 . Of these, only the last three have been defined on top of RDF(S). Since DAML+OIL is essentially
6094	6107	one class, and thus giving it multiple qualities. Note that this way of representing qualities of properties in RDFS follows the proposed general approach of modeling axioms in RDFS, presented in . In this approach, the same distinction between language-level constructs and schema-level constructs is made. One alternative way of serializing the attributes of properties would be to define the
6094	6107	helper properties connecting them to the class expressions involved in the relation. Since axioms can be considered objects, this is a very natural approach towards modeling them in RDF (see also ). Observe also that binary relations (properties) are modeled as objects in RDFS as well (i.e., any property is an instance of the class rdf:Property). We simply introduce a new primitive alongside
6094	6108	helper properties connecting them to the class expressions involved in the relation. Since axioms can be considered objects, this is a very natural approach towards modeling them in RDF (see also ). Observe also that binary relations (properties) are modeled as objects in RDFS as well (i.e., any property is an instance of the class rdf:Property). We simply introduce a new primitive alongside
6094	6109	support (e.g. class consistency and subsumption checking) can be provided. The FaCT system , a DL reasoner developed at the University of Manchester, can be (and has been) used to this end . It is envisaged that this core language will be extended in the future with sets of additional primitives, with the proviso that full reasoning support may not be available for ontologies using
6110	6118	agents may output so many messages so quickly to a user’s screen that the screen scrolls to quickly to read. One example of this is the agent Cobot in the LambdaMOO Multi-user social environment . This agent could be coerced by a malicious user into outputting so many messages that it drowned out any other conversations in the nline space. Another example is the use of IRC (Internet Relay
6110	6118	abilities of semantic analysis and of knowledge about its environment, and, as  points out, may be computationally intractable. A final approach taken by (among others) the programmers of Cobot  is to allow users to limit or stop the behaviour of an agent with which they are interacting. Our approach, on the other hand, allows agents to limit themselves, without requiring them to perform
6119	6119	additionally includes the EndDate attribute. SQL source code for creation of tables and indexes together with the size estimation of the attribute values of the tables can be found in reference . The experiments were carried out using the Oracle DBMS (version 7.2.2.3). 5 Data Warehouse Size We proceed to compare the sizes of the three warehouses. The data warehouse size is given by the
6119	6119	longest execution time, which is given in seconds at the top of the appropriate column. Additional detail, including the actual tuned SQL queries and their execution plans, are provided elsewhere . 100 80 60 40 20 0.049 0.78 1.547 0.09 0.178 57.63 0.265 9.9 0.22 q.1.1 q.1.2 q.1.3 q.1.4 q.2.1 q.2.2 q.2.3 q.3.1 q.4.1 Figure 6: Query Execution Times in Percent of the Slowest Query Time-series
6119	6119	warehouse always tend to become more complex and take more time to execute, while queries on the time-series and temporal warehouses stay simpler and run faster. The extended version of this paper  elaborates on the issues discussed in this section. 7 Conclusions and Future Research Dimensional data modeling using star schemas is a typical approach when designing a relationally-based data
6119	6123	star schemas with regular star schemas, considering database size and query performance, as well as the ease of formulating queries. The temporal star schemas considered here capture valid time , the time when the facts stored in the database are true in the modeled reality, as an opposite to transaction time, which concerns when the facts are current in the database. The novel notion of
6119	5638	star schemas with regular star schemas, considering database size and query performance, as well as the ease of formulating queries. The temporal star schemas considered here capture valid time , the time when the facts stored in the database are true in the modeled reality, as an opposite to transaction time, which concerns when the facts are current in the database. The novel notion of
6119	1755	we want to record information that remains valid for a duration of time, e.g., when we want to record account balances in a bank. This paper studies the use of techniques from temporal databases  for solving these problems. It describes temporal star schemas and provides a case-based, empirical comparison of temporal star schemas with regular star schemas, considering database size and
6119	6129	table will be required. Another research direction is to investigate advanced join techniques such as the STARjoin using the STARindex  or other techniques involving precomputed joins . The performance gain resulting from the usage of precomputed joins would probably be more visible in the temporal warehouse than in the regular one, because temporal joins are more complex and
6130	6132	of the signature assigned to that user. Unfortunately, when the number of active users changes, the signatures must generally be recomputed and reassigned to maintain the interference invariance . Recently a class of signatures, known as Grassmannian signatures, were introduced that satisfy interference invariance even when subsets of the available users are active . This signature
6130	6132	any user k is simply SINRk = ??? ??1 2 s ? 2 v + ? d N ? d ? ? ?1 ?1 and the performance only depends on N and d. Unfortunately, interference invariance only occurs when the system is fully loaded , , i.e. N users are active. The reason is that a WBE set for N > d users almost always ceases to be a WBE set if any M<dsequences are removed from or added to the set . Thus if ¯ N < N users
6130	6134	WBE with the maximum correlation of the best line packing found for (d, N) without the tight frame constraint and the lower bound on the maximum correlation (5). remaining values of N . In the complex case, the algorithm was able to compute every equiangular tight frame that we know of. Unfortunately, no one has yet developed necessary conditions on the existence of complex,
6130	6135	CA 95616 USA strohmer@math.ucdavis.edu Unfortunately, signatures that are both equiangular and maximally spaced are quite rare. Some explicit constructions are available in the articles , , . Signatures derived from good packings in the Grassman manifold, even the best line packings tabulated by Sloane , do not generally satisfy the WBE property when they are not equiangular. In
6130	6135	Most of the current research has approached the design problem with algebraic tools. A notable triumph of this type is the construction of Kerdock codes over Z2 and Z4 due to Calderbank et al. . Other explicit constructions are discussed in the articles , . In the numerical realm, Sloane has used his Gosset software to produce and study sphere packings in real Grassmannian spaces
6130	6136	quite rare. Some explicit constructions are available in the articles , , . Signatures derived from good packings in the Grassman manifold, even the best line packings tabulated by Sloane , do not generally satisfy the WBE property when they are not equiangular. In cases where such signatures do not exist, we would be satisfied with a WBE whose constituent signatures are close to
6130	6136	Other explicit constructions are discussed in the articles , . In the numerical realm, Sloane has used his Gosset software to produce and study sphere packings in real Grassmannian spaces . Sloane’s algorithms have been extended to complex Grassmannian spaces in . We are not aware of any other numerical methods. Some examples of signatures that achieve the bound in (5) are
6130	6137	In cases where such signatures do not exist, we would be satisfied with a WBE whose constituent signatures are close to equiangular. Recently proposed numerical algorithms for finding WBEs (e.g., , , , ), however, do not easily incorporate equiangular side constraints. In this paper, we present an algorithm for finding Welch bound equality signature sequences that are exactly (or
6130	6138	cases where such signatures do not exist, we would be satisfied with a WBE whose constituent signatures are close to equiangular. Recently proposed numerical algorithms for finding WBEs (e.g., , , , ), however, do not easily incorporate equiangular side constraints. In this paper, we present an algorithm for finding Welch bound equality signature sequences that are exactly (or
6130	6139	where such signatures do not exist, we would be satisfied with a WBE whose constituent signatures are close to equiangular. Recently proposed numerical algorithms for finding WBEs (e.g., , , , ), however, do not easily incorporate equiangular side constraints. In this paper, we present an algorithm for finding Welch bound equality signature sequences that are exactly (or nearly)
6130	6140	such signatures do not exist, we would be satisfied with a WBE whose constituent signatures are close to equiangular. Recently proposed numerical algorithms for finding WBEs (e.g., , , , ), however, do not easily incorporate equiangular side constraints. In this paper, we present an algorithm for finding Welch bound equality signature sequences that are exactly (or nearly)
6130	6142	approach builds on our recently proposed iterative algorithm for constructing CDMA signature sequences , which has also been used to find signatures satisfying peak-to-average ratio constraints . The idea is to alternately solve two matrix nearness problems, one that finds the closest signature set satisfying Welch’s bound with equality and the other that finds the nearest set of
6130	6142	and discussed convergence of the algorithm. This algorithm can also be used to solve for unconstrained optimal CDMA signature sequences, sequences with peak-to-average power ratio side constraints , and spectrum constraints. A major issue with the proposed algorithm is that the resulting sequences are generally complex valued and this may lead to implementation challenges. Incorporating
6130	6143	Welch’s bound with equality and the other that finds the nearest set of equiangular signatures. This algorithm is related to a method used by Chu for solving an inverse eigenvalue problem . Our algorithm can also be used to find Grassmannian frames as well as equiangular tight frames. We argue that our algorithm converges to a fixed point, and we claim that the class of fixed points
6130	6144	that our algorithm converges to a fixed point, and we claim that the class of fixed points contains the desired sequences. Detailed proofs of these results are deferred due to space constraints . II. SIGNATURE DESIGN PRELIMINARIES Consider the uplink of a single cell, short code, synchronous CDMA system with N total signatures and a processing gain d. Let xk denote the d×1 signature, code,
6130	6144	the first d columns of U. Then ? UdUd ? is a matrix in G? that is closest to Z with respect to the Frobenius norm. This closest matrix is unique if and only if ?d strictly exceeds ?d+1. Proof: See  for details. Let H be a closed collection of N ×N Hermitian matrices that satisfy the structural constraint set motivated by the equiangular property: def Hµ = {H ? C N×N : H = H ? , diag H = m1
6130	6144	method such as  for example. VI. SUMMARY OF CONVERGENCE RESULTS The machinery of point-to-set maps is required to understand the convergence of this algorithm, so we must refer the reader to  for details. In this section we shall summarize the convergence results. A. Basic Convergence Results It should be clear that alternating projection never increases the distance between successive
6130	6144	point (Y , Z) satisfies ? ? Y ? Z ? ?F = lim j?? ?Yj ? Zj? F . • Every accumulation point (Y , Z) satisfies ? ? Y ? Z ? ?F =dist(Y , Z )=dist(Z, Y ). For a proof of Theorem 3, see the Appendix in . The convergence of the proposed algorithm is geometric at best , , , . This is the major shortfall of alternating projection methods. Note that the sequence of iterates may have
6130	6144	? Hj?F ? 0. • Either the component sequences both converge in norm, ? ?Gj ? G ? ? ? ? 0 and ?Hj ? H F ? ? ? 0, F or the set of accumulation points forms a continuum. Proof: See the Appendix in . VII. NUMERICAL EXPERIMENTS A. Example Construction First, let us illustrate just how significant a difference there is between vanilla signature matrices and equiangular signature matrices. Here
6130	6146	bound equality sequences  since they satisfy the Welch bound on the total squared correlation with equality.sWBE signature sequences have a number of nice properties as summarized in , . Perhaps the most interesting property is that, using WBE sequences, the interference is uniform across all users . The sum total interference in the system is given by ? ? k l?=k |?xk, xl?| 2
6130	6148	numerical realm, Sloane has used his Gosset software to produce and study sphere packings in real Grassmannian spaces . Sloane’s algorithms have been extended to complex Grassmannian spaces in . We are not aware of any other numerical methods. Some examples of signatures that achieve the bound in (5) are available in  but generally they are hard to find. The reason is that while good
8919416	6160	Methods As a training dataset in this study, we used TMPDB alpha non-redundant dataset, which includes 138 prokaryotic and 93 eukaryotic sequences with experimentally-characterized TM topologies . Completely sequenced genome data of 87 prokaryotic (15 archaebacterial and 72 bacterial) and 12 eukaryotic proteomes were also obtained from the public databanks. The combinations of five methods
916225	6164	(centralized) data mining. A variety of methods have been studied for combining models in an ensemble, including Bayesian model averaging and model selection , stacking , partition learning , and other statistical methods, such as mixture of experts . JAM employs meta-learning, while Kensington employs knowledge probing. Papyrus is designed to support different data, task and model
916225	2996	distributed data mining. Perhaps the most mature are: the JAM system developed by Stolfo et. al. , the Kensington system developed by Guo et. al. , and BODHI developed by Kargupta et. al. . These systems differ in several ways: Data strategy. Distributed data mining can choose to move data, to move intermediate results, to move predictive models, or to move the final results of a
916225	6170	have been used for quite a while in (centralized) data mining. A variety of methods have been studied for combining models in an ensemble, including Bayesian model averaging and model selection , stacking , partition learning , and other statistical methods, such as mixture of experts . JAM employs meta-learning, while Kensington employs knowledge probing. Papyrus is designed to
916225	3006	commodity networks. This section is based in part on . Several systems have been developed for distributed data mining. Perhaps the most mature are: the JAM system developed by Stolfo et. al. , the Kensington system developed by Guo et. al. , and BODHI developed by Kargupta et. al. . These systems differ in several ways: Data strategy. Distributed data mining can choose to move
916225	3006	models with a majority vote . Meta-learning combines several models by building a separate metamodel whose inputs are the outputs of the various models and whose output is the desired outcome . Knowledge probing considers learning from a black box viewpoint and creates an overall model by examining the input and the outputs to the various models, as well as the desired output .
8919418	6176	points to determine their performance. This paper covers only “spacewalking”—evaluation and synthesis issues are being investigated by other members of the CompilerandArchitectureResearchGroup. 1 Cache Ports: d=1,2,... i=1 s = 1, 2, .. u= max(1, s) FIGURE 1. Configuration of embedded computer system to be automatically designed. If both the VLIW processor and systolic array are present,
8919418	6177	points to determine their performance. This paper covers only “spacewalking”—evaluation and synthesis issues are being investigated by other members of the CompilerandArchitectureResearchGroup. 1 Cache Ports: d=1,2,... i=1 s = 1, 2, .. u= max(1, s) FIGURE 1. Configuration of embedded computer system to be automatically designed. If both the VLIW processor and systolic array are present,
8919418	6177	points to determine their performance. This paper covers only “spacewalking”—evaluation and synthesis issues are being investigated by other members of the CompilerandArchitectureResearchGroup. 1 Cache Ports: d=1,2,... i=1 s = 1, 2, .. u= max(1, s) FIGURE 1. Configuration of embedded computer system to be automatically designed. If both the VLIW processor and systolic array are present,
8919418	6178	points to determine their performance. This paper covers only “spacewalking”—evaluation and synthesis issues are being investigated by other members of the CompilerandArchitectureResearchGroup. 1 Cache Ports: d=1,2,... i=1 s = 1, 2, .. u= max(1, s) FIGURE 1. Configuration of embedded computer system to be automatically designed. If both the VLIW processor and systolic array are present,
6182	6186	feedback equalizer (DFE) is also possible and desirable, since DFEs generally outperform LEs for the same complexity. A generic strategy for blind DFE initialization in such setting was proposed in ; see also . We develope a similar strategy for blind iterative DFE computation in block transmission systems with quadrature amplitude modulation (QAM). An iterative CMA-based LE for
6182	6187	filter seen ? in (4) is imposed in order to avoid the trivial ? ? , ? £ £?? ? ??¤ . The solution to this minimization problem can be solution? ? ? given in ??????? terms © of the channel?????? £ : ????? ? ????? ¤ ? £ ???? ? ????? ? ¤ ? £ ???? ? §?? ???? (5) ??? ????????? (6) ? ? ????????????????? ? where denotes the ‘paraconjugate’ of???????,i.e. ¤ ??,and ?? ??? ??????? ???????¤£ ? ? ? ,
6182	6187	have proposed a block-oriented bidirectional DFE, which uses a data-dependent soft nonlinearity instead of a hard decision device. In systems incorporating channel coding, turbo equalizers ), which exploit information from the error correction decoder, can also be used. Both approaches are characterized by a symbol-by-symbol recomputation of the FFF of the equalizer, which amounts to
6182	6188	energy. In this way, if ????? ????? ? the? the resulting FFF is seen as degree-? a , causal FIR filter, we ? ????? ?? would have ? (15) A similar energy maximization approach was suggested in  in order to resolve delay ambiguities in a blind channel identification setting. ¥ ? ????? ? ? ? ? ?© ????????? ? ?? 7. DECISION FEEDBACK EQUALIZATION STAGE With the initialization procedure
6182	6190	for blind iterative DFE computation in block transmission systems with quadrature amplitude modulation (QAM). An iterative CMA-based LE for packet-based systems was recently proposed by Regalia in . Its computational simplicity makes it an obvious candidate for the first iterations, during which hard decisions would not be reliable. Once the eye has been opened the DFE mode is switched on,
6182	6190	adaptation of a DFE from a cold start (unless the channel is sufficiently mild), and therefore we initially resort to an LE. In particular,? iterations of Regalia’s CM block-oriented algorithm  are run initially. We briefly describe the process here.sLetsbe the ¡£¢¥¤ vector comprising the LE taps, so that the equalizer output is £¨§ ¡???©?? ???s¢¡?? ? ?????s£?? ? Let ? £?§ ? ??? ¦¡ , and
6182	6193	estimation. For example, the fourth power estimator is well suited ? ? for QAM constellations, and it is known to yield ?? almost maxi? ? mum likelihood performance in the limit of small SNR (see  ??? ? for a performance analysis of several blind phase estimation techniques). It is given by ? ¤ ????????????? ? § ? £ ? ? ¤ ? ¡ ? ¦ ? ¡?? ? (3) ? ¡¨ As with any blind procedure, a residual phase
6182	6194	have proposed a block-oriented bidirectional DFE, which uses a data-dependent soft nonlinearity instead of a hard decision device. In systems incorporating channel coding, turbo equalizers ), which exploit information from the error correction decoder, can also be used. Both approaches are characterized by a symbol-by-symbol recomputation of the FFF of the equalizer, which amounts to
6197	6204	using some kind of policy. Adaptive (persistent) locking, which introduces a time-limit and expiry policy on transactions, can be used to deal with repetition of an event over shorter time granules. The transaction locking mechanism, used in cfengine for instance, can break cycles in many graphs. They are set by two parameters: IfElapsed = ( num_of_minutes ) ExpireAfter = ( num_of_minutes )
6197	6206	events and as a general matter of the maintenance of the system. Policy based administration, employing agents or software robots, is an administrative method which scales to large numbers of hosts in distributed systems, precisely because every host is essentially responsible for its own state of configuration. However, the inter-dependencies of such networked machines mean that
6197	6208	the end, of the graph. Note that there is a relationship between the sequences generated during scheduling, and the paths obtained during reachability and data-flow analysis of distributed systems . 6.3 Decentralized scheduling and global goals Configuration management tools administer checks and controls to a distributed network of hosts. The policy broker is the agent located at each host,
6197	6213	such as misunderstandings between humans, undisciplined maintenance and regular usage. Throughout any change, one is interested in upholding consistency of environment for any dependent processes. The aim of the current work is to summarize the core of an approach to system management, based on regulation of system state. 2 Configuration management and cfengine Configuration management is
3748359	6245	time performance to O(n log 2 n/ log log n + k) has been proposed by . The first O(n log n + k) algorithm due to  used O(n + k) storage. The O(n log n + k) algorithm with O(n) space is due to . In typical travel networks, the number of edges are proportional to the number of nodes, because the node degree is bounded by a small constant. If one can assert that the graph is planar – as in
3748359	6249	line algorithm  and the divideand-conquer strategy of separating paths . Building a query structure on top of the Voronoi diagram is available in O(log n) time by hierarchical subdivision . Probably the best practical option to generate the Voronoi diagram is via its geometric dual - the Delaunay triangulation - since it yields a simple randomized strategy in expected time O(n log n)
3748359	4417	queries based on geometric cuts can be made available even in negatively weighted graphs through h. VIII. GPS-ROUTE We have built our generic system GPS-ROUTE 8 on top of the LEDA algorithm library  that supports accurate and efficient geometry and graph algorithms. To read GPS data, we wrote a GPS trace parser that generates the set of LEDA points and segments and that extends the existing
3748359	6254	of rectangles. Using a sweep line algorithm, this problem can be solved in O(n log n + k) time, where n is the total number of rectangles of both types and k is the number of intersections reported . The second approach that we called as Exploration-time Checking or on-line approach utilizes the observation that it is possible that some of the constraints have terminated and no longer be there
3748359	6255	XII. RELATED WORK This paper does not address the issue of statistical clustering of GPS data to automatically infer a map by condensing the data set through road centerlines and clustering like . In the context of car navigation, lane-precise maps are inferred. The work provides a domain dependent system that automatically generates digital road maps that are significantly more precise and
3748359	4436	evaluation of shortest path computations based on multi-level graph decomposition. An experimental study of the impact of geometric pruning cuts for the setting of train graphs is presented in . In the algorithm portfolio, bounding boxes appear to be superior to annotations of angular sectors. Bounding-box pruning extends early observations of , where angular sectors of all shortest
3748359	4426	setting of train graphs is presented in . In the algorithm portfolio, bounding boxes appear to be superior to annotations of angular sectors. Bounding-box pruning extends early observations of , where angular sectors of all shortest paths that pass an considered edge were used. Similar to pattern databases , shortest path bounding boxes are memory intense approximations of shortest
3748359	6258	graph exploration, by referring to a Fibonacci heap implementation and bounded node degree. As in the current implementation the constructed graph is planar, using the graph separator algorithm of  would lead to a theoretically faster algorithm, with linear run time for nonnegative edge cost. This reduces the pre-computation time to O(n 2 ). However, to the best of the authors’ knowledge,
3748359	4420	(due to bridges or tunnels). Linear time algorithms for a broader graph classes have mostly be devised for restricted weight functions only . Recent experimental results on shortest path search  have also only limited impact on our work, since the authors consider undirected graphs and multiple queries to the same target only. For the case of search in a dynamic scenario  presented an
3748359	4437	path search  have also only limited impact on our work, since the authors consider undirected graphs and multiple queries to the same target only. For the case of search in a dynamic scenario  presented an approach where the authors have obtained a speed-up of up to a factor of three in re-computing the bounding boxes in case of a change in the weight of an edge. The main idea behind
3748359	6260	the dynamics of our route planning system. The former issue calls for external shortest path graph search algorithms  that exploit the spatial structure to minimize secondary memory accesses . ACKNOWLEDGMENTS Thanks to Lioudmila Belenkaia for links to trace data reduction. The work is supported by the DFG under grants ED 74/2-1 and ED 74/3-1, by the Human Potential Programme of the
6261	6263	a reasonable value in between 1 and N. This can be achieved by having buffers at both the input and output ports. IQ and CIOQ switches are employed in many high speed switch architectures, e.g.,  - . One key factor in achieving high performance using VOQ switches is the scheduling algorithm that is responsible for the selection of packets to be transmitted from the input queues to the
6261	6263	approaches to this scheduling problem depending on the level at which the scheduling is done: cell scheduling and rate reservation based scheduling. In the approaches based on cell scheduling (- ), the problem of finding the appropriate connections between the inputs and the outputs of a crossbar is posed as a matching problem in a bipartite graph. Every service time slot, a request
6261	6263	are updated every link time slot and the new matching which maximizes some objective function is found subject to the crossbar constraint. The objective may be to achieve a stable marriage match (, ), to achieve a maximal match (, ) or to maximize the sum of edge weights which are in the connect graph (, ). Edge weights for each VOQ are usually chosen to be the queue size
6261	6268	the new matching which maximizes some objective function is found subject to the crossbar constraint. The objective may be to achieve a stable marriage match (, ), to achieve a maximal match (, ) or to maximize the sum of edge weights which are in the connect graph (, ). Edge weights for each VOQ are usually chosen to be the queue size (-), the delay experienced by the
6261	6268	utilized and no output link is oversubscribed. It has also been shown that with a speedup of 2, work conservation and certain delay guarantees can be achieved with a maximal matching algorithm (). Rate reservation based algorithms were originally proposed for circuit switches in traditional voice networks to provide constant bit rate (CBR) guarantees for voice traffic that is rather static
6261	6268	source of rate r, this delay is N (S?1)r over that for an output queued switch. This delay is a constant factor lower than the delay for such a source with the maximal matching algorithm given in  and a speedup of 2. However, we emphasize that rate quantization provides this QoS with any speedup between 1 and 2. • With rate quantization, the schedule generated by any scheduler is periodic.
6261	6271	approaches to this scheduling problem depending on the level at which the scheduling is done: cell scheduling and rate reservation based scheduling. In the approaches based on cell scheduling (- ), the problem of finding the appropriate connections between the inputs and the outputs of a crossbar is posed as a matching problem in a bipartite graph. Every service time slot, a request graph
6261	6271	matching which maximizes some objective function is found subject to the crossbar constraint. The objective may be to achieve a stable marriage match (, ), to achieve a maximal match (, ) or to maximize the sum of edge weights which are in the connect graph (, ). Edge weights for each VOQ are usually chosen to be the queue size (-), the delay experienced by the packet
6261	6272	using a priori knowledge. A number of rate reservation based scheduling algorithms have been proposed to provide rate guarantees for multirate circuit switches. The BATCH-TSA algorithm proposed in  is a rate reservation based scheduling algorithm that guarantees bounded service lag. The algorithm treats the switch as a time division multiple access (TDMA) network and the problem of providing
6261	6277	desired rate of that user. Users are assumed to be backlogged all the time and the finishing times of the next unserved token for each user in the corresponding Generalized 2 PGPS is introduced in  Processor Sharing (GPS) system is calculated starting at time 0. It is illustrated in  that, with a fairly large class of schedulers, a maximum service lag of O(N 2 ) is unavoidable for input
6311	6312	project 34672/99 and the European Commission, project SmartSketches IST-2000-28169.sIn the past years there have been some research works in retrieving drawings. Gross’ Electronic Cocktail Napkin  addressed a visual retrieval scheme based on diagrams, to indexing databases of architectural drawings. Berchtold’s S3 system  supports managing and retrieving industrial CAD parts, through
6311	6315	industrial CAD parts, through contour matching. Park’s approach  retrieves mechanical parts based on dominant shapes and spatial relationships. Leung proposed a sketch retrieval method  for general unstructured free-form hand-drawings. We can observe two things from existing content-based retrieval systems for drawings. The first is scalability: most published works use databases
6311	6317	partial drawing matches, we also compute descriptors for sub-graphs of the main graph. Moreover, we use a new way to describe drawings hierarchically, by dividing them in different levels of detail  and then computing descriptors at each level. This combination of sub-graphs descriptors and levels of detail, provides a powerful way to describe and search both for drawings or sub-parts of
6311	6318	and search both for drawings or sub-parts of drawings, which is a novel feature of our work. To acquire geometric information about drawings we use a general shape recognition library called CALI . This enables us to use either drawing data or sketches as input, which is a desirable feature of our system. We use CALI to compute a set of geometric features such as area and perimeter ratios
6311	6319	area enclosing rectangle, among others. Using geometric features instead of polygon classification, allows us to index and store potentially unlimited families of shapes. Experimental evaluation  revealed that this technique outperforms other methods to describe shapes, such as Fourier descriptors, grid-based descriptors or Delaunay triangulation, yielding better precision figures for all
6311	6320	to our method. In an elegant fashion two types of information (vector drawings + sketches) are processed by the same pipeline. We developed a new multidimensional indexing structure, the NB-Tree , which provides an efficient indexing mechanism for high-dimensional data points. The NB-Tree is a simple, yet efficient indexing structure, using dimension reduction. It maps multidimensional
6311	6320	the Pyramid Technique. From Figure 3 we can see that the NB-Tree outperforms all the structures evaluated when data dimension and data set size increases. A more detailed evaluation can be found in . Time (sec) 25 20 15 10 5 SR-Tree A-Tree Pyramid NB-Tree 0 0 50 100 150 Dimension 200 250 300 Time (sec) 30 25 20 15 10 5 SR-Tree Pyramid NB-Tree 0 250000 500000 750000 1e+06 Dataset Size Fig. 3.
6321	6322	cost. The effect of patching on server load has not been studied. A different approach to content delivery is the use of periodic broadcasting of encoded content as was done over broadcast disks  using IDA , and more recently using the Digital Fountain approach which relies on Tornado encoding , . These techniques enable end-hosts to reconstruct the original content of size n
6321	6323	the use of periodic broadcasting of encoded content as was done over broadcast disks  using IDA , and more recently using the Digital Fountain approach which relies on Tornado encoding , . These techniques enable end-hosts to reconstruct the original content of size n using a subset of any n symbols from a large set of encoded symbols. Reliability and a substantial degree of
6321	6324	is the use of periodic broadcasting of encoded content as was done over broadcast disks  using IDA , and more recently using the Digital Fountain approach which relies on Tornado encoding , . These techniques enable end-hosts to reconstruct the original content of size n using a subset of any n symbols from a large set of encoded symbols. Reliability and a substantial degree of
6321	6325	VI. RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on dedicated channels, and asynchronous clients join one or more
6321	6326	by other nodes in the system who request the feed within some bounded delay. This process of caching and relaying the content was shown to scale well in terms of server as well as network loads. In , a detailed analysis of this approach was presented. There are two problematic aspects of the cache-andrelay approach. First, when a node leaves the system, any other nodes receiving the feed from
6321	6326	with independent arrivals and departures. Our analysis is backed up by extensive simulations, which demonstrate the superiority of dPAM when compared to the cacheand-relay approach studied in , . Fig. 1. Overlay-based asynchronous streaming II. PRE-FETCH-AND-RELAY: DETAILS In this section, we present the Pre-fetch-and-relay strategy for asynchronous delivery of streams through overlay
6321	6326	(as discussed in Section III-D). We also compare the performance of Pre-fetch-and-relay with Cache-and-relay (CR) with respect to savings in server bandwidth. We refer to the protocols oStream  and OSMOSIS  by the generic term Cache-and-relay because they correspond to the situation when ? =1(hence, a client cannot pre-fetch any content). C. Server Load Let event S represent the
6321	6326	able to locate another source in the peer-to-peer network. In this section, we analytically compare the performance of our Pre-fetchand-relay scheme against the Cache-and-relay scheme proposed in . Suppose R0 starts downloading the stream from R1 upon arrival. Let the inter-arrival time between R0 and R1 be w1 and the time spent by Ri (i =0, 1) downloading the stream be ti. Under the
6321	6326	to support a particular request arrival rate decreases, both in Cache-and-relay as well as Prefetch-and-relay (for all values of ?). This observation is in agreement with the results obtained in . The amount of time that clients spend downloading a stream is an important factor in determining server bandwidth requirements. Peer-to-peer network based asynchronous media content distribution
6321	6327	RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on dedicated channels, and asynchronous clients join one or more broadcasting
6321	6327	indicates that the delays were considered. We assume that the total delay involved in switching the streaming session to another client after a departure is uniformly distributed over the interval  to generate the results of Figures 12 and 13. With a buffer size of 10 time units and ? = 100, 000, a client will have 9.999 time units of “future” content in its buffer after it achieves the ratio
6321	6327	after incorporating the various delays into our model. In Figure 12, with ? =1, clients have 5 time units of “future” content whereas the delays involved are uniformly distributed over the interval . Hence, in a significant number of cases, a client would be forced to download the stream from the server after a departure because it would be unable to find another client to 5 As discussed in
6321	6327	The approach of patching  allows asynchronous client requests to “catch up” with an ongoing multicast session by downloading the missing portion through server unicast. In merging techniques , clients merge into larger and larger multicast session repeatedly, thus reducing both the server bandwidth and the network link cost. These 6 In our dPAM scheme, a node arriving at t0 needs to
6321	6329	protocols, ranging from widely deployed controlled flooding protocols (e.g., gnutella ) to more scalable DHT-based protocols (e.g., CAN  and CHORD ), or recently proposed hybrids thereof . Upon joining (leaving) the asynchronous multicast of a feed f, a node would advertise the availability of its cached portion of f by adding (removing) the appropriate reference string into (from)
6321	6330	VI. RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on dedicated channels, and asynchronous clients join one or more broadcasting
6321	6331	EIA-0202067 and ITR ANI-0205294. Technical Report BUCS-TR-2004-026 practical. Also, using asynchronous multicast approaches requiring multicast capabilities (e.g., periodic broadcasts , , ) is not practical. For starters, the server may not even realize that the feed it is sharing with its P2P community is popular enough that it is “worth” multicasting! Finally, assuming that
6321	6331	Such prefetching is possible if we assume that a node can retrieve content at a rate higher (even if by a small factor) than the playout rate. Such an assumption is common (and realistic) , , . This “lookahead” buffering capability provides each node with an opportunity to recover from the premature departures of its source. Not only does this allow the node to avoid a disruption
6321	6331	name space), the node will resort back to the server. VI. RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on dedicated channels,
6321	6333	EIA-0202067 and ITR ANI-0205294. Technical Report BUCS-TR-2004-026 practical. Also, using asynchronous multicast approaches requiring multicast capabilities (e.g., periodic broadcasts , , ) is not practical. For starters, the server may not even realize that the feed it is sharing with its P2P community is popular enough that it is “worth” multicasting! Finally, assuming that every
6321	6333	prefetching is possible if we assume that a node can retrieve content at a rate higher (even if by a small factor) than the playout rate. Such an assumption is common (and realistic) , , . This “lookahead” buffering capability provides each node with an opportunity to recover from the premature departures of its source. Not only does this allow the node to avoid a disruption of its
6321	6333	the P2P name space), the node will resort back to the server. VI. RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on dedicated
6321	6334	t0). techniques rely on the availability of a multicast delivery infrastructure at the lower level. The idea of utilizing client-side caches has been proposed in several previous work , , . The authors of , propose an overlay, multicast strategy, oStream, that leverages client-side caching to reduce the server bandwidth as well the network link cost. Assuming the client arrivals
6321	6335	nodes must be treated as new arrivals, which in turn presents added load to the server (and network). This latter issue is especially significant because recent results by Jin and Bestavros  have shown that asynchronous multicast techniques do not scale as advertised when 1scontent is not accessed from beginning to end (e.g., due to nodes prematurely leaving the multicast and/or when
6321	6336	in receiving the content, but not beyond. Leveraging Local Storage for Scalable Asynchronous Multicast in P2P Systems: Recently Jin and Bestavros proposed a scalable “cache-and-relay” approach  that could be used for scenarios similar to the one motivated above. Using this approach, a recipient of the feed would “cache” the most recently played out portion of the feed (after playing it
6321	6336	with independent arrivals and departures. Our analysis is backed up by extensive simulations, which demonstrate the superiority of dPAM when compared to the cacheand-relay approach studied in , . Fig. 1. Overlay-based asynchronous streaming II. PRE-FETCH-AND-RELAY: DETAILS In this section, we present the Pre-fetch-and-relay strategy for asynchronous delivery of streams through overlay
6321	6336	in Section III-D). We also compare the performance of Pre-fetch-and-relay with Cache-and-relay (CR) with respect to savings in server bandwidth. We refer to the protocols oStream  and OSMOSIS  by the generic term Cache-and-relay because they correspond to the situation when ? =1(hence, a client cannot pre-fetch any content). C. Server Load Let event S represent the situation that a
6321	6336	a Cache-and-relay strategy, and hence, does not incorporate patching techniques to reduce server bandwidth when the download rate is high. The main objective of the protocol, OSMOSIS, proposed in  is to reduce the network link cost. The effect of patching on server load has not been studied. A different approach to content delivery is the use of periodic broadcasting of encoded content as
6321	6337	space), the node will resort back to the server. VI. RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on dedicated channels, and
6321	6338	of patching on server load has not been studied. A different approach to content delivery is the use of periodic broadcasting of encoded content as was done over broadcast disks  using IDA , and more recently using the Digital Fountain approach which relies on Tornado encoding , . These techniques enable end-hosts to reconstruct the original content of size n using a subset of
6321	6339	? W, t0). techniques rely on the availability of a multicast delivery infrastructure at the lower level. The idea of utilizing client-side caches has been proposed in several previous work , , . The authors of , propose an overlay, multicast strategy, oStream, that leverages client-side caching to reduce the server bandwidth as well the network link cost. Assuming the client
6321	939	be done using any number of existing P2P content location protocols, ranging from widely deployed controlled flooding protocols (e.g., gnutella ) to more scalable DHT-based protocols (e.g., CAN  and CHORD ), or recently proposed hybrids thereof . Upon joining (leaving) the asynchronous multicast of a feed f, a node would advertise the availability of its cached portion of f by
6321	943	any number of existing P2P content location protocols, ranging from widely deployed controlled flooding protocols (e.g., gnutella ) to more scalable DHT-based protocols (e.g., CAN  and CHORD ), or recently proposed hybrids thereof . Upon joining (leaving) the asynchronous multicast of a feed f, a node would advertise the availability of its cached portion of f by adding (removing)
6321	6341	ANI-9986397, EIA-0202067 and ITR ANI-0205294. Technical Report BUCS-TR-2004-026 practical. Also, using asynchronous multicast approaches requiring multicast capabilities (e.g., periodic broadcasts , , ) is not practical. For starters, the server may not even realize that the feed it is sharing with its P2P community is popular enough that it is “worth” multicasting! Finally, assuming
6321	6341	future. Such prefetching is possible if we assume that a node can retrieve content at a rate higher (even if by a small factor) than the playout rate. Such an assumption is common (and realistic) , , . This “lookahead” buffering capability provides each node with an opportunity to recover from the premature departures of its source. Not only does this allow the node to avoid a
6321	6341	the P2P name space), the node will resort back to the server. VI. RELATED WORK Delivery of streams to asynchronous clients has been the focus of many studies, including periodic broadcasting , , ,  and stream patching/merging techniques , , , . In periodic broadcasting, segments of the media object (with increasing sizes) are periodically broadcasted on
6352	6355	this paper, we consider support vector machines (SVMs) as our classifier since it is known to have good classification performance in the biological domain, such as gene expression profile analysis  and translation initiation site prediction in DNA sequences . SVMs is a kind of blend of linear modeling and instance-based learning . It originates from research in statistical
6352	6356	these data points, b and ?0 i are parameters to be determined. The training of a SVM is a quadratic programming and here, we omit the detailed description about this. Please refer to the tutorial  for a better understanding of SVMs. There are several ways to train support vector machines. One of the fastest algorithms was developed by Platt , which solves the above quadratic programming
6352	6359	this paper, we consider support vector machines (SVMs) as our classifier since it is known to have good classification performance in the biological domain, such as gene expression profile analysis  and translation initiation site prediction in DNA sequences . SVMs is a kind of blend of linear modeling and instance-based learning . It originates from research in statistical
6352	2249	initiation site prediction in DNA sequences . SVMs is a kind of blend of linear modeling and instance-based learning . It originates from research in statistical learning theory . An SVM selects a small number of critical boundary samples (called support vectors) from each class of training data and builds a linear discriminant function that separates them as widely as
6352	6370	We generate a feature space using k-gram (k = 1, 2, 3, ...) nucleotide acid patterns. A k-gram is simply a pattern of k consecutive letters, which can be nucleotide symbols or amino acid symbols . At first, we use each k-gram nucleotide acid pattern as a new feature. For example, AT is a 2-gram pattern and ATC is a 3-gram pattern. In order to separate upstream and downstream sequence
6352	6371	the cleavage stimulation factor (CStF), the cleavage factors I and II (CF I and CF II), and poly(A) polymerase (PAP) in most cases. CPSF, PAP and poly(A) binding protein 2 are involved in poly(A) . The assembly of the cleavage/poly(A) complex, which contains most or all of the processing factors and the substrate RNA, occurs cooperatively. CPSF consists of four subunits and binds to the
6352	6371	CstF. The DSE is poorly conserved and two main types have been described as a U-rich, or GU-rich element, which locates 20 to 40 bases downstream of the cleavage site (for reviews, please refer to ).sPASPrediction 85 DSE is present in a large proportion of genes and can affect the efficiency of cleavage . Although in few cases, an upstream element (denoted as USE) is required for the
6352	6371	unexpected flexibility and their activity depends on not only the hexameric signal but also the up/down elements. Figure 1 is a schematic representation of PAS in human mRNA 3’end processing site . USE PAS PolyA site 5’ U-rich AAUAAA CA G/U-rich 3’ 10-30 bases Figure 1: Schematic representation of PAS in human mRNA 3’end processing site. Distances are as described in . There are several
6352	6372	since it is known to have good classification performance in the biological domain, such as gene expression profile analysis  and translation initiation site prediction in DNA sequences . SVMs is a kind of blend of linear modeling and instance-based learning . It originates from research in statistical learning theory . An SVM selects a small number of critical boundary
10088247	6374	in parallel implementations of direct element-by-element or patch-by-patch solution techniques that are possible on spacetime meshes that conform to an appropriate causality cone constraint . We consider applications to linearized elastodynamics  as well as to nonlinear conservation laws. Our implementation is based on the Charm++  Finite Element Method (FEM) framework . The
10088247	6376	to an appropriate causality cone constraint . We consider applications to linearized elastodynamics  as well as to nonlinear conservation laws. Our implementation is based on the Charm++  Finite Element Method (FEM) framework . The FEM framework provides automatic mesh partitioning. In addition, it updates boundary conditions and boundary element data across the partitioned mesh.
8919440	6380	Agency site (Leon version ) and we address label instr 0 zz instr0 1 instr1 2 brcond ss 3 tt instr3 4 instr4 5 brcond zz 6 brcond tt 7 ss instr7 line value of cond Prog Counter values 2 true  2 false  5 then 6 false then false  5 then 6 true then false  5 then 6 false then true  5 then 6 true then true [3, 4, 5, 6, 0,
8919440	6383	Obviously this does not allow to deal with big automata. We shall give some details on the techniques used to describe the source FSM. The techniques used to compute this expansion are presented in  and this present contribution is not about such C. A. D tool but about a new way to use it. 2.1. How do we describe ? All the descriptions are given in the language LUSTRE ( and ). LUSTRE
8919440	6383	instr0 1 instr1 2 brcond ss 3 tt instr3 4 instr4 5 brcond zz 6 brcond tt 7 ss instr7 line value of cond Prog Counter values 2 true  2 false  5 then 6 false then false  5 then 6 true then false  5 then 6 false then true  5 then 6 true then true  Figure 3. A SPARC program with intricated branches and the
8919440	6384	can be different (cooperating) automata. The language is such that, basically, all the automata share the same clock. Due to this feature, LUSTRE is often referred to as a synchronous language (). 2.2. An example of describing circuits in Lustre Here is an example of a n bits adder and a n bus multiplexor.X:boolˆn definesXbeing a bus of n wires, each of which being a boolean. The least
8919440	6384	instr0 1 instr1 2 brcond ss 3 tt instr3 4 instr4 5 brcond zz 6 brcond tt 7 ss instr7 line value of cond Prog Counter values 2 true  2 false  5 then 6 false then false  5 then 6 true then false  5 then 6 false then true  5 then 6 true then true  Figure 3. A SPARC program with intricated branches and the
8919440	5136	are presented in  and this present contribution is not about such C. A. D tool but about a new way to use it. 2.1. How do we describe ? All the descriptions are given in the language LUSTRE ( and ). LUSTRE looks like Lola, the language used by Wirth in his book. () Description may be of different types : Circuits described as a set of nodes : the nodes contain ¡ logic gates and
8919440	5136	Agency site (Leon version ) and we address label instr 0 zz instr0 1 instr1 2 brcond ss 3 tt instr3 4 instr4 5 brcond zz 6 brcond tt 7 ss instr7 line value of cond Prog Counter values 2 true  2 false  5 then 6 false then false  5 then 6 true then false  5 then 6 false then true  5 then 6 true then true [3, 4, 5, 6, 0,
8919440	5136	address 6 are both true, the sequence of values of the Program Counter is 3, 4, 5, 6, 0, 3. 3.4. Boolean level description Let us recall that the automata computing facility of the Lustre compiler ,  can compute the minimal automaton from one of its descriptions. For instance a description given in logic gates and flip-flops. To use this facility, we restricted the data path to 3 address
8919440	5137	presented in  and this present contribution is not about such C. A. D tool but about a new way to use it. 2.1. How do we describe ? All the descriptions are given in the language LUSTRE ( and ). LUSTRE looks like Lola, the language used by Wirth in his book. () Description may be of different types : Circuits described as a set of nodes : the nodes contain ¡ logic gates and
8919440	6386	of proof would have needed a specification of the delayed branch mechanism. Such a specification is difficult to establish. A proof of the implementation of a delayed branch mechanism appears in  and is based on the use of the theorem prover P.V.S. () We can certainly not give general conclusions from this study : in particular it is unrealistic to try to generalize such a study to a
6397	6404	reasoning that employs a graphical structure to capture explicit dependencies among domain variables . Belief networks are the basis of diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network
6397	6408	diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network applications. More generally, applied statisticians have recognized the importance of graphical structures and the fundamental notion of
6397	6408	of probabilistic inference. 4.1 Dependency Structure Speci cation Classical work in belief-network speci cation algorithms has focused explicitly on the construction of models for static domains . These algorithms automate the speci cation of belief networks from large databases of domain information. We can apply these same algorithms to specify models of dynamic domains. We decompose the
6397	6408	maximum likelihood estimates of these probabilities. An alternative approach is to search over all possible dependency structures for the one that maximizes the likelihood of the observed data . In contrast, classical approaches for specifying the domain dependencies, such as AR models, dynamic linear models, or transfer-function models, use cross correlations between the variables to
6397	6409	domain variables . Belief networks are the basis of diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network applications. More generally, applied statisticians have recognized the
6397	6413	domain variables . Belief networks are the basis of diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network applications. More generally, applied statisticians have recognized the
6397	6409	e. Probabilistic inference in large multiply-connected belief networks is di cult. Complexity analyses show that both exact and approximate algorithms pose intractable problems in the worst case . Nevertheless, for many problems, inference approximation procedures provide useful estimates of posterior probabilities in acceptable computation times. 3.2 Additive Belief-Network Models There
6397	6409	converges to these parameters. 4.3 Inference Algorithms For complex applications, the size of belief-network models for dynamic domains prohibits tractable computation of inference probabilities . Dynamic network models, however, 17sinherit the properties of additive belief-network models . As a result, dynamic network models bene t from an exact inference algorithm that exploits the
6397	6421	reasoning that employs a graphical structure to capture explicit dependencies among domain variables . Belief networks are the basis of diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network
6397	6421	domain variables. E ective diagnostic reasoning systems use belief networks to assign probabilities to alternative hypotheses about a patient's health|for example, MUNIN , ALARM , Path nder , VPnet , and QMR-DT |or about the source of failure in complex machinery, including jet engines, electric power generators, and copy machines . These applications of belief networks
6397	6425	a state-space forecasting model. Since then, few techniques have been developed to address the forecasting problem directly. Several approaches for probabilistic reasoning about change over time  and for temporal reasoning with belief networks and in uence diagrams  have been proposed. Real-world applications of forecasting with belief networks include forecasting crude-oil
6397	3106	diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network applications. More generally, applied statisticians have recognized the importance of graphical structures and the fundamental notion of
6397	3106	of probabilistic inference. 4.1 Dependency Structure Speci cation Classical work in belief-network speci cation algorithms has focused explicitly on the construction of models for static domains . These algorithms automate the speci cation of belief networks from large databases of domain information. We can apply these same algorithms to specify models of dynamic domains. We decompose the
6397	3106	dependency structure exhibits an implicit dependency between Xt,2 and Yt. (b) This structure exhibits a spurious correlation between Xt,1 and Yt. instantiate these nodes, we break their dependency . Thus, two nodes X and Y are explicitly dependent if and only if there does not exist a set of nodes S, such that Pr = Pr: (5) We can determine the dependency structure of a
6397	6431	diagnostic systems for many real-world applications . Techniques for probabilistic inference in belief networks  and for speci cation of belief networks  have emerged in response to the diversity of belief-network applications. More generally, applied statisticians have recognized the importance of graphical structures and the fundamental notion of
6397	6431	of probabilistic inference. 4.1 Dependency Structure Speci cation Classical work in belief-network speci cation algorithms has focused explicitly on the construction of models for static domains . These algorithms automate the speci cation of belief networks from large databases of domain information. We can apply these same algorithms to specify models of dynamic domains. We decompose the
6397	6431	dependency structure exhibits an implicit dependency between Xt,2 and Yt. (b) This structure exhibits a spurious correlation between Xt,1 and Yt. instantiate these nodes, we break their dependency . Thus, two nodes X and Y are explicitly dependent if and only if there does not exist a set of nodes S, such that Pr = Pr: (5) We can determine the dependency structure of a
33007	919	Internet path. The RON project experimentally confirmed the Detour observations, and showed that an overlay network that performs its own network measurements can provide improved reliability . Content Delivery Networks such as Akamai , and Cisco’s Overcast  use overlay networks to provide faster service to clients by caching or eliminating redundant data transmission. The ideas
33007	919	pass the filter. Proximity Routing: By picking the overlay node nearest the client (similar to Akamai and other CDNs ) or the node that provides the best performance between client and server , the system can provide high performance with low and filtered the old address to successfully protect themselves from the attack. overhead. In fact, when combined with overlaylevel caching, this
33007	6483	We then examine more sophisticated flooding attacks, and examine the effects of eavesdropping and compromise attacks. We assume that attackers may learn the ISP router topology by various means  because it is a shared resource. 4.1 Probing Several lightweight authenticators, such as destination port or destination address, allow arbitrary hosts to communicate directly with the target.
33007	6486	anonymizing overlays like Tarzan  are designed to prevent observers from determining the identity of communicating hosts. The principles used in these overlays, primarily Chaumian Mixnets , can be directly used in a system such as Mayday to provide greater protection against certain adversaries. We discuss this further in Section 3.5. 3 Design The design of Mayday evolved from one
33007	6486	the lightweight authenticator. Addss¢¡¤£¦¥ additional overlay hops, but provides better compromise containment. In most ways, this routing is inferior to mix routing. Mix Routing: Based on Mixnets  and the Tarzan  anonymous overlay system. A small set of egress nodes configure encrypted forwarding tunnels through the other overlay nodes in a manner such that each node knows only the next
33007	6488	can be deployed using the latest off-the-shelf exploits. Even worm programs have been used to launch DDoS attacks . While technical measures have been developed to prevent  and trace  DDoS attacks, most of these measures require wide-spread adoption to be successful. Unfortunately, even the simplest of these measures, filtering to prevent IP address spoofing, is not globally
33007	6488	sufficient packets have been received by the victim host, it can reconstruct the full path taken by the packets. Dean et al., treat the path reconstruction problem as an algebraic coding problem . These refinements improve the performance and robustness of the packet marking, but the underlying technique is similar to the original. The probabilistic traceback schemes require that a large
33007	6490	with Mayday; in fact, the Akamai network of a few thousand distributed nodes seems like an ideal environment in which to deploy a Mayday-like system. Mixnet-based anonymizing overlays like Tarzan  are designed to prevent observers from determining the identity of communicating hosts. The principles used in these overlays, primarily Chaumian Mixnets , can be directly used in a system such
33007	6490	in which every overlay node can directly access the server (i.e. be an egress node), to Mixnet-style routing (“onion routing”), in which the other overlay nodes do not know which is the egress node . The choice of lightweight authenticator affects which overlay routing techniques can be used. For instance, using source address authentication with proximity routing is extremely weak, because an
33007	6490	Addss¢¡¤£¦¥ additional overlay hops, but provides better compromise containment. In most ways, this routing is inferior to mix routing. Mix Routing: Based on Mixnets  and the Tarzan  anonymous overlay system. A small set of egress nodes configure encrypted forwarding tunnels through the other overlay nodes in a manner such that each node knows only the next hop to which it
33007	6492	of platforms, and can be deployed using the latest off-the-shelf exploits. Even worm programs have been used to launch DDoS attacks . While technical measures have been developed to prevent  and trace  DDoS attacks, most of these measures require wide-spread adoption to be successful. Unfortunately, even the simplest of these measures, filtering to prevent IP address
33007	6492	many of the probing attacks we discuss in Section 4 can be used against these solutions as well. Pushback provides a mechanism for pushing ratelimiting filters to the edges of an ISP’s network . If attack packets can be distinguished from legitimate traffic (as in the case of a SYN flood), Pushback’s mechanisms can effectively control a DoS attack. In the general case, Pushback will also
33007	6493	observations, and showed that an overlay network that performs its own network measurements can provide improved reliability . Content Delivery Networks such as Akamai , and Cisco’s Overcast  use overlay networks to provide faster service to clients by caching or eliminating redundant data transmission. The ideas behind these networks would integrate well with Mayday; in fact, the
33007	6493	pass the filter. Proximity Routing: By picking the overlay node nearest the client (similar to Akamai and other CDNs ) or the node that provides the best performance between client and server , the system can provide high performance with low and filtered the old address to successfully protect themselves from the attack. overhead. In fact, when combined with overlaylevel caching, this
33007	6496	of platforms, and can be deployed using the latest off-the-shelf exploits. Even worm programs have been used to launch DDoS attacks . While technical measures have been developed to prevent  and trace  DDoS attacks, most of these measures require wide-spread adoption to be successful. Unfortunately, even the simplest of these measures, filtering to prevent IP address
33007	6496	such as Cisco’s Reverse Path Filtering. However, ingress filtering is most effective at the edge; deployment in the core, even if it becomes technically feasible, is not completely effective . Mazu Networks  and Arbor Networks  provide DoS detection and prevention by creating models of “normal” traffic and detecting traffic that violates the model. If an attack is detected, Mazu’s
33007	6499	Networks Overlay networks have long been used to deploy new features. Most relevant to this work are those projects that used overlays to provide improved performance or reliability. The Detour  study noted that re-routing packets between hosts could often provide better loss, latency, and throughput than the direct Internet path. The RON project experimentally confirmed the Detour
33007	6500	can be deployed using the latest off-the-shelf exploits. Even worm programs have been used to launch DDoS attacks . While technical measures have been developed to prevent  and trace  DDoS attacks, most of these measures require wide-spread adoption to be successful. Unfortunately, even the simplest of these measures, filtering to prevent IP address spoofing, is not globally
33007	6500	to determine the path taken by the packets. To avoid out-of-band notifications, Savage et al. use probabilistic inline packet marking to allow victims to trace attack packets back to their source . In this scheme, routers occasionally note in the packet the link the packet has traversed; after sufficient packets have been received by the victim host, it can reconstruct the full path taken by
33007	6501	the overlay itself. For example, the SOS system uses Chord  to perform routing lookups. The Chord system, and similar distributed hash tables, are themselves subject to a variety of attacks . Any other component of the lookup system is similarly a potential source of cascaded compromise when an overlay node is compromised. This observation argues for keeping the overlay routing as
33007	6502	under the control of the system designer. When a connection between the ingress node and server is interrupted by address or port changes, the designer can use mechanisms such as TCP Migrate  or other end-to-end mobility solutions to transparently reconnect the session. Using these mechanisms between ingress node and server would not require client changes. 4 Attacks and Defenses The
33007	6503	We then examine more sophisticated flooding attacks, and examine the effects of eavesdropping and compromise attacks. We assume that attackers may learn the ISP router topology by various means  because it is a shared resource. 4.1 Probing Several lightweight authenticators, such as destination port or destination address, allow arbitrary hosts to communicate directly with the target.
33007	943	the authentication key. SOS: The SOS method uses doubly-indirect routing with source address authentication. In the SOS framework, packets enter via an “access node,” are routed via a Chord overlay  to a “beacon” node, and are sent from the “beacon” node to the “servlet” node. The servlet passes packets to the server. This method provides equivalent security to the singly-indirect scheme
33007	943	across the network. An attacker knows anything a compromised node knows. Furthermore, the attacker can now launch internal attacks against the overlay itself. For example, the SOS system uses Chord  to perform routing lookups. The Chord system, and similar distributed hash tables, are themselves subject to a variety of attacks . Any other component of the lookup system is similarly a
6505	6507	Multiuser collaborative environments combine visualization, video conferencing, and remote application steering into a distributed application with low latency and high bandwidth demands , . The Optiputer project  aims to build a distributed computer using a wide area network path for a backplane. Other tools such as GridFTP , bbcp , DPSS , and PSockets  are used by
6505	5710	with low latency and high bandwidth demands , . The Optiputer project  aims to build a distributed computer using a wide area network path for a backplane. Other tools such as GridFTP , bbcp , DPSS , and PSockets  are used by applications that need to move large amounts of data over wide area networks. Many of these applications use the Transmission Control Protocol
6505	6510	and high bandwidth demands , . The Optiputer project  aims to build a distributed computer using a wide area network path for a backplane. Other tools such as GridFTP , bbcp , DPSS , and PSockets  are used by applications that need to move large amounts of data over wide area networks. Many of these applications use the Transmission Control Protocol (TCP) for accurate and
6505	2993	demands , . The Optiputer project  aims to build a distributed computer using a wide area network path for a backplane. Other tools such as GridFTP , bbcp , DPSS , and PSockets  are used by applications that need to move large amounts of data over wide area networks. Many of these applications use the Transmission Control Protocol (TCP) for accurate and reliable in-order
6505	2993	. Thus, the use of competing Scalable TCP flows on a busy network will lead to a higher packet loss rates and reduce the amount of effective work (”goodput”) performed by the network. PSockets  is an application level library that opens concurrent TCP streams to increase aggregate TCP throughput. PSocket exploits the fact that a set of N TCP streams is functionally equivalent to a virtual
6505	2998	effects –. These effects on wide-area high-speed networks can lead to situations in which it can require over 4 hours for a TCP stream to completely recover from one loss event . Non-congestion loss also has significant effects on satellite links . There are efforts within the networking community to design modifications to TCP congestion avoidance to overcome these
6505	2998	TCP performance over wide area networks. III. RELATED WORK Many papers have been written on the topic of improving TCP performance for long-lived TCP streams on high-speed networks. Scalable TCP  improves TCP performance by using a fixed additive increase (AI) value and a multiplicative decrease (MD) factor that is less than the 1/2cwnd factor used by the standard TCP congestion avoidance
6505	6521	can lead to situations in which it can require over 4 hours for a TCP stream to completely recover from one loss event . Non-congestion loss also has significant effects on satellite links . There are efforts within the networking community to design modifications to TCP congestion avoidance to overcome these effects. Most of these efforts rely on mechanisms that aggressively compete
6505	6522	is underutilized. When the network is fully utilized, our approach is substantially fairer to competing traffic than an equivalent number of parallel TCP streams. II. BACKGROUND In previous work , we performed a series of network measurements between the University of Michigan in Ann Arbor and NASA AMES in Moffett Field, CA over a high speed path to determine if parallel TCP streams were
6505	6522	Maximum Segment Size (MSS) of N*MSSof the MSS used by a single TCP stream. Parallel TCP streams are aggressive on a shared network and can steal bandwidth from competing TCP streams. Previous work  showed that a single application using N parallel TCP streams competing with k other streams will receive N/(N+k) of the network bandwidth, rather than the 1/(N+k) portion the other streams
6505	6523	TCP on other network streams. To gain deeper insight into the effects of parallel TCP, we performed a series of simulations to measure the effects of parallel TCP on effectiveness and fairness . The simulation loss model was based on loss characteristics observed from data transfers over a wide area network. We found that the simulation loss rate was high enough to prevent a single TCP
6505	6523	be able to compete effectively with other unmodified TCP streams, and will each secure much less than a single stream’s fair share of the network bandwidth. Simulation results from previous work  shows that combined parallel TCP streams are more effective than a single unmodified TCP stream when the network is underutilized, and are substantially fairer than an equivalent number of
6505	6523	capacity of the network. This situation arises when the TCP socket buffers are too small, or when the systemic non-congestion packet loss rate is sufficiently high enough to limit TCP throughput . The second observation is that a single TCP stream is not effective at using network capacity over a long period of time. For the UDP stream ranging from 1 Mb/sec to 10 Mb/sec shown in Table I,
6505	6524	long round trip time (RTT) for the stream. Our approach exploits a behavior of TCP in which long RTT TCP streams are unable to compete with short RTT TCP streams for available network bandwidth . We call these modulated TCP streams fractional streams because their aggressiveness is a fixed fraction of the aggressiveness of a single unmodulated TCP stream. We call the combination of the
6505	6524	when the network is busy. Our combined approach exploits the fact that short round trip time TCP streams dominate long round trip time TCP streams when both are competing for network bandwidth , . When the network link is uncongested, either due to underutilization or systemic, non-congestion packet loss, the combined set of parallel TCP streams will consume the unused bandwidth
6505	6526	TCP is used on networks with persistently populated queues, it will incorrectly infer the propagation delay of the network and behave more aggressively than competing unmodified TCP streams. Biaz  found that RTT measurements do not adequately represent queue length, and should not be used as a predictor of congestion. Measurements  show that FAST TCP is unfair to competing unmodified TCP
6505	6527	protocols such as Tsunami without TCP-friendly congestion avoidance put the Internet at risk of suffering excessive congestion induced packet loss events and congestion collapse. Congestion Manager  is a software framework that can be used to implement alternative congestion avoidance mechanisms. The binomial congestion control algorithm  is a nonlinear control system that generalizes the
6505	6528	events and congestion collapse. Congestion Manager  is a software framework that can be used to implement alternative congestion avoidance mechanisms. The binomial congestion control algorithm  is a nonlinear control system that generalizes the AIMD class of congestion control that can be used with Congestion Manager. Binomial congestion control parameters that are TCPcompatible can be
6505	2242	TCP performance over lossy wide-area networks. Unfortunately, binomial congestion control is unfair to competing unmodified TCP streams across networks with droptail queue routers. Since RED  has not been widely deployed, binomial congestion control is not suitable for use on public production networks. The important lesson that can be learned from the approaches described in this
6505	6529	the network is busy. Our combined approach exploits the fact that short round trip time TCP streams dominate long round trip time TCP streams when both are competing for network bandwidth , . When the network link is uncongested, either due to underutilization or systemic, non-congestion packet loss, the combined set of parallel TCP streams will consume the unused bandwidth without
6505	6529	has shown the effects of long round trip times on fairness, there is not a relationship that we can use to tightly bound the minimum and maximum bandwidth for each fractional stream. Lakshman  empirically discovered that the effect of round trip time on throughput when two TCP streams are competing for bandwidth is proportional to RT T ?? , where 1 ? ? ? 2. Unfortunately, the lower and
6505	5723	forced to accept a PT value larger (worse bandwidth) than would be required to achieve a fair bandwidth share with a higher RTT value. From examining the relationship C ? MSS TCPBW ? RT T ? (2) p , it is clear that as RTT increases, a proportionate decrease in p2 is required to maintain the same bandwidth. Thus, the sensitivity to loss for a long RTT stream is quadratic in PT . For our
6505	5723	measures changes in the RTT and the square root of the packet loss rate. These values are related to TCP bandwidth by the fundamental relationship C ? MSS BW ? RT T ? (9) p described by Mathis . Looking at this equation, it is clear that if RTT or the square root of the packet loss rate increases, the TCP bandwidth of an individual stream is reduced. To measure RTT, we used the fping
6505	5723	to 10 Mb/sec. Table V contains the median RTT and square root of the median packet loss rate for the UDP stream ranging from 50 to 80 Mb/sec. The RTT and packet loss rate are factors in the Mathis  and Padhye  TCP equations. The second technique we used to assess fairness was based on measuring the impact of unmodified parallel TCP streams and our combined parallel TCP streams on 5
6505	6533	V contains the median RTT and square root of the median packet loss rate for the UDP stream ranging from 50 to 80 Mb/sec. The RTT and packet loss rate are factors in the Mathis  and Padhye  TCP equations. The second technique we used to assess fairness was based on measuring the impact of unmodified parallel TCP streams and our combined parallel TCP streams on 5 competing unmodified
6505	6534	less forcefully than unmodified TCP. This difference leads to a ”smoother” set of streams with less bursty and selfsynchronized traffic behaviors which are more likely to overflow the queue. Yang  found that smoothness, defined as small sending rate variations over time for a particular flow, and fairness are positively correlated. We believe that the negative effects of increased RTT are
8919449	6538	web crawler to roam the Internet and locate web pages about linguistics and languages. It uses a high-speeded indexer to automatically index located web pages. The database uses barrel structure () and sophisticated hash tables to enhance the speed of indexing and search. As the result, Seven Tones reaches a high speed in a single machine environment, and the speed is ensured even if the
8919449	6538	and also use relevance value to sort the branch walking orders. High Speed Indexer: Google search engine uses 64-bit integer addressable virtual files (BigFiles) spinning multiple file system (). This will be easy for the algorithm design but not flexible for the database updates. Most search engines update their databases monthly 5
8919449	6539	6 http://khnt.hit.uib.no/icame/manuals/LONDLUND/INDEX.HTM i X Y i i Y isor every several months by re-crawling the Web then rebuilding the whole inverted index lists (). Seven Tones uses logically separated files instead of a big virtual file. This makes it possible to update the whole database parallel and part-by-part. As the result, Seven Tones can add a web
8919449	6540	languages. Keywords: intelligent crawling, specialized search engine, information retrieval, automated summarization 1. Introduction There are thousands of specialized search engines on the Web (). These search engines usually are based on indexed databases that are entirely or partially constructed manually (). Seven Tones uses an intelligent web crawler to roam the Internet and locate
8919449	6541	() is trained from a selected web corpus. The crawler will compare a newly detected web page to the series of term vectors and calculate the distances from the new page to each term vector (). The algorithm uses these distance values to judge the relevance of a web page. The traditional method for vector distance detection is using TF*IDF and cosine similarity measure formula. It
8919449	6542	are using the following formula: Sim ( X , Y ) = X In this formula, Xi, Yi are term frequencies in two vectors. We also uses page importance value, which is similar to PageRank ranking in Google (), to help select qualified web pages. The web crawler is one of the basic components of this system. The web crawler used in this system is slightly different from web crawlers used for other
8919449	6546	The reliability of the relevance and quality assessments is not evaluated. Seven Tones search engine is also used in the local archive version () of AnswerBus Question Answering System 7 (). Some work from other researchers could be useful for the future work of Seven Tones. For example,  and  discussed some theoretical and practical approaches for similar classification tasks.
6550	6551	not necessary in our example, we need some extensions to finite automata, mainly to describe related calls like push and pop on a stack, and to overcome other modeling problems with finite automata . It is important, that the added type information is no burden for the software developer. In our type system the required interface description of the use protocol can be partially computed by the
6550	6554	Software architectures allow the reuse of high-level software designs. They are relevant to software component technology, because they focus on the connection of several components . Similarly design patterns  package lower level design information for reuse. Several benefits of design reuse are presented in the literature   . Despite of the
6550	2050	computed by the compiler by control-flow analysis . The description of the call protocol can be given by simple pre- and postconditions of functions and extend the idea of contracts . The VideoMail component of our example has the C-Aut shown in figure 2. The functionplay just uses theplay functions of theVideoPlayer and theSoundPlayer components, thus only having a rather
6550	6561	problems of component technology Software component technology is becoming increasingly popular, since it is considered to be a candidate to accomplish the expected benefits of reuse. . But despite of the advantages 2sof binary component technology, the extensive usage of binary components is impeded by several problems: pre-compilation problem: Currently many important portions
6550	6561	user interface limits the applicability and comfort of this approach. Another technique of binary composition is the use of mobile objects . This approach is based on object composition  well known from patterns and frameworks   . Java objects in byte-code are used to configure other objects. While this composition is surely important for component technology,
6550	6564	some services may exclude others, etc. More concrete: functionality = functions + the protocol to use them. component type: The type of a component describes the applicability of the component . Information of the applicability of a component is given in its interface. component interface: The interface of a component describes its applicability, i.e., it consists of the offered functions
6550	6564	the missing of a definition of a component type. 1 More precisely we should talk of the applicability information. 3sAccording to Nierstrasz a type should describe the typed entity’s applicability . This means the type of a component depends on its applicability. One aspect of a component’s applicability is the availability of services it offers to the applications it is used by. Two
6550	3368	private T content; 7 Related work Figure 10: Implementation of the classMail Basically, the binary form of components may be seen as the major difference to the software modules described in . Language support for this modularization of source code, e.g., in MODULA2 , can be seen as an early attempt of software reuse. Beside source code modules, other approaches of software reuse
6602	6608	to the behavior of single vehicles . The systematic basis of this approach enabled us to extend it in order to generate simple descriptions of the formation and dissolution of vehicle queues (Gerber et al. 2002). Figure 18 shows two representative image frames from a long video sequence illustrating a traffic queue which had built up in front of a red traffic light (left panel) and began to dissolve
6602	6608	frames. The frame rate of this sequence is equivalent to a sampling rate of 50 frames per second, i. e. the entire image sequence covers slightly more than 45 seconds of road traffic (adapted from (Gerber et al. 2002) c?2002 IOS Press, by permission). Figure 19: The left panel shows an image frame from the sequence illustrated in Figure 18, overlaid by the lane model for this intersection. One lane segment of
6602	6608	segment and the ‘outgoing’ segment of this same lane are marked by heavy boundary lines, too. In addition, the lane identifiers for the three segments of this lane are given (adapted from (Gerber et al. 2002) c?2002 IOS Press, by permission). In this case, the polygonal lane structure referred to earlier has been used to select all vehicles approaching and crossing the depicted intersection on the left
6602	6608	??§?? ??©?§???? ©¨? ??§??£§???©?? ? ?£§???¢?¤ ¦??¨? § ????? ©?§ ?¨????? ? ©???? §???§ ??? ? ? §?? ? Figure 20: Output text generated for the vehicle queues illustrated in Figure 18 – adapted from (Gerber et al. 2002) c?2002 IOS Press, by permission. chain. In addition, the system still lacks suitable linguistic abstraction facilities which would allow to replace the detailed recounting by a phrase
6602	6617	increasingly during the past decade – see, for example, (Neumann 1989), (Buxton & Gong 1995), (Dance et al. 1995), (Intille & Bobick 1999), (Howarth & Buxton 2000), (Chella et al. 2000), and (Kojima et al. 2002), to name but a few. It appears too early to decide which (combination of) approach(es) offers the greatest promise, given well defined boundary conditions regarding specifics of the discourse
6602	6627	process Rather than tracking a cluster of optical flow vectors directly, such a cluster may serve merely to initialise a 3D-model-based tracking process, building on ideas reported earlier by (Lowe 1991). A polyhedric vehicle model is tentatively placed in the scene at a location estimated by backprojection of optical flow vectors within a cluster onto a plane somewhat above and parallel to the
6602	6637	excluded so far from our discourse domain. Results about the detection, tracking, and description of the behavior of persons have been reported already, for example by (Remagnino et al. 1998) and (Rota & Thonnat 2000). Given the gamut of problems hinted at during the preceding sections, it appeared recommendable to emphasize robustness in a somewhat restricted discourse domain over attempts to admit
6652	6653	The parameter estimation in HMF is a difficult one and the implementation of the popular “ExpectationMaximization” (EM) method poses problems . So, some alternative methods have been proposed . Here we propose two new methods valid in PMF : the m first one is an adaptation to PMF of the Stochastic Gradient successfully applied in HMF . The second one is of Iterative Conditional
6652	6655	Conditional Estimation (ICE) kind, which also has already been successfully used in different HMF based problems . ICE resembles EM, and a comparative study can be seen in  We will specify the different methods in a simple particular Gaussian PMF defined by (3), with the neighborhood system reduced to four nearest neighbors. On the one hand, its generalization does
6652	6657	that the conditional expectation is computed for some components of ? , and is approximated for the other, where the exact computation is not feasible. Such cases occur in Hidden Markov Chains  or still in spatially independent data . As in PMF the distribution of X conditional on Y = y is a Markov distribution, its simulations are feasible, and thus the condition (ii) is always
6652	6658	Stochastic Gradient successfully applied in HMF . The second one is of Iterative Conditional Estimation (ICE) kind, which also has already been successfully used in different HMF based problems . ICE resembles EM, and a comparative study can be seen in  We will specify the different methods in a simple particular Gaussian PMF defined by (3), with the neighborhood system reduced to four
6652	6660	Y = y , even in the case of very rich sets S . However, the simplicity of p ( y x) required to ensure the Markovianity of p ( x y) can pose problems, in particular in textured images segmentation . To remedy this, Pairwise Markov fields (PMF), in which one directly assumes the Markovianity of ( X , Y ) , have been proposed . Both p ( y x) and p ( x y) are then Markovian, the former
6652	6661	Stochastic Gradient successfully applied in HMF . The second one is of Iterative Conditional Estimation (ICE) kind, which also has already been successfully used in different HMF based problems . ICE resembles EM, and a comparative study can be seen in  We will specify the different methods in a simple particular Gaussian PMF defined by (3), with the neighborhood system reduced to four
6652	6662	can pose problems, in particular in textured images segmentation . To remedy this, Pairwise Markov fields (PMF), in which one directly assumes the Markovianity of ( X , Y ) , have been proposed . Both p ( y x) and p ( x y) are then Markovian, the former ensuring possibilities to model textures without approximations, and the latter allowing Bayesian processing. The aim of this paper is to
6652	6664	? p ( xs xc , y s , y c ) ] , which is ? much richer. Remark 1 Introducing a third, possibly latent, random field U = ( U ) s s? , one can consider a Triplet Markov Field S T = ( X , U, Y ) (TMF ), whose distribution is given by (1), with t = ( x, u, y) instead of z = ( x, y) . If each U takes its values in a finite set = { ? , ..., ? } s 1 xs ? , we can write p( x s = ? j y) = ? p( x s ???
6652	6665	Stochastic Gradient successfully applied in HMF . The second one is of Iterative Conditional Estimation (ICE) kind, which also has already been successfully used in different HMF based problems . ICE resembles EM, and a comparative study can be seen in  We will specify the different methods in a simple particular Gaussian PMF defined by (3), with the neighborhood system reduced to four
6674	7064	the mapping function, and thus the performance of the cross-layer scheduling algorithm. To have a better understanding on the impact of these two parameters, we conduct a simulation study in ns-2 . In the experiments, two service classes with differentiation parameters ?1 : ?2 = 1 : 2 are provided in the wireless LAN. In the first experiment, two flows – each from one service class – are
6674	7064	T . 4 Simulation Study To evaluate the performance of CWTP, we simulate the algorithm under both linear mapping and piecewise linear mapping schemes on a variety of network settings in ns-2 . In the simulation, the number of nodes (N) is a parameter that is used to show the impact of network size on CWTP. Each node in the wireless LAN sets up a connection. The transmission rate of each
6674	7065	network management. node 1 node 2 base station node node n node 1 node 2 (a) centralized wireless LAN (b) distributed wireless LAN Figure 1: Models of wireless LANs. IEEE 802.11 MAC layer protocol  has been a popular standard for wireless LANs. It has two different access modes, namely Distributed Coordination Function (DCF) and Point Coordination Function (PCF). IEEE 802.11 DCF is the basic
6674	7065	evaluates four mechanisms for providing service differentiation in IEEE 802.11 wireless LANs via simulation study. The evaluated schemes are the Point Coordination Function (PCF) of IEEE 802.11 , the Enhanced Distributed Coordination Function (EDCF) of the proposed IEEE 802.11e extension to IEEE 802.11 , Distributed Fair Scheduling , and Blackburst . Service differentiation in
6674	6676	example, the work of  studies the problem of supporting both real-time and non-real-time services in a wireless LAN with dynamic time-division duplexed (D-TDD) transmission. Further the works of  present QoS architectures that support service differentiation in wireless networks from a system point of view. There also exist works that achieve better QoS for wireless LAN without service
6674	6677	traffic in wireless LANs. Towards this goal, existing studies have focused on the design of differentiated media access protocols in wireless LANs to achieve distributed priority scheduling (e.g. ) or fair scheduling (e.g. ). These approaches can only achieve performance differentiation of media access. Some QoS metrics, such as queueing delay can not be completely addressed by these
6674	6677	requirements of multimedia applications. Moreover, without a formalized service differentiation goal that quantifies the outcome of differentiation, the performance of most approaches (e.g., ) may be inconsistent and fluctuating, especially for packet delay over a short time-scale. Though some approaches (e.g., ) are based on the analytical model of the throughput, they
6674	6677	priorities. The packets with different priorities are treated differently at the MAC layer such that high priority packets are transmitted in priori to low priority ones. In particular, the work of  proposes three service differentiation schemes for IEEE 802.11. The first scheme scales the contention window according to the priority of each packet. The second scheme assigns different
6674	6678	and management. Under certain conditions (i.e., the network is well provisioned), applications with absolute delay requirements can select appropriate service classes to meet their requirements , even though the network offers only relative differentiation. One of the packet scheduling algorithms that can realize the proportional delay differentiation model in a short time-scale is the
6674	2146	simulation runs for 100 seconds. The default algorithm parameters are given as follows: L = 2, T = 1sec. cw is configured according to the number of nodes in the network based on the results from , and is also given in Table 4. In the simulation study, packet delay is measured as follows. Let t1 be the time instance at which a packet arrives at the queue in the network layer. Further, let t2
6674	6679	, Distributed Fair Scheduling , and Blackburst . Service differentiation in wireless LAN has also been studied under MAC protocols other than IEEE 802.11. For example, the work of  studies the problem of supporting both real-time and non-real-time services in a wireless LAN with dynamic time-division duplexed (D-TDD) transmission. Further the works of  present QoS
6674	6681	traffic in wireless LANs. Towards this goal, existing studies have focused on the design of differentiated media access protocols in wireless LANs to achieve distributed priority scheduling (e.g. ) or fair scheduling (e.g. ). These approaches can only achieve performance differentiation of media access. Some QoS metrics, such as queueing delay can not be completely addressed by these
6674	6681	requirements of multimedia applications. Moreover, without a formalized service differentiation goal that quantifies the outcome of differentiation, the performance of most approaches (e.g., ) may be inconsistent and fluctuating, especially for packet delay over a short time-scale. Though some approaches (e.g., ) are based on the analytical model of the throughput, they
6674	6681	results of these two performance metrics. To scale the results, we introduce an index I? for ? which is given as follows. I? = 10 · ? ? I? normalizes the value of ? based on ?. It takes a range of  in the Fig. 4, which makes the value of ? to be within the range of . From the results, we have the following observations. First, the I? greatly affects the result of differentiation index,
6674	6681	The second scheme assigns different inter-frame spacings to packets with different priorities. The third one uses different maximum frame lengths for packets with different priorities. The work of  proposes that the high priority packets randomly choose their backoff intervals from , while low priority packets choose from , where i is the number of consecutive
6674	6682	2.1 Proportional Delay Differentiation and Waiting Time Priority Scheduling Proportional service differentiation was first introduced as a per-hop-behavior (PHB) for DiffServ in wireline networks . It states that certain class performance metrics should be proportional to the differentiation parameters. In particular, if we consider the case of delay differentiation, in a network with C
6674	6683	works on service differentiation in wireless LANs. This paper conducts this study in two steps. First, it introduces a service differentiation model, proportional delay differentiation model , to the domain of wireless LAN. In the proportional delay differentiation model, the delay of a service class is proportional to another according to the ratio of their differentiation parameters.
6674	6683	relative differentiation. One of the packet scheduling algorithms that can realize the proportional delay differentiation model in a short time-scale is the waiting time priority (WTP) scheduler . In this algorithm, a packet is assigned with a weight, which increases proportionally to the packet’s waiting time. Service classes with higher differentiation parameters have larger
6674	6683	(3) where P is the set of backlogged packets. It is shown that WTP scheduler is able to approximate the proportional delay differentiation model in wireline networks under heavy traffic condition . 2.2 Wireless LAN and IEEE 802.11 A wireless LAN consists of mobile nodes that are within the transmission range of each other. It can operate in a centralized or distributed manner, as shown in
6674	6687	from a system point of view. There also exist works that achieve better QoS for wireless LAN without service differentiation. For example, the fast collision resolution(FCR) algorithm proposed in  reduces the average number of idle slots and thus can significantly increase throughput and achieve low latency. The 29sfair scheduling FCR (FS-FCR) algorithm could simultaneously achieve high
6674	6689	example, the work of  studies the problem of supporting both real-time and non-real-time services in a wireless LAN with dynamic time-division duplexed (D-TDD) transmission. Further the works of  present QoS architectures that support service differentiation in wireless networks from a system point of view. There also exist works that achieve better QoS for wireless LAN without service
6674	6690	studies have focused on the design of differentiated media access protocols in wireless LANs to achieve distributed priority scheduling (e.g. ) or fair scheduling (e.g. ). These approaches can only achieve performance differentiation of media access. Some QoS metrics, such as queueing delay can not be completely addressed by these approaches. Thus these approaches
6674	6690	differentiation, the performance of most approaches (e.g., ) may be inconsistent and fluctuating, especially for packet delay over a short time-scale. Though some approaches (e.g., ) are based on the analytical model of the throughput, they still do not consider delay as a service differentiation metric. Therefore, a quantitative delay differentiation study is still missing
6674	6690	can be starved and thus suffer long, unbounded and unpredictable delays. Besides priority scheduling, fair scheduling in wireless LANs has been studied in existing literatures. In the work of , the contention window size of each node is properly selected to reflect the relative weights among the flows as well as the number of nodes contending for the wireless channel. Thus it can achieve
6674	6691	traffic in wireless LANs. Towards this goal, existing studies have focused on the design of differentiated media access protocols in wireless LANs to achieve distributed priority scheduling (e.g. ) or fair scheduling (e.g. ). These approaches can only achieve performance differentiation of media access. Some QoS metrics, such as queueing delay can not be completely addressed by these
6674	6691	requirements of multimedia applications. Moreover, without a formalized service differentiation goal that quantifies the outcome of differentiation, the performance of most approaches (e.g., ) may be inconsistent and fluctuating, especially for packet delay over a short time-scale. Though some approaches (e.g., ) are based on the analytical model of the throughput, they
6674	6691	to incorporate service differentiation. The work of  discusses and evaluates EDCF, which is a distributed medium access scheme adopted in IEEE 802.11e to allow prioritized medium access.  presents an adaptive enhanced distributed coordination function (AEDCF) based on this new EDCF in IEEE 802.11e standard, which aims to share the transmission channel efficiently. Furthermore, the
6674	2155	studies have focused on the design of differentiated media access protocols in wireless LANs to achieve distributed priority scheduling (e.g. ) or fair scheduling (e.g. ). These approaches can only achieve performance differentiation of media access. Some QoS metrics, such as queueing delay can not be completely addressed by these approaches. Thus these approaches
6674	2155	differentiation, the performance of most approaches (e.g., ) may be inconsistent and fluctuating, especially for packet delay over a short time-scale. Though some approaches (e.g., ) are based on the analytical model of the throughput, they still do not consider delay as a service differentiation metric. Therefore, a quantitative delay differentiation study is still missing
6674	2155	weights among the flows as well as the number of nodes contending for the wireless channel. Thus it can achieve both weighted fairness and maximizes the aggregated throughput. In the work of , a IEEE 802.11 DCF-based fair scheduling algorithm is proposed to schedule transmission such that the bandwidth allocated to different flows is proportional to their weights. Fair scheduling can
6674	2155	Point Coordination Function (PCF) of IEEE 802.11 , the Enhanced Distributed Coordination Function (EDCF) of the proposed IEEE 802.11e extension to IEEE 802.11 , Distributed Fair Scheduling , and Blackburst . Service differentiation in wireless LAN has also been studied under MAC protocols other than IEEE 802.11. For example, the work of  studies the problem of supporting both
6674	6694	In light of this observation, our priori work  provides end-to-end delay differentiation via priority mapping based on the support of existing MAC priority scheduling. And the work of  achieves end-to-end delay assurances in multihop wireless local area networks via dynamic service class selection. 28sThe work of  makes use of the carrier sense capabilities of network
6674	6696	transmission of high priority packets. The resulting scheme guarantees the strict priority and provides bounded access delay to high priority packets. This scheme is further extended in  and  for multi-hop ad hoc network environments. In these schemes, the low priority packets can be starved and thus suffer long, unbounded and unpredictable delays. Besides priority scheduling, fair
6674	6697	an adaptive enhanced distributed coordination function (AEDCF) based on this new EDCF in IEEE 802.11e standard, which aims to share the transmission channel efficiently. Furthermore, the work of  uses a Markov-chain-based model to analyze the performance of IEEE 802.11e. In these schemes, packets with different priorities may have different queueing delays. However the relation between a
6698	6701	problem. Schuurmans and Patrascu  build on this decomposition technique to design an efficient constraint generation approach for the ALP problem. Alternatively, de Farias and Van Roy  propose a sampling approach for selecting a subset of the exponentially-large constraint set. In purely continuous settings, the HALP reduces to the formulation recently proposed by Hauskrecht and
6698	6702	the solutions obtained by the HALP formulation that also applysto the purely continuous setting of Hauskrecht and Kveton. Specifically, we extend the theoretical analysis of de Farias and Van Roy  for discrete ALPs to the continuous and hybrid settings addressed by our HALP formulation, providing bounds with respect to the best approximation in the space of the basis functions. Although
6698	6702	of original value optimization problem; instead of optimal values for all possible states, only k weights need to be found. Note that this formulation reduces to the standard discrete-case ALP  if the state space x is discrete, or to the continuous ALP  if the state space is continuous. A number of concerns arise in context of the HALP approximation. First, the formulation of the HALP
6698	6702	analysis Our theoretical analysis of the quality of the solution obtained by the HALP formulation follows similar ideas to those used by de Farias and Van Roy to analyze the discrete case . They note that the approximate formulation cannot guarantee an uniformly good approximation of the optimal value function over the whole state space. To address this issue, they define a Lyapunov
6698	6702	? i given by ?·? 1,? , and ?·? ?,1/L is the maxnorm weighted by 1/L. Proof: The proof of this result for the hybrid setting follows the outline of the proof of de Farias and Van Roy???s Theorem 4.2  for the discrete case.sThe main intuition behind this result is that, if our basis functions can approximate the optimal value function in the states emphasized by the Lyapunov function, then minw
6698	6703	(linear) value function , where each basis function depends on a small number of state variables. This architecture is central for obtaining efficient approximation algorithms for factored MDPs . In hybrid settings, each basis function may depend on both continuous and discrete state components. We show that the weights of this approximation can be optimized using a convex formulation we
6698	6703	the ALP approach leads to linear programming (LP) problems with a small number of variables, but with exponentially many constraints; one constraint per each state-action pair. Guestrin et al.  present an efficient LP decomposition technique that exploits the structure of the factored model to represent this LP by an equivalent, exponentially-smaller problem. Schuurmans and Patrascu
6698	6703	using a new factored discretization technique that exploits structure in the continuous components. Once discretized, this formulation can be solved efficiently by existing factored ALP methods . We finally provide a bound on the loss in the quality of the solution of this relaxed relaxed ?-HALP with respect to that of the complete HALP formulation. We illustrate the feasibility of our
6698	6703	of values of variables in Xi and Ai (Xj and Aj), for continuous variables use their discretizations. 4. Calculate basis state relevance weights ?i. 5. Use the ALP algorithm by Guestrin et al  or Schuurmans and Patrascu  for factored discrete-valued variables to solve for the optimal weights w. Figure 1: A summary of the ?-HALP algorithm. as the factored LP decomposition of Guestrin
6698	6703	The complexity of this new problem will only be exponentially in the tree-width of a cost network formed by the restricted scope functions in our LP, rather than in the complete set of variables . Alternatively we can also apply the approach by Schuurmans and Patrascu  that incrementally builds the set of constraints using a constraint generation heuristic and often performs well in
6698	6704	the agents must coordinate to choose the action that jointly maximizes the expected value for the current state. We can achieve this by extending the coordination graph algorithm of Guestrin et al.  to our hybrid setting with our factored discretization scheme. The result will be an efficient distribute coordination algorithm that can cope with both continuous and discrete actions. Many
6698	6705	function relevance weights ?is in Equation (4) that appear in the objective function of the HALP formulation require us to solve exponentially-large sums and complex integrals. Guestrin et al.  show that if the state relevance density ?(x) is represented in a factorized fashion, these weights can be computed efficiently. In this section, we show that these ideas also extend for continuous
6698	6705	)dxiC , x iC (10) where ?(xiD ) is the marginal of the density ?(x) to the discrete variables XiD , and ?(xiC ) is the marginal to the continuous variables XiD . As discussed by Guestrin et al. , we can compute these marginal densities efficiently, for example, by representing ?(x) as the product of marginals, or as a Bayesian network. Using these marginals, we can compute ?iD and ?iC
6698	6705	The complexity of this new problem will only be exponentially in the tree-width of a cost network formed by the restricted scope functions in our LP, rather than in the complete set of variables . Alternatively we can also apply the approach by Schuurmans and Patrascu  that incrementally builds the set of constraints using a constraint generation heuristic and often performs well in
6698	6706	a sampling approach for selecting a subset of the exponentially-large constraint set. In purely continuous settings, the HALP reduces to the formulation recently proposed by Hauskrecht and Kveton  for factored continuous-state MDPs (CMDPs). This formulation requires the computation of several complex integration problems, which Hauskrecht and Kveton address by identifying conjugate classes
6698	6706	compactly. To model the transitions of a continuous state variable in  with hybrid parents we assume beta or mixture of beta densities as proposed recently by Hauskrecht and Kveton . Here, the CPF is given by: p(X ? i | Par(X ? i)) = Beta(X ? i | h 1 i (Par(X ? i)),h 2 i (Par(X ? i))), where h1 i (Par(X? i )) > 0,h2i (Par(X? i tive functions of the value of the parents of X ?
6698	6706	all possible states, only k weights need to be found. Note that this formulation reduces to the standard discrete-case ALP  if the state space x is discrete, or to the continuous ALP  if the state space is continuous. A number of concerns arise in context of the HALP approximation. First, the formulation of the HALP appears to be arbitrary, that is, it is not immediately clear
6698	6706	in the context of discrete factored MDPs. An important issue in hybrid settings is that the problem formulation incorporates integrals, which may not be computable. Hauskrecht and Kveton  propose conjugate transition model and basis function classes that lead to closed-form solutions of all integrals in the strict continuous case. The matching pairs include transitions based on beta
6698	6706	continuous states X ? C . For discrete settings, Koller and Parr  show that, for basis functions with restricted scope these backprojections can be computed efficiently. Hauskrecht and Kveton  extend this idea by showing that for polynomial basis functions and beta transition models, the integrals can be computed in closed-form. In hybrid settings, we combine these two ideas in a
6698	6708	popular in recent years are approximations based on linear representations of value functions, where the value function V (x) is expressed as a linear combination of k basis functions fi(x) : V (x) = k? wifi(x). i=1 In the general case, basis functions are defined over complete state space X, but very often they are restricted only to subsets of state variables . The goal of the
6698	6709	present an efficient LP decomposition technique that exploits the structure of the factored model to represent this LP by an equivalent, exponentially-smaller problem. Schuurmans and Patrascu  build on this decomposition technique to design an efficient constraint generation approach for the ALP problem. Alternatively, de Farias and Van Roy  propose a sampling approach for selecting a
6698	6709	using a new factored discretization technique that exploits structure in the continuous components. Once discretized, this formulation can be solved efficiently by existing factored ALP methods . We finally provide a bound on the loss in the quality of the solution of this relaxed relaxed ?-HALP with respect to that of the complete HALP formulation. We illustrate the feasibility of our
6698	6709	of original value optimization problem; instead of optimal values for all possible states, only k weights need to be found. Note that this formulation reduces to the standard discrete-case ALP  if the state space x is discrete, or to the continuous ALP  if the state space is continuous. A number of concerns arise in context of the HALP approximation. First, the formulation of the HALP
6698	6709	Xi and Ai (Xj and Aj), for continuous variables use their discretizations. 4. Calculate basis state relevance weights ?i. 5. Use the ALP algorithm by Guestrin et al  or Schuurmans and Patrascu  for factored discrete-valued variables to solve for the optimal weights w. Figure 1: A summary of the ?-HALP algorithm. as the factored LP decomposition of Guestrin et al. . We can use the same
6698	6709	of a cost network formed by the restricted scope functions in our LP, rather than in the complete set of variables . Alternatively we can also apply the approach by Schuurmans and Patrascu  that incrementally builds the set of constraints using a constraint generation heuristic and often performs well in practice. Figure 1 summarizes the main steps of the algorithm to solve the hybrid
8919471	7103	knowledge, the use of increasingly sophisticated technology, and the high level of physician training, measures of the quality of care, return on investment  and the incidence of medical errors  depict a severely under performing system. This is the first of four articles that addresses the healthcare system using recent fundamental advances in complex systems research. The central
6715	6716	is a hardware-like virtual machine. Traditional VMMs and microkernels can be thought of as the extremes of a continuum. Recent additions to this continuum are paravirtualized systems, such as Xen  and Denali , and systems that “virtualize” a user-mode–only process model, such as Fluke , which reside in proximity to traditional VMMs and classic microkernels, respectively. We propose to
6715	6716	“?” represents an optional feature. does not enable the reuse of untrusted components and does not offer IPC; communication between virtual machines is possible only using an emulated network. Xen  is a paravirtualizing VMM. It supports a Linux kernel with minor modifications. The Xen kernel includes device drivers, which are controlled by privileged, fully trusted virtual machine. It
6715	6717	a general-purpose security architecture that uses L 4 Linux as its legacy component. Like Terra, it provides attestation and a trusted path to the user using a small trusted windowing environment . µSINA (which we discussed in Section 3.4) is a specialized implementation of the Nizza security architecture. 6 Conclusions In this paper, we have proposed extending traditional VMMs with features
6715	6718	extremes of a continuum. Recent additions to this continuum are paravirtualized systems, such as Xen  and Denali , and systems that “virtualize” a user-mode–only process model, such as Fluke , which reside in proximity to traditional VMMs and classic microkernels, respectively. We propose to allocate another spot in the center of this space: VMMs with microkernel-like features, or
6715	6719	we somewhat confusingly called tunneling) . Since then, several authors proposed conventional virtual-machine technology without support for using untrusted components. The authors of Terra  even denounced microkernel technology as “exotic,” implying that nothing can be learned from it. Therefore, in this paper we analyze the misconceptions that lead to this view and provide a
6715	6719	developed our IPSec IP-interface driver, which defers trusted functions to the external trusted servers via IPC, in the context of original Linux instead of that of L 4 Linux. 5 Related work Terra  has been proposed as a virtual-machine–based security architecture for trusted systems. Terra enhances traditional VMM technology with features for attestation, trusted user communication, and
6715	6721	gateway that uses trusted wrappers to cut down the TCB . For IP routing, this application uses two untrusted instances of L 4 Linux, a port of the Linux kernel to the L4 microkernel interface , running as user-mode programs on one machine. While the Linux instances are untrusted, µSINA uses a set of trusted device drivers (running in their own address spaces). We have to trust these
6715	6722	secure systems should be based on small isolated components, and shortly outlined the reuse of untrusted legacy components through trusted wrappers (which we somewhat confusingly called tunneling) . Since then, several authors proposed conventional virtual-machine technology without support for using untrusted components. The authors of Terra  even denounced microkernel technology as
6715	6722	a driver that does not return control. In earlier work, we have designed and implemented microkernel-based security architectures called Nizza that make extensive use of untrusted components . Nizza is a general-purpose security architecture that uses L 4 Linux as its legacy component. Like Terra, it provides attestation and a trusted path to the user using a small trusted windowing
6715	6724	fully trusted virtual machine. It currently does not support IPC or the use of untrusted components. Xen supports communication across protection boundaries using a virtual network device. Nooks  is an architecture for using potentially erroneous device drivers within the Linux kernel. The encapsulation of these drivers ensures the integrity of the Linux kernel. However, Linux cannot
6715	6725	security categories. Therefore, it is important to define them precisely and to point out potential misunderstandings. We use the following definitions (derived from security measures introduced in ): 2 Confidentiality: Only authorized users (entity, principal, etc.) can access information (data, programs, etc.). Integrity: Either information is current, correct, and complete, or it is
6715	6726	virtual machine. Traditional VMMs and microkernels can be thought of as the extremes of a continuum. Recent additions to this continuum are paravirtualized systems, such as Xen  and Denali , and systems that “virtualize” a user-mode–only process model, such as Fluke , which reside in proximity to traditional VMMs and classic microkernels, respectively. We propose to allocate
4194	4174	or preferential pathways. However, it is only recently that the assumption of a finite-variance velocity field has been addressed Že.g., Painter, 1996; Liu and Molz, 1996, 1997; Molz et al., 1997; Benson, 1998 . . The purpose of this paper is to demonstrate that highly skewed, non-Gaussian contaminant plumes with heavy leading edges can be a result of the infinite-variance particle jump distributions
4194	4174	media. Additional factors such as long-term velocity dependence serve to enhance non-Gaussian plume growth, but are not required for this type of evolution. We demonstrate that a fractional ADE Ž Benson, 1998. is a governing equation for conservative solute transport in porous media in cases where temporally correlated velocity fields do not dominate transport processes. Section 2 explains that Fickian
4194	6741	and engineering fields, including fluid flow, electrical networks, electromagnetic theory, and probability and statistics Že.g., Miller and Ross, 1993; Oldham and Spanier, 1974; Zaslavsky, 1994; Gorenflo and Mainardi, 1998a . . Fig. 2 demonstrates that fractional-order derivatives form a continuum between their integer-order counterparts. Benson et al. Ž 2000a. and Gorenflo and Mainardi Ž 1998a. provide an
4194	6741	or forwards. D q q , known as the Riemann–Liouville operator, is the derivative of a function from y` to x while D q y , known as the Weyl fractional derivative, is the derivative from x to ` Ž Gorenflo and Mainardi, 1998b . . If the series definition of the Riemann–Liouville operator Ž Miller and Ross, 1993 . , 1 1 where y1FbF1 and 2 1yb and 2 1qb are the probabilities that a particle will Ž . D fŽ x. s lim D x fŽ
4194	6742	and engineering fields, including fluid flow, electrical networks, electromagnetic theory, and probability and statistics Že.g., Miller and Ross, 1993; Oldham and Spanier, 1974; Zaslavsky, 1994; Gorenflo and Mainardi, 1998a . . Fig. 2 demonstrates that fractional-order derivatives form a continuum between their integer-order counterparts. Benson et al. Ž 2000a. and Gorenflo and Mainardi Ž 1998a. provide an
4194	6742	or forwards. D q q , known as the Riemann–Liouville operator, is the derivative of a function from y` to x while D q y , known as the Weyl fractional derivative, is the derivative from x to ` Ž Gorenflo and Mainardi, 1998b . . If the series definition of the Riemann–Liouville operator Ž Miller and Ross, 1993 . , 1 1 where y1FbF1 and 2 1yb and 2 1qb are the probabilities that a particle will Ž . D fŽ x. s lim D x fŽ
4194	4192	demonstrate that the fractional ADE is a special case of the non-local transport equations found in Cushman and Ginn Ž 1993 . . The three dimensional case is more complicated Ž as discussed in Meerschaert et al., 1999 . , as the order of the fractional derivative is not necessarily equal in all directions and the AskewnessB can assume many directions. 5. Stochastic modeling and stochastic hydrogeology Stochastic
4194	6764	processes Ž Serrano, 1995 . . Furthermore, practitioners still use some sort of numerical implementation of the classical ADE or particle tracking methods in modeling contaminant transport ?? Zheng and Jiao, 1998; Pollock, 1994 . . The central limit theorem is valid for sums of independent and identically distributed Ž iid. finite-Õariance random variables Ž Feller, 1968 . . This means that Gaussian
8919482	6766	cryptographic primitives and distributed calculi. It would also be interesting to study sequence types in the context of Ambient Calculi, in particular comparing with the work of Amtoft et.al.  on ML-like polymorphism (where principal typing holds only for a restricted fragment of Ambients), and where the use of shape types may be regarded as a form of finitary polymorphism. Apart from
8919482	6768	to study the formal properties of the restricted subsystem. Our original motivation to study intersection types was the attempt to give an expressive structural type-system for the e ?-calculus , an extension of the ?-calculus where channels are identified by vector of names (synchronisation vectors). In e ?, a natural choice could be to type a channel with the intersection of the types of
8919482	6772	starting from the work by Reynolds on polymorphism for the Forsythe programming language . Apart from the successful use in defining filter models for the ?-calculus and the Ambient Calculus , intersection types have been somehow overlooked in the context of concurrency and mobility. From a programming language perspective, intersection types express a finite, yet unbounded, amount of
8919482	6772	properties are studied in . A comparison between some of these approaches can be found in . Intersection types have been used to define filter-models for the ?-calculus , the ?-calculus , and a variant of Mobile Ambients . Anyway, the approach and the aims of  are different to the ones of this paper. In particular the types for the filter model are assigned to processes, and
8919482	6772	principal typing holds only for a restricted fragment of Ambients), and where the use of shape types may be regarded as a form of finitary polymorphism. Apart from the definitions of filter models  mentioned in the introduction, this work constitutes the first study of intersection types in process calculi, and we hope that will serve as an inspiration for further research in the area.
8919482	6777	can be adopted to type communication in process calculi, and yields more flexibility than the usual existential/universal approach to polymorphism. We study finitary polymorphism for the ?-calculus , a fundamental formalism to reason about concurrent and mobile processes. A recent study by Berger, Honda and Yoshida  on genericity, highlights how the two endpoints of a communication channel
8919482	6777	represents the first attempt to study finitary polymorphism in the context of the ?-calculus. 2 The ?-calculus 2.1 Syntax and Semantics For simplicity, we use the polyadic variant of the ?-calculus  without the silent action prefix and mixed choice. Let N be a denumerable set of channel names, ranged over by x, y, z, w. We use the notation ˜x for the tuple x1, . . . , xn, and we write ˜xn when
8919482	6779	type inductively defined, polymorphic data structures. We leave it to future work a formal study of principal typing and decidability properties. One way to enforce decidability, following Pierce , consists in adopting a typed system (Church-style), annotating restriction and input variables with types. Alternatively, rank 2 intersection types have been studied for example in  as a
8919482	6780	errors, rather than precisely describing the behaviour of terms. Polymorphism in its various flavours has been studied extensively in the ?-calculus: universal , predicative , existential  and generic . A different approach is the one of , which defines a sort inference system based on tuples of input and output sorts (which could resemble our sequences of types). Their system
8919482	6781	type. Since then, intersection types have been studied extensively and have found many applications, starting from the work by Reynolds on polymorphism for the Forsythe programming language . Apart from the successful use in defining filter models for the ?-calculus and the Ambient Calculus , intersection types have been somehow overlooked in the context of concurrency and
8919482	6782	errors, rather than precisely describing the behaviour of terms. Polymorphism in its various flavours has been studied extensively in the ?-calculus: universal , predicative , existential  and generic . A different approach is the one of , which defines a sort inference system based on tuples of input and output sorts (which could resemble our sequences of types). Their system
8919482	6782	= ˜ T ? ? P ? ? z?˜x?.P Table 2 Basic type assignment system for the ?-calculus. 2.2 Basic Type Assignment System The basic Curry-style type assignment system for the ?-calculus, as presented in , is based on an environment ? (a partial function) associating an exchange type to each channel, describing the objects that can be communicated. The formal definition of basic types and
8919482	6782	the basic type system satisfies the standard property of preserving types under reduction, and guarantees that well-typed processes will not incur in communication errors. 4sMaffeis Theorem 2.2 () (i) If ? ? P and P ? Q, then ? ? Q. (ii) If ? ? P then P ? †. Note that from the Theorem 2.2 and rule (Par), follows that, if P does not contain a communication error in an environment ?, neither
8919482	6782	can be typed in the environment ? = a : T, b : , c : , y : ?{ ? ;  ? } Both processes given in Example 3.3 and Example 3.4 can be typed in the system of Turner ; in Section 5 we show an example which cannot be typed in that system. 7sMaffeis 4 Encoding of the ?-calculus 4.1 Strict intersection types for the ?-calculus We report below a definition of the
8919482	6782	a = (?b, x)(b | b(f).(f?x, a? | !x(c).c)) 8sMaffeis We give a translation of strict types into sequence types which is a proper extension of the one for basic types given by Turner  (for the purpose of the encoding we assume to have type variables ? also in our type system). ????? = ? ???{?1; . . . ; ?n} ? ??? = ]; . . . ; ]}, ] A type variable
8919482	6784	Related Work. Some important intersection type assignment systems are proposed in . Strict intersection types are introduced in , and their principal typing properties are studied in . A comparison between some of these approaches can be found in . Intersection types have been used to define filter-models for the ?-calculus , the ?-calculus , and a variant of Mobile
8919482	6785	are proposed in . Strict intersection types are introduced in , and their principal typing properties are studied in . A comparison between some of these approaches can be found in . Intersection types have been used to define filter-models for the ?-calculus , the ?-calculus , and a variant of Mobile Ambients . Anyway, the approach and the aims of  are different
8919482	6786	Pierce , consists in adopting a typed system (Church-style), annotating restriction and input variables with types. Alternatively, rank 2 intersection types have been studied for example in  as a decidable restriction of the intersection typing discipline which still enjoys many interesting properties. A corresponding notion of rank 2 for our system is straightforward to define, and
8919482	6788	aims at preventing communication errors, rather than precisely describing the behaviour of terms. Polymorphism in its various flavours has been studied extensively in the ?-calculus: universal , predicative , existential  and generic . A different approach is the one of , which defines a sort inference system based on tuples of input and output sorts (which could
2127	6793	Kraft Drive, Suite 111 Blacksburg, Virginia 24060, U.S.A. 279 developed at Virginia Tech in July 1995 (Balci et al. 1995). Technology transfer, enabled by the establishment of Orca Computer, Inc. (Balci et al. 1997d), produced the first commercial version of VSE in November 1996 for the NEXTSTEP operating system. Subsequently, VSE was released for OPENSTEP Mach Unix, Windows NT 4.0, Windows 95, and Windows
2127	6793	technical characteristics. Section 3 provides some examples. Concluding remarks are given in Section 4. 2 VSE CHARACTERISTICS 2.1 Toolset VSE consists of four software tools as described below (Balci et al. 1997c): • VSE Editor: Lets you build your simulation model graphically using the object-oriented paradigm, with inheritance, message passing, and encapsulation. • VSE Simulator: Provides animation and
2127	6793	domains such as manufacturing, health care, transportation, business process reengineering, and networks. A model can be constructed by reuse with no programming. For more information, see (Balci et al. 1997b, 1998). 2.5 Model Maintainability You can build highly maintainable simulation models using VSE. You can create a class and instantiate objects belonging to that class. Each instantiated object
2127	6793	In essence, their collecVisual Simulation Environment 281 tive behavior is represented by the enclosing dynamic object (aircraft) but each passenger object retains its individual representation. (Balci et al. 1997a) 2.11 Graphical and Picture-Based A VSE model is graphically structured in a hierarchical manner. You can drag and drop a graphical image in the VSE Editor in many different formats including EPS
2127	6793	capability inherent to its conceptual framework. Model factories can be established for manufacturing model components that can be reused by others in the development of VSE models. (Balci et al. 1997b) The VSE technology enables the establishment of a component-based simulation modeling marketplace so that the customers of the simulation industry can observe large economic benefits such as
8919491	6137	leverage existing algorithms for solving inverse singular value problems to develop numerically stable, finite algorithms for solving the sequence design problem. Unlike iterative algorithms, e.g. , , , convergence is not an issue for finite-step algorithms since they are guaranteed to solve the stated problem. The specific contributions of this paper are two-fold. First we present a
8919491	6809	the n-th column of X as xn. For each n, one has the relationship (X ? X )nn = ?xn? 2 2 = wn. (1) Finally, let ? denote the covariance matrix of the noise. Viswanath and Anantharam have proven in  that, for real signatures, the sum capacity of the S-CDMA channel per degree of freedom is given by the expression (in the complex case, the sum capacity differs by a constant factor) Csum = 1 2d
8919491	6809	optimal sequence design still boils down to constructing a matrix with given column norms and singular spectrum. Viswanath and Anantharam show that the following procedure will solve the problem . 1) Compute an eigenvalue decomposition of the noise covariance matrix, ? = QDQ? , where D =diag?for some non-negative vector ?. 2) Use Algorithm A of  to determine µ, the Schurminimal element
8919491	6810	Despite this work, the algorithms proposed to find optimal signatures have not exploited the observation that the signature design problem can be characterized as an inverse singular value problem . Thus researchers have been unable to exploit the wealth of algorithms developed in the matrix theory community during the past two decades , . In this paper we present the signature design
8919491	6138	existing algorithms for solving inverse singular value problems to develop numerically stable, finite algorithms for solving the sequence design problem. Unlike iterative algorithms, e.g. , , , convergence is not an issue for finite-step algorithms since they are guaranteed to solve the stated problem. The specific contributions of this paper are two-fold. First we present a summary of
8919491	6812	relevant finite-step algorithms from matrix analysis that will be useful for researchers working in sequence design. We focus on the algorithms by BendelMickey , Chan-Li  and Davies-Higham . Second, we present a new one-sided, finite algorithm that produces the signature sequences directly instead of computing their Gram matrix. This method has very modest time and space requirements
8919491	6812	values of X . This is another inverse singular value problem. III. CONSTRUCTING CORRELATION MATRICES A positive semi-definite Hermitian matrix with a unit diagonal is known as a correlation matrix . The Gram matrix A def = S ?S of an optimal signature matrix S is always a correlation matrix. Every correlation matrix with the appropriate spectrum can be factored to produce optimal signature
8919491	6812	sequence of rotations to convert an arbitrary N × N Hermitian matrix with trace N into a unit-diagonal matrix that has the same spectrum . We follow the superb exposition of Davies and Higham . Brief discussions appear on page 76 of Horn and Johnson  and in Problems 8.4.1 and 8.4.2 of Golub and van Loan . Suppose that A ? MN is a Hermitian matrix with Tr A = N. (Let MN denote the
8919491	6812	not yield ajj =1. A better implementation sets ajj =1explicitly. Davies and Higham prove that the Bendel-Mickey algorithm is backward stable, so long as it is implemented the way we have described . We restate the algorithm. Algorithm 1 (Bendel-Mickey): Given Hermitian A ? MN with Tr A = N, this algorithm yields a correlation matrix whose eigenvalues are identical with those of A. 1) While
8919491	6812	?? SQ. In consequence, the machinery of the Bendel-Mickey algorithm requires little adjustment to produce these factors. We refer to the one-sided version as the Davies-Higham algorithm in view of . We have observed that it can also be used to find the factors of an N-dimensional correlation matrix with rank r<N, in which case S may take dimensions d × N for any d ? r. Algorithm 2
5799924	6817	chips. The most important ones are: crosstalk (signal distortion due to cross coupling effects between signals)  , overshoot (signal rising momentarily above the power supply voltage)  , reflection (echoing back a portion of a signal), electro-magnetic interference (resulting from the antenna properties) , power supply noise  and signal skew (delay in arrival time to
5799924	6823	3-D layout modeling and parasitic extraction , accurate RLC simulation of on-chip power grid , using decoupling capacitors to limit the maximum dV¢ dt  and to improve IR-drop , inserting buffers on the interconnects Mehrdad Nourani and Amir Attarha Center for Integrated Circuits & Systems The University of Texas at Dallas Richardson, TX 75083-0688snourani,attarha¡
5799924	6824	Center for Integrated Circuits & Systems The University of Texas at Dallas Richardson, TX 75083-0688snourani,attarha¡ @utdallas.edu 1  and shielding wires (e.g. grounding every other line) . Noise and skew imposed by interconnects have emerged as main concerns in the interconnect design of gigahertz SoCs. Buffer insertion and transistor resizing methods   are used as design
5799924	6826	every other line) . Noise and skew imposed by interconnects have emerged as main concerns in the interconnect design of gigahertz SoCs. Buffer insertion and transistor resizing methods   are used as design techniques to achieve better power-delay and area-delay tradeoffs. Self-test methodologies have been developed to test signal integrity in high-speed SoCs. Testing crosstalk in
5799924	4337	to achieve better power-delay and area-delay tradeoffs. Self-test methodologies have been developed to test signal integrity in high-speed SoCs. Testing crosstalk in chip interconnects  and a BIST (built-in self-test) structure using D flip-flops that detects the propagation delay deviation of operational amplifiers  are among such methods. 1.2 Contribution and Paper
5799924	4337	methods to solve Equation 2 is found. This is the main reason that we used pseudorandom patterns, generated by conventional TPG, in our approach. 3.2 Inaccuracy of Single-Victim (RC) Model In  and , the worst case test patterns associated with a specific fault model (MAFM) were presented. They used the RC interconnect for test pattern generation. Techniques, such as the one
5799924	4337	. . . . . . 0 . 1 . 0 Victim (Wi): 0 0 1 1 0 0 1 1 0 1 1 0 Others (Wj;j = i): 0 1 0 1 1 0 1 0 1 0 0 1 . 1 Crossing TRmax Crossing TFmax Figure 6: Creating maximal integrity loss using RC model. in , apply identical transitions to all wires except the victim net to create maximal integrity loss in the victim wire. Six scenarios and typical test patterns used in such techniques (e.g. ) have
5799924	4337	These examples clearly show that there are scenarios for test patterns that create worse delay and/or noise on the signal and cause more integrity loss compared to those commonly reported earlier . Thus, RLC model of interconnect needs to be consolidated for test pattern generation. We have used an RLC model of seven parallel interconnect lines (indexed “7654321”) for our
5799924	6830	of operation increases. There are many efficient distributed models in the literature  . Figure 1 shows an accurate equivalent RLC circuit for several parallel interconnect lines . This model comprises resistance (R), partial self inductance (L) and capacitance (C) for each segment, mutual inductances (M) and coupling capacitance (Cc) between alls. . . R L R L R L Line i-1
5799924	6830	technology. The number of segments can be selected based on the length of the interconnect and the operating frequency. All results reported in this work are based on this distributed RLC model . 2.2 A Model for Signal Integrity True characteristics of a signal is reflected in its waveform. Recent interconnect simulation, design and optimization methods not only consider peak voltage and
5799924	6831	of a signal is reflected in its waveform. Recent interconnect simulation, design and optimization methods not only consider peak voltage and delay, but also take into account the signal waveform  . In reality, electronic components can tolerate certain level of noise. For example, a CMOS gate interprets any voltage in thesVHmin¡ Vdd¢ range as logic “1” and any voltage in thesVss¡ VLmax¢
5799924	7234	pattern generation. We have used an RLC model of seven parallel interconnect lines (indexed “7654321”) for our experimentation. The R, L and C values are extracted using accurate extraction tools . TISPICE has been used to simulate the complete RLC model. Typical CMOS gates are considered as driver and driven gates in two sides of the interconnect and Vdd ¢ 1? 2 Volt. Note that in the
5799924	7234	function near core input ports. They do, however, influence the cost of test overhead (e.g. cells and FFs) and test time (e.g. scan-out time). The experimental results here are reported using OEA  and SPICE  simulators. We analyzed five main buses (data, address, control and two internal) of the famous 8051 microprocessor . In our implementation the 7 cores communicate through these
5799924	6838	threshold voltages are V£ ¢ VHthr ¢ 2? 75 and V¤ ¢ VHmin ¢ 1? 60 when Vdd ¢ 2? 5. A similar cell can be designed to detect crossing VLthr and VLmax threshold voltages. Details can be found in . What level of overshoot is acceptable, and what level of voltage should be recognized as a logic “1” and “0” is debatable. Specific choice of VHthr and VLthr (and also VHmin and VLmax) depends on
5799924	6838	as interconnects may not eventually be kept within such bound since the layout generation tools and the fabrication process each may add additional delays. More importantly, as we have discussed in  signal integrity factor in general and noise/skew in particular are data-dependent phenomena that cannot be predicted accurately through analytical or simulation based approaches. Thus, similar to
5799924	6838	terms of test overhead, n ND cells, n SD cells and 2n scan flip-flops are needed. Two other alternatives for the test architectures using compressor/adder and dedicated counters are introduced in  and analyzed for their cost and test time. Briefly, these alternatives are more expensive but can supply data of integrity loss occurrences more accurately for applications such as interconnect
5799924	6842	were developed to approximate the behavior of long interconnect, power and clock networks. They have been extensively used in the simulation and evaluation of high-speed VLSI systems . They comprise the key factors of the original system with much lower complexity; therefore, they significantly reduce the required computation and thereby simulation time, with slight loss of
5799924	6842	residues. To overcome some numerical limitations that AWE method suffers, researchers have developed reduction methods based on Bi-orthogonalization algorithms such as Pade Via Lanczos (PVL)  or orthogonalized Krylov subspace methods , which are computationally convenient and numerically better behaved. Their experimental results reveal the high accuracy within less than 5% of SPICE
5799924	6842	, which contains n ? m transfer functions relating n inputs and m outputs. Note that in general we have m ? n for the possibility of fanout on some wires. Using an order reduction method (e.g. PVL  or ENOR  meth3 1 2 H(s)= n h 11(s) h 1m(s) h n1(s) h nm (s) Figure 5: Transfer function of an interconnect network ods), the transfer function of a specific output fr (1 ? r ? m) becomes of
5799924	6846	interconnect effect) may cause functional error or significant degradation of performance. Numerous research endeavors have been dedicated to the testing of logic gates for their timing behavior . Since the interconnect skew delay will be the dominant factor in determining the clock period of future technologies, it is essential to detect the skew violation on the interconnects.
5799924	6848	of the SD cell after delay associated with the NOR gate. Notice that possible clock skew does not have significant effect on the behavior of the SD cell. There are many approaches such as  to minimize the clock skew in a large chip. However, in the presence of clock skew TSI changes to TSI ¤ Tclock skew where the Tclock skew is the skew on the clock wire attached to the SD cell. This
5799924	6848	Many researchers have extensively investigated how to achieve the optimal driver for long VLSI interconnects considering different objectives such as minimum delay, area or power consumption  . For instance, several design parameters, such as the number of drivers and their aspect ratio, are adjusted using optimization techniques to obtain an optimal driver configuration. Similarly,
6854	7265	but examining pages by hand has obvious 4 limitations. To find additional evidence for the patterns that we spotted, we relied on large-scale statistical analysis of the Wikipedia archives . The statistics in the sections below were derived from data that represents the state of the encyclopedia’s history as of May 2003. To derive statistics, the archives were loaded into a MySQL
8919494	6866	overall number of codewords According to (5), a large increase in the number of nearest neighbors should cause a large degradation in the performance; surprisingly, CA1 performs practically optimal  (see also Section V) in spite of its large number of nearest neighbors. This contradiction can be easily resolved by recalling that there are various types of nearest neighbors differently
8919494	6866	and are the same for all , , and for any “transmitted” codeword. This is evident for , while for it follows from the fact that for a binary linear code with the mapping (2), the analyzed algorithms , , do not make any distinction between codewords or symbol values Since and are close to the transmitted codeword (relative to the complete error region ), the probability ratio provides
8919494	6866	distance from In this work, our attention is restricted to the particular case since the first term of the union bound, corresponding to the nearest neighbors, usually (and the analyzed algorithms , , are no exception) dominates the error performance. Computing involves integrating the term over a volume whose boundaries are rather complex. This is a difficult problem, for which no
8919494	6866	equal to , and symbols equal to . Let be the hyper-sphere with a small radius centered on Chase Algorithm 1: Let be the portion of , such that failure will occur for every vector According to , decoding This proves that is a conventional nearest neighbor, as there is a region with nonzero volume, centered on the midpoint error occurs. , where decoding Chase Algorithm 2: The error
8919494	6870	is not much different than for CA1 and is supported by simulation results (not presented in this paper). Recently, an upper bound has been derived on the bit-error rate (BER) performance of CA2 . This bound is based on a probabilistic rather than a geometrical method and is more complex to evaluate than the proposed bound. C. Chase Algorithm 3 CA3 is the most efficient of the Chase
8919499	6878	and methods. The most commonly used tracers include sodium radiochromate-labelled red blood cells ( 51 Cr-RBC) (Wardle, 1971; Gingerich et al., 1987; Duff et al., 1987; Gingerich and Pityer, 1989; Brill et al., 1998), iodonated bovine ( 125 I-BSA) or human serum albumin ( 131 I-HSA) (Conte et al., 1963; Wardle, 1971; Gingerich et al., 1987; Gingerich and Pityer, 1989; Bushnell et al., 1998), and Evans Blue
8919499	6878	al., 1981; Itazawa et al., 1983; Sleet and Weber, 1983; Nichols, 1987; Tort et al., 1991; Acierno et al., 1995). Studies using fluorescein dye (Ronald et al., 1964) or fluorescein-tagged dextrans (Brill et al., 1998) have also been performed. In the present study Evans Blue dye (EB) was used to determine the volume of the primary and SCSs. EB has been criticised for not being restricted to the circulatory
8919499	6878	barrier (Hargens et al., 1974). This led to the assumption that blood volume determinations using albumin-bound tracer dilution methods typically overestimate the true volumes (Nichols, 1987; Brill et al., 1998; Bushnell et al., 1998). However, it has recently been shown that fish capillaries do in fact have a highly functional barrier to macromolecules, with a reflection coefficient for proteins that
8919499	6878	vascular volume, measured with isotope-labelled albumins, and primary vascular volume, measured with labelled red blood cells (Gingerich and Pityer, 1989; Gingerich et al., 1990; Olson, 1992, 1996; Brill et al., 1998; Bushnell et al., 1998). These concerns are understandable when they are based on a distribution volume several hours after tracer injection. Some of these authors argue against the assumption
6955	6956	theory to online measurements and optimizations can also be employed to address this problem. While no suchs496 V. Sundaram and P. Shenoy study exists for storage systems, both control theory  and online measurements and optimizations  have been employed for dynamically allocating resources in web servers. Utility-based optimization models for dynamic resource allocation in server
6955	6959	storage have been studied in . The design of such systems involves several sub-tasks and issues such self-configuration  , capacity planning , automatic RAID-level selection , initial storage system configuration  , SAN fabric design  and online data migration . These efforts are complementary to our work which focuses on automatic storage bandwidth
6955	6960	can also be employed to address this problem. While no suchs496 V. Sundaram and P. Shenoy study exists for storage systems, both control theory  and online measurements and optimizations  have been employed for dynamically allocating resources in web servers. Utility-based optimization models for dynamic resource allocation in server clusters have been employed in .
6955	6961	Fig. 1. Relationship between application classes, logical volumes and logical units. To enable such allocations, each disk in the system is assumed to employ a QoSaware disk scheduler (such as ). Such a scheduler allows disk bandwidth to be reserved for each class and enforces these allocations at a fine time scale. Thus, if a certain disk receives requests from n application classes,
6955	6963	systems has received significant research attention. For instance, the design of workload monitoring and adaptive resource management for data-intensive network services has been studied in . The design of highly-dependable (“self-healing”) Internet services has been studied . From the perspective of storage systems, techniques for designing self-managing storage have been studied
6955	6964	disk bandwidth and are in (lo ? ,lo ? ). Assume that a workload change causes a transition to (lo ? ,hi + ). Then the system needs to choose one of several possible allocations: (0, 100), (5, 95), (10, 90),..., (100, 0). Choosing one of these allocations allows the system to learn the reward ?sigma + rt that accrues as a result of that action. After trying all possible allocations, the system can use
6955	6968	Finally, reinforcement learning has also been used to address other systems issues such as dynamic channel allocation in cellular telephone systems  and adaptive link allocation in ATM networks . 7 Concluding Remarks and Future Work In this paper, we addressed the problem of dynamic allocation of storage bandwidth to application classes so as to meet their response time requirements. We
6955	6970	can also be employed to address this problem. While no suchs496 V. Sundaram and P. Shenoy study exists for storage systems, both control theory  and online measurements and optimizations  have been employed for dynamically allocating resources in web servers. Utility-based optimization models for dynamic resource allocation in server clusters have been employed in .
6955	6972	Fig. 1. Relationship between application classes, logical volumes and logical units. To enable such allocations, each disk in the system is assumed to employ a QoSaware disk scheduler (such as ). Such a scheduler allows disk bandwidth to be reserved for each class and enforces these allocations at a fine time scale. Thus, if a certain disk receives requests from n application classes,
6955	6972	10,000 RPM Fujitsu MAJ3182MC disk 2 . We use the software RAID driver in Linux to configure the system as a single RAID-0 array. We implement the Cello QoS-aware disk scheduler in the Linux kernel . The disk scheduler supports a configurable number of application classes and allows a fraction of the disk bandwidth to be reserved for each class (these can be set using the scheduler system call
6955	6972	otherwise we assume an array of 8 disks . Each disk in the system is assumed to employ a QoS-aware disk scheduler that supports class-specific reservations; we use the Cello disk scheduler  for this purpose. Observe that the hardware configuration assumed in our simulations is identical to that in our prototype implementation. We assume that the system monitors the response times of
6955	6973	non-linearity, but can get stuck in local maxima. Finally, reinforcement learning has also been used to address other systems issues such as dynamic channel allocation in cellular telephone systems  and adaptive link allocation in ATM networks . 7 Concluding Remarks and Future Work In this paper, we addressed the problem of dynamic allocation of storage bandwidth to application classes so
6955	6974	are complementary to our work which focuses on automatic storage bandwidth allocation to applications with varying workloads. Dynamic bandwidth allocation for multimedia servers has been studied in . Whereas the approach relies on a heuristic, we employ a technique based on reinforcement learning. Several other approaches ranging from control theory to online measurements and optimizations can
6955	6977	Fig. 1. Relationship between application classes, logical volumes and logical units. To enable such allocations, each disk in the system is assumed to employ a QoSaware disk scheduler (such as ). Such a scheduler allows disk bandwidth to be reserved for each class and enforces these allocations at a fine time scale. Thus, if a certain disk receives requests from n application classes,
6978	6979	immature for it to happen anytime soon. An alternative approach called Optical Burst Switching (OBS) that combines the best of optical circuit switching and optical packet switching was proposed in  , and has received increasing amount of attention from both academia and industry worldwide - . In an OBS network, an ingress OBS node assembles client data units (e.g. IP packets) into
6978	6979	bandwidth on a desired output channel) for the following burst, which will be disassembled at the egress node. A prevailing reservation protocol in OBS networks is called Just-Enough-Time (JET) . In JET a control packet reserves an output wavelength channel for a period of time equal to the burst length l, starting at the expected burst arrival time r, which can be determined based on the
6978	6981	that combines the best of optical circuit switching and optical packet switching was proposed in  , and has received increasing amount of attention from both academia and industry worldwide - . In an OBS network, an ingress OBS node assembles client data units (e.g. IP packets) into bursts and sends out a corresponding control packet for each data burst. This control packet is
6978	6981	INFOCOM 2004swell-known burst scheduling algorithms such as Latest Available Unused Channel with Void Filling (LAUC-VF), First Fit with Void Filling and Round Robin with Void Filling described in , Min-SV, Min-EV, Max-SV, Max-EV and Best-Fit described in  and the algorithm in . By using competitive analysis, we establish a number of interesting upper and lower bounds on the
6978	6981	Fit. Most of their algorithms have a loss rate as low as LAUC-VF, and run as almost fast as Horizon. Other scheduling algorithms include First Fit with Void Filling, Round Robin with Void Filling  and the algorithm presented in . Although the above algorithms are very much different from each other, they (except Horizon) share the following two common features. First, they schedule
6978	6989	combines the best of optical circuit switching and optical packet switching was proposed in  , and has received increasing amount of attention from both academia and industry worldwide - . In an OBS network, an ingress OBS node assembles client data units (e.g. IP packets) into bursts and sends out a corresponding control packet for each data burst. This control packet is delivered
6978	6992	to be the ”best” for the time being, but such a scheduling maybe undesirable for future bursts. III. PREVIOUS WORK So far, several scheduling algorithms have been proposed for OBS networks. Horizon  does not utilize any “closed” intervals, and thus is fast but not bandwidth efficient. On the other hand, LAUC-VF can schedule a burst in a closed interval (i.e., as long as it is possible) but has
6978	6993	It is interesting to point out that although OBS is a new research area, the burst scheduling problem can actually be modelled as a special case of the Job Interval Selection Problem (JISP)  that has a rich research history. In the JIST problem, a set of jobs are provided as input to one or more machines. Each job includes a set of intervals on the time axis representing the possible
6978	7001	JET protocol itself cannot support a fixed offset time value for a given burst, not to mention for all bursts. In order to simplify signaling protocol design and system implementation, Xu et al.  proposed a protocol called Only Destination Delay (ODD), which associates with each burst a constant offset time and uses FDLs at each intermediate node to delay the burst by a time period equal to
7003	7005	u and v remain connected in G p . This had been the focus of much research concerning the existence of giant components in such graphs, and the critical values of p for the existence of those, cf. . But in many applications the fact that a path between u and v exists is not sufficient, one wants to be able to find the path in a distributed manner. It is known that if the topology of a graph
7003	7006	these results imply that if the failure probability is small enough then it is possible to find paths efficiently between nodes in the giant component. On the other hand Angel and Benjamini  showed that if ps1 # n then the hypercube could not be embedded in its giant component with constant distortion. This result suggests that when 1 n 1 # n then even though a giant component exists
7003	7006	structural properties of H n , in particular it has diameter poly(n) (w.h.p.), and has roughly the same expansion of H n , yet the ability to find short paths is lost. Angel and Benjamini proved in  that for these failure probabilities H n could not be embedded in H n,p with constant distortion, so the result of Part (i) is not entirely surprising, yet the techniques we use are different then
7003	7006	it is necessary to find a leaf which is connected to v via an open path, an event which is proven to be rare. Our technique is general enough to be used on other families of graphs. It is proven in  that if #s1/2 then there is an embedding of H n in H n,p with constant distortion. This embedding is used to derive the matching upperbound of part (ii). Note that the algorithm of (ii) does not
7003	7006	Pr # 0. 3.2 The Upper Bound Next we show that when p is large, local routing on the hypercube may be performed using n k probes with high probability. This is a variation on the result of  showing that in this regime the metric distortion of the percolation is bounded. This shows that there is indeed an asymptotic phase transition in the complexity of routing on the hypercube. The
7003	7006	to the metric of the hypercube before percolation. Percolation neighbor and percolation distance are used for the percolated hypercube H n,p . We refer to the definition of a good vertex from , which roughly means having a high degree in H n,p . The condition that a vertex is good is determined by the neighborhood of percolation radius 2 around it. In , Section (2) the following is
7003	7008	in social networks (and not merely their existence). In the context of P2P several randomized topologies were proposed along with routing algorithms that find short paths in the random graph cf. . In the context of P2P networks, many routing algorithm are able to find paths between nodes even when nodes or links fail cf. . While our findings do not apply directly to these
7003	7008	no faults, then the algorithm reduces to a greedy algorithm which routes along the hypercube's shortest paths. Many popular P2P topologies share some structural similarities with the hypercube cf. . We did not prove that Theorem 3 holds for these topologies, yet it is reasonable to assume that that this is the case. If so then the Theorem implies that if the network suffers many faults,
7003	7009	has some randomness, then the existence of short paths in a graph does not guarantee the ability of efficiently finding them. For instance a cycle with a random matching has a logarithmic diameter , yet paths connecting a given pair of nodes can not be found in less than # n time . This phenomenon is especially acute when considering `natural' networks such as the world wide web, social
7003	7011	a giant component (i.e. a component with #(2 n ) nodes), while if #s0 then w.h.p a giant component will not exist. This result was sharpened by Bollobas et al in  and then by Borgs et al in . A related notion to routing complexity is that of emulation. Roughly speaking, network A emulates network B if A can perform any computation B performs with a constant slowdown. When the emulating
7003	7013	bottlenecks in the computation. Hastad et al  considered node failures, and showed that if p is a constant close enough to 1, then H n,p could emulate H n with a small slowdown. Cole et al  proved that a faulty butterfly network can perform efficient permutation routing even if each node or edge fails with some constant probability. Emulation under worst case faults were considered by
7003	7014	n-dimensional hypercube, when each edge is deleted with probability 1-p and survives with probability p. Random subgraphs of the hypercube had been the focus of much research. It is known (see eg. ) that if ps1 2 then with high probability H n,p is not connected and if p > 1 2 then with high probability H n,p is connected. A classic result by Ajtai, Komlos and Szemeredi  states that if p #
7003	7016	in social networks (and not merely their existence). In the context of P2P several randomized topologies were proposed along with routing algorithms that find short paths in the random graph cf. . In the context of P2P networks, many routing algorithm are able to find paths between nodes even when nodes or links fail cf. . While our findings do not apply directly to these
7003	7019	algorithms that find short paths in the random graph cf. . In the context of P2P networks, many routing algorithm are able to find paths between nodes even when nodes or links fail cf. . While our findings do not apply directly to these networks, we expect that are main result would hold for them as well. See Section 1.3 for more details. In this paper we analyze the algorithmic
7003	7020	probabilities are not always known. It is known that p 2 c = 1 2 and that p d c = (1 + o(1))/2d and is decreasing in d. See the book by Grimmett  and the references therein. Kaklamanis et al  showed that if p is large enough then M 2 p can emulate M d with O(log n) slowdown. Mathies  extended this result for any p > p 2 c = 1 2 . These results do not imply an efficient routing
7003	7021	u and v remain connected in G p . This had been the focus of much research concerning the existence of giant components in such graphs, and the critical values of p for the existence of those, cf. . But in many applications the fact that a path between u and v exists is not sufficient, one wants to be able to find the path in a distributed manner. It is known that if the topology of a graph
7003	5901	ability of efficiently finding them. For instance a cycle with a random matching has a logarithmic diameter , yet paths connecting a given pair of nodes can not be found in less than # n time . This phenomenon is especially acute when considering `natural' networks such as the world wide web, social networks, P2P networks etc, in which typically the network size is huge, the diameter of
7003	5901	the diameter of the network is small and the challenge is to find short paths within a time complexity that is comparable to the diameter. Indeed Kleinberg's model of the small world phenomenon , is aimed at explaining the ability to find short paths in social networks (and not merely their existence). In the context of P2P several randomized topologies were proposed along with routing
7003	7022	u and v remain connected in G p . This had been the focus of much research concerning the existence of giant components in such graphs, and the critical values of p for the existence of those, cf. . But in many applications the fact that a path between u and v exists is not sufficient, one wants to be able to find the path in a distributed manner. It is known that if the topology of a graph
7003	7023	butterfly network can perform efficient permutation routing even if each node or edge fails with some constant probability. Emulation under worst case faults were considered by Leighton et al . In particular these results imply that if the failure probability is small enough then it is possible to find paths efficiently between nodes in the giant component. On the other hand Angel and
7003	7024	in social networks (and not merely their existence). In the context of P2P several randomized topologies were proposed along with routing algorithms that find short paths in the random graph cf. . In the context of P2P networks, many routing algorithm are able to find paths between nodes even when nodes or links fail cf. . While our findings do not apply directly to these
7003	7026	enough then M 2 p can emulate M d with O(log n) slowdown. Mathies  extended this result for any p > p 2 c = 1 2 . These results do not imply an efficient routing algorithm. Naor and Wieder  used planar duality to prove that efficient routing is possible in M 2 p whenever p > 1 2 . Cole et al  proved that a two dimensional array can tolerate a constant fraction of worst case faults
7003	7026	from 0, both u and v are in the giant component and therefore connected. We give an algorithm that efficiently finds a short path from u to v. The case of d = 2 was solved by Naor and Wieder in , where planar duality is used to show that in a two dimensional grid with n 2 vertices, the routing complexity is O(n) w.h.p. It is important to note that it is fairly easy to find a path between
7003	7027	algorithms that find short paths in the random graph cf. . In the context of P2P networks, many routing algorithm are able to find paths between nodes even when nodes or links fail cf. . While our findings do not apply directly to these networks, we expect that are main result would hold for them as well. See Section 1.3 for more details. In this paper we analyze the algorithmic
7003	5902	no faults, then the algorithm reduces to a greedy algorithm which routes along the hypercube's shortest paths. Many popular P2P topologies share some structural similarities with the hypercube cf. . We did not prove that Theorem 3 holds for these topologies, yet it is reasonable to assume that that this is the case. If so then the Theorem implies that if the network suffers many faults,
7003	943	algorithms that find short paths in the random graph cf. . In the context of P2P networks, many routing algorithm are able to find paths between nodes even when nodes or links fail cf. . While our findings do not apply directly to these networks, we expect that are main result would hold for them as well. See Section 1.3 for more details. In this paper we analyze the algorithmic
7003	943	no faults, then the algorithm reduces to a greedy algorithm which routes along the hypercube's shortest paths. Many popular P2P topologies share some structural similarities with the hypercube cf. . We did not prove that Theorem 3 holds for these topologies, yet it is reasonable to assume that that this is the case. If so then the Theorem implies that if the network suffers many faults,
8896927	7030	also useful in video-on-demand systems. These systems are bandwidth-intensive, and will benefit from load-balancing by multiple servers as well as from efficient multicast-based data distribution . Furthermore, these systems are marked by a heterogeneous client population in which clients, depending on their resource constraints, are interested in different subsets of the data (base layers,
8896927	7031	server systems like peer-to-peer systems, data placement on peers cannot be centrally controlled, and the download process must necessarily occur in an ad-hoc manner. Yet another solution  is to encode documents using a long erasure-resilient code. The client can download enough distinct data items that will enable it to recover the original content. This approach, while
8896927	7032	downloads, has its own shortcomings: first, for large data files, the encoding and decoding costs can be prohibitive. Although efficient codes suitable for bulk data distribution have been proposed , they do not solve the more significant problem of flexible access to documents that have an inherent structure. For example, for movie files, storing video and sound-tracks in several languages
8896927	7032	can be prohibitive. In practice, we envision the use of codes with much more efficient encoding and decoding . Specifically, we propose the use of the recently discovered rateless codes . Rateless codes, instead of having a fixed encoding length, use random encoders to generate code symbols in a stateless manner, and are ideally suited for large-scale data distribution
8896927	7033	co-ordinate its download process with each server in a manner that reconciles the servers’ capabilities with its own download requirements. We propose the use of priority encoded transmission (PET)  for parallel retrieval of documents. The basic idea is that each client should determine priorities for each part of the document it retrieves from each server. Servers encode their data according
8896927	943	D. We do not concern ourselves with how a client can locate servers on the network, which is a well-researched topic in its own right. Rather we assume that some mechanism such as described in e.g.  is available by which a client can locate enough servers whose combined output will enable it to download D in its entirety. Assume that in the previous step, the client has located a set of
8896927	2264	can be significant for large documents. More efficient communication can be achieved at the cost of a little efficiencty with a more compact representation of enumerations by using Bloom Filters  to send descriptions and requests. Specifically, to represent a set of counters compactly, a counting Bloom filter  is used. We propose the use of Bloom filters in the following way: Each server
8896927	7035	from multiple servers. However, for large docments, the encoding and decoding complexity can be prohibitive. In practice, we envision the use of codes with much more efficient encoding and decoding . Specifically, we propose the use of the recently discovered rateless codes . Rateless codes, instead of having a fixed encoding length, use random encoders to generate code symbols in a
8896927	7037	a data file in parallel from multiple servers. In streaming media distribution, a set of servers co-operate to transmit a video over a multicast session using a scalable transmission scheme as in . Bulk distribution In bulk data distribution, a receiver downloading from all the servers in parallel is interested in recovering the file in its entirety after receiving the minimal number of
8896927	188	|Pj| = ?(Pj) ?j 7. Performance ?j?1 ? w ? |Pk| (9) k=0 The performance of the abovementioned methods is summarized in Figures 2(a) and 2(b). We used a trace of the Gnutella peer-to-peer network  collected during May 2001. These traces include information about bottleneck bandwidth of peers as well as the amount of data shared. We used the traces in the following way: first we limited
7038	7039	namely, using the thread abstraction. Our focus is on determining the effectiveness and limitations of using thread-allocation to provide QoS guarantees or differentiation. Several studies  have used thread allocation to provide application-level QoS. Vasiliou  focused his thread-based approach on providing a simple method for creating new scheduling disciplines. Similarly, Pandey
7038	7041	requests. B. Workload Model In an Internet server, the workload model captures the arrival of requests and service that each request requires. Both have been studied extensively in the literature . In general, they have been observed to follow heavy-tail distributions. In fact, we have observed similar behavior during our workload analysis (omitted for space considerations). Heavy-tail
7038	7043	requests. B. Workload Model In an Internet server, the workload model captures the arrival of requests and service that each request requires. Both have been studied extensively in the literature . In general, they have been observed to follow heavy-tail distributions. In fact, we have observed similar behavior during our workload analysis (omitted for space considerations). Heavy-tail
7038	7044	(QoS) has been introduced to manage resources when user demands exceed resource supplies. Supporting QoS in servers has been addressed extensively in the literature, for example, in . In particular, application-level QoS mechanisms are designed to provide the necessary QoS guarantees with little or no support from the end-server’s OS . However, since the
7038	7044	and heterogeneous workloads, using thread allocation to provide QoS differentiation is not an effective approach. Fine-grain resource management must be used to provide effective QoS guarantees . Unfortunately, these techniques require substantial changes to the application or the OS. We, however, show that if services are configured with homogeneous workload to provide client-side
7038	7044	improve the performance of existing QoS techniques. VIII. RELATED WORK The design and analysis of server QoS management techniques have been addressed extensively in the literature, for example, in . In general, our work complements existing QoS techniques by providing a rigorous analysis of one particular approach, namely, using the thread abstraction. Our focus is on determining the
7038	7045	purely static, purely dynamic, and mixture of the two — mixed for short. Each workload adheres to the specification provided by SPECWeb99; in general, the requested files follow a Zipf distribution  regardless of whether they are statically or dynamically generated. Figure 3 shows Gk(m) for the three workloads, with the abscissa drawn in log-scale. The first two regions outlined earlier are
7038	7047	(QoS) has been introduced to manage resources when user demands exceed resource supplies. Supporting QoS in servers has been addressed extensively in the literature, for example, in . In particular, application-level QoS mechanisms are designed to provide the necessary QoS guarantees with little or no support from the end-server’s OS . However, since the
7038	7047	improve the performance of existing QoS techniques. VIII. RELATED WORK The design and analysis of server QoS management techniques have been addressed extensively in the literature, for example, in . In general, our work complements existing QoS techniques by providing a rigorous analysis of one particular approach, namely, using the thread abstraction. Our focus is on determining the
7038	7048	for example, in . In particular, application-level QoS mechanisms are designed to provide the necessary QoS guarantees with little or no support from the end-server’s OS . However, since the underlying OS enforces resource transparency (i.e., hides resource management), applicationlevel mechanisms have limited capabilities in enforcing strict service guarantees and
7038	7048	improve the performance of existing QoS techniques. VIII. RELATED WORK The design and analysis of server QoS management techniques have been addressed extensively in the literature, for example, in . In general, our work complements existing QoS techniques by providing a rigorous analysis of one particular approach, namely, using the thread abstraction. Our focus is on determining the
7038	7038	that no matter how high the load increases for the other service classes, it can still (statistically) meet its QoS objective. We studied a more general case in the extended version of this paper , where we also characterized the behavior of the system for multiple classes for any thread allocation. The derivation was based on extending the Markov chain that was presented in Section IV to be
7038	7052	multithreading and process-sharing abstractions of real systems. The MTRR model is an extension of traditional round-robin servers, which are used in the analysis of polling and timeshared systems . Unlike traditional approaches, our model incorporates the performance benefits of increased concurrency into the interaction between the running threads. Using this MTRR model, we are able to
7038	7052	MTRR SERVER We focus in this section on the single service class MTRR server. The analysis, however, requires extension of some of the existing results from queueing theory and time-sharing systems  to include the effects of concurrency gains. This is done by introducing state-dependent service rates through the speedup function. We first consider an idealized model where the scheduling
7038	7052	in the analysis and control of multi-class servers. Direct interdependencies have predictable behavior that can be accurately captured by an analytical model. An ideal time-sharing system, e.g., , is a good example where a thread will run for its entire scheduling quantum, Q, 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004swithout blocking. Unfortunately, this is seldom the case for
7038	7056	Grant CCR-0216977. examine and evaluate the extent to which application-level mechanisms can provide QoS support. One of the more popular application-level solutions is thread-based QoS mechanisms  in which the allocation of threads or processes to each application or service is adjusted (either statically or dynamically) based on some target QoS objectives (Figure 1). Two design principles
7038	7056	final remarks. II. QUANTIFYING CONCURRENCY GAINS Using concurrency to improve server performance is one of the guiding principles for providing thread-based QoS support. This notion was explored in  as an integral part of their feedback control mechanism that increases the allocation of threads to running applications when better performance is needed. Implicit to the effective operation of
7038	7057	Grant CCR-0216977. examine and evaluate the extent to which application-level mechanisms can provide QoS support. One of the more popular application-level solutions is thread-based QoS mechanisms  in which the allocation of threads or processes to each application or service is adjusted (either statically or dynamically) based on some target QoS objectives (Figure 1). Two design principles
7038	7057	final remarks. II. QUANTIFYING CONCURRENCY GAINS Using concurrency to improve server performance is one of the guiding principles for providing thread-based QoS support. This notion was explored in  as an integral part of their feedback control mechanism that increases the allocation of threads to running applications when better performance is needed. Implicit to the effective operation of
7038	7057	namely, using the thread abstraction. Our focus is on determining the effectiveness and limitations of using thread-allocation to provide QoS guarantees or differentiation. Several studies  have used thread allocation to provide application-level QoS. Vasiliou  focused his thread-based approach on providing a simple method for creating new scheduling disciplines. Similarly, Pandey
7038	7057	the authors of  proposed a feedback control mechanism to adjust the allocation of threads to different service classes based on on-line measurement of QoS metrics. Particularly in , a control-theoretic approach is used to implement a Required Thread Alocation (threads) Required Thread Alocation (threads) 25 20 15 10 5 Measured Predicted 0 0 500 1000 1500 2000 2500 3000
7038	7060	requests. B. Workload Model In an Internet server, the workload model captures the arrival of requests and service that each request requires. Both have been studied extensively in the literature . In general, they have been observed to follow heavy-tail distributions. In fact, we have observed similar behavior during our workload analysis (omitted for space considerations). Heavy-tail
7038	7061	and heterogeneous workloads, using thread allocation to provide QoS differentiation is not an effective approach. Fine-grain resource management must be used to provide effective QoS guarantees . Unfortunately, these techniques require substantial changes to the application or the OS. We, however, show that if services are configured with homogeneous workload to provide client-side
7038	7064	Chain representation of MTRR server. We assume that Gk(m) only operates in regions I and II (Figure 2). The point where Gk(m) collapses is hard to predict apriori, but not very difficult to detect . For the purpose of our analysis, we assume that detection/prevention from server thrashing is handled by a separate mechanism. Therefore, we define the speedup function as follows: Gk(m) = ? ?k(m
7038	7065	(QoS) has been introduced to manage resources when user demands exceed resource supplies. Supporting QoS in servers has been addressed extensively in the literature, for example, in . In particular, application-level QoS mechanisms are designed to provide the necessary QoS guarantees with little or no support from the end-server’s OS . However, since the
7038	7065	Grant CCR-0216977. examine and evaluate the extent to which application-level mechanisms can provide QoS support. One of the more popular application-level solutions is thread-based QoS mechanisms  in which the allocation of threads or processes to each application or service is adjusted (either statically or dynamically) based on some target QoS objectives (Figure 1). Two design principles
7038	7065	final remarks. II. QUANTIFYING CONCURRENCY GAINS Using concurrency to improve server performance is one of the guiding principles for providing thread-based QoS support. This notion was explored in  as an integral part of their feedback control mechanism that increases the allocation of threads to running applications when better performance is needed. Implicit to the effective operation of
7038	7065	(1) they share a bottleneck resource such as a disk and (2) they are organized as a series of stages where an incoming request must be processed by multiple services in a particular order . The complexities that are introduced by the latter is akin to those in network of queues , but with dependent service distributions. In this paper, we restrict our analysis to single-stage
7038	7065	Chain representation of MTRR server. We assume that Gk(m) only operates in regions I and II (Figure 2). The point where Gk(m) collapses is hard to predict apriori, but not very difficult to detect . For the purpose of our analysis, we assume that detection/prevention from server thrashing is handled by a separate mechanism. Therefore, we define the speedup function as follows: Gk(m) = ? ?k(m
7067	7068	of the same size, while minimizing the cut size, which, in our problem, is the frequency of transitions from one subset to the other. This problem is NP-hard . Several heuristics are known , but they are not suitable for an implementation in hardware. Our problem is specific. We want a hardware mechanism that learns the program behavior automatically. We are not relying on information
7067	7071	facilitate snooping, one could maintain the inclusion of the L1 caches into all the L2 caches considered as a whole. However in this study, we assumed that the L1 tags were simply replicated, as in . We assume a write-back, write-allocate L2. Because we P P P P active processor broadcasts architectural updates L3 L2 L2 L2 L2 IL1 DL1 IL1 DL1 IL1 DL1 IL1 DL1 migration controller Figure 1:
7067	7071	the one that was fetched the latest), marking this instruction as the transition instruction T , and recording the 1 Accessing a remote cache may take longer than accessing a shared cache level . On the other hand, if there is a direct access to remote caches, a cache-to-cache miss might be faster. transition PC, i.e., the PC of the instruction following T . The I-fetch unit returns the
7067	7072	a total 2-Mbyte on-chip L2 capacity. The cache line size is still 64 bytes. The IL1 and DL1 caches are both 16 Kbytes, 4-way set-associative. The L2 cache is 512-Kbyte, 4-way skewed-associative . The DL1 is write-through, non write-allocate, while the L2 is write-back, writeallocate. The affinity cache has 8k entries and is 4-way skewed-associative. We apply 25% working-set sampling, as
7067	7073	execution resources can be replicated not only for exploiting execution parallelism, but also for dealing with a large amount of memory. This idea was re-visited for the DataScalar architecture . The concept of execution migration is somewhat related to DataScalar. However, there are significant differences. DataScalar works at the physical memory level, with a static binding between
7067	7074	infrastructure . Our simulator is built on top of the PISA functional simulator. We show results for 13 benchmarks from the SPEC CPU2000 suite , and 5 benchmarks from the Olden suite  (we used the sequential version by Amir Roth ). Benchmarks from these 164.gzip 171.swim 172.mgrid 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 16k 64k 256k 1M 4M 16M normal split trans: 0.0026 0 0.1
7067	7074	on a single-chip multi-core. We believe this approach is not viable on a traditional parallel machine. The term &quot;computation migration&quot; has been used in the context of parallel machines (e.g., ). The goal was to speed-up parallel programs. Execution migration, on the other hand, is a microarchitectural technique that concerns sequential programs. An approach related to ours is the
7067	3869	problem, by analogy with the so-called &quot;memory-wall&quot;. We propose execution migration as a possible answer. Our approach is based on the assumption that future processors will be multi-cores . Some commercial multi-cores are already available, e.g., the IBM Power4 , featuring 2 execution cores on the same chip. As it becomes difficult to exploit higher degrees of instruction-level
7067	7080	is less than 60 times the L2-miss/L3-hit penalty, i.e., P mig ! 60, we will observe performance gains on 181.mcf. 5. Related work The &quot;cache-wall&quot; problem has been pointed out in a recent study . It was observed that by breaking a large L2 cache into several banks, it is possible to access some parts of the cache faster. This leads to a non-uniform cache structure which tries to
7067	7082	on top of the PISA functional simulator. We show results for 13 benchmarks from the SPEC CPU2000 suite , and 5 benchmarks from the Olden suite  (we used the sequential version by Amir Roth ). Benchmarks from these 164.gzip 171.swim 172.mgrid 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 16k 64k 256k 1M 4M 16M normal split trans: 0.0026 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 16k 64k 256k 1M
7067	7083	microarchitectures (e.g., HP PA-8x00, Intel P6). Some processors, on the other hand, merge the architectural and rename registers in a single physical register file (e.g., MIPS R10000, Alpha 21264) . These processors should be modified so that the active retirement unit is able to broadcast register values. This can be done either by having extra read ports on the physical register file, or by
530294	7087	Griffin and Wilfong in  used those conditions to give a simple path-vector routing algorithm that dynamically detects policy-induced route oscillations. In addition, Gao, Griffin, and Rexford in  combine the results from these papers to modify a simplified version of BGP to perform stable back-up routing. Building on this work, the papers by Griffin, Jaggard, and Ramachandran  and
530294	7087	specification of the protocol itself. They present an application of their frameworks that generalizes systems like Hierarchical BGP (mentioned above), incorporating the policy guidelines from  into the design of protocol systems. This paper completes the analysis of this application: It answers the questions left open in  and gives results that can be used in the general design of
530294	7088	network. 1.1 Related Work Several papers have presented policy-configuration guidelines or design principles for path-vector routing so that global routing anomalies can be avoided. Gao and Rexford  showed that route preferences and scoping consistent with the Hierarchical-BGP example mentioned above give stable path-vector routing without global coordination. Griffin, Shepherd, and Wilfong in
530294	7088	specification of the protocol itself. They present an application of their frameworks that generalizes systems like Hierarchical BGP (mentioned above), incorporating the policy guidelines from  into the design of protocol systems. This paper completes the analysis of this application: It answers the questions left open in  and gives results that can be used in the general design of
530294	7089	configuration, and, on the other hand, guaranteeing that the protocol will converge robustly, i.e., predictably, even in the presence of link and node failures. Griffin, Jaggard, and Ramachandran  showed that achieving all three of these design goals requires a non-trivial global constraint on the network, but they left open the question of how to identify and enforce the constraint. This
530294	7089	Rexford in  combine the results from these papers to modify a simplified version of BGP to perform stable back-up routing. Building on this work, the papers by Griffin, Jaggard, and Ramachandran  and Sobrinho  developed formal models for path-vector routing so that properties of path-vector protocols can be studied without involving details of protocol dynamics or of actual networks.
530294	7090	attach information to a route as it is shared throughout the network. However, without global coordination, interaction of locally configured policies can lead to global routing anomalies , e.g., route oscillation and inconsistent recovery from link failures. ? This work was partially supported by the U.S. Department of Defense (DoD) University Research Initiative (URI) program
530294	7090	showed that route preferences and scoping consistent with the Hierarchical-BGP example mentioned above give stable path-vector routing without global coordination. Griffin, Shepherd, and Wilfong in  defined an abstraction of path-vector routing and identified a sufficient condition for local policies that prevents policy-induced route oscillation, and Griffin and Wilfong in  used those
530294	7090	specification of the protocol itself. They present an application of their frameworks that generalizes systems like Hierarchical BGP (mentioned above), incorporating the policy guidelines from  into the design of protocol systems. This paper completes the analysis of this application: It answers the questions left open in  and gives results that can be used in the general design of
530294	7091	and Wilfong in  defined an abstraction of path-vector routing and identified a sufficient condition for local policies that prevents policy-induced route oscillation, and Griffin and Wilfong in  used those conditions to give a simple path-vector routing algorithm that dynamically detects policy-induced route oscillations. In addition, Gao, Griffin, and Rexford in  combine the results
530294	7092	preference routing. Furthermore, any protocol specification that can be described by a countableweight, monotone path-vector algebra  can also be described by a class-based path-vector system . In this paper, we provide the best known robustness constraint for classbased systems. The constraint ensures the robustness of networks that satisfy it; it is in fact the best possible robustness
7094	7096	the operation to be performed. CODEX does not include an implementation for |=? nor does it fix a representation for credentials. However, contemporary authorization engines, like SDSI, KeyNote or the work of Hayton et al. , do provide such implementations and could well be incorporated into CODEX. 2 The lack of any sort of key deletion operation in CODEX is deliberate. After a key
8919538	7117	not surprising, given the experience that correctness proofs can often be difficult. In practical languages such as ML, it is therefore difficult to include dependent types. The approach taken in  is to restrict index objects to be drawn Draft of November 29, 2001s7.2 Dependently Typed Data Structures 151 from a domain with a decidable equality theory. This appears to be a reasonable
8919538	7118	we will use lists with elements of some type A, indexed by their length. This is of practical interest because dependent types may allow us to eliminate bounds checks statically, as demonstrated in . Natural Numbers. First, the formation and introduction rules for natural numbers in unary form. natI0 ?; ·?0:nat natF ?; ·?nat : type ?; ? ? M : nat natI1 ?; ? ? s(M):nat There a two destructors:
10780	7129	a significant loss of precision. One such possibility is random sampling where only a subset of the transitions present in the trace is used. More sophisticated methods include sequence compaction  and sequence synthesis. These approaches aim at reproducing accurately the input correlations with traces that are much smaller than the initial trace. Another possibility is to model the temporal
10780	7130	and spatial correlations at the primary inputs by generalizing the concept of a trace and creating a finite state machine that creates a sequence of inputs with the statistics of that trace . This generalized trace is not necessarily obtained from an actual run of the circuit since it may include don’t cares, allowing the user to model accurately a large number of input dependences.
10780	7131	accurately a large number of input dependences. However, this approach requires a complex procedure to synthesize the finite state machine and to perform the power estimation. Entropy based methods  can also be included in this class since they use an input sequence to extract input and output entropies of a circuit. These methods use the concept of computational work (which is obtained from
10780	7132	accurately a large number of input dependences. However, this approach requires a complex procedure to synthesize the finite state machine and to perform the power estimation. Entropy based methods  can also be included in this class since they use an input sequence to extract input and output entropies of a circuit. These methods use the concept of computational work (which is obtained from
10780	7132	and output signals), as a measure of the average activity in a circuit. Najm observes that average power is proportional to the product of the circuit area and the average node transition activity . However, these methods may incur in a significant loss of accuracy since they do not take into account the details of the circuit structure. All these methods require the existence of a trace to
10780	7133	methods have the potential to be used independently of the existence of an actual trace. The simplest approach is based on the assumption that the primary inputs are uncorrelated in time and space . The user may specify a probability P1 that a given bit is at 1, which, under the independence assumptions used, gives the probability of a transition in that bit as 2P1(1 ? P1). This modeling is
10780	7133	modules that can be combined with the input modeling described in the previous section to perform accurate power estimation.sThis approach can be viewed as a generalization of the work of Ghosh . His approach is based on the computation, for each node, of a function (represented by a BDD) that describes the conditions under which that node switches. With this function it is possible to
10780	7133	The results obtained assuming both spatial and temporal correlations of the signals are compared with those obtained if independence between input bits, both in time and space, is considered . The fourth and fifth column of the table show the power estimated using word level transition statistics, where the input words follow binary and Gray codes, respectively. These estimations use
10780	7134	leads, in many cases, to estimates that are not precise enough. Several methods that aim at modeling spatial and temporal correlations more accurately have been presented. One such method  models the pairwise spatial correlations of the primary inputs and propagates them through the circuit. The advantage of this procedure is that, by using these coefficients, the process only
10780	7135	accurately spatial and temporal correlations is based on the use of algebraic decision diagrams (ADDs) to characterize the dissipation incurred by each module under each particular input transition . This method has the significant advantage that it permits the exact computation of the power dissipated in a combinational module given the ADD that represents the switched capacitance as a
10780	7136	procedure is outlined in section 5. At present, the functions Fi, as well as all other functions required by the approach, are manipulated using a standard binary decision diagram (BDD) package . 4 Pattern independent characterization of modules This section presents a new approach for the characterization of combinational modules that can be combined with the input modeling described in
10780	7139	be used. The most commonly used approach to determine the coefficients used in the RTL power models is to simulate the module using random inputs and to adjust the model using linear regression . This procedure has several disadvantages: • the model may be biased due to the input-pattern dependency problem associated with simulation-based techniques • the model is independent of the
10780	7140	be used. The most commonly used approach to determine the coefficients used in the RTL power models is to simulate the module using random inputs and to adjust the model using linear regression . This procedure has several disadvantages: • the model may be biased due to the input-pattern dependency problem associated with simulation-based techniques • the model is independent of the
9986972	7143	traffic conditions betweensthe involved parties. A simulative comparison between this novel mechanism and other existing adaptive algorithms revealed that BoAT succeeds in offering an adequate QoS . On the other hand, BoAT employs a cryptographic protocol, which adopts a lightweight securing mechanism based on the use of a stream cipher (see, e.g., ) and of a sequence of secret keys
9986972	7143	in the next section, we employ such an approach to model the temporal behavior of each component of the audio protocol specification. 4 Performance and Security Analyses of BoAT In a previous work  we conducted a simulative analysis on an algebraic specification of BoAT (based on the process algebra EMPAgr ) to get some performance measures related to the QoS offered by the adaptive
9986972	7144	. Such an analysis is automatically conducted with the software tool TwoTowers . From a security viewpoint, the same system model is analyzed by employing a probabilistic extension  of the noninterference approach  to the information flow theory. The rest of the paper is organized as follows. In Sect. 2, we briefly recall BoAT and its main features. In Sect. 3, we
9986972	7144	within P are high-level actions.sNow, we informally describe the semantics of the operators and the probabilistic model through some examples (for a formal presentation the reader should refer to ). Example 1. Let us consider the system Writer ? p {produce} Buffer, described as the interaction of two processes, Writer and Buffer. The communication interface {produce} says that the two
9986972	7144	is not appropriate to reveal those interferences that are not solely nondeterministic, since they may depend on additional information, like probabilities and time . Along this line, in  we have proposed a probabilistic extension to the nondeterministic information flow theory of  based on the probabilistic process algebra surveyed above, which is intended to: – capture those
9986972	7144	2. checking the semantic equivalence between such derived models. The definition of the submodels to be compared depends on the definition of SP. One of the most intuitive properties described in  is the Probabilistic Bisimulation Nondeducibility on Composition property (PBNDC ), which informally says that the probabilistic low-level view of a system P in isolation is not to be altered when
9986972	7144	the external environment, represented by any process ? (enabling high-level actions only) put in parallel with P . Finally, ?P B is the equivalence relation, called weak probabilistic bisimulation , which is a probabilistic version (inspired by ) of the classical weak bisimulation of . If the two views of the system are indistinguishable from the standpoint of an external
9986972	7145	dev equal to 50 ms, the lifetime of each session key is about three times the expected value. The unwanted behavior described above can be avoided by implementing a version of BoAT proposed in , which suggests to vary, during the conversation lifetime, the time interval between two consecutive handshaking phases. More precisely, in order to make difficult for the intruder a precise
9986972	7147	(enabling high-level actions only) put in parallel with P . Finally, ?P B is the equivalence relation, called weak probabilistic bisimulation , which is a probabilistic version (inspired by ) of the classical weak bisimulation of . If the two views of the system are indistinguishable from the standpoint of an external observer that can access the low-level part only, then no
9986972	7149	based performance measures (expressible by attaching rewards to actions) through markovian analysis techniques . Such an analysis is automatically conducted with the software tool TwoTowers . From a security viewpoint, the same system model is analyzed by employing a probabilistic extension  of the noninterference approach  to the information flow theory. The rest of
9986972	7149	in order to obtain steady state based performance measures, we adequately attach rewards to actions and we analyze the related DTMC. To this end, we resorted to the software tool TwoTowers , which has been extended to support the generative-reactive approach of our probabilistic calculus. Such a tool also implements the algebraic reward based method needed to specify and derive
9986972	7152	and buffer size are always proportioned to the traffic conditions, and (ii) the brief lifetime of each session key makes it harder any cryptanalysis attack conducted by an adversary (see, e.g., , where it is shown that a few seconds of conversation are enough to complete a cryptanalysis attack against a stream cipher). 3 A Process-algebraic Framework 3.1 The Probabilistic Calculus Basic
9986972	7152	i.e. the time between two consecutive handshaking phases), and the robustness of the stream cipher used by BoAT may depend on the quantity of data encrypted with the same key (see, e.g., ), then the probability of cracking a session key increases if several consecutive handshaking protocols fail, because in such a case the same key is used to encrypt several seconds of conversation.
9986972	7152	of each session key noticeably greater than 5 sec (instead of 1 sec as expected by the protocol), which in many cases is more than enough to crack the session key and the encrypted data (see, e.g., ). Anyway, in such a scenario we also have that the audio packet loss measured at the receiving site is about 16%, which represents a limiting performance typical of highly overloaded channels.
9986972	7156	described in Sect. 3 we have to single out the high-level actions and the low-level ones, so that the high and low behaviors of the system can be specified. According to an approach proposed in  for the analysis of noninterference properties of cryptographic protocols, the high level expresses the external, possibly dishonest environment, where the intruders act in order to interfere with
9986972	7157	and the algebraic operators, which in our calculus are equipped with probabilistic information. The model of probabilities we adopt is a mixture of the generative and reactive approaches of . In particular, we assume the output actions behaving as generative actions (a generative process autonomously decides, on the basis of a probability distribution, which action will be executed and
9986972	7157	actions of P are proportionally redistributed (similarly for Q), as shown both in Example 1 and in Example 2 (note that this is a standard approach when restricting actions in the generative model ); ??? in case of synchronizing generative actions a of P , their probability is distributed among the multiple actions a obtained by synchronizing with reactive actions a? executable by Q, according
8832677	7178	characteristics (e.g., power-law connectivity distributions of Autonomous Systems, Faloutsos, Faloutsos, and Faloutsos 1999), and instability of convergence using the Border Gateway Protocol (BGP) (Labovitz et al. 2001). There is a strong suspicion that network behaviors are heavily influenced by network topology. Research projects for mapping the Internet have produced diverse tools and techniques for observing
8832677	7178	shortest path (Tangmunarunkit, Govindan, and Shenker 2001). Moreover, it will take some time for the routing protocol to respond to changes in the network so connectivity may temporarily be lost (Labovitz et al. 2001). Consequently, for a realistic model of connectivity it is necessary to have a realistic model of routing. The SSFNet simulator contains detailed models of the Border Gateway Protocol (BGP)
8832677	7186	in itself. Finding representative generalizations of network topologies is an active research area, and there appears to be an ongoing debate as to what the most relevant features to capture are (Tangmunarunkit et al. 2002). Moreover, the relative importance of features of the topology depends on the system being studied and the effects of topology on protocols is not well known in many cases. When contemplating the
7189	7191	feature-based, seeding strategies such as those presented by Verma et al.  or Sanna et al. , or (3) interactive seeding strategies using a streamline seeding rake used by Bryson and Levit  or Schultz et al. . Our approach falls into the third category–an interactive streamline seeding strategy. A schematic of our interactive streamline seeding tool is shown in Figure 9. This tool
7189	7196	seeding strategies are often used: (1) image-based seeding strategies such as that described by Turk and Banks  or the evenly spaced-streamline seeding strategy presented by Jobard and Lefer , (2) topological or feature-based, seeding strategies such as those presented by Verma et al.  or Sanna et al. , or (3) interactive seeding strategies using a streamline seeding rake used
7189	7198	of the flow with complete coverage of the vector field. In this paper we use advection approaches according to Image Based Flow Visualization (IBFV)  and Image Space Advection (ISA) , which can generate both Spot Noise  and LIC-like  imagery. These approaches are related to LagrangianEulerian Advection (LEA) . We note that a full comparison of texture-based flow
7189	7198	is clear in an animated sequence. In general, computation time is a disadvantage with many texture-based flow visualization techniques. Recently, this hurdle has been overcome with some techniques . However, this approach does not generally provide as much spatial continuity as geometric techniques. In Figure 5 both the dye injection and the texture-based flow visualization techniques reveal
7189	7198	proper rendering order to the objects. All other objects such as streamlines must be rendered before applying texture synthesis so that the image overlay from ISA does not cover these other objects . One perceptual problem with the result in Figure 1 left is occlusion. Figure 1 middle-left illustrates the use of a clipping plane to reveal occluded flow strucFigure 10: The visualization of 3D
7189	7199	is clear in an animated sequence. In general, computation time is a disadvantage with many texture-based flow visualization techniques. Recently, this hurdle has been overcome with some techniques . However, this approach does not generally provide as much spatial continuity as geometric techniques. In Figure 5 both the dye injection and the texture-based flow visualization techniques reveal
7189	7199	normal component of the flow with respect to the isosurface in the visualization include: (1) varying the texture convolution filter  or the surface color opacity according to the cross flow  or the possibility of not projecting the velocity vectors onto the surface . For a more detailed discussion on this topic we refer to previous literature . Figure 1 left shows a hybrid
7189	7206	flow field is via a streamline seeding strategy. In general, three popular streamline seeding strategies are often used: (1) image-based seeding strategies such as that described by Turk and Banks  or the evenly spaced-streamline seeding strategy presented by Jobard and Lefer , (2) topological or feature-based, seeding strategies such as those presented by Verma et al.  or Sanna et al.
7189	7208	methods offer a dense representation of the flow with complete coverage of the vector field. In this paper we use advection approaches according to Image Based Flow Visualization (IBFV)  and Image Space Advection (ISA) , which can generate both Spot Noise  and LIC-like  imagery. These approaches are related to LagrangianEulerian Advection (LEA) . We note that a full
7189	7208	is clear in an animated sequence. In general, computation time is a disadvantage with many texture-based flow visualization techniques. Recently, this hurdle has been overcome with some techniques . However, this approach does not generally provide as much spatial continuity as geometric techniques. In Figure 5 both the dye injection and the texture-based flow visualization techniques reveal
7189	7209	include: (1) varying the texture convolution filter  or the surface color opacity according to the cross flow  or the possibility of not projecting the velocity vectors onto the surface . For a more detailed discussion on this topic we refer to previous literature . Figure 1 left shows a hybrid visualization using direct colormapping, streamlines, a velocity isosurface, and
7189	7210	Turk and Banks  or the evenly spaced-streamline seeding strategy presented by Jobard and Lefer , (2) topological or feature-based, seeding strategies such as those presented by Verma et al.  or Sanna et al. , or (3) interactive seeding strategies using a streamline seeding rake used by Bryson and Levit  or Schultz et al. . Our approach falls into the third category–an
7189	7211	is clear in an animated sequence. In general, computation time is a disadvantage with many texture-based flow visualization techniques. Recently, this hurdle has been overcome with some techniques . However, this approach does not generally provide as much spatial continuity as geometric techniques. In Figure 5 both the dye injection and the texture-based flow visualization techniques reveal
7189	7212	to velocity magnitude and, (c) 3D streamlines. We have also looked at 3D texture-based flow visualization based on a 3D IBFV implementation  and a programmable graphics hardware implementation  in the context of investigating swirl and tumble flow. We generally find perceptual problems to be a great challenge with this approach. Similar to the resampling approach, a trade-off must be made
7213	7215	of its environment as it operates. 6 RELATED WORK Many proposals for models and architectures of individual agents exist. Some of these proposals are based on logic programming, for example IMPACT , Minerva , GOLOG  , and IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and
7213	7215	whereas in the KGP model they change dynamically via the Goal Decision capability. There are features in some other approaches that are absent in the KGP model. BDI and more so the IMPACT system  allow agents to have in their knowledge bases representations of the knowledge of other agents. These systems allow the agents both some degree of introspection and ability to reason about other
7213	7216	and they have to function in circumstances where they have incomplete information. KGP is motivated, on the one hand, by the existing gap between modal logic specifications  of BDI agents  and their implementation (see issues raised by Rao in ) and, on the other, to make available and extend many useful computational logic (CL)  tools and techniques whose synthesis can
7213	7216	on logic programming, for example IMPACT , Minerva , GOLOG  , and IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and its variants, 3APL , and DESIRE . At a high level of comparison there are similarities in the objectives of these models and the KGP model, in that they all
7213	7216	of capabilities. Each one of these is specified declaratively and equipped with its own provably correct computational counterpart. There is an obvious similarity between the KGP and the BDI models  given by the correspondence between KGP’s Knowledge, Goals and Plan and BDI’s Beliefs, Desires and Intentions, respectively. However, the BDI model is based on modal logic and the gap between its
7213	7217	IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and its variants, 3APL , and DESIRE . At a high level of comparison there are similarities in the objectives of these models and the KGP model, in that they all aim at specifying knowledge-rich agents with certain desirable
7213	7219	pattern of behaviour. Cycle theories are written in the framework of LPP, for which we adopt the concrete framework of LPwNF  suitably extended to deal with conditional, dynamic priorities . Other frameworks for LPP (e.g. ) or for the declarative specification of preference policies, (e.g. ), can be used. The LPwNF framework, as any other LPP framework, is equipped with a
7213	7221	WORK Many proposals for models and architectures of individual agents exist. Some of these proposals are based on logic programming, for example IMPACT , Minerva , GOLOG  , and IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and its variants, 3APL , and DESIRE . At a
7213	7221	semantics to its agents. It has a number of differences, e.g. MINERVA relies on Multidimensional Dynamic Logic Programming  and uses explicit rules for updating its knowledge bases. IndiGolog  is a high-level programming language for robots and intelligent agents that supports, like KGP, on-line planning, sensing and plan execution in dynamic and incompletely known environments. It is a
7213	7223	the model synthesises Abductive Logic Programming (ALP)  and Logic Programming with Priorities (LPP) , both extended to deal with constraint solving as in Constraint Logic Programming (CLP) . CL is used in KGP to specify the individual state of the agent, its reasoning capabilities, state transitions, and its control. Using 1 Department of Computer Science, University of Cyprus,
7213	7224	(Societies of Computees) http://lia.deis.unibo.it/research/projects/socs. Details of the declarative and computational models together with examples can be found in the deliverables of this project . The rest of the paper is structured as follows. We describe the organisation of the internal state of a KGP agent in section 2 and the way reasoning capabilities access this state in section 3. In
7213	7225	useful computational logic (CL)  tools and techniques whose synthesis can produce executable specifications of agents. For this purpose, the model synthesises Abductive Logic Programming (ALP)  and Logic Programming with Priorities (LPP) , both extended to deal with constraint solving as in Constraint Logic Programming (CLP) . CL is used in KGP to specify the individual state of
7213	7229	operates. 6 RELATED WORK Many proposals for models and architectures of individual agents exist. Some of these proposals are based on logic programming, for example IMPACT , Minerva , GOLOG  , and IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and its variants, 3APL , and
7213	7229	robots and intelligent agents that supports, like KGP, on-line planning, sensing and plan execution in dynamic and incompletely known environments. It is a member of the Golog family of languages  that use a Situation Calculus theory of action to perform the reasoning required in executing the program. Instead in the KGP model we rely on ALP and LPP combined with an Event Calculus approach
7213	7230	whose synthesis can produce executable specifications of agents. For this purpose, the model synthesises Abductive Logic Programming (ALP)  and Logic Programming with Priorities (LPP) , both extended to deal with constraint solving as in Constraint Logic Programming (CLP) . CL is used in KGP to specify the individual state of the agent, its reasoning capabilities, state
7213	7230	are written in the framework of LPP, for which we adopt the concrete framework of LPwNF  suitably extended to deal with conditional, dynamic priorities . Other frameworks for LPP (e.g. ) or for the declarative specification of preference policies, (e.g. ), can be used. The LPwNF framework, as any other LPP framework, is equipped with a notion of entailment, that we refer to as
7213	7231	have incomplete information. KGP is motivated, on the one hand, by the existing gap between modal logic specifications  of BDI agents  and their implementation (see issues raised by Rao in ) and, on the other, to make available and extend many useful computational logic (CL)  tools and techniques whose synthesis can produce executable specifications of agents. For this purpose,
7213	7231	IMPACT , Minerva , GOLOG  , and IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and its variants, 3APL , and DESIRE . At a high level of comparison there are similarities in the objectives of these models and the KGP model, in that they all aim at specifying
7213	7231	between its specification and its practical realisation is much wider than with KGP. The same difference exists between the KGP model and Agent0 and its later refinement PLACA . AgentSpeak(L)  attempts to narrow the gap between the specification and executable model of BDI and in that it shares one of the objectives of the KGP. Two other differences between the KGP and Agent0 andsPLACA
7213	7232	in their environment and they have to function in circumstances where they have incomplete information. KGP is motivated, on the one hand, by the existing gap between modal logic specifications  of BDI agents  and their implementation (see issues raised by Rao in ) and, on the other, to make available and extend many useful computational logic (CL)  tools and techniques whose
7213	7232	on logic programming, for example IMPACT , Minerva , GOLOG  , and IndiGolog . Other proposals are based on modal or first order logic or are not logic based, for example the BDI model , Agent0 , AgentSpeak  and its variants, 3APL , and DESIRE . At a high level of comparison there are similarities in the objectives of these models and the KGP model, in that they all
7213	7234	of the model and proof of various properties. For the purposes of experimentation, a first prototype implementation has already been completed successfully using SICStus Prolog, Java, and JXTA . The KGP model was developed as part of the EU (IST: FET - Global Computing) research project, SOCS (Societies of Computees) http://lia.deis.unibo.it/research/projects/socs. Details of the
7237	7243	is allowed, we provide a straightforward proof that verification is co-NP hard even in the case where the pi’s and qj’s are finite and determinate. More results of this kind can be found in . Hence, polynomial verification is rather unlikely in this case. In order to understand whether this intractability result rules out application of our techniques, we consider an example. This is
7237	7243	where there is communication between the components, we show that the verification is co-NP hard, and hence inherently intractable. The proof that we give is a simplified variant of those given in . From these observations we draw the conclusion that verification via decomposition is especially worthwhile when there are relatively many asynchronous or non-communicating actions, and that its
7237	7243	and bisimulation equivalence. We give a straightforward proof of this fact, actually showing that in the case that p i and qj are all finite and determinate, this verification is co-NP complete. In  it is shown that this verification becomes P-space hard if pi and qj are finite state. It also gives an EXPSPACE completeness result in case abstraction of actions is allowed. The proof technique
7237	7243	· ak is always a trace of p, it must also be a trace of (?i=1 pi) ? p, while it cannot be a trace of p ? . ? It is not difficult to extend the proof above to include only two-way communication (see ) or to use only two actions. However this is outside the setting of this paper, and it complicates matters slightly. 6 An application In this section, we apply the decomposition theorem to Milner’s
9006067	7248	is a shared and common understanding of some domain that can be communicated across people and computers . Ontologies can therefore be shared and reused among different applications . An ontology can be defined as a formal, explicit specification of a shared conceptualization . “Conceptualization” refers to an abstract model of some phenomenon in the world by having
9006067	7248	KIM , which is based on GATE , Annotea  of W3C., Amilcare  of the Open University (also based on GATE), and AeroSWARM 1 . For an overview of those approaches and others, see . All approaches use NLP as an important factor to extract semantic information. Our approach is innovative in the sense that it combines four different techniques for Information Extraction in a
9006067	6101	. Ontologies can therefore be shared and reused among different applications . An ontology can be defined as a formal, explicit specification of a shared conceptualization . “Conceptualization” refers to an abstract model of some phenomenon in the world by having identified the relevant concepts of that phenomenon. “Explicit” means that the type of concepts used, and
9006067	7250	with actual information on most countries of the world, along with relevant information in the fields of geography, politics, society, economics, etc. We have used the competency questions approach  to determine the scope and granularity of the domain ontology. Some examples of competency questions that we considered include: What countries are participating on Iraq campaign? Who is the head
9006067	7252	In our case semantic access is provided through: 1. Semantic publishing and navigation 2. Semantic search engine 3. 3D Visualization 3.1 Semantic Publishing and Navigation Experience has shown  that the knowledge base as modeled by domain experts and knowledge engineers is not always a good candidate to visualize it as is.sFig. 2. Explicit visualization using direct translator The main
9006067	7254	5. Related Work Our Knowledge Parser is related to several other initiatives in the area of automatic annotation for the Semantic Web, including KIM , which is based on GATE , Annotea  of W3C., Amilcare  of the Open University (also based on GATE), and AeroSWARM 1 . For an overview of those approaches and others, see . All approaches use NLP as an important factor to
8919553	7262	in a well defined way, while requiring minimal change to current patterns of use. 1. INTRODUCTION Agent communication languages (ACLs) such as the Knowledge Query and Manipulation Language (KQML)  and the Foundation for Intelligent Physical Agents (FIPA) ACL  are based on the idea that “communication can be best modelled as the exchange of declarative statements” . Under this paradigm,
8919553	7265	Language (KQML)  and the Foundation for Intelligent Physical Agents (FIPA) ACL  are based on the idea that “communication can be best modelled as the exchange of declarative statements” . Under this paradigm, agents send, receive and reply to requests for services and information, with the intent of the message specified by a performative (such as ‘inform’ or ‘request’) describing
8919555	7271	ranks can be performed using more sophisticated techniques involving linear combination of individual ranks and parameter optimization, similar to work on combining multiple ranked document lists (Bartell et al., 1994). The results of this experiment should be taken with caution and cannot be easily generalized without further evidence, because they were obtained for specific ensembling methods and parameter
8919555	4750	has shown that ensembling multiple classifiers, whether produced by single or different learning algorithms, may be a viable technique for improving classification accuracy (Larkey and Croft, 1996; Breiman, 1996; Dietterich, 1997). Two keys to success are that the individual classifiers must disagree with one another and that their average accuracies must be comparable. In this case one can try to guess
8919555	7273	historically, work well, due to losses in precision being higher than gains in recall, it has recently received renewed attention for its successful application to large scale collections (e.g., Buckley et al., 1995; Xu and Croft, 1996; Fitzpatrick and Dent, 1997, Mitra et al., 1998). In the TREC environment, for instance, more recently almost all groups have been using variations on expanding queries using
8919555	7273	fact, variants of Robertson’s ranking scheme for expansion terms have subsequently been used by various systems, with different weighting functions and different methods for estimating pt and qt (Buckley et al., 1995; Robertson et al., 1995, Hawking et al., 1998). An alternative, more recent, approach to using the differences in term distribution for selecting expansion term relies on the relative entropy, or
8919555	7273	affect performance results. Although we have not fully worked out this aspect, we tried also different estimation functions such as the number of pseudo-relevant documents that contain the term (Buckley et al., 1995; Robertson et al., 1995), which however seemed to produce worse retrieval effectiveness. Finally, all term-ranking methods tested in the experiment required two values for practical implementation:
8919555	7274	of a query “context” by using such diverse knowledge sources as user’s relevance feedback (Harman, 1992), thesauri (Cooper and Byrd, 1997), 1sand conceptual clustering of documents and terms (Carpineto and Romano, 1998), rather than concentrating on better ways of matching queries against documents. One well known, automatic approach to adding contextual information to user queries is based on the extraction of
8919555	7278	ensembling multiple classifiers, whether produced by single or different learning algorithms, may be a viable technique for improving classification accuracy (Larkey and Croft, 1996; Breiman, 1996; Dietterich, 1997). Two keys to success are that the individual classifiers must disagree with one another and that their average accuracies must be comparable. In this case one can try to guess the right prediction
8919555	7281	in response to the query from the collection at hand, although more sophisticated schemes for locating the candidate expansion terms have been proposed, such as using passages (Xu and Croft, 1996, Hawking et al., 1998), or using the result of past similar queries (Fitzpatrick and Dent, 1997), or running the initial pass on a much larger collection than the target collection (Singhal et al., 1999). 2.2 Expansion
8919555	7281	for expansion terms have subsequently been used by various systems, with different weighting functions and different methods for estimating pt and qt (Buckley et al., 1995; Robertson et al., 1995, Hawking et al., 1998). An alternative, more recent, approach to using the differences in term distribution for selecting expansion term relies on the relative entropy, or KullbackLieber distance, between the two
8919555	7283	in general, a small improvement over the scores obtained by the best individual method. However, as combination strategies work best when the results being combined are 10sgenerated independently (Hull et al., 1996), there are reasons to believe that such an improvement could be higher if we weakened some experimental parameters that are likely to increase the correlation between the term-relevance estimates
8919555	7284	information retrieval has shown that ensembling multiple classifiers, whether produced by single or different learning algorithms, may be a viable technique for improving classification accuracy (Larkey and Croft, 1996; Breiman, 1996; Dietterich, 1997). Two keys to success are that the individual classifiers must disagree with one another and that their average accuracies must be comparable. In this case one can
8919555	7285	gains in recall, it has recently received renewed attention for its successful application to large scale collections (e.g., Buckley et al., 1995; Xu and Croft, 1996; Fitzpatrick and Dent, 1997, Mitra et al., 1998). In the TREC environment, for instance, more recently almost all groups have been using variations on expanding queries using information from the top retrieved documents, but the benefits of
8919555	7285	main conceptually distinct approaches. One straightforward solution is to rank the candidate expansion terms using the (primary) term weights w(t) computed for document ranking (Srinivasan, 1996; Mitra et al., 1998; Singhal et al., 1999). Usually, the score used for inclusion in the expanded query is given by r ? k= 1 wt () Doc k , where the summation index ranges over the first r retrieved documents. This
8919555	7287	ranking scheme for expansion terms have subsequently been used by various systems, with different weighting functions and different methods for estimating pt and qt (Buckley et al., 1995; Robertson et al., 1995, Hawking et al., 1998). An alternative, more recent, approach to using the differences in term distribution for selecting expansion term relies on the relative entropy, or KullbackLieber distance,
8919555	7287	results. Although we have not fully worked out this aspect, we tried also different estimation functions such as the number of pseudo-relevant documents that contain the term (Buckley et al., 1995; Robertson et al., 1995), which however seemed to produce worse retrieval effectiveness. Finally, all term-ranking methods tested in the experiment required two values for practical implementation: the number of
14560490	7296	taking Distributed Database (or Database Implementation courses) can gain a better understanding and experience of issues involved in transaction processing in a distributed environment . The second motivation is to use Rainbow as a research tool to conduct scientific experiments on distributed database and transaction processing. To achieve these goals, Rainbow allows the user to
94971	7298	specification discovery tool to discover a specification (ArrayList.spec) from an implementation (ArrayList.java). While specification discovery tools are effective in discovering specifications , the specifications they produce may be both unsound and incomplete. This is because the most effective specification discovery tools (to our knowledge) are based on analyzing program runs rather
94971	665	they do not interact with a client written in a modern programming language. Thus, these systems do not provide the software engineering benefits that our approach offers. Antoy and Hamlet  propose self-checking ADTs, which integrate rewriting into C++ and Java classes. Among other details, our system differs by (i) fully automating the integration of Java code and the algebraic
94971	7714	and continue to execute by using results from the simulation subjects. We use a custom Java class loader to load the simulation client. The class loader uses the bytecode engineering library  to redirect references to classes belonging to the simulation set to simulation stubs. In other words, once we load the simulation client, it references simulation stubs instead of classes that are
94971	7300	which might be a useful addition to our current system. Other previous work uses algebraic specifications as assertions to check whether implementations are consistent with a given specification . Some of these systems require test drivers to be written (e.g. ), others generate test cases by themselves from the algebraic specifications . Sankar  uses a theorem prover to
94971	7301	a programmer writes an algebraic specification, the system automatically provides an implementation. Our tool interprets algebraic specifications using term rewriting, which is a well studied area . However, to our knowledge our system is the first to seamlessly integrate fully automatic algebraic rewriting techniques with Java classes. Our system provides three main benefits. First, it gives
94971	7301	a term if, for example, it is necessary to increase the size of the term before it can be ultimately reduce. Our current implementation does not check the set of rewriting rules for confluence  or for consistency, which means: (i) it may allow a term to be reduced to two distinct constants; and (ii) it may not find the desirable rewriting sequence, even though it only consists of steps
94971	7301	techniques more appealing for practical use. Both techniques use the same specification language and are designed to be used together. There is a vast body of prior work on term rewriting systems . Prior work has also studied the idea of using term rewriting to simulate a software component. For example, Wang and Parnas proposed the trace rewriting method to simulate software modules .
94971	7303	which might be a useful addition to our current system. Other previous work uses algebraic specifications as assertions to check whether implementations are consistent with a given specification . Some of these systems require test drivers to be written (e.g. ), others generate test cases by themselves from the algebraic specifications . Sankar  uses a theorem prover to
94971	676	(e.g., ) are well suited for describing how methods manipulate the state of an object. They are thus valuable for programmers who try to understand and extend an existing implementation . On the other hand, for programmers who are interested in using a particular class without worrying about the implementation details, understanding axiomatic specifications can be cumbersome. In
94971	676	specification discovery tool to discover a specification (ArrayList.spec) from an implementation (ArrayList.java). While specification discovery tools are effective in discovering specifications , the specifications they produce may be both unsound and incomplete. This is because the most effective specification discovery tools (to our knowledge) are based on analyzing program runs rather
94971	7308	requires more than 100 axioms to completely specify its 39 methods). Figure 1 shows how our specification interpreter complements our own previous work on algebraic specification discovery . We use our algebraic specification discovery tool to discover a specification (ArrayList.spec) from an implementation (ArrayList.java). While specification discovery tools are effective in
94971	7308	the most common, and from a software engineering viewpoint are the most desirable, our language supports only these. (iv) is trivial. Elsewhere we describe how to extend our language to support (v) . We do not yet know of a good way to address (vi) or (vii). Algebraic specifications have two parts: an algebraic signature and a set of axioms . The algebraic signature itself has two parts:
94971	7308	our algebraic specification interpreter. Section 5.1 gives an example of developing a specification from scratch. Section 5.2 shows how we used axioms generated by our specification discovery tool  and then debugged the specification using a client application. Section 5.3 provides evidence that the prototype generated by our tool from the specification has acceptable performance to be usable
94971	7308	clients for the hash set with the intention of “testing” the specification of the hash set. 5.2. Debugging a Discovered Specification In this case study, we used the specification discovery tool  to generate a specification for the java.util.ArrayList class contained in Sun’s Java Development Kit. We then used the algebraic interpreter to debug the discovered specification. Our client
94971	7308	Second, some specifications may execute much faster than other (equivalent) specifications, depending on the match between the specification and the simulation client. 6. Related Work Previously  we described a system that can discover algebraic specifications automatically from Java classes. The output of that system can be used as a starting point for developing a specification of an
94971	7310	which might be a useful addition to our current system. Other previous work uses algebraic specifications as assertions to check whether implementations are consistent with a given specification . Some of these systems require test drivers to be written (e.g. ), others generate test cases by themselves from the algebraic specifications . Sankar  uses a theorem prover to
94971	7312	within a given context, e.g., within a client that uses the specified class (BibtexParser.java). This approach for addressing unsoundness and incompleteness is complementary to Nimmer and Ernst , who address unsoundness for a subset of discovered invariants by using the static checker ESC/Java for validation. We demonstrate the applicability of our tools for the scenario shown in Figure 1
94971	7316	was successful). Note that rather than inventing new syntax, we have tried to use Java syntax as much as possible. We borrow thesSoot syntax for fully qualified names of Java classes and methods . Third, the specification file gives the equational axioms. For example, consider the following two axioms from our linked list specification. The first argument to each operation is the receiver
7321	7744	modules and its branches, connected to submodules or individual nodes of the original graph. For the purpose of extracting the hierarchical modularization we use the Horton-Strahler (HS) . From the bottom to the top of the tree, the HS index changes only when it joins a community of a similar index: the individual nodes (HS=1) join to form a group (with HS=2), which in turn join
8884337	7337	fitness and size separately . Edit distances were used in a multi-objective method , with fitness sharing , and in a linear representation to first select for fitness and then diversity . Fitness sharing and negative correlation learning were studied as ways to improve diversity , and a selection method that is uniform over the fitness values  was suggested as an alternative
8884337	7338	to programs? Our previous research suggests that the measures which are used to control and describe diversity are often conflicting and do not necessarily correlate well with fitness improvement . To better understand the effects diversity has on performance, we have used a simple method based on genetic lineages to increase the genetic diversity of populations. We add no elitism, size,
8884337	7338	One of the main challenges in genetic programming is preventing the system from getting stuck in local optima. An often cited cause is the loss of diversity and convergence within the population . Diversity has been measured as genetic variety , edit distances between trees , unique subtrees , initial genetic material , and the entropy of a population . These
8884337	7338	behavioural properties that might cause convergence and eventual fitness stagnation. However, diversity measures can imply different objectives and do not necessarily correlate well with fitness . Convergence (the population-wide loss of diversity) was suggested to mark the beginning of a local search phase in genetic programming . In typical genetic programming systems, the size of
8884337	7338	and behaviour. 2.3 Entropy and Edit Distance The phenotypic entropy is defined as the distribution of the proportion of the population with the same fitness value , (also investigated in ). Specifically, entropy is defined as ??????? ? log? ? , where ? ? is the proportion of the population with the same fitness value. Higher entropy values represent more chaos of the system, where
8884337	7338	Even-5-Parity and symbolic regression of the Binomial-3 function. These problems and algorithmic parameters are commonly used in numerous theoretical studies of genetic programming and diversity . The Ant problem attempts to pick up 89 food pellets on a grid with the functions ¡ if-foodahead, and the terminals prog2n¢ ¡ move¢ left, right, . The Parity problem attempts to ??? classify all
8884337	7339	the experiment tended to have better fitness. The early difference in entropy values between the control and lineage selection experiments appears to be somewhat correlated to fitness improvement . Fig. 4 shows the last generation where fitness improved plotted against the best fitness of the run for the Ant problem. Under lineage selection, the Ant problem finds better fitness on average 20
8884337	7340	. The Parity problem attempts to ??? classify all combinations of 5-bit length strings of ¡ with the 1,0¢ functions ¡ and, or, and five boolean terminals. The Binomial-3 nand¢ regression problem  attempts to approximate ????????????????????? ? the ? function using the terminals , ephemeral ???¨? random constants in the ??? range of , and the functions if the denominator is extremely small.
8884337	7340	the fitness results. Also, for problems where it is known that solutions will need particular terminals or functions, it would make sense to encourage their inclusion. 4.4 Binomial-3 Daida et al  provide a thorough investigation of why increasing the ephemeral constant range makes the Binomial3 problem ‘harder’. They draw attention to the inter-play between content and context of functions
8884337	7341	in local optima. An often cited cause is the loss of diversity and convergence within the population . Diversity has been measured as genetic variety , edit distances between trees , unique subtrees , initial genetic material , and the entropy of a population . These measures identify different aspects of a population’s structure and behavioural properties that
8884337	7341	for genetic programming . Crossover partners were selected in the Pygmie algorithm from lists based on fitness and size separately . Edit distances were used in a multi-objective method , with fitness sharing , and in a linear representation to first select for fitness and then diversity . Fitness sharing and negative correlation learning were studied as ways to improve
8884337	7341	between two nodes is 1 if they are not equal and 0 if they are equal. The distance between two trees is the summation of the distance between their nodes, normalised by dividing by the smaller tree . The second edit distance divides the total distance (defined above) within the same depth by an increasing weight. The distance between two trees is then the total distance within each depth,
8884337	7341	improve performance by becoming more of a hill-climber? We first look at a modification to genetic programming that claims superior performance on the Even5-parity problem. De Jong et al  used a multi-objective method that keeps only non-dominated individuals according to an individual’s fitness, size and diversity. Diversity is based on an edit distance between trees. Small
8884337	7345	for fitness and then diversity . Fitness sharing and negative correlation learning were studied as ways to improve diversity , and a selection method that is uniform over the fitness values  was suggested as an alternative way to preserve diversity. Island models, or demes, are commonly suggested as ways to improve diversity , but are typically used for easy parallelisation of
8884337	7346	locations. The exchanges of genetic material in these areas represent a local search as they are less likely to be detrimental to fitness and modify only a small part of the overall structure  (assuming a correlated landscape). The quick loss of diversity followed by small improvements to a genetically similar population has been likened to blind random search  where genetic
8884337	7349	as it is makes the population more susceptible to local optima that can vary widely . Genetic programming was compared with hill-climbing methods using similar representations and operators . Some problems, Artificial Ant for instance, were often solved better using genetic programming. On other problems, such as Multiplexer, hill-climbing methods performed considerably better than
8884337	7349	Even-5-Parity and symbolic regression of the Binomial-3 function. These problems and algorithmic parameters are commonly used in numerous theoretical studies of genetic programming and diversity . The Ant problem attempts to pick up 89 food pellets on a grid with the functions ¡ if-foodahead, and the terminals prog2n¢ ¡ move¢ left, right, . The Parity problem attempts to ??? classify all
8884337	7350	distances between individuals were used to select diverse crossover partners in a genetic algorithm . The distance between trees was used to define a homologous crossover for genetic programming . Crossover partners were selected in the Pygmie algorithm from lists based on fitness and size separately . Edit distances were used in a multi-objective method , with fitness sharing ,
8884337	7351	One of the main challenges in genetic programming is preventing the system from getting stuck in local optima. An often cited cause is the loss of diversity and convergence within the population . Diversity has been measured as genetic variety , edit distances between trees , unique subtrees , initial genetic material , and the entropy of a population . These
8884337	7351	as it is makes the population more susceptible to local optima that can vary widely . Genetic programming was compared with hill-climbing methods using similar representations and operators . Some problems, Artificial Ant for instance, were often solved better using genetic programming. On other problems, such as Multiplexer, hill-climbing methods performed considerably better than
8884337	7351	quickly loose genetic lineages and soon descend from one individual. This is critical because genetic lineages, defined in context with crossover, tend to share common root shapes and contents . Thus, they allow us to approximate the loss of genetic diversity andsconvergence without expensive measures. We then improve diversity by slowing the loss of genetic lineages. Fig. 1 is an example
8884337	7351	Even-5-Parity and symbolic regression of the Binomial-3 function. These problems and algorithmic parameters are commonly used in numerous theoretical studies of genetic programming and diversity . The Ant problem attempts to pick up 89 food pellets on a grid with the functions ¡ if-foodahead, and the terminals prog2n¢ ¡ move¢ left, right, . The Parity problem attempts to ??? classify all
8884337	7351	program. If we consider this as a metaphor for standard genetic programming search, then what changes to the algorithm might weaken or strengthen performance?s4.2 Artificial Ant Langdon and Poli  described the Ant problem to be highly deceptive for genetic programming. This is because of numerous solutions with a lot of symmetry, and also because there is no ‘guiding’ force to encourage the
8884337	7353	locations. The exchanges of genetic material in these areas represent a local search as they are less likely to be detrimental to fitness and modify only a small part of the overall structure  (assuming a correlated landscape). The quick loss of diversity followed by small improvements to a genetically similar population has been likened to blind random search  where genetic
8884337	7354	that reducing the size of solutions in Parity and Multiplexer problems (in both size restricted and unrestricted spaces) had the effect of also worsening fitness when compared with standard runs . In both problems, the solving of all the fitness cases requires the use of all the terminal values. For example, in the Parity domain, the absence of one of the boolean variable from a program
8884337	7355	fitness sharing , and in a linear representation to first select for fitness and then diversity . Fitness sharing and negative correlation learning were studied as ways to improve diversity , and a selection method that is uniform over the fitness values  was suggested as an alternative way to preserve diversity. Island models, or demes, are commonly suggested as ways to improve
8884337	7356	One of the main challenges in genetic programming is preventing the system from getting stuck in local optima. An often cited cause is the loss of diversity and convergence within the population . Diversity has been measured as genetic variety , edit distances between trees , unique subtrees , initial genetic material , and the entropy of a population . These
8884337	7356	structure  (assuming a correlated landscape). The quick loss of diversity followed by small improvements to a genetically similar population has been likened to blind random search  where genetic programming will “behave like a set of parallel stochastic hillclimbers”. With added elitism or over-selection of similar individuals, the algorithm begins to look more like a
8884337	7356	on different problems. Early convergence, however, is the likely cause of large performance variance across runs as it is makes the population more susceptible to local optima that can vary widely . Genetic programming was compared with hill-climbing methods using similar representations and operators . Some problems, Artificial Ant for instance, were often solved better using
8884337	7356	selection method that is uniform over the fitness values  was suggested as an alternative way to preserve diversity. Island models, or demes, are commonly suggested as ways to improve diversity , but are typically used for easy parallelisation of the algorithm. While some methods of diversity show improvement of fitness, they typically add elitism, suffer from additional computation and
8884337	7356	a subtree from the other parent. A genetic lineage is defined as the connection from the root parent to those individuals which were created, via crossover, from that individual. McPhee and Hopper  show how enroot?parent non?root?parent I1 I2 3 I I1 I2 3 I ‘‘ I1 I2 3 I lineage I 3 crossover ‘ ‘ ‘ crossover ‘‘ ‘‘ lineage I 3 lineage I 3 lineage I 3 Figure 1: An example of three individuals
8884337	7358	in local optima. An often cited cause is the loss of diversity and convergence within the population . Diversity has been measured as genetic variety , edit distances between trees , unique subtrees , initial genetic material , and the entropy of a population . These measures identify different aspects of a population’s structure and behavioural properties that
8884337	7358	near the root of the trees. The same distances were studied previously , the second was adapted from a similar distance measure  and both are similar to previous ones in the literature . 3 Experimental Results To explore the effectiveness of this strategy, we examine three different problem domains with two experiments: a control experiment with tournament selection and an
8884337	7359	as it is makes the population more susceptible to local optima that can vary widely . Genetic programming was compared with hill-climbing methods using similar representations and operators . Some problems, Artificial Ant for instance, were often solved better using genetic programming. On other problems, such as Multiplexer, hill-climbing methods performed considerably better than
8884337	7359	is also reduced, as seen next in the Parity problem. 4.3 Parity O’Reilly and Oppacher studied the 6 and 11 Multiplexer problems with genetic programming and similar hillclimbing type methods . The Parity and Multiplexer problems have similar functions, terminals and objectives, though they are not identical. Hill-climbing techniques appeared superior in this type of problem. Could
8884337	7360	as it is makes the population more susceptible to local optima that can vary widely . Genetic programming was compared with hill-climbing methods using similar representations and operators . Some problems, Artificial Ant for instance, were often solved better using genetic programming. On other problems, such as Multiplexer, hill-climbing methods performed considerably better than
8884337	7360	is also reduced, as seen next in the Parity problem. 4.3 Parity O’Reilly and Oppacher studied the 6 and 11 Multiplexer problems with genetic programming and similar hillclimbing type methods . The Parity and Multiplexer problems have similar functions, terminals and objectives, though they are not identical. Hill-climbing techniques appeared superior in this type of problem. Could
8884337	7362	and do not necessarily correlate well with fitness . Convergence (the population-wide loss of diversity) was suggested to mark the beginning of a local search phase in genetic programming . In typical genetic programming systems, the size of genetic material exchanged during recombination becomes smaller and more concentrated to similar locations. The exchanges of genetic material in
8884337	7362	by small improvements to a genetically similar population has been likened to blind random search  where genetic programming will “behave like a set of parallel stochastic hillclimbers”. With added elitism or over-selection of similar individuals, the algorithm begins to look more like a ‘hill-climbing’ method. However, it is a loose metaphor used here to help explain why
8884337	7362	quickly loose genetic lineages and soon descend from one individual. This is critical because genetic lineages, defined in context with crossover, tend to share common root shapes and contents . Thus, they allow us to approximate the loss of genetic diversity andsconvergence without expensive measures. We then improve diversity by slowing the loss of genetic lineages. Fig. 1 is an example
8884337	7363	. Diversity has been measured as genetic variety , edit distances between trees , unique subtrees , initial genetic material , and the entropy of a population . These measures identify different aspects of a population’s structure and behavioural properties that might cause convergence and eventual fitness stagnation. However, diversity measures can imply
8884337	7363	according to genetic differences and behaviour. 2.3 Entropy and Edit Distance The phenotypic entropy is defined as the distribution of the proportion of the population with the same fitness value , (also investigated in ). Specifically, entropy is defined as ??????? ? log? ? , where ? ? is the proportion of the population with the same fitness value. Higher entropy values represent
7366	7368	edges. The objective is to minimize a cut criterion, given that any cut on this graph yields a partition of the image into (hopefully) coherent visual patterns. Cut criteria range from conventional  to more sophisticated criteria, tailored to grouping , , . These are basically global criteria; however, the strategies adopted for their minimization range through a broad spectrum, from
7366	7369	that any cut on this graph yields a partition of the image into (hopefully) coherent visual patterns. Cut criteria range from conventional  to more sophisticated criteria, tailored to grouping , , . These are basically global criteria; however, the strategies adopted for their minimization range through a broad spectrum, from local  to global optimization , through
7366	7369	complexity of the scene, and makes it possible to control the coarseness of the segmentation, with the possibility to build a hierarchy of coarse-to-fine (multiscale) segmentations of an image . 3 THEORETICAL ANALYSIS AND ALGORITHMS For the sake of simplicity, we first state our theoretical results for a single color band (e.g., gray-level). On this basis, the extension of the results to
7366	7369	despite the relative noise of this video-extracted picture). 4.4 Controlling the Scale of the Segmentation Some authors have emphasized the need to control the coarseness of a segmentation , , . The objective of multiscale segmentation is to get a hierarchy of segmentations at different scales, and get at each scale a level of details compatible with the perceptual organization of
7366	7370	any cut on this graph yields a partition of the image into (hopefully) coherent visual patterns. Cut criteria range from conventional  to more sophisticated criteria, tailored to grouping , , . These are basically global criteria; however, the strategies adopted for their minimization range through a broad spectrum, from local  to global optimization , through intermediate
7366	7370	the relative noise of this video-extracted picture). 4.4 Controlling the Scale of the Segmentation Some authors have emphasized the need to control the coarseness of a segmentation , , . The objective of multiscale segmentation is to get a hierarchy of segmentations at different scales, and get at each scale a level of details compatible with the perceptual organization of the
7366	7371	cut on this graph yields a partition of the image into (hopefully) coherent visual patterns. Cut criteria range from conventional  to more sophisticated criteria, tailored to grouping , , . These are basically global criteria; however, the strategies adopted for their minimization range through a broad spectrum, from local  to global optimization , through intermediate choices
7366	7372	sophisticated criteria, tailored to grouping , , . These are basically global criteria; however, the strategies adopted for their minimization range through a broad spectrum, from local  to global optimization , through intermediate choices , . Global optimization strategies have the advantage to directly tackle the problem as a whole, and may offer good approximations
7366	7374	offer good approximations , at possible algorithmic expenses though , . In this paper, we focus on a different strategy which belongs to the family of region growing and merging techniques , . In region merging, regions are sets of . R. Nock is with the Université Antilles-Guyane, Département Scientifique Inter-facultaire/GRIMAAG Lab., B.P. 7209, 97278 Schoelcher, Martinique,
7366	7374	builds the segmentation on the basis of (essentially) local decisions. This locality in decisions has to preserve global properties, such as those responsible for the perceptual units of the image . In Fig. 1, the grassy region below the castle is one such unit, even when its variability is high compared to the other regions of the image. In that case, a good region merging algorithm has to
7366	7374	of image generation makes implicitly the assumption that observed color variations inside true regions should reasonably be smaller than between true regions. Such an assumption is made explicit in . Thus, a good way to approximate A is to capture the between-pixel local gradients, and then compute their maximal per-channel variation in fð:; :Þ: fðp; p 0 Þmax a2fR;G;Bg faðp; p 0 Þ. Below, we
7366	7374	gradient estimation ((8), center images) and with convolution kernels (right images). Regions are color averaged with white borders. Fig. 6. Sample results comparing both versions of SRM and . Segmentation conventions are s: region colors are chosen at random. gradients, there is a visual advantage to SRM(w) (e.g., cup). Notice also the segmentation of SRM(w) on bldg, a picture
7366	7374	noise (tðqÞ), or . chosen uniformly in f1;gg (the extremes) for salt and pepper noise (sðqÞ). Fig. 6 shows different images corrupted with increasing amounts of noise, and the results of  and SRM. From 45 percent noise, the results of  appear to be random, whereas SRM still manages to find most interesting regions of the images. However, on some images, SRM obtained a brutal
7366	7376	distributions, more or less restrictive, which would make any theoretical insight into how region merging works restricted to such settings and, therefore, of possibly moderate interest (see, e.g.,  for related criticisms). Our aim in this paper is to propose a path and its milestones from a novel model of image generation, the theoretical properties of possible segmentation approaches to a
7366	7376	observed image (see also Fig. 2), and b is grouping’s objective (i.e., find the statistical regions’ borders, given I). chosen not to use complex formulations of the colors, such as the L u v space . I is an observation of a perfect scene I we do not know of, in which pixels are perfectly represented by a family of distributions, from which each of the observed color channel is sampled. In I ,
7366	7376	modifications of SRM becomes OðjIjðlog g þ ÞÞ, which is still linear in jIj if is constant. Fig. 7 reports results on the castle of Fig. 1. Conventions for the segmentations results are as follows: ’s regions are averaged with the original colors, ’s are averaged with random colors, and SRMs follow ’s (with white bordered regions). Notice that the number of regions found by ,
7366	7378	pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi the error incurred by our segmentation is driven by g js ðIÞj=ðjIjQÞ,a close order approximation to the optimum . 3.3 Color Images The merging predicate for the RGB setting is: PðR; R 0 true if 8a 2fR; G; Bg; Þ  jR0 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi a
7380	7381	advanced fairly quickly, culminating in the excellent introductory text by Bohner and Peterson  and their more recent advanced monograph . A succint survey on time scales can be found in . A time scale T is any nonempty closed subset of the real numbers R. Thus time scales can be any of the usual integer subsets (e.g. Z or N), the entire real line R, or any combination of discrete
7380	7387	order the evaluation of (3.5) before the evaluation of (3.4). 4. Stability Preliminaries The stability properties of systems of the form x ? (t) = Ax(t) have quite recently been extensively studied . Since A(t) is time-varying and not obviously Jordan reducible in this case, the results from  for matrix systems are not directly applicable. However, our approach in the next section will
7429	7430	original values of a modified attribute can be estimated. Traditionally, the security provided by a perturbation method has been measured as the variance between the actual and the perturbed values . This measure is given by Var(X ? Y )whereX represents a single original attribute and Y the distorted attribute. This measure can be made scale invariant with respect to the variance of X by
7429	7432	the four factors above, RBT can be seen as a technique on the border with obfuscation. Obfuscation techniques aim at making information highly illegible without actually changing its inner meaning . In other words, using RBT the original data is transformed so that the transformed data captures all the information for clustering analysis while protecting the underlying data values. Now we
7429	7433	different scales (e.g. centimeters and kilograms). For this reason, it is common to standardize the data so that all attributes are on the same scale. There are many methods for data normalization . We review only two of them in this section: min-max normalization and z-score normalization. Min-max normalization performs a linear transformation on the original data. Each attribute is
7429	7434	as PPC over centralized data, and the latter as PPC over vertically partitioned data. The problem of PPC over vertically and horizontally partitioned data has been addressed in the literature , while the problem of PPC over centralized data has not been significantly tackled. In this paper, we focus on PPC over centralized data. There is very little literature regarding the problem of
7429	7434	conclusions. 2 Related Work Some effort has been made to address the problem of privacy preservation in data clustering. The class of solutions has been restricted basically to data partitioning  and data distortion . The work in  addresses clustering vertically partitioned data, whereas the work in  focuses on clustering horizontally partitioned data. In a horizontal partition,
7429	7434	distances of objects in the process of moving them in the Euclidean space. More recently, a new method, based on generative models, was proposed to address privacy preserving distributed clustering . In this approach, rather than sharing parts of the original data or perturbed data, the parameters of suitable generative models are built at each local site. Then such parameters are transmitted
7429	7434	Carlo techniques. This approach achieves high quality distributed clustering with acceptable privacy loss and low communication cost. The work presented here is orthogonal to that one presented in  and differs in some aspects from the work in . In particular, we build on our previous work. First, instead of distorting data for clustering using translations, scaling, rotations or even some
7429	7437	tackled. In this paper, we focus on PPC over centralized data. There is very little literature regarding the problem of PPC over centralized data. A notable exception is the work presented in . The key finding of this study was that adding noise to data would meet privacy requirements, but may compromise the clustering analysis. The main problem is that by distorting the data, many data
7429	7437	Transformation (RBT). The major features of our data transformation are: a) it is independent of any clustering algorithm, which represents a significant improvement over our previous work ; b) it has a sound mathematical foundation; c) it is efficient and accurate since the distances between data points are preserved; and d) it does not rely on intractability hypotheses from algebra
7429	7437	Some effort has been made to address the problem of privacy preservation in data clustering. The class of solutions has been restricted basically to data partitioning  and data distortion . The work in  addresses clustering vertically partitioned data, whereas the work in  focuses on clustering horizontally partitioned data. In a horizontal partition, different objects are
7429	7437	about the attributes at other sites. This work ensures reasonable privacy while limiting communication cost. The feasibility of achieving PPC through geometric data transformation was studied in . This investigation revealed that geometric data transformations, such as translation, scaling, and simple rotation are unfeasible for privacypreserving clustering if we do not consider the
7429	7437	are limited when the perturbed attributes are considered as a vector in the n-dimensional space. Such methods would exacerbate the problem of misclassification. A promising direction of the work in  was that PPC through data transformation should be to some extent possible by isometric transformations, i.e., transformations that preserve distances of objects in the process of moving them in
7429	7440	as PPC over centralized data, and the latter as PPC over vertically partitioned data. The problem of PPC over vertically and horizontally partitioned data has been addressed in the literature , while the problem of PPC over centralized data has not been significantly tackled. In this paper, we focus on PPC over centralized data. There is very little literature regarding the problem of
7429	7440	conclusions. 2 Related Work Some effort has been made to address the problem of privacy preservation in data clustering. The class of solutions has been restricted basically to data partitioning  and data distortion . The work in  addresses clustering vertically partitioned data, whereas the work in  focuses on clustering horizontally partitioned data. In a horizontal partition,
7429	7440	Carlo techniques. This approach achieves high quality distributed clustering with acceptable privacy loss and low communication cost. The work presented here is orthogonal to that one presented in  and differs in some aspects from the work in . In particular, we build on our previous work. First, instead of distorting data for clustering using translations, scaling, rotations or even some
7441	7442	problem solvers that work through the collaboration of independent reasoning modules. They were developed in the 1970s and originally applied to signal-processing tasks. The first, HEARSAY-II , was used for speech recognition, employing acoustic, lexical, syntactic, and semantic knowledge. Other systems were applied to problems as diverse as interpretation of sonar data, protein folding,
7441	7444	blackboard systems can search opportunistically, dynamically rating KSARs based on the current data and on the partial solutions that exist so far. Heuristic methods generally have been used  to represent uncertainty: for example, HEARSAY-II used a numerical confidence score that ranged from 1 to 100. One of our contributions is to provide hypotheses on the blackboard with a real
7441	7445	constructing the network need to specify separate, identical CPTs for each unit, which is impractical because there could be many units, and we do not know in advance how many. Several authors  have addressed this problem by breaking up large belief networks into smaller subnetworks. Subnetworks have designated input nodes—which have no conditional distribution, requiring that their
7441	7446	constructing the network need to specify separate, identical CPTs for each unit, which is impractical because there could be many units, and we do not know in advance how many. Several authors  have addressed this problem by breaking up large belief networks into smaller subnetworks. Subnetworks have designated input nodes—which have no conditional distribution, requiring that their
7441	7446	nodes of another. Repetitive structure can be specified just once in a subnetwork and instantiated multiple times to exploit redundancies in the domain. Object-oriented Bayesian networks (OOBNs)  employ strong encapsulation between subnetworks. Each subnetwork defines a set of output variables, and combinations between subnetworks are made only by connecting the output variables of one
7441	7448	nodes of another. Repetitive structure can be specified just once in a subnetwork and instantiated multiple times to exploit redundancies in the domain. Object-oriented Bayesian networks (OOBNs)  employ strong encapsulation between subnetworks. Each subnetwork defines a set of output variables, and combinations between subnetworks are made only by connecting the output variables of one
7441	7449	the blackboard, where we are considering things like meetings and current locations, each network node is indexed by the time it occurs, and the entire network is a Dynamic Bayesian Network (DBN) . At higher levels of the blackboard, which correspond to long-term actions and intentions, we represent events by the interval in which they occur. Each event has a starttime and an end-time that
7441	7451	We have built a prototype of AIID in the domain of military analysis. We simulate military engagements at the battalion level (roughly a thousand troops), using the Capture the Flag simulator . The simulator includes such effects as terrain, fog of war, artillery, combat aviation, and morale. The data consist of reports about friendly and enemy units, for example, “Unit Red-1 has sound
7441	7452	We have built a prototype of AIID in the domain of military analysis. We simulate military engagements at the battalion level (roughly a thousand troops), using the Capture the Flag simulator . The simulator includes such effects as terrain, fog of war, artillery, combat aviation, and morale. The data consist of reports about friendly and enemy units, for example, “Unit Red-1 has sound
560376	7564	and perhaps learn from the results, whereas we treat that as a feature of what we call the ‘deliberative’ layer. The processes in our third layer, the meta-management layer, described in (Beaudoin, 1994), are concerned with observing, evaluating, and controlling information-processing processes within the rest of the architecture. Insofar as this requires manipulating information about
560376	7584	286’, where Joe Bloggs happens to be thing number 286. 12 We do not claim to be the first to make this observation! McCarthy, for example, has been aware of this problem for many years (e.g., McCarthy, 1979), partly because he realised very early on that meta-semantic competence is hard to accommodate within first-order logic. 30sorganisms and in robots. But it is important to make some observations:
560376	7587	whereas the CogAff schema allows both neural and non-neural reactive mechanisms, for instance implemented as forward-chaining conditionaction rules, like Nilsson’s ‘teleo-reactive’ systems (Nilsson, 1994). As far as H-CogAff is concerned, we leave open whether there are many kinds of reactive mechanisms, including reactive rule-sets, or whether they are restricted to neural, distributed
560376	7595	Omega architecture and architectures involving true, multi-level perception and action (such as H-CogAff, figure 5). The latter satisfy specific requirements on the high-level perceptual processes (Sloman, 1989). For example, for multi-level perception, we would require there to be higher-level representations (such as affordances involving more abstract ontological categories) which are the product of
560376	7601	employed in the physical sciences. Yet the niche is causally very important, both in the way that the organism works (e.g. as an information processor) and in the way that a species evolves (Sloman, 2000a). A niche is part of what determines features of new generations, and in some cases may be involved in reproducing itself also, for instance if members of a species all alter the environment in
560376	7604	to suggest research questions that will extend what we know about the organisms, generating new requirements for explanatory architectures. 7.1 CogAff and emotions For example, we have shown in (Sloman, 2001a) how a full three level architecture, of the sort represented within the CogAff schema, can explain at least three different classes of emotions found in humans namely primary emotions involving
560376	7604	conforming architectures, including both simpler, insect-like architectures, and more complex additional mechanisms required for more sophisticated deliberative and meta-management processes. In (Sloman, 2001a) and other papers listed in the references, we outline such an elaborated instance, the H-CogAff architecture, illustrated sketchily in figure 5. There is much to be said about the additional
560376	7605	E.g. the question whether a certain animal, or robot, has emotions or is conscious or feels pain suffers from the multiple confusions in our current notion(s) of mental states and processes (Sloman, 2002a, 2001a; Sloman et al., 2005). So, in part, our task is to explain how to make those obscure concepts clearer, for instance by interpreting them as ‘architecture-based’ concepts (op.cit). 2 1.2
560376	7607	systems, finding out what they do as controllers or as information processors is a very different task from observing physical behaviour, whether internal or external (Sloman, 1993; Sloman and Chrisley, 2003). 1 That is because the most important components of an information processor may be 1 Throughout this paper, we use ‘information’ in the colloquial sense in which information is about something
560376	7607	to whether a movement you are making is done in a relaxed or tense way, or with or without precise control. (Relaxed precision is a requirement for many sporting and artistic achievements.) In (Sloman and Chrisley, 2003) we have tried to show how this ability to monitor internal information-processing states can involve mechanisms that account for many of the features of what philosophers refer to as ‘qualia’.
560376	7608	whether a certain animal, or robot, has emotions or is conscious or feels pain suffers from the multiple confusions in our current notion(s) of mental states and processes (Sloman, 2002a, 2001a; Sloman et al., 2005). So, in part, our task is to explain how to make those obscure concepts clearer, for instance by interpreting them as ‘architecture-based’ concepts (op.cit). 2 1.2 Orders of ontology The fact that
560376	7608	about emotions of other people) and using third-order ontologies without realising that the concepts in their second-order ontologies will not suffice for use in scientific third-order ontologies (Sloman et al., 2005). 7s2 Ontologies in science, and how they change Progress in science takes many forms, including discovering generalisations, refuting generalisations, and discovering new observable types of
560376	7609	within the space defined by our framework are reported in Scheutz and Sloman (2001). There is much more exploration to be done of this sort and we have developed tools to help with the task (Sloman and Logan, 1999). 9.1 Using the CogAff framework to guide research For decades AI has suffered from swings of fashion in which people have felt they have to propose and defend a particular type of architecture as
7616	7618	can therefore backtrack nonchronologically. In addition, there have been significant contributions in terms of formula manipulation techniques which can in some cases yield competitive approaches . It is generally accepted that the ability to reduce either the number of variables or clauses in instances of SAT impacts the expected computational effort of solving a given instance. This
7616	7618	of Theorem 2.4: if ????????????¤?¥¨§?©???????????? then create clause ©?????????? . More recently, a competitive SAT solver incorporating hyper-resolution with binary clauses has been proposed . Given the set of clauses ©???? ? ????????©???? ? ????????????????©???????? ? ????? ? ??????????????????? , hyper-resolution allows infer??????©?? ©???????? ring . Once again, observe that this
7616	7620	can therefore backtrack nonchronologically. In addition, there have been significant contributions in terms of formula manipulation techniques which can in some cases yield competitive approaches . It is generally accepted that the ability to reduce either the number of variables or clauses in instances of SAT impacts the expected computational effort of solving a given instance. This
7616	7620	necessary assignments, the table of assignments can also be used for inferring new clauses. 3 This condition is the inference rule used in the St?almarck’s method . 4 This rule is utilized in  for deriving shared implications.sLet us consider the triggering assignment ????????? and the respective implied assignment ????????? . Hence, the clause ©?????????? can be inferred. Clearly, for
7616	7620	techniques. 4.1. Probing-Based Techniques In the SAT domain, the idea of establishing hypotheses and inferring facts from those hypotheses has been extensively studied in the recent past . The failed literal rule is a well-known and extensively used probing-based technique (see for example ): if the assignment ? ? ? yields a conflict (due to BCP), then we must assign ? ? ? . This
7616	7620	Common assignments are deemed necessary for a clause to become satisfied and consequently for the formula to be satisfied. These techniques have been applied to SAT in  and more recently in . In our framework, clause probing is captured by Theorem 2.2. To the best of our knowledge, no other work proposes the joint utilization of variable and clause probing. The notion of literal
7616	7621	is declared, and the assignment ????????? is deemed necessary. Observe that the necessary assignments obtained from unsatisfiability conditions correspond to the well-known failed-literal rule . 2.2.2. Inferred Clauses Besides the identification of necessary assignments, the table of assignments can also be used for inferring new clauses. 3 This condition is the inference rule used in the
7616	7621	We now proceed describing the identification of necessary assignments based on formula unsatisfiability conditions. As mentioned earlier, these conditions correspond to the failed-literal rule . Theorem 2.3 Given a CNF £ formula , ¤?¥¨§?©?????????????? if yields a conflict, then the ??????????? assignment is deemed necessary. The previous theorem includes the conditions regarding both the
7616	7621	techniques. 4.1. Probing-Based Techniques In the SAT domain, the idea of establishing hypotheses and inferring facts from those hypotheses has been extensively studied in the recent past . The failed literal rule is a well-known and extensively used probing-based technique (see for example ): if the assignment ? ? ? yields a conflict (due to BCP), then we must assign ? ? ? . This
7616	7622	if the condition ????©?¤?¥¨§?©?????? ? ? is used, at the cost of additional computational overhead. Moreover, observe that the result of Theorem 2.8 is related with a technique proposed in . For a clause ©?????????? , where ? is a disjunction of literals, if assigning value 0 to all literals in ? yields a conflict, then ©???? is an implicate of £ . The two techniques are related since
7616	7622	techniques. 4.1. Probing-Based Techniques In the SAT domain, the idea of establishing hypotheses and inferring facts from those hypotheses has been extensively studied in the recent past . The failed literal rule is a well-known and extensively used probing-based technique (see for example ): if the assignment ? ? ? yields a conflict (due to BCP), then we must assign ? ? ? . This
7616	7622	and clause probing. The notion of literal dropping, that considers applying sets of simultaneous assignments for inferring clauses that subsume existing clauses, is described for example in . As mentioned earlier, some of the clause inference conditions proposed by Theorem 2.8 can be related with previous techniques for literal dropping, proposing more general conditions for inferring
7616	7624	can therefore backtrack nonchronologically. In addition, there have been significant contributions in terms of formula manipulation techniques which can in some cases yield competitive approaches . It is generally accepted that the ability to reduce either the number of variables or clauses in instances of SAT impacts the expected computational effort of solving a given instance. This
7616	7624	formula are captured from the construction of the assignment table and the application of Theorem 2.4. Furthermore, sophisticated techniques have been developed to detect chains of biconditionals . The 2-closure of a 2CNF sub-formula  allows to infer additional binary clauses. The identification of the transitive closure of the implication graph is obtained from the construction of the
7616	7626	assignments to variables. Common assignments are deemed necessary for a clause to become satisfied and consequently for the formula to be satisfied. These techniques have been applied to SAT in  and more recently in . In our framework, clause probing is captured by Theorem 2.2. To the best of our knowledge, no other work proposes the joint utilization of variable and clause probing. The
7616	7627	formula are captured from the construction of the assignment table and the application of Theorem 2.4. Furthermore, sophisticated techniques have been developed to detect chains of biconditionals . The 2-closure of a 2CNF sub-formula  allows to infer additional binary clauses. The identification of the transitive closure of the implication graph is obtained from the construction of the
7629	7630	out of one system with the emphasis on technical security and thus formulated. Recent studies have moved more towards user requirements, with its main focus on the system. cf. ,  and . The requirements specified in the above mentioned literature can be summarized in five categories: Security, confidentiality, fungibility, user friendliness and efficiency. Due to its selective
7629	7632	management, where demands such as pseudo anonymity, inability to connect chain links, inability to identify locations or inability of observation can be considered and precisely formulated , . Within the context of role behaviour the dynamic aspect of trust is captured. With every role there are certain activity sequences connected during the payment process. The execution of these
7629	7643	considered. Therefore, electronic payment should be realized in accordance with the principles of multilateral security, which are focused on an equilibrium between different security interests . Trust Electronic payment is confidential when all phases of the procedure are designed to satisfy the participants and their security expectations. Therefore, credibility and authenticity are
7647	7648	functions. 1 Introduction Pattern discovery from biosequences is an important topic in Bioinformatics, which has been, and is being, studied heavily with numerous variations and applications (see  for a survey on earlier work). Although finding the single, most significant pattern conserved across multiple sequences has important and obvious applications, it is known that in many, if not
7647	7649	cases, more than one sequence element is responsible for the biological role of the sequences. There are several methods which address this observation, focussing on finding composite patterns. In , they develop a suffix tree based approach for discovering structured motifs, which are two or more patterns separated by a certainsdistance, similar to text associative patterns . MITRA is
7647	7650	patterns. In , they develop a suffix tree based approach for discovering structured motifs, which are two or more patterns separated by a certainsdistance, similar to text associative patterns . MITRA is another method that looks for composite patterns  using mismatch trees. Bioprospector  applies the Gibbs sampling strategy to find gapped motifs. The main contribution of this paper
7647	7651	discovering structured motifs, which are two or more patterns separated by a certainsdistance, similar to text associative patterns . MITRA is another method that looks for composite patterns  using mismatch trees. Bioprospector  applies the Gibbs sampling strategy to find gapped motifs. The main contribution of this paper is to present an efficient O(N 2 ) algorithm (where N is the
7647	7652	are two or more patterns separated by a certainsdistance, similar to text associative patterns . MITRA is another method that looks for composite patterns  using mismatch trees. Bioprospector  applies the Gibbs sampling strategy to find gapped motifs. The main contribution of this paper is to present an efficient O(N 2 ) algorithm (where N is the total length of the input strings) and
7647	7658	whose occurrences in the sequences correlate with their numeric attributes. For example, ri could be the expression level ratio of a gene with upstream sequence si. The scoring function used in  is the inter class variance, which can be maximized by maximizing the scoring function score(x,y) = y2 /x+(y? ? |m| i=1 ri) 2 /(m?x), where x = |M(?)| and y = ? M(?) ri. We will later describe how
7647	7660	whose occurrences in the sequences correlate with their numeric attributes. For example, ri could be the expression level ratio of a gene with upstream sequence si. The scoring function used in  is the inter class variance, which can be maximized by maximizing the scoring function score(x,y) = y2 /x+(y? ? |m| i=1 ri) 2 /(m?x), where x = |M(?)| and y = ? M(?) ri. We will later describe how
7647	7664	is thus implemented. We apply our algorithm to 3’UTR (untranslated region) of yeast and human mRNA, together with data obtained from microarray experiments which measure the decay rate of each mRNA . We were successful in obtaining several interesting pattern pairs where some correspond to known mRNA destabilizing elements. 2 Preliminaries 2.1 Notation Let ? be a finite alphabet. An element of
7647	7664	that some proteins bind to the UTR of the mRNA to promote its decay, while others inhibit it. Recently, the comprehensive decay rates of many genes have been measured using microarray technology . We consider the problem of finding substring pattern pairs related to the rate of mRNA decay to find possible binding sites of the proteins, in order to further understand this complex mechanism.
7647	7664	Patterns from Human Sequences For our second experiment, we used the decay rate measurements of the human hepatocellular carcinoma cell line HepG2 made available as Supplementary Table 9 of . 3???UTR sequences for each mRNA was retrieved using the ENSMART  interface. We were able to obtain 2306 pairs of 3’UTR sequences and their decay rates, with the average length of the sequences
7647	7665	(and generalized suffix trees) can be represented in linear space and constructed in linear time  with respect to the length of the string (total length of the strings for GST). A suffix array  As for a given string s of length n, is a permutation of the integers 1,...,n representing the lexicographic ordering of the suffixes of s. The value As = j in the array indicates that s
7647	7669	three methods for constructing the suffix array directly from a string in linear time have been developed . The lcp array can be constructed from the suffix array also in linear time . It has been shown that several algorithms (and potentially many more) which utilize the suffix tree can be implemented very efficiently using the suffix array together with its lcp array
7647	7669	for each fixed l(v1) are independent of each other. 4 Implementation Using Suffix Arrays The algorithm on the suffix tree can be simulated efficiently by a suffix array. We modify the algorithm of  that simulates a bottom-up traversal of a suffix tree, using a suffix array. Notice that since each suffix of the string correspondssto a leaf in the suffix tree, each position in the suffix array
7647	7670	time . It has been shown that several algorithms (and potentially many more) which utilize the suffix tree can be implemented very efficiently using the suffix array together with its lcp array  (the combination termed in  as the enhanced suffix array). This paper presents yet another example for efficient implementation of an algorithm based conceptually on suffix trees, but uses the
7647	7671	is common to the paths from the root to each of the nodes. The tree can be pre-processed in linear time to answer the lowest common ancestor (lca-query) for any given pair of nodes in constant time . In terms of the suffix array, the lca-query is almost equivalent to a range minimum query (rm-query) on the lcp array, which, given a pair of positions i and j, rmq(i,j) returns the position of
7647	7672	of positions i and j, rmq(i,j) returns the position of the minimum element in the sub-array lcp. The lcp array can also be pre-processed in linear time to answer the rm-query in constant time . The linear time bounds mentioned above for the construction of suffix trees and arrays, as well as the preprocessing for lca- and rm-queries are actually not required for the O(N 2 ) overall time
7647	7674	for each fixed l(v1) are independent of each other. 4 Implementation Using Suffix Arrays The algorithm on the suffix tree can be simulated efficiently by a suffix array. We modify the algorithm of  that simulates a bottom-up traversal of a suffix tree, using a suffix array. Notice that since each suffix of the string correspondssto a leaf in the suffix tree, each position in the suffix array
7680	7681	(Szent-Gyorgyi and Cohen, 1957) and that these correlations may be used to predict the secondary structure (Rost, 2001; Przybylski and Rost, 2002) or as a contribution to threading potentials (Alexandrov et al., 1996; McGuffin and Jones, 2003) and other tertiary structure prediction algorithms (Bowie et al., 1991). The effectiveness of local secondary structure prediction, and the utility of secondary structure
7680	7681	and Protein secondary structure Jones, 2003), but some of these only consider the direct correlation between an amino acid and the secondary structure class at that one residue (Bowie et al., 1991; Alexandrov et al., 1996). By ignoring the correlations between an amino acid and an extended segment of local secondary structure, such methods lose over half of the available local signal and, unlike secondary structure
7680	7696	biased significantly (Miller, 1955), resulting in a systematic underestimation of the true entropy or overestimation of the mutual information. We used non-parametric bootstrap resampling (Efron and Tibshirani, 1993) to correct for this bias and to estimate standard statistical errors. A total of 50 replicas of the original data are generated by sampling, with replacement, from the available sequences. This
7680	7697	Contact: gec@compbio.berkeley.edu INTRODUCTION The secondary structure of a protein is a summary of the general conformation and hydrogen bonding pattern of the amino acid backbone (Frishman and Argos, 1995). This structure is frequently simplified to a sequence (one element per residue) of helixes (H), extended strands (E) and unstructured loops (L). It has long been recognized that each residue’s
7680	7697	2002), which provide a more reliable and convenient representation of the true sequence than the PDB ATOM or SEQRES records. The secondary structure sequences were determined by the program STRIDE (Frishman and Argos, 1995) using each protein’s hydrogen bonding pattern and backbone torsional angles. STRIDE was unable to process a small fraction of SCOP domains that were consequentially removed from further
7680	7697	structure assignment and due to variations in the underlying data set itself. Our standard data set consists of 2853 sequences derived from the 40% subset of SCOP release 1.61, with STRIDE (Frishman and Argos, 1995) secondary structure assignments. We also considered prediction 1608 Fig. 5. The HMM defined in  is able to extract over 95% of the available inter-sequence information. Here,
7680	5458	the single secondary structure element at the center of that window and often assume that inter-amino acid correlations are informative. However, even nearest neighboringsFig. 4. A factor graph (Kschischang et al., 2001) for P(S|R), representing the decomposition of this complex, many-variable function into simpler parts . Circles represent variables and squares represent factors, local
7680	7702	measures. To ensure accurate results, we employ a large, carefully curated collection of protein structures derived from the structural classification of proteins (SCOP) (Murzin et al., 1995; Lo Conte et al., 2002) database, which contains 2853 sequences. Sequence information Entropy is a measure of the information needed to describe a random variable. Specifically, the entropy, H(X), of a discrete random
7680	7702	(Berman et al., 2000) currently contains over 20 000 publicly accessible structures, but many of these are very similar, and many are of relatively low quality. The SCOP (Murzin et al., 1995; Lo Conte et al., 2002) database provides a convenient decomposition of PDB structures into domains, and the ASTRAL (Brenner et al., 2000; Chandonia et al., 2002) compendium provides representative subsets of SCOP
7680	7703	These error estimates were not significantly improved when the number of replicas was increased from 50 to 500. The requisite pseudorandom numbers were drawn from the Mersenne Twister generator (Matsumoto and Nishimura, 1998; Gough, 2003). RESULTS Entropy and correlations In Figure 2, we plot the entropies for secondary structure sequence blocks of length up to 9 (39 = 19 683 states). Of the half million residues in
7680	7704	1957) and that these correlations may be used to predict the secondary structure (Rost, 2001; Przybylski and Rost, 2002) or as a contribution to threading potentials (Alexandrov et al., 1996; McGuffin and Jones, 2003) and other tertiary structure prediction algorithms (Bowie et al., 1991). The effectiveness of local secondary structure prediction, and the utility of secondary structure potentials, depends upon
7680	7706	x. A useful alternative approach is to construct an approximation of the true probabilities, g(x) ? P(x), and then estimate the entropy by the mean log-likelihood of the data (Moddemeijer, 2000). H(X) = E ? log2 P(X) ? ? E ? log2 g(X) ? ? N? i=1 1 N log 2 g(xi). (7) Similarly, the mutual information can be related to the mean log odds since ? ? P(X|Y) I(X; Y) = E log2 . (8) P(X) A serious
7680	7708	quantified using entropic measures. To ensure accurate results, we employ a large, carefully curated collection of protein structures derived from the structural classification of proteins (SCOP) (Murzin et al., 1995; Lo Conte et al., 2002) database, which contains 2853 sequences. Sequence information Entropy is a measure of the information needed to describe a random variable. Specifically, the entropy, H(X),
7680	7708	Data Bank (PDB) 1604 (Berman et al., 2000) currently contains over 20 000 publicly accessible structures, but many of these are very similar, and many are of relatively low quality. The SCOP (Murzin et al., 1995; Lo Conte et al., 2002) database provides a convenient decomposition of PDB structures into domains, and the ASTRAL (Brenner et al., 2000; Chandonia et al., 2002) compendium provides representative
7680	1069	structure profile. Computationally, the conditional secondary structure probabilities can be derived from the amino acid sequence using the standard forward–backward dynamic programming algorithm (Rabiner, 1989). The time and memory complexities for a naive implementation are O(L3k ), which, despite the exponential scaling, is feasible for moderate k. For example, with k = 7, training on one half of our
7680	7711	secondary structure is appreciably correlated with the local amino acid sequence (Szent-Gyorgyi and Cohen, 1957) and that these correlations may be used to predict the secondary structure (Rost, 2001; Przybylski and Rost, 2002) or as a contribution to threading potentials (Alexandrov et al., 1996; McGuffin and Jones, 2003) and other tertiary structure prediction algorithms (Bowie et al., 1991).
7680	7711	many profile-based secondary structure prediction algorithms are essentially equivalent and that the differing results are due, primarily, to differences in the quality of the input alignments (Rost, 2001; Kloczkowski et al., 2002). DISCUSSION Although local inter-sequence information is insufficient to determine secondary structure accurately, such correlations are still useful to statistical
7680	7711	employed in secondary structure prediction (Rost and Sander, 1993), and improvements in accuracy to about Q3 ? 75% ± 3 are routine (Cuff and Barton, 1999, 2000; Chandonia and Karplus, 1999; Rost, 2001; Eyrich et al., 2003). By modifying our HMM to use evolutionary profiles, we find that even a modest increase in prediction accuracy represents a substantial increase in secondary structure
7680	7712	(Chandonia and Karplus, 1995; Frishman and Argos, 1997; Cuff and Barton, 1999) E ? E; H ? H; all others ? L. We also considered another common reduction, the ‘EHL’ mapping (Moult et al., 2001; Rost and Eyrich, 2001) E, B ? E; H, G, I ? H; all others ? L. Entropy estimation and bias correction The entropy of a discrete probability can be estimated by sampling from the distribution and then replacing the
7680	7712	However, given these small sample errors and the variation due to changes in secondary structure assignment, we cannot statistically distinguish accuracies separated by less than about two points (Rost and Eyrich, 2001; Przybylski and Rost, 2002). Since the range of reported accuracies is about 65–68% (Cuff and Barton, 1999; Schmidler et al., 2000; Kloczkowski et al., 2002), we are obliged to conclude that many,
7680	7713	inter-sequence correlations. It has been found that the secondary structure prediction accuracy can be enhanced substantially by basing the prediction upon a MSA of homologous protein sequences (Rost and Sander, 1993) rather than just a single sequence. Since protein structure tends to evolve relatively slowly, the MSA essentially represents many semi-independent amino acid sequences, each associated with
7680	7713	is then limited by the size of the family, the divergence of structure across the family and the quality of the alignment. This strategy is commonly employed in secondary structure prediction (Rost and Sander, 1993), and improvements in accuracy to about Q3 ? 75% ± 3 are routine (Cuff and Barton, 1999, 2000; Chandonia and Karplus, 1999; Rost, 2001; Eyrich et al., 2003). By modifying our HMM to use
7680	7714	associated with helixes, while glycines (G) are often located near helix breaks. Also note that secondary structure is strongly persistent. Helixes, e.g. are on average about 10 residues long (Schmidler et al., 2000). a protein’s structure, particularly the secondary structure, is determined by local, short-ranged interactions between residues closely spaced along the backbone, as opposed to non-local or
7680	7714	distinguish accuracies separated by less than about two points (Rost and Eyrich, 2001; Przybylski and Rost, 2002). Since the range of reported accuracies is about 65–68% (Cuff and Barton, 1999; Schmidler et al., 2000; Kloczkowski et al., 2002), we are obliged to conclude that many, very different secondary structure prediction algorithms are statistically indistinguishable. Our HMM model is almost optimal, in
7680	7714	as the maximum accuracy of a variety of other secondary structure prediction methods that utilize only local sequence–sequence correlations (Chandonia and Karplus, 1996; Cuff and Barton, 1999; Schmidler et al., 2000; Kloczkowski et al., 2002). This suggests that these algorithms are also almost optimal and that the modest prediction accuracy is due to the fundamental lack of local structure information.
7680	7714	this modest accuracy increase actually represents a considerable increase in information. This accuracy is similar to reported accuracies of a number of other algorithms tested on this data set (Schmidler et al., 2000; Kloczkowski et al., 2002). It may be that many profile-based secondary structure prediction algorithms are essentially equivalent and that the differing results are due, primarily, to differences
8919622	7727	step proceeds as follows. We start with the dispersion of resources. Since we use a grid based method, we first map the triangular mesh onto a three-dimensional grid using a voxelization method . This grid-based representation of the mesh is then filled to generate a solid obstacle. Resources are dispersed from the top plane of the simulation box, and absorbed by the “polyps” 1 . As soon
8919622	7727	on a cubic lattice. For this, the triangular mesh describing the surface of the coral colony is mapped onto the cubic lattice, for which we use the triangle voxelization method by Huang et al . The resulting hollow “shell” is filled using a fast, heuristic three-dimensional seed fill algorithm. The diffusion equation is solved using the moment propagation method . This method is
5581135	7733	execution phase is driven by these constraints. As a result, our solution can satisfy not only common requirements of query applications but also contextspecific. Adaptive techniques such as , , ,  have been proposed for deciding the best way to execute a query faced on changes of environment during query execution. Several works, such as , , focus on scheduling query
5581135	7735	query processing; (ii) a rewriting phase (rewriter) which translates global query into local queries on sources schemas. This phase depends on the way mappings between schemas are defined ; (iii) a preparing phase (preparation) which generates a query evaluation plan (QEP). We will present the form of QEP in the next section; (iv) a evaluation phase (evaluator) which communicates
5581135	1189	into sub-queries, called local queries. The sub-queries are evaluated by their appropriated sources and intermediate results are assembled by mediators. Many works in data integration such as , , , , ,  follow this reference mediation architecture. However, these works aim at solutions for different problems of data integration so they have different mediation architectures
5581135	7740	Children brokers push data to its parent. Pull mode means, parent brokers call data from their children. As a result, our execution model is not limited to only the classical iterator model  but apt to different natures of data sources like data sensors . Please note that buffers sizes and buffers strategies can be dynamically modified according to changes of the execution
5581135	7741	model, the level of distribution and autonomy of their sources systems. The focuses of works in ,  are on developing models for integrating heterogeneous sources. The emphasis of work in  are generating wrappers or translators. The authors of ,  focus on problems of reformulating global queries into local queries. The works in  stress on modeling restricted capabilities
5581135	7742	generates a feedback loop between environment and behaviors (context and rules). These three tasks of our evaluator component fulfill the features of an adaptive query processing defined in . We believe that our approach is suitable for executing queries in the unpredictable environment of large-scale mediation systems. Acknowledgements We would like to thank Beatrice Finance for many
5581135	7743	to go beyond what has been proposed and developed in the context of query processing in distributed database management systems and integration systems, i.e. dynamic optimization, caching, etc. , , , . We propose Query Brokers as basic units for evaluating queries. The query evaluation process can be compared as asApplication/User Application/User Wrapper Mediator Wrapper
5581135	7744	local queries. The sub-queries are evaluated by their appropriated sources and intermediate results are assembled by mediators. Many works in data integration such as , , , , ,  follow this reference mediation architecture. However, these works aim at solutions for different problems of data integration so they have different mediation architectures in terms of their data
5581135	7744	user-defined events/actions. This allows the query evaluation satisfying application contextspecific. V. RELATED WORK Many works on adaptive query processing have been done such as , , , . Nevertheless, these works only focussed on some specific problems of an adaptive query processing. The work in  addressed the problem of parallelization in query processing distributed over
5581135	7745	phase is driven by these constraints. As a result, our solution can satisfy not only common requirements of query applications but also contextspecific. Adaptive techniques such as , , ,  have been proposed for deciding the best way to execute a query faced on changes of environment during query execution. Several works, such as , , focus on scheduling query operators faced
5581135	7746	go beyond what has been proposed and developed in the context of query processing in distributed database management systems and integration systems, i.e. dynamic optimization, caching, etc. , , , . We propose Query Brokers as basic units for evaluating queries. The query evaluation process can be compared as asApplication/User Application/User Wrapper Mediator Wrapper Wrapper
5581135	7747	into sub-queries, called local queries. The sub-queries are evaluated by their appropriated sources and intermediate results are assembled by mediators. Many works in data integration such as , , , , ,  follow this reference mediation architecture. However, these works aim at solutions for different problems of data integration so they have different mediation
5581135	7747	systems. The focuses of works in ,  are on developing models for integrating heterogeneous sources. The emphasis of work in  are generating wrappers or translators. The authors of ,  focus on problems of reformulating global queries into local queries. The works in  stress on modeling restricted capabilities of data sources as Vu Tuyet Trinh, LSR-IMAG Laboratory, BP
5581135	7747	and execution tasks are not separated. They are included into the evaluation phase. Our work focuses on the evaluation phase. We suppose that there exists an algorithm such as the one in ,  finding relevant sources of a given global query and rewriting the global query into local queries. As a result, we have a query formulated on local schemas terms and our effort aims at
5581135	7748	parent brokers call data from their children. As a result, our execution model is not limited to only the classical iterator model  but apt to different natures of data sources like data sensors . Please note that buffers sizes and buffers strategies can be dynamically modified according to changes of the execution environment. Buffer Management is responsible for instantiating QB buffers,
5581135	7750	systems. The focuses of works in ,  are on developing models for integrating heterogeneous sources. The emphasis of work in  are generating wrappers or translators. The authors of ,  focus on problems of reformulating global queries into local queries. The works in  stress on modeling restricted capabilities of data sources as Vu Tuyet Trinh, LSR-IMAG Laboratory, BP 72,
5581135	7750	and execution tasks are not separated. They are included into the evaluation phase. Our work focuses on the evaluation phase. We suppose that there exists an algorithm such as the one in ,  finding relevant sources of a given global query and rewriting the global query into local queries. As a result, we have a query formulated on local schemas terms and our effort aims at evaluating
5581135	7752	problems of data integration so they have different mediation architectures in terms of their data model, the level of distribution and autonomy of their sources systems. The focuses of works in ,  are on developing models for integrating heterogeneous sources. The emphasis of work in  are generating wrappers or translators. The authors of ,  focus on problems of
5581135	7753	and user-defined events/actions. This allows the query evaluation satisfying application contextspecific. V. RELATED WORK Many works on adaptive query processing have been done such as , , , . Nevertheless, these works only focussed on some specific problems of an adaptive query processing. The work in  addressed the problem of parallelization in query processing
5581135	7753	to the classical iterator model in . This allows us to adapt our execution model to different context in a largescale mediation system. Our Query Broker framework looks like the one proposed in . However, instead of a centralized “bidder”, this evaluation is distributed through an interconnected QBs graph. Besides,  focuses on an economic model for optimizing queries while we do not
5581135	7754	phase is driven by these constraints. As a result, our solution can satisfy not only common requirements of query applications but also contextspecific. Adaptive techniques such as , , ,  have been proposed for deciding the best way to execute a query faced on changes of environment during query execution. Several works, such as , , focus on scheduling query operators
5581135	7755	a classical query processor . It uses a library of query operators including adaptive operators. The adaptive operators are non-blocking operators such as doublepipelined hash join , XJoin , etc. We also consider some special operators such asXUnion - exclusive union - which is designed for handling the case of duplication. XUnion produces at output the data coming from only one of
5581135	7755	approach for defining behaviors of QB. EC-A rules allow to specify Query Brokers behaviors towards execution circumstances. The techniques for re-scheduling and re-optimizing queries , ,  can be integrated in QBs as rules. We distinguish two types of events which are built-in event and user-defined event. Built-in events are primary events supported by system. We classify built-in
5581135	7755	the best way to execute a query faced on changes of environment during query execution. Several works, such as , , focus on scheduling query operators faced on delays. The others ,  concentrate to develop autoadaptive operators, i.e. non-blocking operators. These techniques can be easily integrated in query brokers. Moreover, different from all of above works, our goal is not
5581135	7758	Engine of a classical query processor . It uses a library of query operators including adaptive operators. The adaptive operators are non-blocking operators such as doublepipelined hash join , XJoin , etc. We also consider some special operators such asXUnion - exclusive union - which is designed for handling the case of duplication. XUnion produces at output the data coming from
5581135	7759	what has been proposed and developed in the context of query processing in distributed database management systems and integration systems, i.e. dynamic optimization, caching, etc. , , , . We propose Query Brokers as basic units for evaluating queries. The query evaluation process can be compared as asApplication/User Application/User Wrapper Mediator Wrapper Wrapper Mediator Fig.
7760	7761	over the last few years. Thomas Hofmann’s aspect model was capable of inducing latent word and document models from occurrence statistics of words within documents. Latent Dirichlet Allocation added the ability to represent the process by which documents are drawn as mixtures of latent word classes as determined by a Dirichlet prior.sFigure 1: The graphical model used in Factored Latent
7760	7762	for each of a number of different observable characteristics. Graphical models including latent class models have become increasingly popular over the last few years. Thomas Hofmann’s aspect model was capable of inducing latent word and document models from occurrence statistics of words within documents. Latent Dirichlet Allocation added the ability to represent the process by which
7760	7763	models that are consistent with those statistics across multiple aspects. Many varieties of data-driven perceptual data mining techniques have been applied to activity analysis. Johnson and Hogg  clustered trajectories in a scene into 400 representative clusters allowing generalization and prediction, but not compact description. Stauffer and Grimson  used a hierarchal clustering
7760	7764	is somewhat more compact but also not segmental. Because these approaches describe entire paths holistically it is unclear how they can be adapted to describe compound activities. Makris and Ellis also clustered entire trajectories into independent paths, but they extracted common features of paths to describe way points enabling some segmentation of the entire sequence but still a limited
14370	7770	and the semantics of TSQL2 was also given in the format of the SQL–92 standard. Some 500 pages of technical commentaries accompany these specifications. Upward compatibility of TSQL2 is studied in . In TSQL2, there are six kinds of tables: snapshot tables, valid-time event tables, valid-time state tables, transaction-time tables, bitemporal event tables, and bitemporal state tables. The first
3899838	7794	consider the application of clustering to the selforganization of a textual document collection. Clustering is the operation by which similar objects are grouped together in an unsupervised manner . Hence, when clustering textual documents, one is hoping to form sets of documents with similar content. Instead of exploring the whole collection of documents, a user can then browse the resulting
3899838	7796	clustering quality. We will consider other versions of ART in future work. II. RELATED WORK We consider one of the many applications of text clustering in the field of Information Retrieval (IR) , namely clustering that aims at self-organizing textual document collections. This application of text clustering can be seen as a form of classification by topics, hence making it the unsupervised
3899838	7796	It uses the same underlying pair-wise counting procedure as Jaccard and Fowlkes-Mallow to establish a count of false negatives and false positives, but combines those values following the F-measure  formulae: Fs¡£¢ ¤ 2¥§¦©¨?????????? 2 p+r] (3) where p = a/( a + b) is known as the precision and r = a/( a ???????©???????£??????????????????????????????????????????? ?????????????????????????
3899838	7797	textual document collections. This application of text clustering can be seen as a form of classification by topics, hence making it the unsupervised counterpart to Text Categorization (TC) . Text self-organization has become increasingly popular due to the availability of large document collections that change rapidly and that are quasi-impossible to organize manually. Even TC becomes
3899838	7798	of quality by setting it in a wider framework. Some investigators have evaluated clustering quality with other algorithms on Reuter-21578 and with the F1 measure, but have used non-standard splits . So our results cannot be compared directly with theirs. We plan to eventually evaluate other clustering methodologies to compare their clustering quality to ART’s. As well, testing on other text
3899838	7804	results were not very good with F1 = 0.25 and 0.15 (minimum quality value is 0 and maximum 1), but were comparable with other nonneural clustering results published on the same data sets. Merkl  compared ART with Self-Organizing Maps (SOM)  for the clustering of a small collection of documents and concludes that SOM forms better clusters based on a visual qualitative evaluation. We
3899838	7807	is deemed adequate based on the percentage of overlap between clusters and expected classification. The evaluation measure and text collection were non-standard. Finally, Kondadadi and Kozma  compare KMART, their soft clustering version of ART, to Fuzzy-ART and k-means. The text data consist of 2000 documents downloaded from the web as well as another 2000 newsgroup documents. Quality
3899838	7807	We are currently investigating these avenues. As well, in the FMsReuter collection, topics are not mutually exclusive, while ART1 clustering is. ART-based soft clustering such as with KMART  will be explored as yet another possible way to improve clustering. We are also exploring issues related to lower quality with other orders of presentation and the impact of time requirements for
3899838	7810	p+r] (3) where p = a/( a + b) is known as the precision and r = a/( a ???????©???????£????????????????????????????????????????????? ????????????????????????? precision and recall. The &quot;ModApte&quot; split  of the Reuter-21578 Distribution 1.0 1 data set is used for our experiments. This data sets is known to be challenging because of skewed class distribution, multiple overlapping categories, and its
3899838	7811	the clusters have been formed. Reuter is a benchmark data set for TC. Using this data set specific split and the F1 quality measure makes comparison with published TC results on the same split  possible. This is an important and we believe innovative aspect of our experimental approach: TC F1 quality results are used as an upper bound for cluster quality since learning in a supervised
3899838	7811	We now compare ART1 clustering quality to the upper bound expected for cluster quality: the best TC results obtained with Support Vector Machines (SVM) and k-Nearest Neighbors (kNN) published in  (Fig. 3). ART1 achieves 51.2% of the TC quality. Comparing to 16.3% level of quality for kmeans, the lower bound, ART1 does much better but is still only about half-way to the optimal expected
3899838	7812	This way of evaluating clustering quality allows one to clearly establish the level of quality obtained by a clustering algorithm as a percentage of the upper bound quality. We use the k-means  clustering algorithm to establish a lower bound for quality. Our rationale is that since k-means represents one of the simplest possible approaches to clustering, one would expect that any slightly
3899838	3167	parameter ? ? (0,1] determines the level of abstraction at which ART discovers clusters. Moreover, the minimal number of clusters present in the data can be determined by minimal vigilance , computed as ?min < 1/N where N is the number of features (words) used to represent a document. We chose a value of ?min = 0.0005 as the initial vigilance parameter and we increment it until we
3899838	7815	feature reduction by term selection based on term frequency was applied to reduce the dimensionality of the original documents feature space. This approach was judged very effective for TC by . IV. EXPERIMENTAL RESULTS We eliminated words that appear in 10, 20 40 and 60 or less documents. In the first case, a total of 2282 term features were retained while in the last only 466 were. Our
3899838	7818	that do not penalize specializations and generalizations. Moreover, more advanced ART architectures with non-binary representation (such as ART2 , fuzzy ART , MacLeod’s ART  and FOSART ) and better feature selection may improve cluster quality. We are currently investigating these avenues. As well, in the FMsReuter collection, topics are not mutually exclusive, while ART1
3899838	7819	of quality by setting it in a wider framework. Some investigators have evaluated clustering quality with other algorithms on Reuter-21578 and with the F1 measure, but have used non-standard splits . So our results cannot be compared directly with theirs. We plan to eventually evaluate other clustering methodologies to compare their clustering quality to ART’s. As well, testing on other text
8919636	7821	on protein-RNA interactions are available in the literature, we are planning to include these data also in the database from the next update. Recently, ProNIT is linked with the “ProTherm” database  with respect to the particular protein involved in the protein-nucleic acid interaction. 4 Links to Other Databases ProNIT is cross-linked with other relevant databases we have developed,
446314	7826	in engineering and out-of-core workloads, and optimizing them has been the subject of considerable research, although it is usually attacked at the application level or as a virtual memory issue . ¤ ? ????? ¤ ????????????????? ), In the ordinary implementation, the nfsheur contains a single offset and sequentiality count for each file handle. In order to handle stride read patterns, we add
446314	7828	has centered on increasing the effectiveness of client-side caching. Dahlin et al. provide a survey of several approaches to cooperative client-side caching and analyze their benefits and tradeoffs . The NQNFS (Not Quite NFS) system grants short-term leases for cached objects, which increases performance by reducing both the quantity of data copied from the server to the client and the number
446314	7830	of much research. Dube et al. discuss the problems with NFS over wireless networks, which typically suffer from packet loss and reordering at much higher rates than our switched Ethernet testbed . We believe that our SlowDown heuristic would be effective in this environment. Much research on increasing the read performance of NFS has centered on increasing the effectiveness of client-side
446314	5880	from the buffer cache instead of from disk. In an earlier study of NFS traffic, we noted that many NFS requests arrive at the server in a different order than originally intended by the client . In the case of read requests, this means that the sequentiality metric used by FFS is undermined; read-ahead can be disabled by a small percentage of out-of-order requests, even when the overall
446314	7833	increases performance by reducing both the quantity of data copied from the server to the client and the number of NFS calls that the client makes to check the consistency of their cached copies . Unfortunately, adding such constructs to NFS version 2 or NFS version 3 requires non-standard changes to the protocol. The NFS version 4 protocol does include a standard protocol for read leases,
446314	7834	the number of times the data buffers are copied. The zero-copy NFS server shows that this technique can double the effective read throughput for data blocks that are already in the server cache . In contrast to most previous work in NFS read performance, we focus on improving read performance for uncached data that must be fetched from disk. In this sense, our work is more closely related
446314	7835	sequential access – but because sequential access is the common case, this is quite effective for most workloads. The Fast File System (FFS) was the pioneering implementation of these ideas on UNIX . FFS assumes that most file access patterns are sequential, and therefore attempts to arrange files on disk in such a way that they can be read via a relatively small number of large reads, instead
446314	7836	bewildering variety of benchmarks, and there have been calls for still more . Nearly all benchmarks can be loosely grouped into one of two categories: micro benchmarks, such as lmbench  or bonnie , which measure specific lowlevel aspects of system performance such as the time required to execute a particular system call, and macro or workload benchmarks, such as the SPEC SFS
446314	7839	in engineering and out-of-core workloads, and optimizing them has been the subject of considerable research, although it is usually attacked at the application level or as a virtual memory issue . ¤ ? ????? ¤ ????????????????? ), In the ordinary implementation, the nfsheur contains a single offset and sequentiality count for each file handle. In order to handle stride read patterns, we add
446314	7840	amortize the cost of these operations over the set of synchronous operations that would otherwise be necessary. For write operations, some techniques for doing this are log-structured file systems , journalling, and soft updates . For reading, the primary mechanism is read-ahead or prefetching. When the file system detects that a process is reading blocks from a file in a predictable
446314	7842	did not investigate this phenomenon. The tradeoffs between throughput, latency, fairness and other factors in disk scheduling algorithms have been well studied and are still the subject of research . Despite this research, choosing the most appropriate algorithm for a particular workload is a complex decision, and apparently a lost art. We find it disappointing that modern operating systems
446314	7843	over the set of synchronous operations that would otherwise be necessary. For write operations, some techniques for doing this are log-structured file systems , journalling, and soft updates . For reading, the primary mechanism is read-ahead or prefetching. When the file system detects that a process is reading blocks from a file in a predictable pattern, it may optimistically read
446314	7844	and the heuristics used by FFS . 3 Benchmarking File Systems and I/O Performance There has been much work in the development of accurate workload and micro benchmarks for file systems . There exists a bewildering variety of benchmarks, and there have been calls for still more . Nearly all benchmarks can be loosely grouped into one of two categories: micro benchmarks,
446314	7845	Ellard, Margo Seltzer Harvard Universitysellard,margo¡ @eecs.harvard.edu ous blocks is small. This technique can be beneficial even when the disk blocks are not adjacent, as shown by Shriver et al. . Although there has been research in detecting and exploiting arbitrary access patterns, most file systems do not attempt to recognize or handle anything more complex than simple sequential access
446314	7845	work in NFS read performance, we focus on improving read performance for uncached data that must be fetched from disk. In this sense, our work is more closely related to the studies of read-ahead  and the heuristics used by FFS . 3 Benchmarking File Systems and I/O Performance There has been much work in the development of accurate workload and micro benchmarks for file systems .
446314	7846	and the heuristics used by FFS . 3 Benchmarking File Systems and I/O Performance There has been much work in the development of accurate workload and micro benchmarks for file systems . There exists a bewildering variety of benchmarks, and there have been calls for still more . Nearly all benchmarks can be loosely grouped into one of two categories: micro benchmarks,
446314	7846	the raw read performance of large files. We do not attempt to age the file system at all before we run our benchmarks. Aging a file system has been shown to make benchmark results more realistic . For most benchmarks, fresh file systems represent the best possible case. For our enhancements, however, fresh file systems are one of the worst cases. We are attempting to measure the impact of
446314	7847	has been much work in the development of accurate workload and micro benchmarks for file systems . There exists a bewildering variety of benchmarks, and there have been calls for still more . Nearly all benchmarks can be loosely grouped into one of two categories: micro benchmarks, such as lmbench  or bonnie , which measure specific lowlevel aspects of system performance such as
446314	7848	seeks, since each track at the outside of the disk contains more sectors). This effect has been measured and analyzed and is the basis of some well-studied techniques in file system layout tuning . Beyond papers that explicitly discuss methods for measuring and exploiting disk properties, however, mention of this effect is rare. The ZCAV effect can skew benchmark results enormously,
446314	7849	adaptive configuration the situation is even more complex; in such devices the relationship between logical and physical block addresses may be arbitrary, or even change from one moment to the next . For these situations it is better to let the device schedule the requests because it has more knowledge than the kernel. Even for ordinary single-spindle disks, there is a small amount of
8919638	7859	other: ????????¥¤??????s? ? ? ????? ? ??????s? £ ? £?????? § ¥¤??? §s? £ ? ? ¡ §???????? ????? ??? ??????????? (5) The existence of an inverse for ¥ follows from a semimonotonicity assumption . The goal is to estimate A, b, c, K, and F, from various input images, which can be achieved using a generalization of motion estimation, within an iterative framework, as follows: First
8919638	7859	a ? value at the same spatial location that a pixel in the second image has the ? value . Compara? grams capture everything that can be known about the amplitude response function ¥ of a camera ; Slenderize the comparagrams to obtain the compara? metric functions (comparagraphs) ; Unroll the comparagraphs to obtain an estimate of ? the response function. The unrolling can be
8919638	7863	have been published on the problems of motion estimation and frame alignment , and much of this work is based on the so-called Brightness Constancy Constraint Equation (BCCE) of Horn and Schunk . However, a more general formulation, suitable for mediated reality, is based on the Lightspace Change Constraint Equation (LCCE) . Tsai and Huang  pointed out that the elements of the
8919638	7863	presented in this paper (which does not require prior solution of correspondence) also relies on projective group theory . When the change from one image to another is small, optical flow  may be used. In 1-D, the traditional optical flow formulation assumes each point ? in frame ? is a translated version of the corresponding point in ? ??? ? frame , and ? ? are chosen in the ratio ?
8919638	7864	Equation (BCCE) of Horn and Schunk . However, a more general formulation, suitable for mediated reality, is based on the Lightspace Change Constraint Equation (LCCE) . Tsai and Huang  pointed out that the elements of the projective group give the true camera motions with respect tosa planar surface. Tsai and Huang explored the group structure associated with images of a 3-D
718752	7866	software. Architects are proposing heavily partitioned execution substrates with many ALUs to deal with limits in pipeline depth, clock rate growth , and slowing of on-chip wires . To achieve high performance, these architectures must execute many instructions in parallel and tolerate multi-cycle onchip delays between dependent producing and consuming instructions. The
718752	7866	the execution order and the placement of instructions onto ALUs. Centralized superscalar processor are limited because of the quadratic hardware required for both the issue width and window size  to check for data dependences and broadcasts results. While researchers have proposed clustered (partitioned) superscalar architectures to improve their scalability, the hardware instruction
718752	7866	3b shows the encoding. Instructions do not encode their source operands — they encode only the physical locations of their dependent instructions. For example, theadd instruction placed at location , upon execution, forwards its result to theLSH instruction placed at location . The hardware maps the hyperblock to the execution array, reads the input registers from the register file, and
718752	7866	ports Operand buffers A B C Instruction buffers 4 TRIPS Processor. LD AND ADD SUB LD LSH Frame C Frame B Frame A a) Instruction Placement b) Instruction Encoding READ R2 READ R4 READ R5 WRITE R7  AND   LD  ADD  SUB   LD  LSH READ R2  READ R4   READ R5  LD #0  AND  ADD  LD #0  SUB R7 LSH
718752	7869	presented in this paper, and run on average 18% slower than the oracular mechanism. We added a TRIPS scheduler to the Trimaran compiler tool set, which is based on the Illinois Impact compiler . Trimaran applies aggressive VLIW optimizations and produces code in the Elcor intermediate format, based on the Hewlett-Packard PD  instruction set. In addition to the usual set of classic
718752	7871	and latency tolerance that clustered VLIWs present to software schedulers. Farkas attempts to reduce intercluster communication in a clustered superscalar using compile-time cluster assignment . However, performance gains are underwhelming, in part because the scope of scheduling is limited to basic blocks, rather than the larger regions of instructions available with hyperblock formation
718752	7876	chooses both the order and the ALU on which to issue each instruction. Despite techniques to move instructions across branches, such as predication , trace scheduling , and region formation , instruction schedulers often cannot find enough instructions to pack into each VLIW instruction. In addition, instructions with unpredictable latency, such as load misses, stall the entire VLIW
718752	7877	software. Architects are proposing heavily partitioned execution substrates with many ALUs to deal with limits in pipeline depth, clock rate growth , and slowing of on-chip wires . To achieve high performance, these architectures must execute many instructions in parallel and tolerate multi-cycle onchip delays between dependent producing and consuming instructions. The
718752	7878	changes in architectures and their accompanying software. Architects are proposing heavily partitioned execution substrates with many ALUs to deal with limits in pipeline depth, clock rate growth , and slowing of on-chip wires . To achieve high performance, these architectures must execute many instructions in parallel and tolerate multi-cycle onchip delays between dependent producing
718752	7879	of the central role that they play in obtaining good VLIW performance. However, conventional architectures and their schedulers are ill equipped to solve these emerging problems. VLIW architectures , including EPIC architectures , use an execution model in which the compiler chooses both the order and the ALU on which to issue each instruction. Despite techniques to move instructions
718752	7880	latencies between functional units and variable latencies from the memory hierarchy. Partitioned VLIW: The SPDI scheduling problem bears the most resemblance to scheduling for a partitioned VLIW . For RAW, which uses a 2-D VLIW execution model, the convergent scheduler handles complexity by computing an ALU preference for each scheduling heuristic . These preferences are weighted, and
718752	7880	critical paths in the same cluster is best in their setting. The CARS approach is similar to UAS but performs register allocation concurrently with scheduling and has lower algorithmic complexity . In comparison to VLIW approaches, EDGE architectures using SPDI scheduling provide two major features that improve the effectiveness of the scheduler. First, the scheduler is freed from some of
718752	7881	balanced scheduler of Kerns and Eggers spreads ILP to cover variable latencies in a superscalar execution model, resulting in a schedule with available ILP spread evenly between load instructions . Later work spread ILP to the loads with the highest latencies . Clustered superscalar processors present to the hardware scheduler the same problems of load balancing and latency tolerance
718752	7883	of the central role that they play in obtaining good VLIW performance. However, conventional architectures and their schedulers are ill equipped to solve these emerging problems. VLIW architectures , including EPIC architectures , use an execution model in which the compiler chooses both the order and the ALU on which to issue each instruction. Despite techniques to move instructions
718752	7884	16.41 15.78 14.24 20.45 HMEAN 2.52 2.45 2.49 2.59 2.69 2.78 2.74 2.36 3.33 AMEAN 4.37 3.87 4.61 5.18 5.38 5.64 5.61 4.85 7.09 Table 1. Performance improvements from scheduler optimizations. bench  benchmark suites. The Trimaran front end currently compiles only C benchmarks, so we convert a number of the SPECFP benchmarks to C, and we present results for all of the SPEC benchmarks that the
718752	7885	by 29% over a naive greedy scheduler. We also show that this simple algorithm outperforms a much more complex scheduling algorithm, the recently developed convergent scheduling framework  both tuned with the same heuristics. On at 64-wide issue machine, we achieve 5.6 IPC and are within 80% of the performance of an idealized machine without on-chip latency. We conclude that by
718752	7885	can balance reduced latency with better speculation depth to attain better performance. We also compare the SPDI scheduling algorithm with an implementation of the convergent scheduling framework  and find that the framework does not offer any improvements over the best set of SPDI scheduler heuristics. 5.1 Scheduler Evaluation This section evaluates the scheduler heuristics from Section 3,
718752	7885	thus far tightly integrate all heuristics within one single algorithm. A competing approach that decouples different scheduler optimizations through a flexible interface is Convergent Scheduling , which has been proposed as a framework for clustered architectures. The framework composes independent phases that each address a particular constraint. All phases share a common interface that
718752	7885	the phases successively, one or more times and in any order until it converges on a final schedule. Lee et al. show that this scheduling approach works well on clustered-VLIW and RAW architectures . We implemented the convergent scheduling algorithm for TRIPS and compared the results with our approach. Unlike VLIW architectures, temporal scheduling preferences are not required for TRIPS, so
718752	7885	latencies between functional units and variable latencies from the memory hierarchy. Partitioned VLIW: The SPDI scheduling problem bears the most resemblance to scheduling for a partitioned VLIW . For RAW, which uses a 2-D VLIW execution model, the convergent scheduler handles complexity by computing an ALU preference for each scheduling heuristic . These preferences are weighted, and
718752	7886	latencies in a superscalar execution model, resulting in a schedule with available ILP spread evenly between load instructions . Later work spread ILP to the loads with the highest latencies . Clustered superscalar processors present to the hardware scheduler the same problems of load balancing and latency tolerance that clustered VLIWs present to software schedulers. Farkas attempts to
7893	7895	such as membership management and data replication are promoted to the application layer. Here, to distinguish it from a p2p network, an overlay network is equivalent to a proxy-based network 1 . In a p2p multicast network, each node in the multicast tree can also be a multicast client (receiver). In a (proxy-based) overlay network, only the leaf nodes are clients. Within both networks,
7893	7895	trees (e.g., ). These studies are based on the IP multicast model, in which intermediate nodes do not 1 Please note that both p2p and proxy-based networks are forms of overlay networks . In this paper, we use the term overlay networks to refer to proxy-based networks.sparticipate in FEC. Here, we study the packet-loss model of a multicast tree when FEC codecs are placed in the
7893	7897	in (sub-)optimally selected intermediate nodes of a network can improve the throughput and overall reliability dramatically. 1. INTRODUCTION Overlay and peer-to-peer (p2p) networks (e.g., =) are becoming increasingly popular for the distribution of shared content over the Internet. Most of the studies conducted for these networks have focused on multicast tree building. Further, these
7893	7897	FEC (b) A NEF codec in a multicast tree can recover lost data and parity packets and send these lost packets downstream. In the two forms of networks considered here, “overlay” and “p2p” (e.g., -), multicast functions such as membership management and data replication are promoted to the application layer. Here, to distinguish it from a p2p network, an overlay network is equivalent to a
7893	7898	presented in Section 4. A brief summary is presented in Section 5. 2. ANALYSIS OF NETWORK-EMBEEDED FEC ROUTS Previous studies analyzed the packet-loss model for FECenhanced multicast trees (e.g., ). These studies are based on the IP multicast model, in which intermediate nodes do not 1 Please note that both p2p and proxy-based networks are forms of overlay networks . In this paper, we
7893	7899	in Section 4. A brief summary is presented in Section 5. 2. ANALYSIS OF NETWORK-EMBEEDED FEC ROUTS Previous studies analyzed the packet-loss model for FECenhanced multicast trees (e.g., ). These studies are based on the IP multicast model, in which intermediate nodes do not 1 Please note that both p2p and proxy-based networks are forms of overlay networks . In this paper, we use
7902	7903	in studying static network motifs as a tool for understanding regulatory networks . Complex networks have previously been classified by global characteristics such as scale–free  and small world network connection topologies . In order to investigate networks further beyond their global features requires an understanding of the potential basic structural elements
7905	7907	layer to the corresponding units in the input layer, then it always settles down in a unique stable state (given any input vector, that is, any interpretation) when P is an acceptable program (see ) 1 . This stable state is the unique stable model of P, that is the least fixed point of T P . In  Towell and Shavlik presented KBANN 2 , a system for rule insertion, refinement and
7905	7910	system for fault diagnosis of a simplified power system generation plant, obtaining good preliminary results. These results indicates the usefulness of C-IL 2 P as a tightly coupled hybrid system  for the solution of problems related to signal processing. As future work, we would like to submit the application here presented to a more expressive quantity of tests, to apply C-IL 2 P’ s
7905	1478	knowledge while learning, like the symbolic systems. Therefore, it is very important to outline tight correlation between symbolic knowledge and Artificial Neural Networks. Toward this goal, in  Holldobler and Kalinke presented a massively parallel method for logic programming . They have shown that for each logic program P there exists a three-layer feedforward neural network R with
7905	1478	computational model can represent and process symbolic knowledge in a massively parallel way, performing learning from examples with background knowledge as in , and logic programming as in . This integration also offers explanation 1 It guarantees that P has exactly one stable model. 2 Knowledge-based Artificial Neural Network. capability to the system through the extraction of
7905	7912	one of the basic attributes of intelligent comportment, can be defined as the change of behavior motivated by changes in the environment in order to perform better in many knowledge domains . Learning strategies can be classified as: learning from instruction, learning by deduction, learning by analogy, learning from examples and learning by observation and discovery ; the latter
7905	7919	into account the evolution of computer science for massively parallel architectures , it is desirable that intelligent hybrid systems could be based on some artificial neural network’s model. In  and  is empirically showed that neural networks using the backpropagation learning algorithm  are at least as effective as purely symbolic inductive learning systems .
7922	7924	model is described in , where a 3-component mixture for the stable process, the outlier data and the wandering term is designed to capture rapid temporal variations in the model. Cham and Rehg  introduce a piecewise Gaussian representation to specify the tracker state, in which the selected Gaussian components characterize the neighborhoods around the modes. This idea is applied to
7922	7925	of our algorithm are 0.284 and 0.136 respectively, which are better than the classical particle filter (MSE = 0.340, variance = 0.294). 4.2 Object Tracking in Video Color-based trackers such as  search the image space deterministically, and they might fall into a local minimum. To overcome this limitation, the color-based multihypothesis tracking was proposed in  which is based on the
7922	7926	introduces a density approximation methodology that is an alternative to kernel density estimation, but computationally as simple as parametric methods. It is based on the mode finding algorithm  by variable-bandwidth mean-shift. The density is represented with a weighted sum of Gaussians, whose number, weights, means and covariances are automatically determined. Instead of a batch
7922	7927	examples that involve statistical estimation and propagation of the underlying density. Real-time object tracking is a challenging computer vision task. Tracking based on the mean-shift algorithm  searches for the local maximum of the object appearance model. However, because it is a deterministic algorithm, it generally cannot recover from a failure. This problem can be ameliorated by
7922	7927	of our algorithm are 0.284 and 0.136 respectively, which are better than the classical particle filter (MSE = 0.340, variance = 0.294). 4.2 Object Tracking in Video Color-based trackers such as  search the image space deterministically, and they might fall into a local minimum. To overcome this limitation, the color-based multihypothesis tracking was proposed in  which is based on the
7922	7928	introduces a density approximation methodology that is an alternative to kernel density estimation, but computationally as simple as parametric methods. It is based on the mode finding algorithm  by variable-bandwidth mean-shift. The density is represented with a weighted sum of Gaussians, whose number, weights, means and covariances are automatically determined. Instead of a batch
7922	7928	for object tracking in video. 2 Mode Detection and Density Approximation In this section, we present the iterative procedure for mode detection based on the variable-bandwidth mean-shift , and the batch density approximation using the mode detection technique. Then, an efficient alternative method – incremental approximation – is presented. 2.1 Batch Density Approximation Denote ¦¨§
7922	7928	, ? ? ? ??? ? ????? , ????? ? ? ??? ??? ? , ? sentation is incorporated into the particle filter, and how to propagate the density through Bayesian filtering based on variable-bandwidth mean-shift . 3.1 Bayesian Filtering ? The ¦ ? (??? state variable ? (??? ??????? measurements ??? ). ) is characterized by its ??????? probability density function estimated from the sequence of The process
7922	7929	This idea is applied to multiple hypothesis tracking in a high dimensional space body tracker, but the sampling and the posterior computation are not straightforward. Kernel density estimation  is a widely used non-parametric approach in computer vision. Its major advantage is the flexibility to represent very complicated densities effectively. But its very high memory requirements and
7922	394	it generally cannot recover from a failure. This problem can be ameliorated by probabilistic trackers using the Kalman filter and its extensions , or more generally particle filters  that achieve robustness to clutter and occlusion by maintaining multiple hypotheses in the state space. Particle filtering provides a convenient framework for estimating and propagating the density
7922	394	Mode Propagation through Bayesian Filtering In this section we will show how to use the approximation technique to propagate the density modes in the particle filter framework. The particle filter  is a stochastic framework to propagate the conditional density; it originated from statistics and control theory. The algorithm combines the dynamical models and measurement by sampling to
7922	7930	it generally cannot recover from a failure. This problem can be ameliorated by probabilistic trackers using the Kalman filter and its extensions , or more generally particle filters  that achieve robustness to clutter and occlusion by maintaining multiple hypotheses in the state space. Particle filtering provides a convenient framework for estimating and propagating the density
7931	7937	parameters correspond to traditional notions of register variation—i.e., depending on the situational features holding, differing linguistic features occur with higher or lower probability (cf. Biber, 1993). Another way of viewing this process is to say that the domain ontology configurations are associated with differing ‘views’ of the linguistic system. As the register changes, so does the
7931	7943	and thus that the communication with artificial systems is not homogeneous. Instead, the linguistic properties of the utterances depend very much on the way speakers conceptualize the system (cf. Fischer 2000) and on the system's output (cf. Fischer & Batliner, 2000). Since speakers design their speech for their recipients, the way they think about their communication partner may have a strong impact on
7931	7957	categorizations (Garrod and Sanford 1988; Coventry, 1998); while work from robotics has introduced the notion of a field potential for describing degrees of likelihood of positioning (cf. Stopp et al., 1994). Psycholinguistic experimental studies on spatial situations focus on different kinds of mental representations that are reflected verbally in the speakers’ utterances. A central concept
8709212	7968	identified the origins of certain pathological behavior. They also showed that routing instability had been significantly reduced in the core network by software improvements. Govindan and Reddy  studied the Internet topology and routing stability several years ago. They found that routes to prefixes were highly available and stable at that time, but the mean reachability duration for a
8709212	7969	update” (or “BGP update” for brevity). 2. Related work While BGP has been widely used in the Internet, its behavior in this real-world environment is yet to be fully understood. Labovitz, et. al.  studied BGP routing messages exchanged between US service providers and reported that the majority of BGP messages consisted of redundant pathological announcements.  further identified the
8709212	2711490	Labovitz, et. al.  studied BGP routing messages exchanged between US service providers and reported that the majority of BGP messages consisted of redundant pathological announcements.  further identified the origins of certain pathological behavior. They also showed that routing instability had been significantly reduced in the core network by software improvements. Govindan and
8709212	7970	for a prefix decreases with the Internet growth. The Internet has grown rapidly since this study and more recent data is needed to help better understand current Internet performance. Paxson  studied the routing behavior from an endto-end communication point of view. The results showed that Internet paths are heavily dominated by a single prevalent route. These measurements were
8709212	4498	route. These measurements were conducted based on traceroute data. In contrast, our study uses a different data collection methodology that focuses on BGP routing updates. Rexford, et. al.  studied the routing stability of popular destinations. They found popular destinations have remarkably stable BGP routes, while a small number of unpopular destinations are responsible for the
8709212	4498	is a change in some attribute associated with the route. Thus ideally, a prefix would have a stable route that is announced once and no additional updates would be sent for the prefix. In practice,  shows that routes to some popular prefixes tend to be quite stable and these prefixes contribute only a few updates to the volume of BGP updates seen in the Figure 2. Globally Unreachable DoD
8709212	7973	that there were some “BGP storms”, i.e., excessive numbers of BGP updates over short periods of time. However, after looking into the BGP traffic and classifying it into different categories,  found that 40% of BGP storm was caused by a measurement artifact: BGP session resets at the monitoring point. The work reported in this paper represents another step toward a comprehensive
8709212	7973	resumes. We attribute this behavior to lower stability of the multi-hop BGP sessions. We pre-process the update files to remove the updates that are generated due to session resets. Our work in  discusses problems associated with multi-hop sessions and techniques for cleaning the data in more detail. Our pre-processing of BGP updates results in a clean set of BGP updates to analyze. The
8709212	7973	2001 appear to be more sensitive to events such as Code Red and Nimda. To better understand the behavior of our sample set of DoD prefixes, we examined the type of updates being sent. Our work in  defines the update class hierarchy (shown in Figure 8) that is based on the timing of an update and its relationship to previous updates. We examine the type of updates sent for our set of sample
7974	7977	rate (60% per year) than important factor is the degree of locality in data memory speeds (10% per year) as shown in Figure 1 references for a given algorithm. Good data locality (borrowed from ). Thus, the relative cost of leads to fewer (expensive) cache misses, and better a cache miss has increased by two orders of magni- performance. tude since 1986. As a result, we cannot assume that
7974	7977	doesn’t have direct control of which block to bring into a cache. This makes cache optimization more subtle. Typical cache optimization techniques include clustering, compression and coloring . Clustering tries to pack, in a cache block, data structure elements that are likely to be accessed successively. Compression tries to remove irrelevant data and thus increases cache block
7974	7977	improved performance by restructuring these algorithms to exploit caches. In addition, they constructed a cache-conscious heap structure that clustered and aligned heap elements to cache blocks.  demonstrated that cache optimization techniques can improve the spatial and temporal locality of pointer-based data structures. They showed improvement on various benchmarks. In , Nyberg
7974	7977	and B+-trees. Our results show the contrary conclusion. The explanation is that the CPU speed has improved by two orders of magnitude relative to memory latency during the past thirteen years . time(s) 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 T tree enhanced B+ tree full CSS-tree level CSS-tree hash 0 0 20 40 60 80 100 number of entries per node 120 140 Figure 7: Varying node size,
7974	7978	relative outcomes from  for lookup speed. We study a variety of existing techniques, including hash indexes, binary search on a sorted list of record identifiers, binary trees, B+-trees , T-trees , and interpolation search. We also introduce a new technique called “Cache-Sensitive Another recent development has been the explo- Search Trees” (CSS-trees). CSS-trees augment
7974	7992	the problems of concurrency, transaction processing and logging , and recovery . Systems with a significant query-processing component include OBE , MM-DBMS , and Starburst . More recently, the TimesTen corporation (formerly the Smallbase project at Hewlett-Packard) has developed a commercial main-memory system, with claims of tenfold speedups
7974	7993	has attacked the processormemory gap using the above techniques. Wolf and Lam  exploited cache reference locality to improve the performance of matrix multiplication. LaMarca and Ladner  considered theseffects of caches on sorting algorithms and improved performance by restructuring these algorithms to exploit caches. In addition, they constructed a cache-conscious heap structure
7974	7994	has attacked the processormemory gap using the above techniques. Wolf and Lam  exploited cache reference locality to improve the performance of matrix multiplication. LaMarca and Ladner  considered theseffects of caches on sorting algorithms and improved performance by restructuring these algorithms to exploit caches. In addition, they constructed a cache-conscious heap structure
7974	7999	et al. have shown that for achieving high performance sorting, one should focus carefully on cache memory behavior. Cache conscious algorithms have been considered in database systems also. In , the authors suggested several ways to improve the cache reference locality of query processing operations such as joins and aggregations. They showed that the new algorithms can run 8% to 200%
7974	7999	be optimized for space allocation and cache related issues such as alignment. As a result, we believe our implementation will run faster. Since cache optimization can be sensitive to compilers , we also chose two different compilers: one is Sun’s native compiler CC and the other is GNU’s g++. We used the highest optimization level of both compilers. However, the graphs for
7974	8000	existing index structures for searching and point out their shortcomings. Cache memories are small, fast static RAM memories that improve program performance by holding recently referenced data . Memory references satisfied by the cache, called hits, proceed at processor speed; those unsatisfied, called misses, incur a cache miss penalty and have to fetch the corresponding cache block from
7974	8001	has addressed the problems of concurrency, transaction processing and logging , and recovery . Systems with a significant query-processing component include OBE , MM-DBMS , and Starburst . More recently, the TimesTen corporation (formerly the Smallbase project at Hewlett-Packard) has developed a commercial main-memory system, with claims of
7974	8001	loop join is pipelinable, requiring minimal storage for intermediate results and is relatively easy to implement. As a matter of fact, indexed nested loop join is the only join method used in . This approach requires a lot of searching through indexes on the inner relations. Last but not least, transforming domain values to domain IDs (as described in the previous section) requires
7974	8001	problem with binary search is that it requires a calculation to be performed log 2 n times to find the next element to search. Even if this calculation uses a shift rather than a division by two , the calculation represents a significant portion of the execution time needed. Nevertheless, binary search has the benefit that no additional space beyond a is needed to perform a search. Our goal
4217056	8004	Wilhelm Hasselbring University of Oldenburg Software Engineering Group Department of Computing Science D-26111 Oldenburg, Germany Email: hasselbring@informatik.uni-oldenburg.de supporting systems . On the other hand, the tools have an impact on the process models. The main concern for this direction is the consideration that software development processes are similar to software or at least
4217056	8006	such a PML. For this purpose a flexible approach of a PML is needed, which is extensible. Typical PML representations focus on specific application areas or they are not extensible , , . Furthermore, we need the above mentioned connection from process elements to components. For several reasons the UML is an interesting candidate for this purpose: • Firstly, a lot of successful
4217056	8008	steadily increasing — especially in the domain of system development — since only tools are able to support the efficient development of the systems that have a continuously increasing complexity . The relationship between process models and tools is relevant in both directions. On one hand, process models have an impact on the tools — this is often referred to as “enactment”  usually
4217056	8014	from process elements to components. For several reasons the UML is an interesting candidate for this purpose: • Firstly, a lot of successful approaches to express process models in UML exist . The suitability of UML as PML was as well confirmed in the project ViSEK (Virtual Software Engineering Competence Center) , in which the so called V-Model  — the standard process model
4217056	8017	that base on component protocols  can provide useful information about the predicted properties of a component  including the best (re-)configuration time to avoid unexpected behaviour . III. Choosing an XPM Frame In this section, we discuss the requirements on a PML and possible instances of such a PML. For this purpose a flexible approach of a PML is needed, which is extensible.
4217056	6564	Language” . The interface specification has to reflect all required tracking and control functions for the relevant actions. Such “component contracts” that base on component protocols  can provide useful information about the predicted properties of a component  including the best (re-)configuration time to avoid unexpected behaviour . III. Choosing an XPM Frame In
4217056	8020	and control functions for the relevant actions. Such “component contracts” that base on component protocols  can provide useful information about the predicted properties of a component  including the best (re-)configuration time to avoid unexpected behaviour . III. Choosing an XPM Frame In this section, we discuss the requirements on a PML and possible instances of such a
8919657	8483	be progressively incorporated into policy goals, institutions, and new poverty programs, and there will be an increasing understanding of the larger issues that affect community well-being. Myer (1984) refers to scaling up through three different approaches—expansion, explosion, and association. Another classification, used in the past by IIRR (2001), suggests that scaling up needs to be viewed
8373515	8517	Frisian, Gaelic, French, etc. • Level of abstraction. What level of abstraction, with respect to the systems under consideration, is the methodological knowledge focussed at? Examples, derived from , would be: conceptual level, logical level and physical level. • Systems scope. What is the scope of the methodological knowledge in terms of the system(s) considered? Some examples would be:
8373515	8518	other examples of, ICT related, bodies of knowledge with ensuing standardisation, certification and training activities are: 1. The IT Infrastructure Library (ITIL) for IT infrastructure management . 2. The Project Management Body of Knowledge (PMBOK) for project management . 3. The Information Services Procurement Library (ISPL) for procurement of information services . We
8919674	8084	using content understanding methods to approximate the content flow between zones. This analysis is to be based on natural language processing involving contextual grammar and vector modeling . This would involve knowledge models and information retrieval techniques to define the relationship between various zones . Once relationships between various zones are established, this can be
8103	8104	vehicles or other robots. The navigation algorithm should be able to evaluate and take advantage of each of these types of objects if they prove beneficial to the robot’s task. In our previous work , we demonstrated a method for stealthy multi-robot navigation in the presence of an observer using static objects in outdoor environments. In contrast, the goal of the research presented here is to
8103	8104	for low-visibility path planning without the use of a priori maps,  presents a reactive method for a robot to use stationary objects in the environment for cover while navigating to a goal.  extends this concept for multiple robots conducting sequential traverses. Path quality is improved on successive traverses from the integration of environment and path information from the
8103	8104	Our work focuses on these criteria and allows the environment to contain dynamic objects beneficial for providing cover to the robot during its traverse. III. THE APPROACH In our previous work , task and environment-related information is combined to embody the stealth-efficiency cost function. This information consists of: - objects in the environment modeled on an occupancy grid
8103	8104	in this manner, the robot is able to integrate new information about the environment as it is sensed and use it for reactive decisions about the next waypoint to traverse to. Previous results  demonstrate that this approach produces intuitive low-visibility paths in unknown static environments. The extension discussed in this paper allows the robot to reactively take advantage of
8103	8106	compute the visibility constraints of each cell using information about the mobile observers in the environment. Combining this information allows a reactive path to be determined for the robot.  analyzes digitized terrain features for visual servoing to a goal in the presence of an observer.  models observers and potential navigation waypoints using virtual springs and masses. The
8103	8113	In all experiments, the observer has infinite range, omni-directional sensing. IV. SIMULATION EXPERIMENTS The simulation experiments were conducted using Player devices and the Stage simulator . The environment measures 35m by 35m and is configured as shown in Figure 3. It consists of three static and one mobile barricade with the linear reversible path shown in the figure. The mobile
8114	8116	Algorithms for positioning a mobile sensor networksSensor Networks and Autonomous Flying Robots 3 includes even dispersal of sensors from a source point and redeployment for network rebuilding . Other important contributions include . In  we describe a decentralized and localized algorithm called robot-assisted localization for localizing a sensor network with a robot
8114	8119	localization for localizing a sensor network with a robot helicopter. In  we describe an algorithm called network-assisted navigation in which a sensor network guides a robot helicopter. In  we describe an algorithm and preliminary experiments for deploying a sensor network with a robot helicopter. Here we extend this work to include deployment and connectivity repair and discuss our
8114	8119	sensor is fitted to the helicopter to allow communications with the deployed sensor network and is connected to the helicopter’s Linux-based computer. For further details the reader is referred to . Several applications were run onboard the helicopter, depending on the experiment. The ping application sends a broadcast message with a unique id once per second and logs all replies along with
8114	483	Peterson, Rus, Saripalli and Sukhatme Fig. 1. AVATAR Autonomous Helicopter with a sensor interface for deploying sensors 2 Related Work Our work builds on important previous work in sensor networks  and unmanned aerial vehicles . It bridges the two communities by integrating autonomous control of flying vehicles with multi-hop message routing in ad-hoc networks. Autonomous aerial
8114	483	of helicopters  and pursuitevasion games . Research in sensor networks has been very active in the recent past. An excellent general introduction on sensor networks can be found in . An overview of hardware and software requirements for sensor networks can be found in  which describes the Berkeley Mica Motes. Algorithms for positioning a mobile sensor networksSensor
8114	8122	the various types of vehicles and the algorithms used for control of these vehicles can be found in  . Recent work has included autonomous landing , aggressive maneuvering of helicopters  and pursuitevasion games . Research in sensor networks has been very active in the recent past. An excellent general introduction on sensor networks can be found in . An overview of hardware
8114	3551	networksSensor Networks and Autonomous Flying Robots 3 includes even dispersal of sensors from a source point and redeployment for network rebuilding . Other important contributions include . In  we describe a decentralized and localized algorithm called robot-assisted localization for localizing a sensor network with a robot helicopter. In  we describe an algorithm called
8114	8123	Peterson, Rus, Saripalli and Sukhatme Fig. 1. AVATAR Autonomous Helicopter with a sensor interface for deploying sensors 2 Related Work Our work builds on important previous work in sensor networks  and unmanned aerial vehicles . It bridges the two communities by integrating autonomous control of flying vehicles with multi-hop message routing in ad-hoc networks. Autonomous aerial
8114	5746	been very active in the recent past. An excellent general introduction on sensor networks can be found in . An overview of hardware and software requirements for sensor networks can be found in  which describes the Berkeley Mica Motes. Algorithms for positioning a mobile sensor networksSensor Networks and Autonomous Flying Robots 3 includes even dispersal of sensors from a source point and
8114	5746	fitted with a PC-104 stack augmented with sensors (Figure 1). Autonomous flight is achieved using a behavior-based control architecture . Our sensor network platform is the Berkeley Mica Mote . The operating system support for the Motes is provided by TinyOS, an eventbased operating system. Our testbed consists of 50 Mote sensors deployed in the form of a regular 11 × 5 grid, see Figure
8114	8124	Algorithms for positioning a mobile sensor networksSensor Networks and Autonomous Flying Robots 3 includes even dispersal of sensors from a source point and redeployment for network rebuilding . Other important contributions include . In  we describe a decentralized and localized algorithm called robot-assisted localization for localizing a sensor network with a robot
8114	8125	Peterson, Rus, Saripalli and Sukhatme Fig. 1. AVATAR Autonomous Helicopter with a sensor interface for deploying sensors 2 Related Work Our work builds on important previous work in sensor networks  and unmanned aerial vehicles . It bridges the two communities by integrating autonomous control of flying vehicles with multi-hop message routing in ad-hoc networks. Autonomous aerial
8114	8127	1. AVATAR Autonomous Helicopter with a sensor interface for deploying sensors 2 Related Work Our work builds on important previous work in sensor networks  and unmanned aerial vehicles . It bridges the two communities by integrating autonomous control of flying vehicles with multi-hop message routing in ad-hoc networks. Autonomous aerial vehicles have been an active area of
8114	8127	problems with helicopters. A good overview of the various types of vehicles and the algorithms used for control of these vehicles can be found in  . Recent work has included autonomous landing , aggressive maneuvering of helicopters  and pursuitevasion games . Research in sensor networks has been very active in the recent past. An excellent general introduction on sensor networks
8114	8127	is a gaspowered radio-controlled model helicopter fitted with a PC-104 stack augmented with sensors (Figure 1). Autonomous flight is achieved using a behavior-based control architecture . Our sensor network platform is the Berkeley Mica Mote . The operating system support for the Motes is provided by TinyOS, an eventbased operating system. Our testbed consists of 50 Mote
8114	8129	networksSensor Networks and Autonomous Flying Robots 3 includes even dispersal of sensors from a source point and redeployment for network rebuilding . Other important contributions include . In  we describe a decentralized and localized algorithm called robot-assisted localization for localizing a sensor network with a robot helicopter. In  we describe an algorithm called
8114	8131	and the algorithms used for control of these vehicles can be found in  . Recent work has included autonomous landing , aggressive maneuvering of helicopters  and pursuitevasion games . Research in sensor networks has been very active in the recent past. An excellent general introduction on sensor networks can be found in . An overview of hardware and software requirements for
4632584	8133	(Voorhees, 2001) uses more than 3Gb of source text. Competing systems often exploit the data redundancy existing in the source text. Some of them even use the Web to increase the data redundancy (Brill et al., 2001; Clarke et al., 2001, for example). These systems typically trade accuracy for speed and avoid the use of intensive natural language processing techniques. Most of the current QA systems are based
4632584	8136	indicates people or organisations). The module may also produce an image of the question. This image may be similar in format to the document images and can range from a simple bag of words (Cooper and Rüger, 2000, for example) to a fairly complex logical form (Harabagiu et al., 2001; Elworthy, 2000, for example). Once the question is analysed, a document preselection module identifies the documents that are
4632584	8139	to add the additional modules that further enhance the expressivity of the sentence image. For the present evaluation we used the Remedia Publications Reading Comprehension corpus used by DeepRead (Hirschman et al., 1999). The corpus is aimed at testing the degree of reading comprehension by children, and the documentssAnswer candidate Flat Logical Form John saw Mary object(’john’,o1,), object(’mary’,o3,),
4632584	8143	than the representation provided by dependency arcs. Mollá and Hutchinson (2003) used the grammatical relations to compare the accuracy of two broad-coverage dependency-based parsers, Link Grammar (Sleator and Temperley, 1993) and Conexor Functional Dependency Grammar (Tapanainen and Järvinen, 1997) — henceforth referred to as Conexor FDG. The evaluation used a subset of the original relations: SUBJ, OBJ, XCOMP, and
8919686	8147	any of the following: arbitrary, arbitrary-npv, arbitrary-upv, matching, paths, regions, regions-npv, regions-upv, scheduling, L1, L2, L3, L4, L5, L6, L7, and L8. The distributions are defined in . The optional suffixes -npv and -upv available for the regions and arbitrary distributions specify that valuations are to be drawn from either a normal or uniform distribution, respectively. If a
8919686	8147	other distributions before a valuation type is chosen. 1s2.2 Optional Parameters CATS can be given several optional parameters that are listed and described here, with their default values: • =n : number of instance files that you want CATS to generate • -seed : random seed • -seed2 : random seed 2: used only for normal distributions • -bid alpha :
8919686	8147	a usage screen. If a distribution is selected with -d, help specific to that distribution is also printed. 3 Advanced Features CATS 2.0 has several advanced features over CATS 1.0 as described in , related to the work of Kevin Leyton-Brown, Eugene Nudelman, Galen Andrew, Jim McFadden and Yoav Shoham during 20022003  . They enable the creation of new distributions by skewing the
8919686	8148	CATS 2.0 has several advanced features over CATS 1.0 as described in , related to the work of Kevin Leyton-Brown, Eugene Nudelman, Galen Andrew, Jim McFadden and Yoav Shoham during 20022003  . They enable the creation of new distributions by skewing the built-in distributions to emphasize desired regions of the problem space. The features are first described, and an explanation of
8919686	8148	is their minimum that is used as the weight for sampling. The original purpose of this feature was to weight instances according to their estimated hardness when solved using an algorithm portfolio , which is the minimum estimated run-time of the portfolio’s constituent algorithms. 3.4 Interaction of Parameter Distributions and Feature Weighting Both parameter distributions and feature
8919686	8148	implementing these advanced features was for use with models of algorithm runtime. Thus, provided with CATS are default feature and parameter models of run-time for the three algorithms studied in . The -default hard flag causes CATS to use these models, weighting the problems generated according to the minimum predicted runtime of three different algorithms, in order to produce harder
8919686	8149	CATS 2.0 has several advanced features over CATS 1.0 as described in , related to the work of Kevin Leyton-Brown, Eugene Nudelman, Galen Andrew, Jim McFadden and Yoav Shoham during 20022003  . They enable the creation of new distributions by skewing the built-in distributions to emphasize desired regions of the problem space. The features are first described, and an explanation of how
8919686	8149	distribution. Only instead of speeding up sampling, more samples would be necessary than if no parameter model were used at all. For a more formal description of the sampling method used, see . 3.5 Default Hard Distributions One of the original motivations for implementing these advanced features was for use with models of algorithm runtime. Thus, provided with CATS are default feature
8150	8151	paper can castigate this kind of active attacks: the global reputation value is calculated giving more relevance to the enforcement of critical functions such as packet forwarding. Furthermore, in  it has been showed that the impact of a erroneous execution of the packet forwarding function has more relevance on network performances compared to the erroneous execution of the routing g h c b:
8150	935	at the ad hoc networking environment have been proposed. However, very few researchers focus on the selfishness problem in MANET and existing work in this area is still in its infancy. In , the authors consider the case in which some misbehaving nodes agree to forward packets but fail to do so. In order to solve this problem, they propose two mechanisms: a watchdog, in charge of
8150	8155	not allow a node to distribute negative ratings about other nodes, so unlike the pathrater technique, our scheme can resist to simple denial of service attacks exploiting this vulnerability. In , the authors present two important issues targeted specifically at the ad hoc networking environment: first, end-users must be given some incentive to cooperate to the network operation (especially
8150	8159	nonnegative for all k. The incentive structure given by (1) and (2) is modelled by the reputation technique used in the cooperative security scheme presented in this paper. The reputation metric  represents the payoff that a node of the network receives or loses while operating the network: if the node cooperates its reputation increases, if the node misbehaves itssreputation decreases
8150	8160	The requestor validates the result of the execution of f and, based on the outcome of the validation phase, it updates the ratings relative to the monitored providers using the reputation technique . 4.2.2 The provider As a provider receives a request for the execution of a function f, based on the reputation rating associated to the requestor it accepts or denies to serve the request. If the
8150	8160	of the Route Request is a Route Reply message which is sent back to node a and which contains the route to the destination. The Route Reply message corresponds to the ACK message we described in  and contains the list of the ? ? ? ? ? ? ? nodes that correctly participated to the DSR protocol. Direct reputation b c i b: d: e: a S g d h e l m D n h: m: f Indirect reputation Figure 4. MANET
8150	8160	simplicity the picture doesn’t represent every local validation mechanism for all the nodes of the network. On the other hand, the heavy lines represent the second validation mechanism described in : the ACK message (which corresponds in this case to the result of the execution of the function f) is used to update indirect reputation ratings and it’s validated by the corresponding mechanism.
8150	8160	DSR routing protocol and obtained the following route: <l, g, h, e>. Node h does not execute the packet forwarding function. The dotted line represent the first validation mechanism described in : node g detects that node h is misbehaving with respect to function f and decreases the corresponding reputation rating in its local reputation basis. If node g misbehavior continues its reputation
8150	8160	to the routing function. However, in the second phase of the attack, node g does not perform correctly the packet forwarding function: its global reputation rating should heavily degrade. In  we describe how the mechanism outlined in this paper can castigate this kind of active attacks: the global reputation value is calculated giving more relevance to the enforcement of critical
8150	8168	can be modelled as thespayoff structure of the m-dimensional PD game. 6.1 The preference structure The analysis presented in this paper relies on a preference structure given by the ERC theory . This theory explains most of the behaviour of players observed in diverse experiments 1 , but deviates from the traditional utility concept. The utility of a player is not solely based on the
8150	8168	noted by Bolton and Ockenfels, this theory can generate cooperation in the standard prisoner’s dilemma. 2 Note that such a preference for equity is self-centered only and is distinct from altruism . A player’s utility is determined solely by its own absolute and relative payoff. 3 By the Folk theorems, basically any payoff vector can be sustained as a Nash equilibrium under certain
8150	8170	but the most interesting result has been depicted in Figure 3. The last family of simulations showed an interesting characteristic of the global network throughput. It has already been showed  that the global network throughput decreases when the node mobility increases: the reason is that link outage becomes more frequent causing a higher packet loss probability. On the other side, when
8150	941	of the network during which key-pairs are generated and public key certificates are issued by a common, centralized certification authority. This is the case of managed environment, as defined in . If tamper-proof hardware and strong authentication infrastructure are not available, the reliability of basic functions like routing can be endangered by any node of an ad hoc network. No
2635666	8174	of the Web, similar to our approach, consisting of the sites, Web pages, and intra-page structuring, is presented in ; an extension of W3QS/W3QL  to this model is planned. Also, WebOQL  makes parse-trees first-class citizens of the model which can be queried directly. W3QL and WebOQL use tags in SQL-style SELECT clauses – unlike FLORID, which uses rules and path expressions for
2635666	8175	Different languages are used for wrapper and mediator specification, and for querying.  describes a grammarbased tool for coding wrappers producing OEM output. In the ARIADNE project, e.g., , the Web is also not modeled explicitly. Instead, for every relevant Web page, a grammar for wrapping it directly into the target model is derived semi-automatically. Their approach does not
2635666	8176	wrapping it directly into the target model is derived semi-automatically. Their approach does not support HTML tables or any textual formatting. As a non-graph based approach, the ARANEUS project  uses a hypertextbased model. Different languages are used for extracting data and defining views on it. A hierarchical graph modelssenko altman 76 is1 is5 vldb is conf jrn dblp dblp.url Figure 5.
2635666	1188	wrappers. The above approaches do not deal with Web exploration. For a complete overview of existing systems, see . Information brokering using several sources is investigated in InfoSleuth , Information Manifold , and Observer . In all these approaches, it is assumed that site descriptions and appropriate wrappers are given for each accessible data source – i.e., they cannot
2635666	8177	F-Logic, it is not fully object-oriented; the only objects are “rel-infons” which simulate in some sense the nodes of the parse-tree. There is no reported implementation of WebLog. The idea of UnQL  can also be seen as an integrated approach, using a graph-based model for semistructured data and defining a language for navigating and querying this model. Most recent prototypes for information
2635666	8178	can be queried directly. W3QL and WebOQL use tags in SQL-style SELECT clauses – unlike FLORID, which uses rules and path expressions for parsetree navigation. The YATL language of the YAT system  is a rule-based querying and transformation language for XML/SGML wich is also based on a tree representation. Here, generic trees can be instantiated to be used in mediator programs. The Web
2635666	8179	Web pages; Web exploration and information integration is left to the application. Another tool for interactive generation of wrappers using the DOM model is presented in NoDoSE .  and  present methods for automatical wrapper generation. Most of the above models focus either on the Web representation of information (UnQL, STRUDEL), neglecting the application semantics of the
2635666	8180	prototypes for information extraction follow the layered approach, using separate programs for wrapping (often, an individual wrapper is designed for every source) and mediating. In the STRUDEL  system, the Web is also mapped to a graph representation. Here, all relevant pages must be accessed at the same time before the querying (and reasoning) phase is started. In the TSIMMIS project
2635666	8181	then leaving the mapping from the Web representation to the data model to external wrappers. The above approaches do not deal with Web exploration. For a complete overview of existing systems, see . Information brokering using several sources is investigated in InfoSleuth , Information Manifold , and Observer . In all these approaches, it is assumed that site descriptions and
2635666	8182	on previously accessed pages as shown in Section 6. This can especially be exploited in a rule-based framework as presented in this work. In the following sections, we describe the FLORID system  1 which follows this architecture. Based on the object-oriented deductive language F-Logic, it demonstrates the ideas and advantages of the architecture. Although, other implementations (E.g., in
2635666	8183	F-Logic representation of the DBLP server . For readability, we use mnemonic oid's of the form oname. In addition to the basic F-Logic syntax, the FLORID system also supports path expressions  in place of id-terms for navigating in the object-oriented model. Especially, singlevalued references can create anonymous objects when used in the head of rules. A rule of the form
2635666	1189	system, the Web is also mapped to a graph representation. Here, all relevant pages must be accessed at the same time before the querying (and reasoning) phase is started. In the TSIMMIS project , the graphbased OEM (Object Exchange Model) is used as a common data model for the extracted, application-level information. The Web structure and the page markup is not modeled. Instead, every Web
2635666	8185	The Web structure and the page markup is not modeled. Instead, every Web source is mapped to OEM by a wrapper. Different languages are used for wrapper and mediator specification, and for querying.  describes a grammarbased tool for coding wrappers producing OEM output. In the ARIADNE project, e.g., , the Web is also not modeled explicitly. Instead, for every relevant Web page, a grammar
2635666	8187	The Web structure is not modeled, also the relevant part of the Web cannot be extended at run-time. An early approach for using F-Logic for data integration has been reported in . Jedi  is a tool for manually specifying Web access and wrappers for HTML pages by a framework combining grammars and rules. W4F  is a toolkit for interactively generating wrappers for HTML pages
2635666	8188	for data-driven Web exploration based on previously accessed pages. The architecture is implemented in FLORID, an implementation of the deductive object-oriented database language F-Logic  extended with Web access functionality. F-Logic serves as data model, and as wrapper, mediator, and query language. Generic rule patterns are used for structured document analysis
2635666	8188	combining the rich modeling capabilities (objects, methods, class hierarchy, inheritance, signatures) of the objectoriented data model with the advantages of deductive database languages, F-Logic  provides a suitable framework for modeling and handling Web information. At a short glance, the syntax is as follows (we omit inheritable methods and signature specifications; for the full syntax
2635666	8188	hold whenever <body(o,m)> is satisfied. Object creation is used for constructing an application-level model. Negation-free F-Logic programs have a standard logic programming semantics . For programs with negation, FLORID allows inflationary and user-defined stratified semantics. Since path expressions and F-Logic atoms may be arbitrarily nested, a concise and extremely flexible
2635666	8189	the Web by iteratively following hyperlinks of the Web, similar to our approach, consisting of the sites, Web pages, and intra-page structuring, is presented in ; an extension of W3QS/W3QL  to this model is planned. Also, WebOQL  makes parse-trees first-class citizens of the model which can be queried directly. W3QL and WebOQL use tags in SQL-style SELECT clauses – unlike FLORID,
2635666	8190	dblp dblp.url Figure 5. Exploring the Web by iteratively following hyperlinks of the Web, similar to our approach, consisting of the sites, Web pages, and intra-page structuring, is presented in ; an extension of W3QS/W3QL  to this model is planned. Also, WebOQL  makes parse-trees first-class citizens of the model which can be queried directly. W3QL and WebOQL use tags in SQL-style
2635666	21199	single Web pages; Web exploration and information integration is left to the application. Another tool for interactive generation of wrappers using the DOM model is presented in NoDoSE .  and  present methods for automatical wrapper generation. Most of the above models focus either on the Web representation of information (UnQL, STRUDEL), neglecting the application semantics of
2635666	8193	do not deal with Web exploration. For a complete overview of existing systems, see . Information brokering using several sources is investigated in InfoSleuth , Information Manifold , and Observer . In all these approaches, it is assumed that site descriptions and appropriate wrappers are given for each accessible data source – i.e., they cannot be used for exploring the
2635666	8194	used for structured document analysis basedonanSGMLparser,andfor processing raw data by pattern matching. Some issues of handling semistructured data and Web access have already been described in . In  and  we dealt with the integrated data model and generic wrapping techniques, respectively. In the present paper, we focus on the architecture and its merits and consequences wrt.
2635666	8194	interface maps the relevant Web documents into the unified world model without interpreting them. This mapping is based on the generic idea of modeling the Web by two classes url and webdoc (cf. ). Thus, Web access via the interface is performed by a single generic method which is based on Internet access using the http/ftp protocols: Every member u of class url provides a special method
2635666	8196	document analysis basedonanSGMLparser,andfor processing raw data by pattern matching. Some issues of handling semistructured data and Web access have already been described in . In  and  we dealt with the integrated data model and generic wrapping techniques, respectively. In the present paper, we focus on the architecture and its merits and consequences wrt. other approaches. The
2635666	8196	skeleton, an application-level model is derived and superposed on the skeleton. Wrapper Functionality. In our approach, the construction of wrappers and mediators is based on generic rule schemata  for standard tasks which can be complemented by application-specific rules and refinements for handling exceptional cases to obtain the required flexibility. For the wrapping task, depending on the
2635666	8197	with Web exploration. For a complete overview of existing systems, see . Information brokering using several sources is investigated in InfoSleuth , Information Manifold , and Observer . In all these approaches, it is assumed that site descriptions and appropriate wrappers are given for each accessible data source – i.e., they cannot be used for exploring the Web itself. Also, the
2635666	8198	for using F-Logic for data integration has been reported in . Jedi  is a tool for manually specifying Web access and wrappers for HTML pages by a framework combining grammars and rules. W4F  is a toolkit for interactively generating wrappers for HTML pages using HEL, a DOMbased language operating on the parse-tree of a document. Comma-lists and name-value pairs are regarded as atomic –
2635666	8199	The Web is now the most popular information repository and there is a strong need for integration of data from different sources. A standard approach for this is the mediator architecture  which comprises wrappers for translat1 ing data from different local languages into a common format, and mediators for providing an integrated view on the data. Existing approaches have a strictly
2635666	8199	interpretation. 2. The Architecture Figure 1 shows our architecture of a system for processing information from the Web. In contrast to the common layered wrapper-mediator-architecture (cf. ), our architecture is based on a single interface to the Web as a whole instead of multiple interfaces to individual Web sources via individual, independent wrappers. The http/ftpprotocol-based Web
8200	8201	for bundle adjustment). However, nonlinearly estimating the fundamental matrix suffers from the lack of a simple technique to represent it efficiently. This paper, which is an extension of , provides such a technique in Section 3, based on the orthonormal representation of the fundamental matrix that we introduce. We show in Section 4 how this method can be used to refine the
8200	8202	camera matrix, while keeping P ðI0Þ. The 12 75 extra parameters are the homo. geneous scale of the second camera matrix, the global scene scale, and the position of the plane at infinity. MAPS ,  is a minimal parameterization based on multiple maps. . ORTHO uses the orthonormal representation proposed in this paper. Measured quantities. We measure two quantities characterisic of a
8200	8203	matrix F is decomposed into the epipoles e and e0 and the epipolar transformation, which is a 1D projective transformation relating the epipolar pencils, represented by a homogeneousð2 2Þ matrix g , , . . The authors are with INRIA Rhone-Alpes, 655 avenue de l’Europe, 38334 Saint Ismier cedex, France. E-mail: {Adrien.Bartoli, Peter.Sturm}@inria.fr. Manuscript received 30 Apr. 2002;
8200	8203	implementation of the optimization process. Note that there are nine different possibilities to form the fundamental matrix—or any other 2D entity such as the extended epipolar transformation  or the canonic plane homography H ? —from e, e 0 , and g . In , , the method has been revised so as to reduce the number of parameterizations using image transformations. In , the
8200	8204	bundle adjustment, minimal parameterization, fundamental matrix. 1 INTRODUCTION æ THE fundamental matrix has received a great interest in the computer vision community, see, e.g., , , , , , , . It encapsulates the epipolar geometry or the projective motion between two uncalibrated perspective cameras and can be used for 3D reconstruction, motion
8200	8204	the fundamental matrix is therefore a major research issue. Most of the time, point correspondences between the two images are used. A linear solution is obtained using the 8-point algorithm ,  optionally embedded in a robust estimation scheme , . This estimate is then nonlinearly refined by minimizing a physically meaningful criterion that may involve reconstructed 3D point
8200	8204	have their own structure parameterization: They optimize the four elements of each point. Initialization. We compute an initial solution for the motion using the normalized 8-point algorithm . Image point coordinates are standardized such that they lie in 1...1Š. Each point is reconstructed by minimizing its reprojection error. Nonlinear optimization. We use the Levenberg-Marquardt
8200	8205	bundle adjustment, minimal parameterization, fundamental matrix. 1 INTRODUCTION æ THE fundamental matrix has received a great interest in the computer vision community, see, e.g., , , , , , , . It encapsulates the epipolar geometry or the projective motion between two uncalibrated perspective cameras and can be used for 3D reconstruction, motion
8200	8205	fundamental matrix, i.e., on the coordinate frame employed. We have chosen the canonic projection matrices (4). This Jacobian matrix is employed directly for the overparameterization proposed in . Deriving its analytical form is straightforward. We therefore concentrate on deriving a closed-form expression for A ? . One of the advantages of the update rule (3) is that there exists a simple
8200	8205	minimization and by including first-order gauge constraints into the minimization. The reconstruction basis, as well as the homogeneous scale of the camera matrices are constrained. . PARFREE  partially fixes the gauge by optimizing only the entries of the second camera matrix, while keeping P ðI0Þ. The 12 75 extra parameters are the homo. geneous scale of the second camera matrix, the
8200	8207	a cost function described in Section 4.1 over structure and motion parameters. In projective space, there are 15 inherent degrees of gauge freedom, due to the coordinate-frame ambiguity. In , a general framework consisting in incorporating gauge constraints up to first order in numerical estimation is introduced. The method of  falls in that category. Another technique is to let
8200	8210	bundle adjustment, minimal parameterization, fundamental matrix. 1 INTRODUCTION æ THE fundamental matrix has received a great interest in the computer vision community, see, e.g., , , , , , , . It encapsulates the epipolar geometry or the projective motion between two uncalibrated perspective cameras and can be used for 3D reconstruction, motion segmentation,
8200	8210	F is decomposed into the epipoles e and e0 and the epipolar transformation, which is a 1D projective transformation relating the epipolar pencils, represented by a homogeneousð2 2Þ matrix g , , . . The authors are with INRIA Rhone-Alpes, 655 avenue de l’Europe, 38334 Saint Ismier cedex, France. E-mail: {Adrien.Bartoli, Peter.Sturm}@inria.fr. Manuscript received 30 Apr. 2002; revised
8200	8210	of each homogeneous entity (in practice, the largest entry), which yields three possibilities for each epipole and four for the epipolar transformation, so 3 3 436 possible parameterizations. In , the authors propose to restrict the two-view configurations considered to the cases where both epipoles are finite and can therefore be expressed in affine coordinates. Consequently, this
8200	8210	of the reprojection error. Similar results can be derived for other criteria, such as the minimization of the distances between points and epipolar lines or the gradient-weighted criterion , . However, in order to obtain the maximum likelihood estimate of the fundamental matrix, one has also to estimate corrected point positions bq i$bq 0 i , i.e., which satisfy exactly the
8200	8210	eliminated. We call any pair of camera matrices P and P 0 a realization. In Section 4.2, we give analytical formulae to compute a realization from the orthonormal representation of F (as opposed to , , , ). The algorithm is summarized in Table 1. 4.1 Cost Function Bundle adjustment consists in solving the following optimization P problem, see e.g., , , : mina;b j r2 j ,
8200	8211	a Realization Due to the projective frame ambiguity, there exists a 15-parameter family of realizations for a given fundamental matrix. A common choice is the canonic projection matrices given by : P ðI ð3 3Þ 0 ð3 1ÞÞ and P 0 ðH ? e 0 Þ; ð4Þ where e0 is the second epipole, given by the left null-vector of F, F > e0 0ð3 1Þ, and H ? e0Š F is the canonic plane homography . The arbitrary
8200	8215	to first order approximation, has been proposed in  for the gradientweighted criterion, which is not the one used for bundle adjustment. Another solution is the point-based parameterization of . The idea is to represent the fundamental matrix by a set of 7-point correspondences. Minimal optimization can then be conducted by varying one coordinate for each point correspondence. The
8200	8215	We call any pair of camera matrices P and P 0 a realization. In Section 4.2, we give analytical formulae to compute a realization from the orthonormal representation of F (as opposed to , , , ). The algorithm is summarized in Table 1. 4.1 Cost Function Bundle adjustment consists in solving the following optimization P problem, see e.g., , , : mina;b j r2 j , where
8200	8216	adjustment, minimal parameterization, fundamental matrix. 1 INTRODUCTION æ THE fundamental matrix has received a great interest in the computer vision community, see, e.g., , , , , , , . It encapsulates the epipolar geometry or the projective motion between two uncalibrated perspective cameras and can be used for 3D reconstruction, motion segmentation,
8200	8216	issue. Most of the time, point correspondences between the two images are used. A linear solution is obtained using the 8-point algorithm ,  optionally embedded in a robust estimation scheme , . This estimate is then nonlinearly refined by minimizing a physically meaningful criterion that may involve reconstructed 3D point coordinates as well (in particular for bundle adjustment).
8200	7864	consider its singular value decomposition F U V > , where U and V are Oð3Þ matrices 2 and a diagonal one containing the singular values of F. Since F has rank 2, diagð 1; 2; 0Þ, where 1 2 > 0 . We can scale such that F U diagð1; ;0Þ V > , where  2= 1 ( 16 0 since F6 0) and 1 >0. This decomposition shows that any fundamental matrix can be represented byðU; V; Þ, i.e., two Oð3Þ matrices
8200	8217	minimal parameterization, fundamental matrix. 1 INTRODUCTION æ THE fundamental matrix has received a great interest in the computer vision community, see, e.g., , , , , , , . It encapsulates the epipolar geometry or the projective motion between two uncalibrated perspective cameras and can be used for 3D reconstruction, motion segmentation, self-calibration, etc.
8200	8217	Most of the time, point correspondences between the two images are used. A linear solution is obtained using the 8-point algorithm ,  optionally embedded in a robust estimation scheme , . This estimate is then nonlinearly refined by minimizing a physically meaningful criterion that may involve reconstructed 3D point coordinates as well (in particular for bundle adjustment).
8200	8217	decomposed into the epipoles e and e0 and the epipolar transformation, which is a 1D projective transformation relating the epipolar pencils, represented by a homogeneousð2 2Þ matrix g , , . . The authors are with INRIA Rhone-Alpes, 655 avenue de l’Europe, 38334 Saint Ismier cedex, France. E-mail: {Adrien.Bartoli, Peter.Sturm}@inria.fr. Manuscript received 30 Apr. 2002; revised 26
8200	8217	distinct parameterizations are still necessary for g. A total of four parameterizations are then needed to represent this restricted set of fundamental matrices. The method has been extended in  to the general case, i.e., when the epipoles can be either finite or infinite. In this case, it is shown that all 36 distinct parameterizations are necessary. This leads to a cumbersome and
8200	8217	are nine different possibilities to form the fundamental matrix—or any other 2D entity such as the extended epipolar transformation  or the canonic plane homography H ? —from e, e 0 , and g . In , , the method has been revised so as to reduce the number of parameterizations using image transformations. In , the image transformations used are metric and the number of distinct
8200	8218	main drawback is that in the transformed image space, the original noise model on the image features is not preserved. A means to preserve it, up to first order approximation, has been proposed in  for the gradientweighted criterion, which is not the one used for bundle adjustment. Another solution is the point-based parameterization of . The idea is to represent the fundamental matrix by
8200	8218	any pair of camera matrices P and P 0 a realization. In Section 4.2, we give analytical formulae to compute a realization from the orthonormal representation of F (as opposed to , , , ). The algorithm is summarized in Table 1. 4.1 Cost Function Bundle adjustment consists in solving the following optimization P problem, see e.g., , , : mina;b j r2 j , where a and b are
8919702	1023	are concerned with the development of methods and tools for the construction of concept ontologies and the definition of thematic views to improve semantic interoperability and knowledge sharing . Several efforts are also devoted to the development of techniques and approaches for the integration of heterogeneous datasources to provide global views on data provided ? This work has been
8919702	8247	for information exchange among different datasources. We assume that, for each datasource, information to be exchanged is described by means of one of defined XML schema languages (e.g. DTD, DSD , XML Schema and RDF Schema). We propose an ontology architecture where information about XML schemas and their contents (e.g., meaning of elements, sub-elements, attributes) is organized in three
8919702	8248	datasource schemas. Once each available schema has been translated into a set of XClasses, these latter are compared to find semantic mappings between them (according to a schema matching process ). Our approach exploits the knowledge provided by XClasses and a thesaurus of weighted terminological relationships (e.g., synonymy, hyperonymy) to semi-automatically identify semantic mappings
8919707	8258	design and implement their own application from scratch requiring them to write code which directly interacting with devices, or use a toolkit that hides a lot of the device details from them . However, even with low-level toolkit support for acquiring context, experienced developers are still required to write a large amount of code to develop relatively simple applications. In order
8919707	8258	them, end users should be given this control. In our previous research, we have looked at making it easier for programmers to build context-aware applications through the use of the Context Toolkit , which removed the need to deal with the underlying details of sensors similar to the way that graphical user interface toolkits removed the need to deal with low-level details for building
8919707	8258	rule set can be tested using the iCAP engine in run mode. The engine can either be set to simulate the context-aware environment, or be used in conjunction with a real context-aware environment . Users can interact with the engine to change the value of defined inputs, and evaluate the behavior of the rules being tested. With the engine, users are able to quickly design and test their
8919707	8258	are hard coded should be subject to manipulation also. 1 The project is funded to 50% by the industrial partners and to 50% by KKS (The Knowledge Foundation). For more detailed information see . 2 To protect the business interest of our industrial partner, we do not tell about the character of contracts.sThe existing software has turned out to be too cumbersome to change. Beside specific
8919707	8258	are able to make sense of it. 4 References  Dittrich, Y., Eriksén, S. and Hansson, C. PD in the Wild; Evolving Practices of Design in Use. Accepted for the Participatory Design Conference 2002.  Dittrich, Y. and Lindeberg, O. Designing for Changing Work and Business Practices. In Patel, N. (ed.) Evolutionary and Adaptive Information Systems. IDEA group publishing (forthcoming).
8919707	8260	to Harrison and Dourish, the ability to change an information space is also an important precondition for the appropriation virtual spaces, turning them into an actual place for cooperation . In contrast to these approaches, the approach taken within the Cooperation Infrastructure aims at flexibility of the information model or domain model. OBJECT METAPHOR Early implementations of the
8919707	8260	in other application domains. There are also theoretical foundations to be relied upon. Semiotic Engineering studies the creation and sharing of meanings and signs within the scope of HCI and EUP . Distributed cognition  encompasses interactions between people and with resources and materials in the environment. More than that, it looks at how representations in the material world provide
8919707	8260	and issues that we will put forward and discuss here. VISIBLE APPLICATION DATA EXAMPLES Our previous experience in this context includes research on an interface development environment, HandsOn , where the interface designer can manipulate explicit examples of application data at design-time to build custom dynamic displays that depend on application data at runtime.  shows how HandsOn
8919707	8260	but work has also been done on inducing wrappers from examples . Most web screen scrapers are written in a scripting language that dwells outside the web browser, like Perl, Python, or WebL . For an end-user, the distinction is significant. Cookies, authentication, session identifiers, plugins, user agents, client-side scripting, and proxies can all conspire to make the Web look
8919707	8260	of the ACM, 43(3):98–103, March 2000.  Gordon Edes. This hack tried but couldn’t connect. Boston Globe, July 1999.  Steve Kangas. Bookmarklets. http://www.bookmarklets.com/, 1998.  Thomas Kistler and Hannes Marais. WebL – a programming language for the Web. In Proceedings of the 7th International World Wide Web Conference (WWW7), 1998.  Bruce Krulwich. Automating the
8919707	8263	with a use perspective on the domain. The presentation of the building blocks and the possible connections between has to be presented in a comprehensible way as well. Mørch’s application units  and Stiemerling et al’s component based approach  are examples for such architecture concepts. In the billing gateway interface partly provides a very intuitive interface from the user’s point
8919707	8263	2002.  Mejstad, V., Tångby, K.-J. and Lundberg, L. Improving Multiprocessor Performance of a Large Telecommunication System by Replacing Interpretation with Compilation. Accepted for the ???  Mørch, Anders I. 2003:” Tailoring as Collaboration: The Mediating Role of Multiple Representations and Application Units”, in N. Patel: Adaptive Evolutionary Information Systems. Idea group Inc.
8919707	8263	Development, co-evolution, requirements engineering, information society INTRODUCTION The subject of End-User Development (EUD) is the focus of the ongoing European research project EUD-Net . The project’s definition of EUD is as follows : “End User Development is a set of activities or techniques that allow people, who are non-professional developers, at some point to create or
8919707	8263	immediate or prior classification would impede the flow of thoughts. According to Cox and Greenberg, who presented a tool to support a similar group process they call “collaborative interpretation” , I name the process of collecting information first and finding a structure for it later “collaborative interpretation”. In several other contexts, processes of alternating collecting and
8919707	8263	metaphor of colored cards pinned to a pinboard, called “Linked Sketches” (Figure 2.) The cards can be spatially structured just like the paper cards. The Linked Sketches tool is similar to PReSS  with the main difference of being integrated with the hypermedia information space of the Cooperation Infrastructure, and allowing for visual and hypertext links between the cards and to other
8919707	8268	Robert C. Miller and Brad A. Myers. Multiple selections in smart text editing. In Proceedings of the Sixth International Conference on Intelligent User Interfaces (IUI 2002), pages 103–110, 2002.  Ian Oeschger, Eric Murphy, Brian King, Pete Collins, and David Boswell. Creating Applications with Mozilla. O’Reilly, 2002.  Richard Potter. Triggers: Guiding automation with pixels to achieve
8919707	8268	to use. This evolution has to take place at the hand of the users, as they are the owners of the problems. The systems must therefore be designed for modifications, to suit evolution of use ; (; ). Tailorable groupware offers end-users the possibility to adapt system behaviour as well as the look and feel of the system in the context of collaboration, not as a separate design
8919707	8268	exemplify the use of a component by a small characteristic example application. Finally, automatically generated and visualized integrity checks can prevent from building pointless application (cf. ). Here integrity checks are used not only to prevent from failures but in the way of supporting users when developing their applications in the way that making failures and getting corrections on
3680859	8274	frameworks that support publication, discovery and composition of Semantic Web Services have been developed in the near past. These initiatives include OWL-S  (formerly DAML-S ), METEOR-S , and WSMO  etc. In general however we feel that no tool/framework provides all that is required for a generic Web services modeling platform for the
9299851	8319	behavior may be attributed to ill-conditioning of the regression problem, which may occur when the regressors are nearly parallel. Although this phenomenon is well-known in multivariate statistics (Stewart, 1987; Belsley, 1991), it does not seem to have been noticed and analyzed in the subspace identi cation literature. The study of numerical conditioning of the regression problem, besides the e ect of
8919723	8330	Much effort is being spent nowadays on the enhancement of formal languages for representing the socalled real-time properties, namely the definition of timing constraints on the execution of events . ELOTOS , which is an improved version of LOTOS that allows the representation of real-time properties, is about to become an international standard. Language enhancements such as E-LOTOS, tend
8919723	8331	has shown that since (Standard) LOTOS  is based on general concepts like events and processes, it can be successfully applied to the specification of a wide range of distributed systems . Limitations and shortcomings of LOTOS are mostly consequences of the interleaving semantics and the clumsy data type part . Much effort is being spent nowadays on the enhancement of formal
8919723	8333	Much effort is being spent nowadays on the enhancement of formal languages for representing the socalled real-time properties, namely the definition of timing constraints on the execution of events . ELOTOS , which is an improved version of LOTOS that allows the representation of real-time properties, is about to become an international standard. Language enhancements such as E-LOTOS, tend
8919723	8336	has shown that since (Standard) LOTOS  is based on general concepts like events and processes, it can be successfully applied to the specification of a wide range of distributed systems . Limitations and shortcomings of LOTOS are mostly consequences of the interleaving semantics and the clumsy data type part . Much effort is being spent nowadays on the enhancement of formal
8919723	8340	to the specification of a wide range of distributed systems . Limitations and shortcomings of LOTOS are mostly consequences of the interleaving semantics and the clumsy data type part . Much effort is being spent nowadays on the enhancement of formal languages for representing the socalled real-time properties, namely the definition of timing constraints on the execution of
13258220	8347	genome size, gene distribution, gene-location distance, cyanobacteria 1 Introduction In the phylum Cyanobacteria, complete genome sequences have been already reported in seven species and strains . Very interestingly, only one speices of these cyanobacteria, Anabaena sp. PCC7120  (hereafter Anabaena), has considerably large genome size of approximately 6.4 Mb, while the remaining species
13258220	8347	patterns of gene distribution on the genomes of three cyanobacteria, Anabaena, Synechocystis sp. PCC6803 (ca. 3.6Mb)  (hereafter Synechocystis) and Thermosynechococcus elongatus BP-1 (ca. 2.6Mb) . 2 Method and Results In order to investigate patterns of gene distribution on three cyanobacterial genomes, we adopt a metric of gene-location distance  to the gene-configuration comparison
13258220	8347	genome. 3 Discussion Anabaena is a filamentous, heterocyst-forming and nitrogen-fixing cyanobacterium, while Synechocystis and T. elongatus are unicellular and non-nitrogen-fixing cyanobacteria . It is of interest that when the Anabaena genome is divided into two regions of 260 ? -80 ? and 80 ? -260 ? , a larger number of house-keeping protein genes involved in cell maintenance such as
8350	8352	XML 1.0 specification published by the W3C  requires schema definitions to be one-unambiguous, i.e. that all regular expressions in the grammar’s rules are deterministic in the following sense : a regular expression is oneunambiguous iff the corresponding Glushkov automaton is deterministic. Note that the terminology is somewhat confusing in the literature: in the context of SGML
8350	8353	label a, the sequence a1 · · · an of labels of its children matches the regular expression d(a). The class of tree languages definable by DTDs is usually referred to as the local tree languages . A simple example of a DTD defining the inventory of a store is the following: store ? dvd dvd ? dvd ? title price For clarity, in examples we write a ? r rather than d(a) = r. We next recall the
8350	8353	a relabeling of the nodes, thus yielding a ?-tree. A ?-tree t then satisfies the SDTD if t can be written as µ(t ? ) where t ? satisfies the DTD ?. As SDTDs are equivalent to unranked tree automata , the class of tree languages definable by SDTDs is the class of regular tree languages. The XML equivalent of that class is captured by the schema language Relax NG . For ease of exposition, we
8350	8359	growing success of XML, combined with certain shortcomings of DTDs, generated a large number of alternative proposals for the description of schemas, such as RELAX , TREX , Relax NG , DSD , and XML Schema . Judging from the number of schemas one can find on the Web, XML Schema seems the most accepted one. The definition of XML Schema is nevertheless quite complicated and
8350	8361	label a, the sequence a1 · · · an of labels of its children matches the regular expression d(a). The class of tree languages definable by DTDs is usually referred to as the local tree languages . A simple example of a DTD defining the inventory of a store is the following: store ? dvd dvd ? dvd ? title price For clarity, in examples we write a ? r rather than d(a) = r. We next recall the
8350	8361	while dvd 2 defines DVDs on sale. The rule for store specifies that there should be at least two of the latter. The following restriction on SDTDs corresponds to the expressiveness of XML Schema : definition 3. A single-type SDTD is an SDTD (?, ? ? , (d, s), µ) with the property that no regular expression d(a) has occurrences of types of the form b i and b j with the same b but with
8350	8363	the inventory of a store is the following: store ? dvd dvd ? dvd ? title price For clarity, in examples we write a ? r rather than d(a) = r. We next recall the definition of a specialized DTD . definition 2. A specialized DTD (SDTD) is a 4-tuple (?, ? ? , ?,µ), where ? ? is an alphabet of types, ? is a DTD over ? ? and µ is a mapping from ? ? to ?. Note that µ can be applied to a ? ?
8350	8364	dvd 1 and dvd 2 , they can only occur in a different context, regulars and discounts respectively. 1.2 Related work In 2000, while the XML Schema specification was still under development, Sahuguet  investigated a sample of DTDs to determine the shortcomings of the Document Type Definition specification. What he found missing has been remedied in XML Schema. Moreover, XML Schema introduces
8919735	8369	level&quot; con ict resolution haves12 Chapter 1. Multidatabase Languages been addressed . The use of semantic values and arbitrary conversion functions is proposed in . \Schema level&quot; resolutions techniques address speci c incongruities. For instance, a query language in uenced by the Datalog paradigm is adopted in  to resolve schematic discrepancy con
8919735	8199	most of these proposals is the acknowledged need for some form of meta-information whose purpose is to describe how data integration is to be performed. The general term \mediators&quot; has been used  to encompass the wide variety of tools used for entity and object description that incorporate forms of meta-information. In this section, we consider four broad approaches to the resolution of
8374	8375	the implicit, distance-field based shape modeling and dynamic, force-based shape design, and overcome some of their limitations. For example, unlike implicit-based haptic editing system such as , no intermediate conversion step is necessary in our system. Besides closed shapes, our system can also work with mesh models with openings and boundaries. In contrast to the classical
8374	8380	field has been used to generate swept volumes , offset surfaces , and to morph between surface models . Recently, Adaptively Sampled Distance Fields (ADF) is proposed by Frisken et al. . ADFs consists of adaptively sampled distance values organized in a spatial hierarchy of data structures, and were later incorporated into a prototype sculpting system called “Kizamu”  developed
8374	8381	et al. . ADFs consists of adaptively sampled distance values organized in a spatial hierarchy of data structures, and were later incorporated into a prototype sculpting system called “Kizamu”  developed by Perry and Frisken. 1sAs for haptics-based computing, a good introduction to haptic rendering can be found in . Salisbury and his colleagues developed the PHANToM haptic interface,
8374	8384	and his colleagues developed the PHANToM haptic interface, which has resulted in many haptic rendering algorithms. Morgenbesser and Srinivasan  pioneered the concept of force shading. Kim et al.  presented a rather different implicit-based haptic rendering technique. Despite the widespread application of haptics in visual computing areas, haptics-based interaction was mainly applied to
8374	8385	objects (i.e., haptic rendering). Whereas, haptic modeling allows designers to directly manipulate objects with force feedback for the purpose of modeling or deforming objects. Foskey et al.  presented a touch-enabled 3D model design and texture painting system based on subdivision surfaces. Balakrishnan et al.  developed ShapeTape, a curve and surface manipulation technique that
8374	8386	for the purpose of modeling or deforming objects. Foskey et al.  presented a touch-enabled 3D model design and texture painting system based on subdivision surfaces. Balakrishnan et al.  developed ShapeTape, a curve and surface manipulation technique that can sense user-steered bending and twisting motions of the rubber tape. PDE-Based Surface Flow The general formulation of
8374	8387	based on the calculus of variation. One of the important PDE we used for distance-field based shape manipulation (Section 0.5) is the simplified version of the weighted minimal surface flow  as the PDE of Equation 1 : ? ? S ?t =(gv + g? ? H?) ? N, (3) where, ? H is the mean curvature of the surface and is acting as a smoothing constraint. v is the constant velocity, which will enable
8374	8388	where, I is the distance field function and ? is the gradient function. The mean curvature vector ? H used in Equation 3 is calculated by the discrete curvature estimator proposed by Desbrun et al. : Surface Flow Simulation 0.2 Model Regularity To ensure the numerical simulation of the surface flow to proceed smoothly, we must maintain the regularity of the model such that the model has a good
8374	8389	these techniques are applied only to regions of the polygonal model that are deforming at the current time step. 0.2.1 Mesh optimization There are three commonly used mesh optimization operations : edge split, edge collapse, and edge swap. Edge split and edge collapse are used to keep an appropriate node density. An edge split is triggered if the edge length is bigger than the maximum edge
8374	8390	than the minimum edge length threshold. Edge swapping is used to ensure a good aspect ratio of the triangles. This can be achieved by forcing the average valence to be as close to 6 as possible . An edge is swapped if and only if the quantity ? ?p?? (valence(?p) ? 6)2 is minimized after the swapping, where ? represents the four vertices in the two adjacent triangles of the current edge.
8391	8392	in a controlled fashion to combat errors in the channel – the purpose of channel coding. In the past decade a series of research breakthroughs in the field of channel coding (most notably ), have rendered a large number of coding schemes which can achieve near-to-optimal performance (i.e. very close to channel capacity) with reasonable complexity. The secret of their success lies in
8391	8399	random variable ? , where for convenience ??????? ????????? , then the L-value is defined as, ??? ? ? ? ? ??? ? ????? ? ???????s¢ ? ? ? ? © ? ? ? ? ?¡s? More details on L-values can be found in . In Fig. £¥¤§¦ 3, £¥¤©¨ , £¥??¦ , £¥?¥¨ , £¥?¥¦ , £¥??¨ , £???¦ , and ? ¢ ? ¢ ? ? ? ? ¢ ? ? (3) ? and stand for “apriori” and “extrinsic” respectively. Since the parity bit sequences ¢ £ ?¦?? and ¢
8391	8402	algorithm, we can employ slightly modified versions of the standard tools used for optimizing turbo codes. A useful tool for analysing iterative decoding algorithms is the EXIT chart (e.g. ,). The principle behind the EXIT chart is that much can be learned about the overall performance of a turbo decoder bysanalysing each of its parts. For this purpose, EXIT charts use mutual
8391	8403	between the © random ¦ variables and as, § ? ©©¨?¦ ? ? ????? £ ???s??¡¥£ © ? ? ????? ? ? ??? © ? ??? ??? ? ? ¥ ?¢¡¤£ © ? ???s??¡¥£ © ? ? ? ? ? ? ? ¢?? ?¨???s??¡¥£ © ??? ¢ ????? ? ¢ (4) Following , which exploits the ergodicity of the source this expression can be simplified to § ? ©©¨?¦ ? ? ? ? ? ? ? ? ??? ¢ ? ? ? ? ? ? ? ?????s? ¢ £ ¡ ? ??? ? ? ¢ ??? ??? ??????? ??? ? ¡ ¥ ????? ??? ??? ? ? ?
8391	8403	tool to optimize the component codes and the puncturing rates near the entropy of the source. Possible directions for future work include the design of irregular turbo source codes with EXIT charts , application to more elaborate discrete sources and entropy-constrained quantization of continuous-valued sources using turbo source codes. Acknowledgements The authors gratefully acknowledge
8919737	8885	those in education (Cuban, 1986). Teachers have long been seen by many advocates of educational computing as a principal barrier to widespread integration of ICT in schools*as Bryson and de Castell (1998) put it, a major &nuisance factor' to the otherwise smooth succession to the information technology revolution. A host of reports have &proved' teachers to be either &technophobic' about using ICT
8919737	8885	schools to be realised. Thus, how ICT is being &sold' to education by the IT industry via the media is of fundamental importance to the eventual use (or non-use) of computers in schools. As Leask (1998) points out, the (print) media remain one of the most signi&quot;cant external in#uences on the development of professional knowledge of teachers in the school system. National dissemination of new
8919737	8910	those in education (Cuban, 1986). Teachers have long been seen by many advocates of educational computing as a principal barrier to widespread integration of ICT in schools*as Bryson and de Castell (1998) put it, a major &nuisance factor' to the otherwise smooth succession to the information technology revolution. A host of reports have &proved' teachers to be either &technophobic' about using ICT
8919737	8910	schools to be realised. Thus, how ICT is being &sold' to education by the IT industry via the media is of fundamental importance to the eventual use (or non-use) of computers in schools. As Leask (1998) points out, the (print) media remain one of the most signi&quot;cant external in#uences on the development of professional knowledge of teachers in the school system. National dissemination of new
8445	8446	be analyzed with these computationally efficient methods. 1. Introduction Approximate Mean Value Analysis (AMVA) is a widely used approach to evaluating key computer system performance questions . The wide applicability of the AMVA technique is due to both its very low computational expense and its high degree of accuracy in producing performance estimates that agree with detailed system
8445	8447	be analyzed with these computationally efficient methods. 1. Introduction Approximate Mean Value Analysis (AMVA) is a widely used approach to evaluating key computer system performance questions . The wide applicability of the AMVA technique is due to both its very low computational expense and its high degree of accuracy in producing performance estimates that agree with detailed system
8445	8447	service times, in which case CV=0. The approximation has also been used in a number of other accurate models that contain FCFS centers with deterministic service times, such as those in . A problem with the accuracy of the above approximation arises for centers that have high service time CV. In this case, the estimated mean residual service time of the customer in service at a
8445	8457	be analyzed with these computationally efficient methods. 1. Introduction Approximate Mean Value Analysis (AMVA) is a widely used approach to evaluating key computer system performance questions . The wide applicability of the AMVA technique is due to both its very low computational expense and its high degree of accuracy in producing performance estimates that agree with detailed system
8445	8462	be analyzed with these computationally efficient methods. 1. Introduction Approximate Mean Value Analysis (AMVA) is a widely used approach to evaluating key computer system performance questions . The wide applicability of the AMVA technique is due to both its very low computational expense and its high degree of accuracy in producing performance estimates that agree with detailed system
8445	8462	service times, in which case CV=0. The approximation has also been used in a number of other accurate models that contain FCFS centers with deterministic service times, such as those in . A problem with the accuracy of the above approximation arises for centers that have high service time CV. In this case, the estimated mean residual service time of the customer in service at a
8445	8464	be analyzed with these computationally efficient methods. 1. Introduction Approximate Mean Value Analysis (AMVA) is a widely used approach to evaluating key computer system performance questions . The wide applicability of the AMVA technique is due to both its very low computational expense and its high degree of accuracy in producing performance estimates that agree with detailed system
8445	8469	mean service times . The work in this paper is motivated by a recent highly efficient heuristic AMVA model for evaluating shared memory architectures that contain complex modern processors . In that architecture model, each processor is modeled by a FCFS queue. Service times at the processor represent the time between memory requests that miss in the second level cache when the
8445	8469	was as high as 13 for the benchmarks and architecture that were modeled. For several of the benchmarks, Figure 1 shows the throughput (in units of instructions per cycle, IPC) obtained in  by (1) a detailed architecture simulator called RSIM, (2) the AMVA model with the standard AMVA approximation for FCFS centers with high service time CV , (3) the AMVA model with a new simple
8445	8469	Throughput Estimates The simple interpolation was found to be sufficiently accurate for evaluating the shared memory architecture performance over a fairly broad region of the design space , but the accuracy of the interpolation has not been investigatedsTable 1: Notation term definition introduced N number of customers in the (closed) network ? mean service time at a queueing center
8445	8469	residence time at a FCFS center with high service time variability. These techniques are the “standard AMVA approximation” , a new simple interpolation used in the previous architecture model , and a decomposition technique proposed by Zahorjan et al. . Other previoussrest of system high-CV server p 1-p rest of system rest of system Figure 2: System Decomposition approaches that are
8445	8469	applications, particularly those that have many FCFS queues with high service time variability and also use decomposition for analyzing other non-product form system features (e.g., the model in ), the exponential cost in the number of highCV FCFS queues may render the approach impractical. 3. FCFS Centers with High Service Time CV In this section, we explore the accuracy of AMVA techniques
8445	8474	be analyzed with these computationally efficient methods. 1. Introduction Approximate Mean Value Analysis (AMVA) is a widely used approach to evaluating key computer system performance questions . The wide applicability of the AMVA technique is due to both its very low computational expense and its high degree of accuracy in producing performance estimates that agree with detailed system
8919754	8481	items are next to each other and thus easy to find. The buttons labeled '+' and '?' are used to add and remove value ranges, respectively. An individual range functions like a thumb in Range Slider , i.e., the size of the range is adjusted by dragging from the edges of the range. Here the sizes of the ranges are constrained to be equal, so changing the size of a range changes the sizes of all
8482	8483	neighbors to send messages, that means that those nodes are involved in a cycle with the neighbors that did not yet send their messages. 4.1 Cycle cutset It has been pointed out in the literature  that breaking a problem with cycles into cycle-free parts can greatly improve the search performance for centralized, crisp CSPs. In the following, we will try to use this idea to find optimal
8482	8485	that each variable and constraint is owned by an agent. Systematic search algorithms for solving DisCSP are generally derived from depth-first search algorithms based on some form of backtracking . Recently, the paradigm of asynchronous distributed search has been extended to constraint optimization by integrating a bound propagation mechanism (ADOPT - ). Backtracking algorithms are very
8482	5458	Report EPFL/IC/2004/65 Abstract. We present in this paper a new complete method for distributed constraint optimization. This is a utility-propagation method, inspired by the sumproduct algorithm . The original algorithm requires fixed message sizes, linear memory, and is time-linear in the size of the problem. However, it is correct only for tree-shaped constraint networks. In this paper,
8482	5458	order would only use a linear number of messages. However, the messages could grow exponentially in size, and the algorithm would not have any parallelism. Recently, the sum-product algorithm  has become popular for certain constraint satisfaction problems, for example decoding. It is an acceptable compromise as it combines a dynamic-programming style exploration of a search space with a
8482	5458	networks For tree-structured networks (see an example in Figure 1), it is possible to devise polynomial-time complete optimization methods (see the sum-product algorithm for instance ) In this problem setting there is a set X of agents (each agent Xi is responsible for a variable), and a set A of agents that are interested in the assignments that are made for the variables X .
8482	8488	form of backtracking . Recently, the paradigm of asynchronous distributed search has been extended to constraint optimization by integrating a bound propagation mechanism (ADOPT - ). Backtracking algorithms are very popular in centralized systems because they require very little memory. In a distributed implementation, however, they may not be the best basis since in
8482	8490	algorithm is formulated for optimization problems, but can be easily applied to satisfaction problems as well. 1 Introduction Distributed Constraint Satisfaction (DisCSP) was first studied by Yokoo  and has recently attracted increasing interest. In distributed constraint satisfaction, variables and constraints are distributed so that each variable and constraint is owned by an agent.
8482	8491	that each variable and constraint is owned by an agent. Systematic search algorithms for solving DisCSP are generally derived from depth-first search algorithms based on some form of backtracking . Recently, the paradigm of asynchronous distributed search has been extended to constraint optimization by integrating a bound propagation mechanism (ADOPT - ). Backtracking algorithms are very
8482	8492	that each variable and constraint is owned by an agent. Systematic search algorithms for solving DisCSP are generally derived from depth-first search algorithms based on some form of backtracking . Recently, the paradigm of asynchronous distributed search has been extended to constraint optimization by integrating a bound propagation mechanism (ADOPT - ). Backtracking algorithms are very
8919755	8495	recapon the Boneh and Franklin encryption algorithm. Although much of what we will discuss also applies to other pairing based schemes such as the short signature scheme of Boneh, Lynn and Shacham  or the identity based signature schemes to be found in ,  and . The scheme of Boneh and Franklin , allows the holder of the private part S ID of an identifier based key pair to decrypt
8919755	8498	Franklin based on the Weil pairing, although using a variant based on the Tate pairing is more efficient. A number of identity based signature algorithms based on the Tate pairing also exist, e.g. ,  and . In this paper we investigate the applications of the Boneh and Franklin scheme in more detail. In particular we examine how the key structure can lead to interesting applications.
8919755	8498	much of what we will discuss also applies to other pairing based schemes such as the short signature scheme of Boneh, Lynn and Shacham  or the identity based signature schemes to be found in ,  and . The scheme of Boneh and Franklin , allows the holder of the private part S ID of an identifier based key pair to decrypt a message sent to her under the public part Q ID . We
8919755	8499	secure and efficient identity based encryption algorithms, but with little success. This state of affairs changed in 2001 when two identity based encryption algorithms were proposed, one by Cocks  based on the quadratic residuosity assumption and another by Boneh and Franklin based on the Weil pairing, although using a variant based on the Tate pairing is more efficient. A number of identity
8919755	8501	key pair to decrypt a message sent to her under the public part Q ID . We present only the simple scheme which is only ID-OWE, for a ID-CCA scheme one applies the Fujisaki-Okamoto transformation . Let m denote the message to be encrypted – Encryption : Compute U = rP where r is a random element of Fq. Then compute V = m ? H2(t(R TA ,rQ ID )) Output the ciphertext (U, V ). – Decryption :
8919755	8502	based on the Weil pairing, although using a variant based on the Tate pairing is more efficient. A number of identity based signature algorithms based on the Tate pairing also exist, e.g. ,  and . In this paper we investigate the applications of the Boneh and Franklin scheme in more detail. In particular we examine how the key structure can lead to interesting applications. Due to
8919755	8502	of what we will discuss also applies to other pairing based schemes such as the short signature scheme of Boneh, Lynn and Shacham  or the identity based signature schemes to be found in ,  and . The scheme of Boneh and Franklin , allows the holder of the private part S ID of an identifier based key pair to decrypt a message sent to her under the public part Q ID . We present
8919755	8503	on the Weil pairing, although using a variant based on the Tate pairing is more efficient. A number of identity based signature algorithms based on the Tate pairing also exist, e.g. ,  and . In this paper we investigate the applications of the Boneh and Franklin scheme in more detail. In particular we examine how the key structure can lead to interesting applications. Due to our later
8919755	8503	we will discuss also applies to other pairing based schemes such as the short signature scheme of Boneh, Lynn and Shacham  or the identity based signature schemes to be found in ,  and . The scheme of Boneh and Franklin , allows the holder of the private part S ID of an identifier based key pair to decrypt a message sent to her under the public part Q ID . We present only the
8919756	8516	algorithm. Briefly, the First-Order Theory ? is divided into sub-domains corresponding to subsets of the predicates and constants of ?. This is done by hand or automatically (e.g., ) such that the predicates and the constants are divided somewhat evenly among the sub-domains (which we call partitions). Figure 4 shows this applied to our example from Section 2.2. Then, each
8919756	8516	be done with human guidance or automatically. Sometimes, we can reduce this problem to finding graph decompositions with minimum treewidth of the intersection graph G(V, E, l). A good reference is . The algorithms above give sound and complete propositionalizations without the DCA by using the E-sets. Even if we are allowed to make the DCA for the entire problem, a partitioned
8919756	8517	of quantifiers in ? and is at most O(2 3P +PC ) (significantly smaller in practice, using current propositional SAT solvers). Secondly, we use structure (in the manner of Partition-based Reasoning ) to obtain a propositionalization that has an exponentialfactor fewer propositions than the one above. Our method is quite general and does not require special knowledge of the underlying domain or
8919756	8517	is to determine which predicates need to be instantiated with which constants by analyzing the global properties of the theory. Our idea is to use the principles of Partition-based Reasoning  to do so. The next section describes an algorithm that finds a more compact propositionalization using partitioning. We present this algorithm, its analysis and application to the machine
8919756	8517	After this we may choose to do either of two things. The partitioning of the theory can be retained and reasoning can be done using the (sound and complete) Message-Passing algorithms described in  (Henceforth, we call this Method 1). Alternatively, the domains can be merged together, creating a single propositionalized theory, to which any propositional SAT solver or theorem prover can be
8919756	8517	between the partitions are labeled with the set of nonlogical (predicate and constant) symbols that are shared between partitions. Figure 3 reproduces the Message Passing algorithm FORWARDMP from . Given a partitioned theory, its intersection graph and query Q in the language of one of the partitions Ak, FORWARDMP will try to prove Q. (If the query is not contained in any partion Ak, then a
8919756	8517	theory, performs Consequence Finding and so on. When the algorithm reaches Ak it attempts to prove the query Q and returns the result. The following recounts a soundness and completeness result of  for partitioned reasoning with Message-Passing on trees. We use this result to show that COMPACT-PROP is correct. 2 If we remove (i, j) from the graph, then each of B1, B2 is the union of the
8919756	8518	for consequence finding in FOL (as opposed to clausal FOL) iff for every non-tautologous logical consequence ? of A, R derives a logical consequence ? of A such that ? |= ? and ? ? L(?). Theorem 3 () Let A = ? i?n Ai be a partitioned theory and assume that the graph G is a tree that has a proper labeling for the partitioning {Ai}i?n. Also assume that each of the reasoning procedures used in
8919756	8518	= O(n · 22dl · fSAT( n2 ) ) fSAT(|P ||C|) where fSAT(n) is the time taken to solve SAT problems over n variables. The proof of Theorem 7 relies on the running-time analysis of FORWARD-MP given in . Since fSAT is typically exponential in the number of propositional symbols, the fraction T(A) will be T(?) small. 4.3 The Factory Example Revisited Recall the machine scheduling problem from
8919756	8524	the Unique Names Assumption (UNA; every constant symbol refers to a unique object). Propositionalization is used in a number of applications involving First-Order representations, such as Planning  and Relational Data Mining . Many specialized propositionalization algorithms exist for such domains that use prior knowledge to construct efficient (small) propositionalizations. ILP systems
8919756	8524	steps. Call the axioms above T . Then our task is to determine if T ? P1(1) |= P125(n). Assume that we try to solve this problem by making the DCA and a naive propositionalization in the manner of . Then the number of propositional symbols is the number of constants times the number of predicates, i.e., 125 · 3n (there are n states, 3 predicates per state, and 125 time steps). This number is
8919756	8524	Some use Lifted Causal Encodings, an idea borrowed from the Theorem Proving community and others reduce the number of variables by compiling away state variables and fluents. A good introduction is .s3.1 Propositionalization Without the DCA We present a technique for constructing a propositionalization of a monadic function-free FOL theory with open domain semantics. It is common to try to do
8919756	8526	to construct efficient (small) propositionalizations. ILP systems such as LINUS  use the training data to guide construction of the propositionalizations. Bottom-Up Propositionalization  is tailored for biochemical databases and chooses propositions by constructing frequently occuring fragments of linearly connected atoms. 1 This is also called the Closed-World Assumption (CWA)
8919756	8527	every constant symbol refers to a unique object). Propositionalization is used in a number of applications involving First-Order representations, such as Planning  and Relational Data Mining . Many specialized propositionalization algorithms exist for such domains that use prior knowledge to construct efficient (small) propositionalizations. ILP systems such as LINUS  use the
8919756	8279	and operator definitions. Several techniques have been employed in the planning literature to obtain optimized propositional encodings of planning problems stated in a Situation Calculus  formalism. Some use Lifted Causal Encodings, an idea borrowed from the Theorem Proving community and others reduce the number of variables by compiling away state variables and fluents. A good
8919757	8546	as the weighted average nonfood expenditure. In F constructing the average, observations closer to z are given a higher weight. The weighting scheme follows a kernel with triangular weights (Hardle 1990). 13 Table 1 lists by region the food and reference (total) poverty lines as well as the implicit spatial price indices. By definition, the differences observed in the poverty lines reflect
8919757	8556	in a subsequent step. Fourth, estimation of the consumption model avoids strong distributional assumptions that are typically necessary for estimating nonlinear limited dependent variable models (Powell 1994). 5 5 Hence, the approach we use in this study will be modeling consumption as in equation (1), and then using equation (2) to infer implications about levels of poverty. In estimating models such
8919757	8559	the effect of a particular variable conditional on the other potential determinants. While there may be certain contexts where unconditional poverty profiles are relevant to a policy decision (see Ravallion 1996), often one would be interested in the &quot;conditional&quot; poverty effects of proposed policy interventions. It is not surprising therefore that empirical poverty assessments in recent years have seen a
8919757	8559	other potential determinants. While there may be certain contexts where unconditional poverty profiles are relevant to a policy decision (for instance, in the context of indicator targeting; see Ravallion 1996), often one is interested in the &quot;conditional&quot; poverty effects of proposed policy interventions. It is not surprising therefore that empirical poverty assessments in recent years have seen a number
8919763	8566	by data mining tasks. A variety of data mining techniques can be applied to the click2 stream or Web application data in the pattern discovery phase, such as clustering, association rule mining , and sequential pattern discovery . The recommendation engine considers the active user session in conjunction with the discovered patterns to provide personalized content. The personalized
8919763	8566	itemsets are stored in a directed acyclic graph, here called a Frequent Itemset Graph. The Frequent Itemset Graph is an extension of the lexicographic tree used in the tree projection algorithm of . The graph is organized into levels from 0 to k, wherek is the maximum size among all frequent itemsets. Each node at depth d in the graph corresponds to an itemset, I, of size d and is linked to
8919764	8586	the allowed angles in A are a subset of those in B. The arrow indicates angles allowed in B which are not allowed in A. All plots shown are equal area Sauson–Flamsteed map projections (22), after (23). and R parameters, assuming that the distribution of internuclear vector orientations was isotropic. The errors in Table 2 are the standard deviations of the two parameters extracted from these
8595	8597	algorithm over the Internet to maximize aggregate utility, and a user’s utility function is (often implicitly) defined by its TCP algorithm, see e.g.  for unicast and  for multicast. All of these papers assume that routing is given and fixed at the time scale of interest, and TCP, together with active queue management (AQM), attempt to maximize aggregate utility
8595	8600	as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility function is (often implicitly) defined by its TCP algorithm, see e.g.  for unicast and  for multicast. All of these papers assume that routing is given and fixed at the time scale of interest, and TCP, together with active queue management (AQM), attempt to
8595	8600	xi. One can think of TCP–AQM as a distributed primal-dual algorithm to maximizing aggregate utility, given a routing matrix R, i.e., it solves the following constrained convex program (see e.g. ): max xi and the associated dual problem : ? ? ? min max Ui(xi) ? xi pl?0 xi?0 i ? Ui(xi) (1) i subject to Rx ? c (2) l Rlipl ? + ? TCP algorithms adapt the primal variables x = (xi, i
8595	2358	as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility function is (often implicitly) defined by its TCP algorithm, see e.g.  for unicast and  for multicast. All of these papers assume that routing is given and fixed at the time scale of interest, and TCP, together with active queue management (AQM), attempt to
8595	2358	xi. One can think of TCP–AQM as a distributed primal-dual algorithm to maximizing aggregate utility, given a routing matrix R, i.e., it solves the following constrained convex program (see e.g. ): max xi and the associated dual problem : ? ? ? min max Ui(xi) ? xi pl?0 xi?0 i ? Ui(xi) (1) i subject to Rx ? c (2) l Rlipl ? + ? TCP algorithms adapt the primal variables x = (xi, i
8595	2358	Di tan?1 (xiDi/ ? 2) where Di is source i’s round trip time, and the utility function of Vegas is ?idi log xi where ?i is protocol parameter and di is round trip propagation delay of source i; see  and references therein for details and other variations. These utility functions are strictly concave increasing, and hence the problem (1)–(6) can be efficiently solved. Note that we can go
8595	2358	as solving a certain utility maximization problem. This simple convex program allows us to understand, and predict, the equilibrium properties of large-scale networks under TCP-AQM control (e.g., ). We now study whether the same methodology can be applied to the understanding of TCP-AQM and shortest-path routing. 2.2 TCP–AQM/IP Consider the problem of maximizing utility over routes as well
8595	2358	of including prices in link cost is that they are precise measure of congestion. Indeed, these prices represent packet loss probability in TCP Reno and its variants and queueing delay in TCP Vegas . The protocol parameters ? and ? determine the responsiveness of routing to network traffic: ? = 0 corresponds to static routing, ? = 0 corresponds to purely dynamic routing, and the larger the
8595	8602	i = 1, . . . , r see price p1(r) on their paths while nodes i = r +1, . . . , N see price pN+1(r) on their paths. Since these rates xi(r) and prices pi(r) are primal and dual optimal, they satisfy  U ? (xi(r)) = p1(r) for i = 1, . . . , r (10) U ? (xi(r)) = pN+1(r) for i = r + 1, . . . , N (11) This implies that x1(r) = · · · = xr(r) and xr+1(r) = · · · = xN(r). It is easy to see that the
8595	8602	update period t, on a slower timescale than congestion control. In each routing period t, we first solve the link prices based on the current routing, using the gradient projection algorithm of . We iterate the source algorithm to update rates and the link algorithm to update prices, until they converge. The link prices are then used to compute the shortest paths for the next period. We
8595	8602	inequality follows from the strict concavity of U. We now show that the right-hand side is the optimal dual objective value, and hence there is a duality gap. The dual problem of (8)–(9) is (e.g., ) min p1,pN+1?0 N? i=1 max xi (U(xi) ? xi min{p1, pN+1}) + (p1 + pN+1) First, note that the minimizing (p1, pN+1) must satisfy p1 = pN+1, for otherwise, if (say) p1 < pN+1, then the dual objective
8595	8603	as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility function is (often implicitly) defined by its TCP algorithm, see e.g.  for unicast and  for multicast. All of these papers assume that routing is given and fixed at the time scale of interest, and TCP, together with active queue management (AQM), attempt to
8595	8603	xi. One can think of TCP–AQM as a distributed primal-dual algorithm to maximizing aggregate utility, given a routing matrix R, i.e., it solves the following constrained convex program (see e.g. ): max xi and the associated dual problem : ? ? ? min max Ui(xi) ? xi pl?0 xi?0 i ? Ui(xi) (1) i subject to Rx ? c (2) l Rlipl ? + ? TCP algorithms adapt the primal variables x = (xi, i
8595	8603	Di tan?1 (xiDi/ ? 2) where Di is source i’s round trip time, and the utility function of Vegas is ?idi log xi where ?i is protocol parameter and di is round trip propagation delay of source i; see  and references therein for details and other variations. These utility functions are strictly concave increasing, and hence the problem (1)–(6) can be efficiently solved. Note that we can go
8595	8603	as solving a certain utility maximization problem. This simple convex program allows us to understand, and predict, the equilibrium properties of large-scale networks under TCP-AQM control (e.g., ). We now study whether the same methodology can be applied to the understanding of TCP-AQM and shortest-path routing. 2.2 TCP–AQM/IP Consider the problem of maximizing utility over routes as well
8595	8603	of including prices in link cost is that they are precise measure of congestion. Indeed, these prices represent packet loss probability in TCP Reno and its variants and queueing delay in TCP Vegas . The protocol parameters ? and ? determine the responsiveness of routing to network traffic: ? = 0 corresponds to static routing, ? = 0 corresponds to purely dynamic routing, and the larger the
8595	8605	as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility function is (often implicitly) defined by its TCP algorithm, see e.g.  for unicast and  for multicast. All of these papers assume that routing is given and fixed at the time scale of interest, and TCP, together with active queue management (AQM), attempt to
8595	8605	xi. One can think of TCP–AQM as a distributed primal-dual algorithm to maximizing aggregate utility, given a routing matrix R, i.e., it solves the following constrained convex program (see e.g. ): max xi and the associated dual problem : ? ? ? min max Ui(xi) ? xi pl?0 xi?0 i ? Ui(xi) (1) i subject to Rx ? c (2) l Rlipl ? + ? TCP algorithms adapt the primal variables x = (xi, i
8595	2360	as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility function is (often implicitly) defined by its TCP algorithm, see e.g.  for unicast and  for multicast. All of these papers assume that routing is given and fixed at the time scale of interest, and TCP, together with active queue management (AQM), attempt to
8595	2360	xi. One can think of TCP–AQM as a distributed primal-dual algorithm to maximizing aggregate utility, given a routing matrix R, i.e., it solves the following constrained convex program (see e.g. ): max xi and the associated dual problem : ? ? ? min max Ui(xi) ? xi pl?0 xi?0 i ? Ui(xi) (1) i subject to Rx ? c (2) l Rlipl ? + ? TCP algorithms adapt the primal variables x = (xi, i
8919772	8610	on a real-time kernel. The performance delivered by each task is obtained under different scheduling algorithms (such as EDF, RM and LEF) by simulation. Simulations have been performed using . A. Plants and controller Plant1 Plant2 Fig. 1. LEF scheduling operation Task1 Task2 From the linear time invariant state-space model that we used to model each inverted pendulum (each mounted on a
8919772	1496	algorithms depend on a priori characterization of the workload to provide performance guarantees in predictable environments. For example, Rate Monotonic (RM) and Earliest Deadline First (EDF)  require complete knowledge about the task set such a resources requirements, precedence constraints, resource contention, and future arrival times. Such algorithms work in “open-loop”. That is,
8919772	5689	future work is outlined. II. RELATED WORK Lately, significant effort has been made in the area of real-time and control systems research. As it has been shown in several works (see for example , , or ), approaches that combine real-time and control disciplines offer effective solutions for the analysis and design of both real-time control systems and feedback scheduling approaches. In
8919772	5689	each plant dynamics in order to assign processor capacity to the tasks that more urgently need to issue the control signal. This may result in slow sampling for other control tasks in the system.  presents a framework for adaptive real-time systems where feedback control scheduling algorithms are designed to satisfy the transient and steady state performance specifications of real-time
8919772	8611	is outlined. II. RELATED WORK Lately, significant effort has been made in the area of real-time and control systems research. As it has been shown in several works (see for example , , or ), approaches that combine real-time and control disciplines offer effective solutions for the analysis and design of both real-time control systems and feedback scheduling approaches. In
8919772	8611	Note that although the framework uses the idea of feedback, it is not intended for control applications. Our scheduling technique is for control tasks. The approach we present follows the work of , in which the authors stressed the need for new scheduling approaches able to optimize control performance according to the control application dynamics. The technique we present is based in the
8919772	8611	between the desired response of the system and the actual response of the system) for any given perturbations. In other words, controllers try to optimize control performance. As it was shown in , upon a perturbation arrival, the higher the frequency a controller is given, the better the control performance. Treating the scheduling policy as a fundamental part of the control application
8919772	8611	which offers the possibility of taking at run time thesscheduling decisions according to the application dynamics. And finally, we plan that LEF schedule will use flexible timing constraints (see ) for the control tasks, which will allow a better both CPU utilization and control performance. V. CONCLUSIONS In this paper we have presented an early specification of a novel scheduling paradigm
8612	8613	2 This view of expectations and sociality follows the Theory of Social Systems (“systems theory”) of the sociologist Niklas Luhmann  and is described from a computer scientific point of view in . P3 P4swe use, and to make certain auxiliary definitions and assumptions. The example network in figure 1 will be used throughout the discussion of ENs to illustrate the purpose of definitions and
8612	8613	LogicalExpr. It can either be (i) an atomic proposition, a (ii) message pattern or physical action term,sVar ? X | Y | Z | . . . AgentVar ? A1 | A2 | . . . PhysicalActVar ? X1 | X2 | . . . Expect ?  Agent ? agent_1 | . . . | agent_n Head ? it_rains | loves | . . . Performative ? accept | propose | reject | inform | . . . PhysicalAction ? move_object | pay_price | deliver_goods | . . . Message
8612	8613	into two categories, cognitive and normative edges. A cognitive edge e (also called observation edge) denotes a correlation in observed communication sequences. Usually, its expectability Exp(e) ???  reflects the probability of target(e) occurring shortly after source(e) in the same communicative context (i.e. in spatial proximity, between the same agents, etc.). Although expectability values
8612	8613	mesg(target(f))) (target node labels of outgoing links never match), – H ? N is a finite communication horizon, – n ? N is the total number of non-continuations, – Expect : C ? , ? : V ? N, Fc : N × N ? R, – subst : C ? N ? SubstList (with SubstList as in table 1). Through this definition, all elements discussed above are included: networks contain cognitive edges
8612	8613	approach (1) lies in its seamless integration of “physical” events into the EN, whereas (2) is probably more easy to apply in practice. 5.2 Mirror Holons: Multi-Stage Observation and Reflection In , we have introduced the social system mirror architecture for open MAS. The main component of this architecture is a so-called social system mirror (or “mirror” for short), a middle agent which
8612	8614	Obviously, its absolute value should decrease with time passing, and it should become zero after some time, i.e. ?n1, n2 ? N.n2 > n1 ? |Fc(e, n2)| < |Fc(e, n1)| (1) ?n0 ? N.?n > n0.Fc(e, n) = 0 (2) Computation of changes in the impact of a normative edge necessitates, of course, keeping track of the time ?(v) that has passed since a message was observed that matched the label of node v. Note
8612	8614	of maximal message sequence length for which the EN is relevant, and the total number of “non-continuations” n. Usually, this will be incremented each time a node (1) is appended to the root node, (2) matches one of the children nodes of the root node. Both are necessary for defining the semantics of the EN, which are discussed in detail in the following section. 5 This comes at the price of
8612	8614	of an application domain. To obtain a social version of this sort of ontology from an EN, two different approaches appear to be reasonable: (1) Inclusion of environment events within the EN and (2) probabilistic weighting of assertions. The former approach treats “physical” events basically as utterances. Similar to the communicative reflection of agent actions by means of do, a special
8612	8614	know its meaning in terms of its consequences. Its meaning has thus to be derived a posteriori from the communicational reflection of how the agents react to its occurrence. In contrast, approach (2), which we proposed for the agent-based competitive rating of web resources , exploits the propositional attitude of utterances. The idea is to interpret certain terms within LogicalExpr as
8612	8614	Unlike approach (1), ontologies are constructed “by description” not “by doing” in this way. The advantage of approach (1) lies in its seamless integration of “physical” events into the EN, whereas (2) is probably more easy to apply in practice. 5.2 Mirror Holons: Multi-Stage Observation and Reflection In , we have introduced the social system mirror architecture for open MAS. The main
8612	8615	all, the sum of expectabilities of all cognitive out-edges of a node should be one for any state (i.e. set of believed facts) of the knowledge base. In other words, the condition ?v ? Expect(e) = 1 (3) e?out(v),KB|=cond(e) should hold. This can be ensured, for example, by guaranteeing that the following condition holds through appropriate construction rules for the EN. Assume the outgoing links
8612	8618	the semantics of agent communication languages (ACLs) are mostly based on describing mental states of communicating agents  or on observable (usually commitment-based) social states . However, both these views fail to recognise that communication semantics evolve during operation of a multiagent system (MAS), and that the semantics always depend on the view of an observer who
8612	8619	systems research in general. 1 Introduction Traditional attempts to model the semantics of agent communication languages (ACLs) are mostly based on describing mental states of communicating agents  or on observable (usually commitment-based) social states . However, both these views fail to recognise that communication semantics evolve during operation of a multiagent system (MAS),
8612	8622	employed by agents in their reasoning) and which are autonomous with respect to how they perform this generation and modification of expectations. Our hypothesis regarding the Socionics endeavour  is that its main contribution lies in the construction of appropriate communication systems for complex MAS, or, to take it to the extreme, we might summarise this insight as Socionics = empirical
8612	8624	by sociological systems-theory  – introduced expectations regarding observable communications as a universal means for the modelling of emergent sociality in multiagent systems, and in , we have presented – influenced by socio-systems and socio-cognitive (pragmatist) theories  – a formal framework for the semantics of communicative action that is empirical, rational,
8612	8625	approach. A very important feature of communication languages is their ability to effectively encode the generalised meaning of utterances by means of syntax. Our computationally tractable approach  to this phenomenon relies on the assumption that the syntax of messages somehow reflects expectation structures which have already been assembled. Structure Expansion. Structure expansion is
8612	8625	an extensible set of speech-act performative types with semantics defined in a mentalistic fashion. In our approach, we can imagine some system component (e.g., a so-called multiagent system mirror ) that provides the agents with a set of performatives without any predefined semantics and wait for the semantics of such “blank” performatives to emerge. To become predictable, it is rational for
8612	8625	for the MAS designer. While a mirror only models a single communication system, and, except for the propagation of expectations, is a purely passive observer, the successor architecture HoloMAS  is able to model multiple communication systems at thessame time through multiple mirror holons in order to model large, heterogenous systems. In addition, a mirror holon can take action itself by
8612	8626	by sociological systems-theory  – introduced expectations regarding observable communications as a universal means for the modelling of emergent sociality in multiagent systems, and in , we have presented – influenced by socio-systems and socio-cognitive (pragmatist) theories  – a formal framework for the semantics of communicative action that is empirical, rational,
8612	8618	the semantics of agent communication languages (ACLs) are mostly based on describing mental states of communicating agents  or on observable (usually commitment-based) social states . However, both these views fail to recognise that communication semantics evolve during operation of a multiagent system (MAS), and that the semantics always depend on the view of an observer who
8612	8630	by sociological systems-theory  – introduced expectations regarding observable communications as a universal means for the modelling of emergent sociality in multiagent systems, and in , we have presented – influenced by socio-systems and socio-cognitive (pragmatist) theories  – a formal framework for the semantics of communicative action that is empirical, rational,
8612	8630	for these performatives, but at least the intentional attitude associated with the respective performative needs to become common ground to facilitate non-nonsensical, non-entropic communication . A particular performative usually appears at multiple nodes within the EN, with different consequences at each position, depending on context (especially on the preceding path), message content
8612	8632	of one old node for each new node; 5. removal of very unlikely paths. 4 Integrating the Agent Perspective In this section, we will explain how the Interaction Frames and Framing Architecture InFFrA  fits into the communication systems view. Interestingly enough, despite the fact that this architecture was developed independently from the CS framework using interactionist theories (yet also
8612	8634	systems research in general. 1 Introduction Traditional attempts to model the semantics of agent communication languages (ACLs) are mostly based on describing mental states of communicating agents  or on observable (usually commitment-based) social states . However, both these views fail to recognise that communication semantics evolve during operation of a multiagent system (MAS),
8612	8635	the semantics of agent communication languages (ACLs) are mostly based on describing mental states of communicating agents  or on observable (usually commitment-based) social states . However, both these views fail to recognise that communication semantics evolve during operation of a multiagent system (MAS), and that the semantics always depend on the view of an observer who
8919773	9113	aspects (e.g., design processes, quality assurance in schools, see). Furthermore, most organizations have developed their own criteria catalogs or evaluation models (e.g. ASTD criteria, see ). Unfortunately, these approaches are only used nationally, regionally, or locally. Secondly, the effort to adapt these approaches to a specific organization is immense. Expensive consulting and
8919773	9117	educational programs (see). Another class of approaches covers only specific parts of educational process or domain specific aspects (e.g., design processes, quality assurance in schools, see). Furthermore, most organizations have developed their own criteria catalogs or evaluation models (e.g. ASTD criteria, see ). Unfortunately, these approaches are only used nationally,
8919773	9118	Other broad approaches for educational organizations are the Quality Mark of the British Association for Open Learning  or the approaches of the Higher Education Funding Council for England . These approaches are generic but specifically adapted for educational processes. Other approaches are specifically designed for Quality Assurance in the field of education – one example is the
8919773	8638	the field of education – one example is the EssenLearning-Model which is a procedural model for planning, design, implementation, and evaluation of learning scenarios and educational programs (see). Another class of approaches covers only specific parts of educational process or domain specific aspects (e.g., design processes, quality assurance in schools, see). Furthermore, most
8640	2656	scaling, or that the finding was merely an artifact of poor estimators in the face of data polluted with nonstationarities. The introduction of wavelet based estimation techniques to traffic in  helped greatly to resolve this question, as they convert temporal LRD to SRD in the domain of the wavelet coefficients, and simultaneously eliminate or reduce certain kinds of nonstationarities. It
8640	8641	that the estimation is heavily weighted toward the smaller scales, where there is much more data. For further details on the method, its use and robustness properties, we refer the reader to , . C. Impact of Large-Time Scaling on Performance Performance studies of networks, whether they be analytic or simulation based, as well as the interpretation of traffic measurements themselves,
8640	8642	e.g., , , , ). A survey of estimation methods for the scaling exponent can be found in , , and comparisons of the wavelet estimator against others is given in  (see also ), where a joint wavelet estimator is given for with excellent statistical properties. Another key advantage of the wavelet approach is its computational complexity (memory complexity is also in an
8640	8644	are always trying to transmit at the maximum rate the protocol will allow. There are also more complex formula that apply over a wider range of values of , or for more complex loss patterns . This body of work however, is largely restricted to coarse measures of traffic such as long term throughput rates and avoids dealing with the large-scale aspect of real networks. A much finer
8640	8646	restricted to coarse measures of traffic such as long term throughput rates and avoids dealing with the large-scale aspect of real networks. A much finer analysis is possible using the machinery , , which is in principle capable of providing rigorous results over quite a wide range of scenarios, including tandem networks. Thus far, only throughput-type measures have been derived using
8640	8649	mitigate somewhat the service degradations seen from the individual users’ point of view, as it means that if they encounter a very large queue they nonetheless begin to receive service immediately –, . However, from a system or network operator’s point of view, simply changing scheduling disciplines cannot make congestion disappear, and we are again reminded of the lesson learned
8640	8653	unexpected finding that on–off-type sources with heavy-tailed on-periods ( ) can lead to an infinite average queue content even though the average input rate is less than the capacity of the queue . Essentially, the infinite variance of the on-times allows very long periods where the input rate exceeds , leading to correspondingly large buffer levels with appreciable probability. Many
8640	8655	predicted within the framework of the traditionally considered traffic models. An example of such an emergent phenomenon was the discovery of the self-similar or fractal nature of network traffic , , . The focus of this paper is the observed scaling behavior of network traffic, and the use of dynamical systems models to understand how feedback mechanisms affect it. In particular, we
8640	8655	in demystifying self-similar traffic modeling and have given rise to new insights and physical understanding of the effects of LRD on the design, management and performance of modern data networks , , , , . On the one hand, the fact that we can explain self-similar scaling in terms of the statistical properties of the individual sessions or connections that make up the
8640	8655	more involved, but the empirical evidence for the heavytailed characteristic of Web-related entities (e.g., HTTP request sizes and durations) has been well-established to date; see for example , , . Typically, these empirical studies rely on more or less sophisticated methods for inferring heavy-tailed behavior, including simple – plots of versus (the so-called complementary
8640	8657	the design, analysis, and implementation of new algorithms for practical engineering problems include 1) CPU load balancing in networks of workstations ; 2) connection scheduling in Web servers ; 3) load-sensitive routing in IP networks ; and 4) detecting certain kinds of network attacks , . Realizing that self-similarity concerns large time scales and leaves the small scale
8640	8659	traffic. In another engineering sense, the presence of self-similarity could have been termed “good news.” The large deviations principles that underpins the Norros’ formula, and its refinements , , , directly relate the traffic characteristics (e.g., distribution of arrival counts) to performance measures (e.g., queue length distributions, loss rates). They indicate that to obtain
8640	8662	traffic description is clearly a more parsimonious and also more viable alternative. The above discussion does not imply that self-similar models are always appropriate. As described in , there are three fundamental conditions that must be satisfied for a Gaussian self-similar traffic description, corresponding to the observation of long-range dependence in traffic, to apply: 1)
8640	8663	above, depending on the networking setting where the traces were collected. For some traditional open loop queueing models, the effects of such variability have been shown to be strongly negative . In each of , , , , unconventional but nevertheless open-loop scaling models were investigated to describe the observed type of variability, which can be thought of as local
8640	8664	involved, but the empirical evidence for the heavytailed characteristic of Web-related entities (e.g., HTTP request sizes and durations) has been well-established to date; see for example , , . Typically, these empirical studies rely on more or less sophisticated methods for inferring heavy-tailed behavior, including simple – plots of versus (the so-called complementary cumulative
8640	3027	self-similar traffic modeling and have given rise to new insights and physical understanding of the effects of LRD on the design, management and performance of modern data networks , , , , . On the one hand, the fact that we can explain self-similar scaling in terms of the statistical properties of the individual sessions or connections that make up the aggregate
8640	3027	of congestion, and so on . The key point is that TCP traffic is shaped by current networking conditions, resulting in TCP traffic in turn dynamically adapting to changing network conditions , . The above observation, and the fact that many of the measured traffic traces which show strong scaling behavior are derived from networks in which TCP is dominant, has led researchers to a
8640	3027	certain kinds of nonstationarities. It is now widely accepted that scaling in traffic is real, and wavelet methods have become the method of choice for detailed traffic analysis (see e.g., , , , ). A survey of estimation methods for the scaling exponent can be found in , , and comparisons of the wavelet estimator against others is given in  (see also ), where a
8640	8667	on-time distribution for a variety of values of ? (top), the Logscale diagrams for a variety of values of ? (bottom). different RTTs, and this can have a strong impact on their relative performance . Moreover, the RTT can vary dynamically within a TCP session. For instance, as the load increases, the queue lengths increase, and, therefore, the RTT increases. This can have as dramatic an impact
8640	8668	past 10 years, paying special attention to the radical changes that have resulted from a gradual improvement of our understanding of the causes and origins underlying the self-similarity phenomenon , , , –, . This work motivates the development of innovative new approaches for modeling and evaluating the performance of large-scale complex communication networks such as
8640	8668	result. However, in the Internet bandwidths can vary from kb/s to Gb/s, while RTTs can range from less than 1 millisecond to 1 second, allowing a variation in send rate of 10 orders of magnitude . The TCP flow control attempts to adaptively choose the window size using four algorithms, Slow start, Congestion avoidance, Fast Recovery, and Fast Retransmission. To illustrate TCP’s potential
8640	8669	(5). Far heavier tails in the queueing distribution are possible, as we explore further in Section II-D. In short, the effect of LRD on performance can be summarized as being mildly negative ,  through to very severe, depending on the circumstances. Often little can be done about the impacts, as they are intrinsic to the sources themselves. The network can try to control traffic
8640	8671	phenomena can be exploited and has led to the design, analysis, and implementation of new algorithms for practical engineering problems include 1) CPU load balancing in networks of workstations ; 2) connection scheduling in Web servers ; 3) load-sensitive routing in IP networks ; and 4) detecting certain kinds of network attacks , . Realizing that self-similarity concerns
8640	2994	There are a number of versions of TCP (Tahoe, Reno, Vegas, New Reno) but the predominant one by far is TCP Reno, which we focus on here; for more TCP-specific details, see, e.g., , ???, . TCP aims to provide a reliable delivery service. To ensure this, each packet has a sequence number, and its successful receipt must be signaled by a returning acknowledgment (ACK) packet.
8640	8673	packet flow emitted from the different sources, which in turn alters the rate process that arrives at the queue or IP router for buffering, which in turn impacts the level of congestion, and so on . The key point is that TCP traffic is shaped by current networking conditions, resulting in TCP traffic in turn dynamically adapting to changing network conditions , . The above
8640	8673	subject run the gamut of possibilities—from some studies that claim that TCP feedback eliminates self-similarity to the extent that fBm queueing predictions do not hold (e.g., ; but see also ); to other studies that claim that self-similarity can in fact be generated by such feedback , . Note that any network scenario in which flow controls substantially alter source behavior
8640	2355	type, and, therefore, models a complex system whose dynamics is stable to perturbations, and even has stochastic features. In this sense, our approach is related to the work by Kelly and co-workers , , who describe dynamical systems that represent TCP-type rate control algorithms and establish stability and fairness properties by showing that, with an appropriate formulation of a
8640	2673	within the framework of the traditionally considered traffic models. An example of such an emergent phenomenon was the discovery of the self-similar or fractal nature of network traffic , , . The focus of this paper is the observed scaling behavior of network traffic, and the use of dynamical systems models to understand how feedback mechanisms affect it. In particular, we survey
8640	8677	In another engineering sense, the presence of self-similarity could have been termed “good news.” The large deviations principles that underpins the Norros’ formula, and its refinements , , , directly relate the traffic characteristics (e.g., distribution of arrival counts) to performance measures (e.g., queue length distributions, loss rates). They indicate that to obtain the
8640	5723	effects, although we allow hybrid deterministic/stochastic approaches. We now provide a short, and necessarily incomplete, survey of this work. There are a number of publications , , ,  which have demonstrated that under suitable assumptions the average window size of a source goes as the inverse square root of the loss probability: . Although the assumptions in any
8640	8678	In another engineering sense, the presence of self-similarity could have been termed “good news.” The large deviations principles that underpins the Norros’ formula, and its refinements , , , directly relate the traffic characteristics (e.g., distribution of arrival counts) to performance measures (e.g., queue length distributions, loss rates). They indicate that to obtain the
8640	8678	of interest is the marginal distribution of the (stationary) queueing process, and the principal result is that it is asymptotically of Weibull type, that is where is a slowly varying prefactor . The slow “stretched exponential” decrease with implies a far higher loss probability than is the case in the traditional Markovian modeling context where exponentially fast decaying tails are the
8640	8679	and deployment, such as frame relay, ATM and Internet access/backbone networks . To answer this FAQ, a good starting point in understanding the impact of self-similarity was provided by Norros , who developed a formula that can be used to estimate buffer overflow probabilities at network switches and routers. The Norros results showed that the queueing backlogs were in general worse with
8640	8679	the fundamental implications of fractal traffic. We describe its main performance implications when used as an input model for simple queueing systems. The model was originally introduced by Norros  and is based on the fBm process. Begin with the stationary traffic rate process , then sum it to form the (nonstationary) process measuring the total number of bytes arriving in the interval . This
8640	8680	effects, although we allow hybrid deterministic/stochastic approaches. We now provide a short, and necessarily incomplete, survey of this work. There are a number of publications , , ,  which have demonstrated that under suitable assumptions the average window size of a source goes as the inverse square root of the loss probability: . Although the assumptions in any one case
8640	6533	effects, although we allow hybrid deterministic/stochastic approaches. We now provide a short, and necessarily incomplete, survey of this work. There are a number of publications , , ,  which have demonstrated that under suitable assumptions the average window size of a source goes as the inverse square root of the loss probability: . Although the assumptions in any one case are
8640	6533	or greedy sources we mean those which are always trying to transmit at the maximum rate the protocol will allow. There are also more complex formula that apply over a wider range of values of , or for more complex loss patterns . This body of work however, is largely restricted to coarse measures of traffic such as long term throughput rates and avoids dealing with the large-scale
8640	2679	within the framework of the traditionally considered traffic models. An example of such an emergent phenomenon was the discovery of the self-similar or fractal nature of network traffic , , . The focus of this paper is the observed scaling behavior of network traffic, and the use of dynamical systems models to understand how feedback mechanisms affect it. In particular, we survey here
8640	2679	such as arrival times and sizes or durations from packet-level measurements is straightforward. For FTP and TELNET, these entities have been shown to be consistent with Cox’s construction; see . For HTTP (i.e., Web sessions), obtaining session information is generally more involved, but the empirical evidence for the heavytailed characteristic of Web-related entities (e.g., HTTP request
8640	8686	estimator is given for with excellent statistical properties. Another key advantage of the wavelet approach is its computational complexity (memory complexity is also in an on-line implementation ), which is invaluable for analyzing the enormous data sets of network-related measurements. 3 However, even wavelet methods have their problems when applied to certain real-life or simulated
8640	8687	of interest to see that the small scale behavior (from scales 1–4) does not follow the strict linear asymptote corresponding to LRD. This is the type of midscale effect characteristic discussed in , and we attribute this at least in part to the on-time compression caused by the exponential window increase of TCP slow start. It is a topic for further research to determine the relation between
8640	4499	algorithms for practical engineering problems include 1) CPU load balancing in networks of workstations ; 2) connection scheduling in Web servers ; 3) load-sensitive routing in IP networks ; and 4) detecting certain kinds of network attacks , . Realizing that self-similarity concerns large time scales and leaves the small scale behavior essentially unspecified has recently
8640	8690	real, and wavelet methods have become the method of choice for detailed traffic analysis (see e.g., , , , ). A survey of estimation methods for the scaling exponent can be found in , , and comparisons of the wavelet estimator against others is given in  (see also ), where a joint wavelet estimator is given for with excellent statistical properties. Another key
8640	8691	setting where the traces were collected. For some traditional open loop queueing models, the effects of such variability have been shown to be strongly negative . In each of , , , , unconventional but nevertheless open-loop scaling models were investigated to describe the observed type of variability, which can be thought of as local irregularity. Indeed, when looking
8640	8692	analysis (see e.g., , , , ). A survey of estimation methods for the scaling exponent can be found in , , and comparisons of the wavelet estimator against others is given in  (see also ), where a joint wavelet estimator is given for with excellent statistical properties. Another key advantage of the wavelet approach is its computational complexity (memory complexity
8640	8692	is that the estimation is heavily weighted toward the smaller scales, where there is much more data. For further details on the method, its use and robustness properties, we refer the reader to , . C. Impact of Large-Time Scaling on Performance Performance studies of networks, whether they be analytic or simulation based, as well as the interpretation of traffic measurements themselves,
8640	8693	kinds of nonstationarities. It is now widely accepted that scaling in traffic is real, and wavelet methods have become the method of choice for detailed traffic analysis (see e.g., , , , ). A survey of estimation methods for the scaling exponent can be found in , , and comparisons of the wavelet estimator against others is given in  (see also ), where a joint
8640	8693	setting where the traces were collected. For some traditional open loop queueing models, the effects of such variability have been shown to be strongly negative . In each of , , , , unconventional but nevertheless open-loop scaling models were investigated to describe the observed type of variability, which can be thought of as local irregularity. Indeed, when looking at
8640	8693	a single parameter, the scaling seen over small scales seems to be more consistent with richer scaling models, such as Multifractals, where an entire spectrum of exponents are required; in ,  an even richer class, Infinitely Divisible Cascades, was proposed. The case for richer scaling models is a topic of current research. It remains inconclusive, partly because the statistical
8640	8695	to all open loop models, self-similar or not. On the other hand, there are other studies which claim that TCP mechanisms can create and propagate the self-similarity in network traffic ,  (for an earlier discussion concerning protocol mechanisms and self-similarity, see ). We discuss the existing confusion over this issue and the resulting dilemma in more detail in Section II-F.
8640	8695	loop.” In particular, as demonstrated in  and , alternative explanations that rely either on phase transition arguments from statistical mechanics , or on network protocols such as TCP , fall well short of satisfying this crucial “closing the loop” requirement. The ability to explain the observed self-similarity phenomenon of aggregate traffic in terms of the high variability
8640	8695	realistic loss and networking assumptions, and if it can provide a convincing mechanism for temporal scaling. A dynamical systems approach, in the sense of chaos theory, has also been attempted. In , it is shown (using simulation) that TCP dynamics, at least in the case of a simple network with small numbers of greedy sources and small buffers, is capable of generating very complex chaos-like
8640	8695	the chaos. Furthermore in real networks large buffers are the norm. Note that the dynamical systems approach we employ in the next two sections is totally different, in a way opposite, to that of . We define a traffic source to have chaotic features, and use it to show how it can impact—using the terminology introduced in Section II-A—the “size” of LRD behavior, and its onset scale, but not
8640	8695	to the extent that fBm queueing predictions do not hold (e.g., ; but see also ); to other studies that claim that self-similarity can in fact be generated by such feedback , . Note that any network scenario in which flow controls substantially alter source behavior violates condition (3) for the validity of the fBm model (which is consistent with the observations of
8640	3039	self-similar traffic modeling and have given rise to new insights and physical understanding of the effects of LRD on the design, management and performance of modern data networks , , , , . On the one hand, the fact that we can explain self-similar scaling in terms of the statistical properties of the individual sessions or connections that make up the aggregate link
8640	8698	attention to the radical changes that have resulted from a gradual improvement of our understanding of the causes and origins underlying the self-similarity phenomenon , , , ???, . This work motivates the development of innovative new approaches for modeling and evaluating the performance of large-scale complex communication networks such as today’s Internet. As an
8640	8698	behavior and can be shown to converge to fGn. For details about Mandelbrot’s construction, its relationship with Cox’s construction, and proofs and generalizations of the basic results we refer to . The beauty of structural models such as Cox’s or Mandelbrot’s construction is that in stark contrast to the conventional black box models mentioned earlier, they not only explain the
8640	8699	but the empirical evidence for the heavytailed characteristic of Web-related entities (e.g., HTTP request sizes and durations) has been well-established to date; see for example , , . Typically, these empirical studies rely on more or less sophisticated methods for inferring heavy-tailed behavior, including simple – plots of versus (the so-called complementary cumulative
8640	8700	traffic modeling and have given rise to new insights and physical understanding of the effects of LRD on the design, management and performance of modern data networks , , , , . On the one hand, the fact that we can explain self-similar scaling in terms of the statistical properties of the individual sessions or connections that make up the aggregate link traffic
8640	8700	at the application layer that causes the aggregate traffic at the IP layer to exhibit self-similar scaling. A closely related earlier construction, originally due to Mandelbrot  (see also , ), relies on the notion of an on–off process (or, more generally, a renewal-reward process), but uses the same basic ingredient of heavy-tailedness to explain the self-similarity property of the
8640	8702	CPU load balancing in networks of workstations ; 2) connection scheduling in Web servers ; 3) load-sensitive routing in IP networks ; and 4) detecting certain kinds of network attacks , . Realizing that self-similarity concerns large time scales and leaves the small scale behavior essentially unspecified has recently motivated researchers to renew their investigations into
8640	9187	load balancing in networks of workstations ; 2) connection scheduling in Web servers ; 3) load-sensitive routing in IP networks ; and 4) detecting certain kinds of network attacks , . Realizing that self-similarity concerns large time scales and leaves the small scale behavior essentially unspecified has recently motivated researchers to renew their investigations into the
8640	8703	result in highly unreliable estimates. 806 PROCEEDINGS OF THE IEEE, VOL. 90, NO. 5, MAY 2002sto come—assuming the way humans tend to organize information will not change drastically in the future . This observation has resulted in a substantial body of literature on performance modeling with heavy-tailed input models, the main driver being the unexpected finding that on–off-type sources with
8640	3027	traffic modeling and have given rise to new insights and physical understanding of the effects of LRD on the design, management and performance of modern data networks , , , , . On the one hand, the fact that we can explain self-similar scaling in terms of the statistical properties of the individual sessions or connections that make up the aggregate link traffic suggests
8640	3027	congestion, and so on . The key point is that TCP traffic is shaped by current networking conditions, resulting in TCP traffic in turn dynamically adapting to changing network conditions , . The above observation, and the fact that many of the measured traffic traces which show strong scaling behavior are derived from networks in which TCP is dominant, has led researchers to a
8640	3027	of nonstationarities. It is now widely accepted that scaling in traffic is real, and wavelet methods have become the method of choice for detailed traffic analysis (see e.g., , , , ). A survey of estimation methods for the scaling exponent can be found in , , and comparisons of the wavelet estimator against others is given in  (see also ), where a joint wavelet
8718103	8714	that provides visual interactive modeling (VIM) capabilities for hierarchical discrete event simulation (DES) models. HiMASS models use the Hierarchical Control Flow Graph (HCFG) Model paradigm (Fritz and Sargent 1995, Sargent 1997.) The other application, HiMASS Engine, provides implementation of simulation algorithms for HCFG models. Section 2 gives a brief overview of the HCFG Model paradigm, a hierarchical
8718103	8715	interactive modeling (VIM) capabilities for hierarchical discrete event simulation (DES) models. HiMASS models use the Hierarchical Control Flow Graph (HCFG) Model paradigm (Fritz and Sargent 1995, Sargent 1997.) The other application, HiMASS Engine, provides implementation of simulation algorithms for HCFG models. Section 2 gives a brief overview of the HCFG Model paradigm, a hierarchical model paradigm
8718103	8724	University (Gan et al. 2000) and the Georgia Institute of Technology (Julka et al. 2002), and also by the Manufacturing Engineering Laboratory of the National Institute of Standard and Technology (McLean and Riddick, 2000). Depending on the challenges to be tackled, two alternative implementation approaches for distributed supply chain simulation can be applied: Simulation models can be developed from scratch,
8731	8734	excessive number of students to try to achieve this threshold level to become eligible for the employment guarantee, it is not surprising that the greatest labor market distortion occurs there (see Assaad 1997b). 5 Radwan (1997) aptly characterizes the evolution of the Egyptian labor market from the 1960s to the 1990s as follows. The 1960s and 1970s saw a transition from a Lewis-type labor surplus
8731	8736	agricultural labor market, and agricultural wages, in particular, to labor supply shocks caused by the reversal of international migration flows as oil prices collapsed after 1986 (Richards 1994; Datt and Olmsted 1998). With the exception of Radwan and Lee (1986) and Commander (1987), studies on agricultural labor markets in Egypt have relied on aggregate time series data on agricultural wages and employment.
8688596	8776	u ?x | u ?y | R ? ?x ?y | R. The term ?x ?y is called an explicit fusion. It has delayed substitutive effect on the rest of the term R. In this respect it is similar to explicit substitutions . As an example, in ux| vy| u v, the atom on u may be renamed to v. This yields 5svx| vy| u v. In contrast to Parrow’s trios (which send the entire environment to every continuation), explicit
8688596	8777	to the heating of a term (a directed implementation of structural congruence). In this respect, the fusion machine is like a distributed 1sversion of the channel machine first described by Cardelli  and later used in Pict . In the fusion machine, rendezvous can result in explicit fusions, namely equational concurrent constraints on names. Upon heating, these give rise to forwarders
8688596	8778	there have been only two distributed implementations of it: Facile  which integrates it with the lambda calculus; and an encoding into the join calculus  which is then implemented on Jocaml . Other implementations  have not used the pi calculus for distributed interaction, for two reasons. First, synchronous rendezvous (as found in the pi calculus) seemed awkward to implement.
8688596	8780	distributed implementations of it: Facile  which integrates it with the lambda calculus; and an encoding into the join calculus  which is then implemented on Jocaml . Other implementations  have not used the pi calculus for distributed interaction, for two reasons. First, synchronous rendezvous (as found in the pi calculus) seemed awkward to implement. Second, the pi calculus has no
8688596	8781	encoding—several pi-like calculi and (following ) some concurrent constraint calculi. In particular, it implements the synchronous and asynchronous pi calculus, and the explicit fusion calculus . Because the fusion machine is so close to the calculi, strong congruence results are possible and easy to prove. We believe that the machine can be adapted to provide a distributed implementation
8688596	8781	at u must look in the global environment to find sufficient names (|?z|) before it can allow reaction. Instead, we implement the solos calculus with the explicit fusions of Gardner and Wischik . This allows local reaction as follows: u ?x | u ?y | R ? ?x ?y | R. The term ?x ?y is called an explicit fusion. It has delayed substitutive effect on the rest of the term R. In this respect it is
8688596	8783	generated by ?x = ?y has exactly one element not in ?z, and the substitution ? collapses each equivalence class to its one element. A single-processor implementation of solos has been described . However, it seems difficult to make a distributed implementation. This is because its reaction is not local: the channel-manager at u must look in the global environment to find sufficient names
8688596	8783	implementation would be substantially the same as those for guarded replication !inx.P . 7sParrow’s trios show how the size of a guarded replication !inx.P can be kept small . A similar result  applies to solo replication !(inx|P ). 4 Fusion machine theory We now develop a formalism for the fusion machine. We use this to prove that it is a fully abstract implementation of the explicit
8688596	8784	model for other pi-like calculi as well, and to evaluate the efficiency of encodings and highlight problems. For instance, we find that the fusion calculus  and the solos calculus  are difficult to implement because they only allow reaction after a global search for restricted names. They can be seen as ‘lazy’ models for the fusion machine. Using the machine as an
8688596	8784	calculus is as expressive as the full calculus with continuations. This is provided by an encoding which is uniform and strongly barbed congruent. The encoding has been inspired by earlier works in . We also introduce a formal technique to argue about efficiency in the machine, in terms of the number of network messages required to execute a program. As an example, we quantify the efficiency
8688596	8784	entire environment to every continuation. The second encoding is based upon the fusion calculus of Parrow and Victor , a calculus in which the input command u?y.P is not binding. The encoding  uses the sub-calculus with only solos u ?x and u ?x. It uses the reaction relation (?z)(u ?x | u?y | R) ? R? where every equivalence class generated by ?x = ?y has exactly one element not in ?z,
8688596	8784	of the full hybrid machine may be found in . Also, the efficiency results in Section 5 refer to the hybrid machine. It is possible to implement replication without continuations, as shown in . First, encode the guarded replication !u(x).P as !(?z)(u ?x | P ? ), where u ?x is the only unrestricted solo. Then remove the structural rule !P ? P |!P and deal with reaction by means of ad-hoc
8688596	8784	compiler can encode a term with nested continuations into one without. This section describes our encoding and discusses its efficiency. 13sTwo different encodings have been given previously . The distinguishing features of ours are that it is a strong congruence rather than just preserving weak congruence, it is uniform, and it uses co-location for increased efficiency. As an example,
8688596	8786	to provide a distributed implementation model for other pi-like calculi as well, and to evaluate the efficiency of encodings and highlight problems. For instance, we find that the fusion calculus  and the solos calculus  are difficult to implement because they only allow reaction after a global search for restricted names. They can be seen as ‘lazy’ models for the fusion machine. Using
8688596	8786	distributed channel machine. Note that this encoding amounts to transporting the entire environment to every continuation. The second encoding is based upon the fusion calculus of Parrow and Victor , a calculus in which the input command u?y.P is not binding. The encoding  uses the sub-calculus with only solos u ?x and u ?x. It uses the reaction relation (?z)(u ?x | u?y | R) ? R? where
8688596	8787	calculus is as expressive as the full calculus with continuations. This is provided by an encoding which is uniform and strongly barbed congruent. The encoding has been inspired by earlier works in . We also introduce a formal technique to argue about efficiency in the machine, in terms of the number of network messages required to execute a program. As an example, we quantify the efficiency
8688596	8787	P is large. There have been two encodings of the pi calculus into a limited calculus without nested continuations. These might solve the efficiency problem. The first encoding, by Parrow , uses a sub-calculus of the pi calculus which uses only trios u(?x).v(y).wy?x and u(?x).vy.w ?x. An encoded term could then be executed directly on the distributed channel machine. Note that this
8688596	8787	of atom !(inx|P ). Its implementation would be substantially the same as those for guarded replication !inx.P . 7sParrow’s trios show how the size of a guarded replication !inx.P can be kept small . A similar result  applies to solo replication !(inx|P ). 4 Fusion machine theory We now develop a formalism for the fusion machine. We use this to prove that it is a fully abstract
8688596	8787	compiler can encode a term with nested continuations into one without. This section describes our encoding and discusses its efficiency. 13sTwo different encodings have been given previously . The distinguishing features of ours are that it is a strong congruence rather than just preserving weak congruence, it is uniform, and it uses co-location for increased efficiency. As an example,
8688596	8788	directed implementation of structural congruence). In this respect, the fusion machine is like a distributed 1sversion of the channel machine first described by Cardelli  and later used in Pict . In the fusion machine, rendezvous can result in explicit fusions, namely equational concurrent constraints on names. Upon heating, these give rise to forwarders between channels. The fusion
8688596	8789	description of the channel machine anticipated the pi calculus by several years.) We provide the proof in the appendix. Sewell has given a weaker result for the version of the machine used in Pict . We remark that the full abstraction result for the join calculus is weaker than Theorem 1. This is because the join calculus encodes each pi channel with two join calculus channels that obey a
8688596	6782	directed implementation of structural congruence). In this respect, the fusion machine is like a distributed 1sversion of the channel machine first described by Cardelli  and later used in Pict . In the fusion machine, rendezvous can result in explicit fusions, namely equational concurrent constraints on names. Upon heating, these give rise to forwarders between channels. The fusion
8688596	8791	granularity and uses the same form of interaction as the pi calculus. The fusion machine is a flexible machine able to implement directly—without encoding—several pi-like calculi and (following ) some concurrent constraint calculi. In particular, it implements the synchronous and asynchronous pi calculus, and the explicit fusion calculus . Because the fusion machine is so close to the
8688596	8792	are largely unrelated. Therefore, for simplicity, our formal treatment of the machine (next section) omits replication and continuations. A formal account of the full hybrid machine may be found in . Also, the efficiency results in Section 5 refer to the hybrid machine. It is possible to implement replication without continuations, as shown in . First, encode the guarded replication !u(x).P
8688596	8792	? calc N if and only if M · ? N. The first part of the theorem is just a special case of this, since calc x ? P . As for the pi calculus result, we refer to Corollary 66 and Proposition 101 of . Together these provide a translation from the pi calculus into the explicit solos calculus which preserves strong barbed bisimulation. ? We now consider behavioural congruence. In this paper, our
8688596	8792	For instance, !u ?x.P ? !u ?x.(flat P ). Therefore, an optimising compiler can locally encode any part of a program, without needing to encode it all. The proof is substantial; it may be found in . Theorem 13 If x takes n inter-location messages to evolve to M ? in the fusion machine with continuations, then x need take no more than 2n inter-location messages in the machine
8688596	8794	distributed implementations of it: Facile  which integrates it with the lambda calculus; and an encoding into the join calculus  which is then implemented on Jocaml . Other implementations  have not used the pi calculus for distributed interaction, for two reasons. First, synchronous rendezvous (as found in the pi calculus) seemed awkward to implement. Second, the pi calculus has no
8795	8800	immersion is debated  but is clearly an important aspect of games and game design. METHOD This study has involved looking at four of the eight game genres defined by Rollings and Adams . While evaluating illumination in computer games we have focused on action, adventure, role-playing and strategy games and have chosen not to regard sports and vehicle games as well as construction
8795	8800	of an adventure games remains to be exploring the environment and manipulating items and/or people. The other important elements are the setting, emotional tone, interaction model, and perspective . The setting and emotional tone are in our case the elements of importance when researching the significance of artificial illumination in adventure games. Rollings and Adams (2003:447) state that
8795	8800	and first person shooter (“FPS”) is today the most popular action game type. Action games are in general simple, the most common task in all FPSs is to kill your opponents and avoid ending up dead . The illumination in an action game helps pump up the adrenaline, for instance, when sneaking among enemies in a dark place, which resurrects the player’s primitive hunting instinct. Often, the
8795	8800	the role-playing genre often take place in either a fantasy or a sci-fi context, since one of the main purposes of the game is for the player to flee into different worlds taking on different roles .sThe role-playing games genre includes games such as Blizzard’s Diablo and BioWare’s Baldur’s Gate and Neverwinter Nights. The latter uses light in the visual representation of magic. The large
8795	8800	and works in a similar manner as the board games. As most games in this genre have complex mechanics it is important to make the design organized and structured so as not to confuse the players . As an example of a strategy game we used Blizzard’s Warcraft III. The 3d environment offers a lot of lighting possibilities, such as shadows from trees, buildings and characters, but the
8801	8802	data request message and acknowledgement message) is set to be 1 KByte. The access probability of data objects follows a Zipf distribution, which is widely adopted as a model for real Web traces . The parameter of the Zipf distribution is set to be 1.1 with a reference to the analyses of real Web traces . Since small objects are much more frequently accessed than large ones ,
8801	8803	clients, and as a result, attracts much research attention, including cache replacement schemes , system architectures  and proxy collaboration . In recent years, data broadcast  has been employed as an important technique to design a power conservation, high scalability and high bandwidth utilization mobile information system. However, most research works in transcoding
8801	8804	clients, and as a result, attracts much research attention, including cache replacement schemes , system architectures  and proxy collaboration . In recent years, data broadcast  has been employed as an important technique to design a power conservation, high scalability and high bandwidth utilization mobile information system. However, most research works in transcoding
8801	8805	query, route guidance and so on. To provide such services, researchers have encountered and are endeavoring to overcome challenges in various research areas including mobile data management, wireless network infrastructure, location-dependent data management , pervasive computing, and so on. In a pervasive computing environment , due to the constraints resulting from
8801	175	data request message and acknowledgement message) is set to be 1 KByte. The access probability of data objects follows a Zipf distribution, which is widely adopted as a model for real Web traces . The parameter of the Zipf distribution is set to be 1.1 with a reference to the analyses of real Web traces . Since small objects are much more frequently accessed than large ones , we
8801	8807	content providers and users, this kind of approaches is able to simplify the design of servers and clients, and as a result, attracts much research attention, including cache replacement schemes , system architectures  and proxy collaboration . In recent years, data broadcast  has been employed as an important technique to design a power conservation, high scalability
8801	8807	of data objects among different versions according to the transformation requests of the cache manager. Since the design of the back-end is similar to the systems proposed in some prior works , we focus in this paper on the design of the front-end to provide scalable and Server Server QoS-aware transcoding proxy services. C. Signalling Procedures Before using the transcoding
8801	8807	follow an exponential distribution with mean 2 seconds . The values of W1 and W2 (i.e., the QoS requirement) are set to be two seconds and five seconds, respectively. In the client model, as in  and , we assume that the mobile clients can be classified into five device profiles, and Profile Viewable version set P1 {2, 1} P2 {4, 3, 2, 1} P3 {6, 5} P4 {8, 7, 6, 5} P5 {10, 9, 8, 7, 6, 5}
8801	8807	can be transcoded into a less detailed one and the transcoding delay is determined as the quotient of the object size to the transcoding rate. The transcoding rate is set to be 30 KBytes/sec . The number of users in the network is set to be 1000. The cell residence time, service holding time and service establishing time for each user are set to be exponential distributions with means
8801	8808	content providers and users, this kind of approaches is able to simplify the design of servers and clients, and as a result, attracts much research attention, including cache replacement schemes , system architectures  and proxy collaboration . In recent years, data broadcast  has been employed as an important technique to design a power conservation, high scalability and
8801	8808	of data objects among different versions according to the transformation requests of the cache manager. Since the design of the back-end is similar to the systems proposed in some prior works , we focus in this paper on the design of the front-end to provide scalable and Server Server QoS-aware transcoding proxy services. C. Signalling Procedures Before using the transcoding
8801	8808	dist. with µ = one hour Transcoding Proxy Fig. 8. The simulation topology TABLE II DEFAULT SYSTEM PARAMETERS LWF (standing for Longest Wait First) as the underlying scheduling algorithm. Scheme AE  is employed as the cache replacement policy since it outperforms the other replacement policies for transcoding proxies. Each cell provides one control channel and one download channel with network
8801	8808	size and its access probability. The default capacity of the cache is set to be 0.01 × ? object size and the fetch delays of data objects follow an exponential distribution with mean 2 seconds . The values of W1 and W2 (i.e., the QoS requirement) are set to be two seconds and five seconds, respectively. In the client model, as in  and , we assume that the mobile clients can be
8801	8808	all objects could be transcoded into ten versions, and the sizes of the ten versions (from version one to version ten) are assumed to be 10%, 20%, 30%, ··· and 100% of the original object sizes . The viewable version set for each device profile is shown in Table III. By a reference to , we assume that a more detailed version can be transcoded into a less detailed one and the transcoding
8801	8811	data formats) and computation power makes the design of mobile information systems more challenging. This diversity also results in an increasing demand on the capability of context awareness  for mobile information systems. Content adaptation, which is an important technique to realize context awareness, emerges to remedy the problem resulting from the said diversity by offering the
8801	8813	a Zipf distribution, which is widely adopted as a model for real Web traces . The parameter of the Zipf distribution is set to be 1.1 with a reference to the analyses of real Web traces . Since small objects are much more frequently accessed than large ones , we assume that there is a negative correlation between the object size and its access probability. The default capacity
8801	8814	research areas including mobile data management, wireless network infrastructure, location-dependent data management , pervasive computing, and so on. In a pervasive computing environment , due to the constraints resulting from power-limited mobile devices and lowbandwidth wireless networks, designing a power conservation, high scalability and high bandwidth utilization mobile
8801	8815	kind of approaches is able to simplify the design of servers and clients, and as a result, attracts much research attention, including cache replacement schemes , system architectures  and proxy collaboration . In recent years, data broadcast  has been employed as an important technique to design a power conservation, high scalability and high bandwidth utilization
8801	8815	objects among different versions according to the transformation requests of the cache manager. Since the design of the back-end is similar to the systems proposed in some prior works , we focus in this paper on the design of the front-end to provide scalable and Server Server QoS-aware transcoding proxy services. C. Signalling Procedures Before using the transcoding proxy, a
8816	8817	future students, a study is being undertaken to observe current students' use of their Pocket PCs. In accordance with Jameson's call for combining research in context-awareness and user modelling , the study aims to identify whether there are sufficient patterns and differences in Pocket PC use with reference to activity and location, to suggest a role for user modelling in this setting. A
8816	8818	which can be consulted away from the tutoring system in which it was generated. The educational benefit of open learner models to promote reflection has been suggested in the desktop PC context , but has not yet been considered for mobile learning. In a mobile environment an open learner model may be even more useful as, similar to the way in which mobile learning materials may be used for
8816	8819	which can be consulted away from the tutoring system in which it was generated. The educational benefit of open learner models to promote reflection has been suggested in the desktop PC context , but has not yet been considered for mobile learning. In a mobile environment an open learner model may be even more useful as, similar to the way in which mobile learning materials may be used for
8816	8822	based on a description of proposed software, the positive response does indicate that it might be worth investigating further. An intelligent tutoring system is therefore being designed (see  for an early version). Users will be able to interact with teaching materials followed by diagnostic multiple choice tests, the results of which will be used to update the learner model. Two
6104556	8836	structure; the Quad-Edge structure; and, for specific applications, the triangulation structure. Worboys (1995) gives a good summary. One way to look at the many possibilities is the PAN-graph (Gold, 1988). Here each entity in the graph (Polygons, Arcs, Nodes: equivalent to Regions, Edges, Nodes) may be connected together in various ways, indicating the pointer structure between adjacent graph
6104556	8838	that can be represented as a graph, possibly stored as Quad-edges, and can be updated using local operations. Nevertheless, the ideas were organized with the Voronoi structure in mind, (for example Gold and Condal, 1995), as it has been shown to be maintainable by local update, can handle time-varying information, and can readily be generated on the sphere. Thus the move towards a global GIS would require a
6104556	8843	the destination point on the “outside” edge, and then moving to that triangle and repeating the process, until the enclosing triangle is found (Gold et al., 1977). The Quad-edge data structure (Guibas and Stolfi, 1985) allows similar navigation, even though all navigation is from edge to edge only. Its advantages are firstly that there is no distinction between the primal and the dual representation (it is
3278	8724	University (Gan et al., 2000) and Georgia Institute of Technology (Julka et al., 2002), and also by the Manufacturing Engineering Laboratory of National Institute of Standard and Technology (McLean and Riddick, 2000). Both groups are leveraging on the High Level Architecture (HLA) as a general-purpose architecture for simulation reuse and interoperability (Kuhl et al., 1999). In conventional approaches,
8919803	8858	a Maximum Likelihood estimator can be formulated that simultaneously estimates the camera parameters and the 3D positions of feature points. This joint optimization is called bundle adjustment . If the errors in the positions of the detected feature points obey a Gaussian distribution, the Maximum Likelihood estimator has to minimize a nonlinear least squares cost function. In this case,
8919818	8919819	activations of a pitch memory task. The use of a sparse temporal sampling technique enabled us to overcome some of the interference between scanner noise and auditory tasks (Hall et al., 1999; Belin et al., 1999; Edmister et al., 1999; Shah et al., 2000). The majority of previous studies using a sparse temporal sampling technique only examined responses in primary and secondary auditory areas, most
8919818	8904	lobule is an important nodal point for integrating multimodal sensory information and for providing guidance to motor operations through intense reciprocal connections with the premotor cortex (Friedman and Goldman-Rakic, 1994; Bushara et al., 1999). Thus, requiring a motor response to an auditory cue could involve the superior parietal lobule. Neuropsychological and functional imaging evidence also suggests that the
8919818	8910	that the parietal lobe, in particular the right parietal lobe, is a key part of a larger network involved in auditory spatial and attentional functions (Clarke et al., 2000; Bushara et al., 1999; Griffiths et al., 2000; Weeks et al., 1999). Satoh et al., (2001) compared a harmony listening condition with a more specific alto-part-listening condition and found bilateral increases in the superior parietal lobules
8919818	8920	task was surprising, although in agreement with reports indicating that the cerebellum has non-motor functions (Bower, 1997; Jueptner et al., 1997; Parsons and Fox, 1997; Desmond et al., 1998; Penhune et al., 1998; Satoh et al., 2001; Schmahmann and Sherman, 1998). Several studies have shown now an involvement of the cerebellum in auditory tasks such as the planning of speech production (Silveri et al.,
8919818	8922	attention and the analysis of pitch information on a mental score is supported by other work showing that selective auditory attention led to significant activation of the superior parietal lobe (Pugh et al., 1996). Of interest is that better performance in the pitch memory task was associated with less activation of the left superior parietal lobule and of a region in the temporo– occipital junction. This
8919818	8931	with reports indicating that the cerebellum has non-motor functions (Bower, 1997; Jueptner et al., 1997; Parsons and Fox, 1997; Desmond et al., 1998; Penhune et al., 1998; Satoh et al., 2001; Schmahmann and Sherman, 1998). Several studies have shown now an involvement of the cerebellum in auditory tasks such as the planning of speech production (Silveri et al., 1998), auditory verbal memory function (Grasby et al.,
8919818	8932	the planum temporale (for definition see Steinmetz et al., 1991, and Schlaug et al., 1995) representing auditory association cortex, the supramarginal gyrus, and lobules V and VI of the cerebellum (Schmahmann et al., 1999). The relationship between performance (percentage correct responses) and task-related activations was examined by weighting the parameter estimates with the performance data of each subject which
8919818	8919829	use of a sparse temporal sampling technique enabled us to overcome some of the interference between scanner noise and auditory tasks (Hall et al., 1999; Belin et al., 1999; Edmister et al., 1999; Shah et al., 2000). The majority of previous studies using a sparse temporal sampling technique only examined responses in primary and secondary auditory areas, most typically using short stimuli such as single
8919818	8942	to its global intensity. We contrasted the pitch memory task with the motor control task for these three clusters of ITP and applied a threshold of P ? 0.05, corrected for multiple comparisons (Worsley et al., 1996). In addition, we also contrasted these imaging clusters with each other using the contrast images to determine significant changes in the time course of the activation. Low-frequency drifts were
8919818	8946	SMG remained active for a longer period of time. The strong leftward activation pattern is somewhat different from other studies showing a more rightward activation pattern in pitch memory tasks (Zatorre et al., 1994; Griffiths et al., 1999). It is not very likely that our subjects were able to verbally encode the target tone, since none of them possessed absolute pitch and it would be very difficult for a
8919818	8946	on both sides. Although neuroimaging studies have shown the involvement of the inferior frontal lobes in auditory processing and auditory working memory (Chao and Knight, 1996; Griffiths, 2001; Zatorre et al., 1994), the role that these regions play in auditory processing remains largely unclear. Several physiological studies of the frontal lobe in nonhuman primates have focused on auditory spatial processing
3596973	8951	2.1 Accessibility of the instrument What would an instrument look like if it had to fulfil the above mentioned conditions? Trying to serve the secondspurpose, developments presented by Mathews , Machover , Young , Trueman  or Nichols  would not fit. The accessibility would be far to small because of the technical know-how needed to implement these devices. Commissions to a
3596973	8952	by reading a paper or a test report. Therefore the instruments have to be available before they are bought or built by the musician. Regarding these requirements the work done by Jehan and Schoner  offers an interesting possibility. They present a sound synthesis that is audio-driven. An electric violin, a laptop for example and demo software would fulfil the complete needs. Unfortunately
3596973	8952	and  Nichols. According to their results, the implementation of haptical feedback into an interface for bowed instruments will increase playability of the system. The instrument offered by Jehan  would have this haptical feedback since it uses the traditional construction of stringed instruments. Also the hyperbow  and the vBow  offer the haptical feedback, however, a pizzicato will
3596973	8952	focus of the string instrument interface has to be set on the string, its modulation by the player and the immediate resonance of the body. A problem in the string-specific playability of Jehahns  instrument can be seen in the latency and mistakes that may be expected by using the pitch, noise and brightness follower. There would obviously be a lack of representation of sounds having the
3596973	8958	to include electronic sounds into her or his musical expression? Usually the traditional string player has no opportunity to get access to hardware like the developments presented by  Young,  Trueman or  Goudeseune. If one cannot manufacture the mentioned instruments oneself one has to deal with systems offered by the commercial market. The essential possibilities there are Zeta midi
3596973	8958	What would an instrument look like if it had to fulfil the above mentioned conditions? Trying to serve the secondspurpose, developments presented by Mathews , Machover , Young , Trueman  or Nichols  would not fit. The accessibility would be far to small because of the technical know-how needed to implement these devices. Commissions to a company would be expensive and would not
3596973	8960	she/he wants to include electronic sounds into her or his musical expression? Usually the traditional string player has no opportunity to get access to hardware like the developments presented by  Young,  Trueman or  Goudeseune. If one cannot manufacture the mentioned instruments oneself one has to deal with systems offered by the commercial market. The essential possibilities there
3596973	8960	the instrument What would an instrument look like if it had to fulfil the above mentioned conditions? Trying to serve the secondspurpose, developments presented by Mathews , Machover , Young , Trueman  or Nichols  would not fit. The accessibility would be far to small because of the technical know-how needed to implement these devices. Commissions to a company would be
3596973	8960	will increase playability of the system. The instrument offered by Jehan  would have this haptical feedback since it uses the traditional construction of stringed instruments. Also the hyperbow  and the vBow  offer the haptical feedback, however, a pizzicato will cause a problem since these systems track the bow, not the string. Regarding string-specific playability the author believes
8919840	800	and robust data transport mechanisms, for example when moving a large dataset to or from a remote compute server, or when fetching data from several locations for the purpose of integration . GridFTP  has become the de facto standard for moving data in many Grid projects. However, GridFTP does not offer a web service interface. In addition, GridFTP requires that the client
8919840	5710	data transport mechanisms, for example when moving a large dataset to or from a remote compute server, or when fetching data from several locations for the purpose of integration . GridFTP  has become the de facto standard for moving data in many Grid projects. However, GridFTP does not offer a web service interface. In addition, GridFTP requires that the client driving a remote
8919840	8971	service for compute jobs, and provides reliability even in the face of local failure. Within the Globus Alliance , we chose to resolve these issues by developing a Reliable File Transfer  (RFT) service. RFT is available in the Globus Toolkit (GT) V3.0 and V3.2 as an OGSI  compliant service, and will be available in GT4.0 as a WSRF  compliant service. (For simplicity, we
8973	8974	approach are compared in . Their experiments show that the tree-based multicast consistently outperforms the flooding approach. The “NICE is the Internet Cooperative Environment” (NICE) protocol  builds and maintains hierarchical topology of multicast members. The multicast routes are implicitly defined by the structure of the hierarchy. A protocol that uses Delaunay triangulation as an
8973	8975	end users require the content delivered to them in different forms, wrapped in different services, depending on their personal preferences, end-system capabilities and network connectivity . For example, one user requires an encrypted version of the content, while another user wants the audio portion of the content in a different language, and a third user wants an encrypted and
8973	8976	in . With the distributed construction of a Delaunay triangulation, multicast paths are embeddedsin the overlay without a routing protocol. Overlay Multicast Network Infrastructure (OMNI)  proposes a two-tier architecture and builds a multicast tree consisting of multicast service nodes (MSN) which in turn connect to clients. This distributed scheme is adaptive with changes in the
8973	7172	application packets from both until the handoff is complete. 3. Experimental Results To evaluate our algorithms, we conduct a simulation study on two transit-stub topologies produced by GTITM , both with approximately 10,000 nodes. The first topology, small-transit, has 25 transit domains, five transit nodes per transit domain, four stub domains attached to each transit node, and 20
8973	8977	a mesh topology of all multicast members, and then compute a multicast spanning tree for each source. Both protocols periodically refresh the mesh to maintain the multicast topology. SplitStream  addresses the problem of load imbalance of interior and leaf nodes in a multicast tree. It constructs a forest, rather than a single tree. The content is partitioned into multiple stripes using
8973	8978	trees offer. 4. Related Work 4.1. Overlay Multicast Trees Several application-level multicast schemes achieve data distribution by implicitly building a multicast structure. For instance, Scribe  is a multicast infrastructure built on top of Pastry . In Scribe, the multicast tree is formed by the union of the Pastry routes from multicast members to the rendezvous point (RP). Bayeux
8973	7897	a mini-CAN and multicast data is distributed by flooding over the mini-CAN, without explicitly building a tree. The Scribe’s tree-based approach and CAN’s flood-based approach are compared in . Their experiments show that the tree-based multicast consistently outperforms the flooding approach. The “NICE is the Internet Cooperative Environment” (NICE) protocol  builds and maintains
8973	7895	tree construction and maintenance. Application Level Multicast Infrastructure (ALMI)  uses a centralized approach to construct shared minimum spanning tree based on network measurements. Narada  and Scattercast  build a mesh topology of all multicast members, and then compute a multicast spanning tree for each source. Both protocols periodically refresh the mesh to maintain the
8973	8980	high-level goal of Ninja and CANS, but focuses on service virtualization by executing multiple UserMode Linux atop a unmodified host OS to achieve fault isolation. Service Overlay Networks (SON)  is pieced together via service gateways, the logical connection between which is provided by the underlying network domain with certain QoS (e.g., bandwidth) guarantees. The Internet indirection
8973	8982	physically close-by neighbors, our approach provides very fast, high quality tree construction and adaptation. Starting from a service definition, we build end-to-end service paths as defined in . As required, new service paths are builtsfrom existing “close-by” service paths, thereby creating an efficient multicast service tree. Our goal is to create highly scalable mechanisms that handle
8973	8982	to trigger tree transformations rather than the use of the periodic random subset distribution approach of SARO. 4.2. Service Paths Ninja  and Composable, Adaptive Network Services (CANS)  are typical examples of infrastructures that support heterogeneous user devices and needs. Ninja is a distributed service architecture that builds paths of composed services. Active proxies are
8973	8983	However, SAM advocates the use of application QoS feedback to trigger tree transformations rather than the use of the periodic random subset distribution approach of SARO. 4.2. Service Paths Ninja  and Composable, Adaptive Network Services (CANS)  are typical examples of infrastructures that support heterogeneous user devices and needs. Ninja is a distributed service architecture that
8973	8984	performs a link-state algorithm on the mesh to construct overlay service paths. SPY-Net requires the proxy to have global network resource availability information. QoS-aware service aggregation  composes overlay service paths by mapping user request into resource requirements (e.g., bandwidth, processor, memory, etc.). The path that satisfies these resource requirements is selected and
8973	6493	is adaptive with changes in the client distribution and network conditions. The following protocols explicitly form the multicast tree. Targeting at content distribution applications, overcast  builds a single source multicast tree rooted at the source. The optimization goal of its “up/down” protocol is to provide each tree node with a high bandwidth path to the root. Yoid  forms a
8973	8985	devices for dynamic service adaptation. CANS is very similar to Ninja, but one of the main differences is that CANS performs resource-aware service adaptation. Service On-Demand Architecture (SODA)  shares the same high-level goal of Ninja and CANS, but focuses on service virtualization by executing multiple UserMode Linux atop a unmodified host OS to achieve fault isolation. Service Overlay
8973	8987	tree is built using three given rules. The tree is periodically reconfigured to balance the load based on the node degree and capacity. The Scalable Adaptive Randomized Overlay (SARO) protocol  has been recently proposed, built on top of a Random Subsets (RanSub) utility. The RanSub utility is used to deliver state information about a random subset of global nodes with each node selected
8973	8988	request into resource requirements (e.g., bandwidth, processor, memory, etc.). The path that satisfies these resource requirements is selected and used. QoS-aware Routing in Overlay Networks (QRON)  builds QoS-satisfied hierarchical service paths using Dijkstra-like algorithms, with computational capacity and available bandwidth as the path weight. Although the above schemes consider building
8973	8989	of multicast members. The multicast routes are implicitly defined by the structure of the hierarchy. A protocol that uses Delaunay triangulation as an overlay network topology is proposed in . With the distributed construction of a Delaunay triangulation, multicast paths are embeddedsin the overlay without a routing protocol. Overlay Multicast Network Infrastructure (OMNI)  proposes
8973	8990	on providing a global view of the system stored in a distributed hash table (DHT), which is scalable, fault-tolerant, and administration-free. The global view is generated from landmark clustering . Combining the landmark information with a small number of round-trip time (RTT) measurements to locate physically close-by neighbors, our approach provides very fast, high quality tree
8973	8990	network connectivity. We represent the position of a node in the physical network using a landmark vector that is produced by measuring round-trip time against a set of well known landmark nodes . The landmark vectors of the nodes define a coordinate space with distances among the landmark vectors reflecting the distances among the corresponding nodes in the physical network. The global
8973	8990	Clustering Landmark clustering is based on the intuition that nodes close to each other are likely to have similar distances to a few selected landmark nodes. Our landmark clustering is based on , where a set of well known landmark nodes is first identified. The landmark nodes measure the RTT among themselves and use this information to compute a coordinate in a Cartesian space (i.e.,
8973	8991	Host Multicast Tree Protocol (HMTP)  builds a shared tree. When a new node joins, it probes the tree at each level, starting from the root, to find the nearest member node as a parent. CoopNet  focuses on using multiple description coding to handle flash crowd while reducing disruption. They rely on a centralized server for tree construction and maintenance. Application Level Multicast
8973	7896	description coding to handle flash crowd while reducing disruption. They rely on a centralized server for tree construction and maintenance. Application Level Multicast Infrastructure (ALMI)  uses a centralized approach to construct shared minimum spanning tree based on network measurements. Narada  and Scattercast  build a mesh topology of all multicast members, and then compute
8973	939	against the set of nodes that is returned through landmark clustering. 2.2.2. Storing and Retrieving Global State on a DHT DHT-based overlays, represented by Content Addressable Networks (CAN) , Chord , and Pastry , are scalable, fault-tolerant, and administration-free. Their basic functionality is to map “keys” onto “values.” Our DHT stores the global state and is based on CAN,
8973	939	multicast members to the rendezvous point (RP). Bayeux  is an architecture built on top of Tapestry  and supports source-specific multicast. The Content-Addressable Network (CAN) framework  is extended for multicast in . In this work, the multicast group members establish a mini-CAN and multicast data is distributed by flooding over the mini-CAN, without explicitly building a
8973	8992	as the hash keys. To take advantage of the physical network topology, we employ landmark clustering when constructing the DHT and storing the information. We build a topologically-aware CAN  where each node joins the Cartesian space owning a zone that contains its landmark vector. When storing information about a node in the multicast service tree, we compute its landmark vector in the
8973	940	that is returned through landmark clustering. 2.2.2. Storing and Retrieving Global State on a DHT DHT-based overlays, represented by Content Addressable Networks (CAN) , Chord , and Pastry , are scalable, fault-tolerant, and administration-free. Their basic functionality is to map “keys” onto “values.” Our DHT stores the global state and is based on CAN, which provides a hash table
8973	940	Several application-level multicast schemes achieve data distribution by implicitly building a multicast structure. For instance, Scribe  is a multicast infrastructure built on top of Pastry . In Scribe, the multicast tree is formed by the union of the Pastry routes from multicast members to the rendezvous point (RP). Bayeux  is an architecture built on top of Tapestry  and
8973	8994	there is a sudden high loss rate or large delay period inflicted by the handoff. To minimize disruption, we use multi-homing at the multicast overlay layer during the handoff period, similar to . The idea is to have a child connected to both the new and old parents, and receive application packets from both until the handoff is complete. 3. Experimental Results To evaluate our algorithms,
8973	8995	via service gateways, the logical connection between which is provided by the underlying network domain with certain QoS (e.g., bandwidth) guarantees. The Internet indirection infrastructure (I 3 )  introduces a level of indirection to avoid the limitations of the current point-to-point communication model of the Internet. It provides the basic primitives to enable efficient multicast, anycast
8973	943	the set of nodes that is returned through landmark clustering. 2.2.2. Storing and Retrieving Global State on a DHT DHT-based overlays, represented by Content Addressable Networks (CAN) , Chord , and Pastry , are scalable, fault-tolerant, and administration-free. Their basic functionality is to map “keys” onto “values.” Our DHT stores the global state and is based on CAN, which
8973	8996	node serves as an interior node of a tree but as a leaf of some other trees in the forest. Similar to most other DHT-based multicast schemes, the trees in the forest are embedded in the DHT. ZIGZAG  proposes a peer-to-peer multicast for streaming media based on an administrative organization in which peers are organized in a multi-layer hierarchy of clusters. Given the administrative logical
8973	8999	that (n, p) and (n, p ? ) distances are 20ms since n is close to p and p ? , and (p, DHT ) and (n, DHT ) are 100ms. Considering that routing in the DHT typically doubles the latency of IP routing , it takes approximately 320ms to obtain the candidate sets. Assume that we perform three rounds of conc f ( o) b O a ( a ) root O d c f ( o) b O a ( b ) root Figure 4. Tree maintenance. current
8973	9000	compare our algorithm against two other algorithms. The first algorithm is an abstraction of the existing level-bylevel (LBL) tree construction schemes such as Host Multicast Tree Protocol (HMTP)  and Yoid . LBL traverses down the tree level-by-level from the root to search for the node that is closest to the new node until it reaches the leaf. This algorithm does not account for the QoS
8973	9000	root. Yoid  forms a shared multicast spanning tree across the end hosts. Yoid builds a mesh structure among members for routing stability. Similar to Yoid, Host Multicast Tree Protocol (HMTP)  builds a shared tree. When a new node joins, it probes the tree at each level, starting from the root, to find the nearest member node as a parent. CoopNet  focuses on using multiple
8973	9002	is a multicast infrastructure built on top of Pastry . In Scribe, the multicast tree is formed by the union of the Pastry routes from multicast members to the rendezvous point (RP). Bayeux  is an architecture built on top of Tapestry  and supports source-specific multicast. The Content-Addressable Network (CAN) framework  is extended for multicast in . In this work, the
9003	9502	system is designed to function under the security constraints enforced by the reference monitor. In a search for a more effective solution, the Multilevel Data Management Security Summer Study  recommended three approaches to solving the multilevel database security problem. The three approaches are: integrity lock, kernelized, and replicated. The integrity lock approach, also known as
9003	9502	to investigate their performance. The study by Thuraisingham and Kamon  was based on the distributed architecture approach recommended by the Multilevel Data Management Security Summer Study . The purpose of the benchmark study was to investigate the performance of query processing algorithms in this particular implementation. The study did not investigate the performance of the
9003	9502	influence on this thesis. Section 3.3 considers the architectures for multilevel database systems that resulted from the Multilevel Data Management Security Summer Study (or Woods Hole study group) ; these include the integrity lock, kernelized, replicated, and trusted subject architectures. Section 3.4 presents a number of MLS/DBMS prototypes that have been developed over the last two
9003	9502	it involves additional security risk since more people are given the highest clearance. In order to try and resolve this problem, the Woods Hole study group, organized by the U.S. Air Force in 1982 , considered different architectures for building MLS/DBMSs. The study group proposed requirements for a purpose-built multilevel DBMS as well as solutions that looked at how existing DBMS
9003	9502	physical storage of multilevel data, three solutions became prominent; these include: the Integrity lock (or spray paint) architecture, the Kernelized architecture, and the Replicated architecture . 3.3.1 The integrity lock (or spray paint) architecture This architecture uses a single DBMS to manage all levels of data . Figure 3.1 is depicted with the four security levels introduced in
9003	9007	to investigate their performance. The study by Thuraisingham and Kamon  was based on the distributed architecture approach recommended by the Multilevel Data Management Security Summer Study . The purpose of the benchmark study was to investigate the performance of query processing algorithms in this particular implementation. The study did not investigate the performance of the
9003	9007	it involves additional security risk since more people are given the highest clearance. In order to try and resolve this problem, the Woods Hole study group, organized by the U.S. Air Force in 1982 , considered different architectures for building MLS/DBMSs. The study group proposed requirements for a purpose-built multilevel DBMS as well as solutions that looked at how existing DBMS
9003	9007	was based on a trusted database management system (TDBMS). The authors implemented and benchmarked a TDBMS based on the distributed architecture approach recommended by the Woods Hole study group . The objective of this study was to validate the security policy for the query operation and to analyze the performance of query processing algorithms in the TDBMS implementation. This was the
9003	9011	used to measure and compare the relative and quantitative performance of two or more database systems through the execution of controlled experiments . Standard benchmarks such as the Wisconsin , TPC-A , TPC-B , TPC-C , and AS 3 AP  benchmarks have been used to assess the performance of relational DBMS software. A wide variety of users have been dependent upon these
9003	9011	and AS 3 AP benchmarks are widely considered as the standard relational query benchmarks , although the AS 3 AP is also a complex mixed workload benchmark. The Wisconsin Benchmark described in  was the result of the first effort to systematically measure and compare the performance of relational database systems withsCHAPTER 3. RELATED RESEARCH AND DEVELOPMENTS 49 database machines. The
9003	9018	levels to low security levels. Some of the models that support MAC include the SeaView model , the Jajodia-Sandhu model , the Smith-Winslett model , and the Multilevel Relational model . These models willsCHAPTER 1. INTRODUCTION 4 be described in more detail in Chapter 3. Providing a database system with multilevel security, or bringing a model to life as a prototype or a
9003	9018	of each single attribute. Instead, access classes can be assigned only to key attributes and to tuples as a whole. The most recent of the MLS data models, the Multilevel Relational model (MLR)  was proposed by Chen and Sandhu and is substantially based on the JajodiaSandhu model. It combines ideas from SeaView, belief-based semantics, and the Lock Data Views model in trying to eliminate
9003	9024	labels to entire relations can be useful but is generally considered inconvenient. For example, if some salaries are secret but others are not, these salaries must be placed in different relations . Assigning labels to an entire column of a relation is similarly inconvenient in the general case . The finest granularity of labelling is at the level of individual data elements — i.e,
9003	9024	of a tuple. Although not so flexible as element-level labelling, this approach is considered more convenient than using relation or attribute-level labelssCHAPTER 2. REVIEW OF BASIC CONCEPTS 24 . The following examples illustrate the three labelling schemes used in this thesis. Although the Jajodia-Sandhu model does not specify how the security labels should be stored, the labels are
9003	6554	in all the facilities except the Storage and Recovery Management Facilities, and portrays a view of the control flow within the DBMS system. A pipeline architecture, described by Garlan and Shaw , is used in the Query Processor Facility between the Data Manipulation Language Precompiler (DMP) and the Query execution engine. For the sake of simplicity, information flows back up the
9003	9040	of functional dependencies. In the MLS relational model such a key is called the apparent key (AK) which we assume is a user-specified primary key consisting of a subset of the data attributes Ai . The notion of apparent key is also discussed by Castano et al. in . MLS integrity property 1: Entity integrity. Let AK be the apparent key of a relation R. A multilevel relation R satisfies
9003	9040	primary key of its parent relation. When a user requests information, a view is created consisting of those single-level fragments that the user is authorised to access. The Jajodia-Sandhu model  is based on the SeaView model and addresses some of the flaws identified in the SeaView model  such as the proliferation of tuples during updates, and spurious joins. The Commutative
9003	9040	into at most 4 resulting relations. This final decomposition is necessary because of SeaView’s support for polyinstantiation. A performance analysis of this algorithm by Jajodia and Sandhu  pointed out that the decomposition algorithm leads to unnecessary single-level fragments, and that performing the recovery of multilevel relations requires repeating joins that may lead to spurious
9003	9041	relational model, relations, tuples, attributes, or elements are assigned security classifications. This thesis uses the definition of a multilevel relation captured by the Jajodia-Sandhu model . The model is based on the security classifications introduced in the BLP model, and it formally defines a multilevel relation as consisting of the following two parts: 1. a state-independent
9003	9041	the attributes in the tuple, and tc indicates the lowest user level that can see the tuple. The instance of a relation at a given access class represents the version of the relation at that class . Basically, each element t in a tuple t is visible in instances at access class t or higher; t is replaced by a null value in an instance at a lower access class. A single-level
9003	9042	downgrading of data by restricting information flow from high security levels to low security levels. Some of the models that support MAC include the SeaView model , the Jajodia-Sandhu model , the Smith-Winslett model , and the Multilevel Relational model . These models willsCHAPTER 1. INTRODUCTION 4 be described in more detail in Chapter 3. Providing a database system with
9003	9042	primary key of its parent relation. When a user requests information, a view is created consisting of those single-level fragments that the user is authorised to access. The Jajodia-Sandhu model  is based on the SeaView model and addresses some of the flaws identified in the SeaView model  such as the proliferation of tuples during updates, and spurious joins. The Commutative
9003	9045	the use of MLS as a foundation for network security. We consider two efforts that influenced this thesis, the seminal work by Rushby and Randell , and the work by Kang, Moore and Moskowitz . The systems proposed by both efforts implement multilevel security and are primarily concerned with secure communication between computers in a network. 3.6.1 A distributed secure system The focus
9003	9045	must be managed very carefully to prevent unauthorised communication between host machines, or between a host machine and a wiretapping accomplice. 3.6.2 NRL pump Kang, Moore and Moskowitz  proposed the NRL pump as a device for enforcing multilevel security between LANs. It is a hardware device similar to the TNIU described earlier in section 3.6.1, and it is configured as a single
9003	9050	the access of subjects to objects; these properties are the simple security property rule (ss-property) and the *-property. Both properties must hold in order for security to be maintained : ss-property. A state (b, M, f) satisfies the ss-property, if for each element (s, o, a) ? b where the access operation a is read, the security level of the subject s dominates the classification
9003	9050	on multilevel security models began in earnest in the early 1970s with the aim of producing high-level, software-independent, conceptual models that describe the protection mechanisms of a system . In order to have assurance that a given model would perform as specified, researchers also recognised the need to formalise these models. Many of the models were based on the military model of
9003	9050	in trying to eliminate ambiguity whilst retaining upward information flow. This model supports labelling at the element-level of granularity. A detailed description of these models may be found in .sCHAPTER 3. RELATED RESEARCH AND DEVELOPMENTS 30 3.3 Architectures for multilevel database systems Mandatory access control is often implemented by classifying both users and data at various
9003	7296	Building C Ebert S D13 U Geriatrics C 12 C B16 Basement U Xavier TS Figure 2.5: Element-level labelling. 2.6 Distributed database systems A distributed database as defined by Ozsu and Valduriez  is a collection of multiple, logically interrelated, databases distributed over a computer network. A distributed database management system (DDBMS) is also defined in  as the software system
9003	7296	common alternative is the element fragmentation, where sub-relation subsets are defined on the elements of the original multilevel relation. The following fragmentation rules taken directly from  are required to ensure that the database does not undergo semantic change during fragmentation. 1. Completeness: If a relation instance R is decomposed into fragments R1, R2, ..., Rn, each data
9003	7296	to several fragment queries. The studies of query processing that have had the most influence on this thesissCHAPTER 2. REVIEW OF BASIC CONCEPTS 27 are those involving distributed query processing . These sources contain detailed treatments of the process of converting global queries into their horizontal, vertical and mixed fragment equivalents. These query fragments are processed at the
9003	9062	systems as the basis of security, we now examine the use of MLS as a foundation for network security. We consider two efforts that influenced this thesis, the seminal work by Rushby and Randell , and the work by Kang, Moore and Moskowitz . The systems proposed by both efforts implement multilevel security and are primarily concerned with secure communication between computers in a
9003	9066	information flow from high security levels to low security levels. Some of the models that support MAC include the SeaView model , the Jajodia-Sandhu model , the Smith-Winslett model , and the Multilevel Relational model . These models willsCHAPTER 1. INTRODUCTION 4 be described in more detail in Chapter 3. Providing a database system with multilevel security, or bringing a
9003	9066	attribute values that the user is not cleared to see with null-values. A shortcoming in the Jajodia-Sandhu model arising from the use of this filter function was pointed out by Smith and Winslett . Smith and Winslett observed that the filter function ? introduces an additional semantics for nulls. In the Jajodia-Sandhu model, a null value can mean ‘information available but hidden’ and
9003	9066	are then added to the original query in order to make it safe. Data objects in the database are assigned security labels by the model based on its own security constraints. The Smith-Winslett model  addresses the semantics of an MLS database based on the concept of belief, where a user sees and believes the contents of the database at its own level, and sees the data objects at lower security
9076	9078	for B&B Mixed Integer Programming, Linderoth and Savelsbergh demonstrate for example that no branching or node selection method outperforms all others when a variety of MIP problems is concerned . The reason for that is that heuristic functions, however robust, cannot be equally effective in all problem instances. This gave rise to our idea: what if we employ the power of metaheuristics and
9076	9078	take on the values of pluss4 Konstantinos Kostikas and Charalambos Fragakis or minus infinity. Thus, a MIP is a linear program (LP) plus some integrality restriction on some or all of the variables . Although other methods also exist, B&B is the predominant technique for solving MIP, employed in most commercial solvers . To precisely define how B&B can be applied for solving MIP problems,
9076	9078	bound on the value that zMIP can have in N i . The list L of problems that must still be solved is called the active set. Denote the optimal solution by x ? . The algorithm in Table 1, adopted from , is a Linear Programming-based Branch and Bound algorithm for solving MIP. Table 1. Branch and Bound Algorithm for Linear Programming based MIP 0. Initialize. L = MIP. zL = ??. x ? = Ø. 1.
9076	9078	N i . For each j = 1, 2, . . . , k, let z ij U = zi LP and add the problem N ij to L. Go to 1. 2.3 Node Selection in B&B MIP A plethora of methods, or “strategies”, exist for node selection in MIP.  is a good survey of lots of them. All such methods, except from pure DFS and BFS, employ in their core an expression which assigns a numerical value to all active nodes of the tree. The node which
9076	9078	of the solutions obtained is reported. As it is apparent from Table 4, no node selection method fully outperforms the others. This is typical for node selection heuristics, and in accordance with . Two methods stand out however as being the most capable of obtaining good integer feasible solutions at the available time: best projection and GP-35 (training set comprised of 35 dives): best
9076	9079	Related Work The field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. A recent, general “overview and conceptual comparison” is . Abramson and Randall  utilize Simulated Annealing, Tabu Search and other metaheuristics for building a ’general purpose combinatorial optimization problem solver’;  and  are also good
9076	9080	metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. A recent, general “overview and conceptual comparison” is . Abramson and Randall  utilize Simulated Annealing, Tabu Search and other metaheuristics for building a ’general purpose combinatorial optimization problem solver’;  and  are also good references for related
9076	9081	for the application to combinatorial optimization problems is a rapidly growing field of research. A recent, general “overview and conceptual comparison” is . Abramson and Randall  utilize Simulated Annealing, Tabu Search and other metaheuristics for building a ’general purpose combinatorial optimization problem solver’;  and  are also good references for related work.
9076	9085	recognized by Lawler and Wood in , which contains also a problem independent description of B&B. A presentation of the most important models of B&B and the related references can be found in . Although the B&B framework is not tied to any particular description, B&B methods are most often described as generating search trees : roughly speaking, each node corresponds to a subset of
9076	9087	design. 4 Evolving Heuristics Offline Genetic Programming has been used in standalone mode for heuristic generation in domains other than B&B search. A representative recent example is  where GP is used “to optimize the priority functions associated with register allocation as well as branch removal via predication”. A number of benchmark programs were used as the training set.
9076	9088	A number of benchmark programs were used as the training set. Such a route for heuristic generation is probably applicable to B&B as well. In the case of B&B MIP, benchmark problems from MIPLIB3  like the ones used for our experimentation (Sect. 6) can be utilized as the training set. An issue with the above approach is the excessive computational time required. However, in the case of B&B
9076	9088	the generated heuristic, and thus its the capability to effectively guide B&B towards good solutions. For the above purposes we used for experimentation problems from the standard MIPLIB3 library . Easy (fully solved to optimality in less than 300 seconds by the unmodified solver) and very difficult problems (no solution of the initial LP relaxation found in the available time) were
9076	9090	5.1 Infrastructure The MIP solver we based our experimentation on is GLPK (Gnu Linear Programming Kit) . GLPK doesn’t provide the plethora of options found in commercial software like CPLEX , but it contains a solid implementation of the simplex method, and, most importantly, comes with full source code. GLPK adopts a backtracking method for node selection: it goes depth first as much
3256	3255	or hosts. Therefore, the functionalities of the core routers are relatively simple - they classify packets according to the DSCP and forward them using corresponding Per-Hop Behaviors (PHBs) . Three service classes have been proposed in the DiffServ model: the premium class, the assured class and the best-effort class . They have different priorities. The premium class has the
3256	3257	is not empty, i.e., it will schedule the packets from the RIO-queue only when the PS-queue is empty. Some papers have already done research on the performance issues for DiffServ model, such as . But in order to get close-form solutions, most of them used simplified assumptions such as Poisson arrivals or exponential distributed service time for packets. And both of them assume a
3256	3257	distribution. Results are analyzed and compared with each other. 4.1 PS-Queue For PS-queue, we develop two different models for preemptive model and non-preemptive model respectively. In  and , only the preemptive model was used and analyzed. But in reality, most of the switches and routers in the Internet are using non-preemptive model. Thus we model both scheme and compare the
3256	3257	load is high and the fraction of premium traffic is high, the best-effort packets have to wait for and even preempted by more number of premium packets. Comparing this results with the results in , we find that they totally agree. Figure 6(b) shows the results for a non-preemptive system. Not surprisingly, the results are almost the same as those of a preemptive system. Comparing Figure 6(b)
3256	3257	very limited impact on the system performance in terms of the loss probability. Therefore, we prove that the traditional analysis based on the assumption of exponential service time are valid. In , the authors proved that different arrival processes (such as On/Off Exponential model and On/Off Pareto model) have small impact on overall system performance too. Due to lack of time and space,
3256	3257	traffic, with exponential service time. Both the loss probability results and the effective throughput results perfectly agree with our expectation and are consistent with the results presented in . 4.2.2 General Service Time and Comparison Loss probability 1 0.8 0.6 0.4 0.2 0 0.6 0.8 1 1.2 1.4 Offered load (Lambda) 1.6 1.8 2 (a) Packet Loss Probability Assured(Exp) Best-effort(Exp)
3256	9094	and 5) Combined model of both PS-queue and RIO-queue for all three service classes. During the progress of building these SAN models, we learned some modeling and simulation techniques from . We also encountered some problems. We will introduce them and our solutions to them in Section 3. Since simulations are used, we can model systems with large queue sizes. For example, in Figure 3,
3256	9095	is not empty, i.e., it will schedule the packets from the RIO-queue only when the PS-queue is empty. Some papers have already done research on the performance issues for DiffServ model, such as . But in order to get close-form solutions, most of them used simplified assumptions such as Poisson arrivals or exponential distributed service time for packets. And both of them assume a
3256	9095	distribution. Results are analyzed and compared with each other. 4.1 PS-Queue For PS-queue, we develop two different models for preemptive model and non-preemptive model respectively. In  and , only the preemptive model was used and analyzed. But in reality, most of the switches and routers in the Internet are using non-preemptive model. Thus we model both scheme and compare the
9096	9097	challenges and how they influenced the architecture we will propose in Section 2. Administration: Administration is a serious concern for systems of many nodes. We leverage ideas in prior work , which describes how a unified monitoring/reporting framework with data visualization support was an effective tool for simplifying cluster administration. Component vs. system replication: Each
9096	9100	and we have successfully demonstrated adaptation to network changes by combining our original WWW proxy prototype with the Event Notification mechanisms developed by Welling and Badrinath , and plan to leverage these mechanisms to provide an adaptive solution for Web access from wireless clients. We have not investigated how well our proposed architecture works outside the
9096	9101	and Availability The technique of constructing robust entities by relying on cached soft state refreshed by periodic messages from peers has been enormously successful in wide-area TCP/IP networks , another arena in which transient component failure is asfact of life. Correspondingly, our SNS components operate in this manner, and monitor one another using process peer fault tolerance 1 :
9096	9101	fault tolerance robustness has been well explored in the wide-area Internet, in the context of IP packet routing , multicast routing , and wireless TCP optimizations such as TCP Snoop ; the lessons learned in those areas strongly influenced our design philosophy for the TACC server architecture. Load balancing and scaling: WebOS  and SWEB++  have exploited the
9096	9103	transformations on content, including Kanji transcoding , Kanji-to-GIF conversion , application-level stream transducing , and personalized agent services for Web browsing . Fault tolerance and high availability: The Worm programs  are an early example of process-peer fault tolerance. Tandem Computer and others explored a related mechanism, process-pair fault
9096	9105	needs: user preference reads are much more frequent than writes, and the reads are absorbed by a writethrough cache in the front end. 3.1.5 Cache Nodes TranSend runs Harvest object cache workers  on four separate nodes. Harvest suffers from three functional/performance deficiencies, two of which we resolved. First, although a collection of Harvest caches can be treated as “siblings”, by
9096	9105	per kilobyte of input. Similar results were observed for the JPEG and HTML distillers, although the HTML distiller is far more efficient. 4.4 Cache Partition Performance GIF size (bytes) In , a detailed performance analysis of the Harvest caching system is presented. We summarize the results here: • The average cache hit takes 27 ms to service, including network and OS overhead,
9096	9108	certainly be delegated to an ACID database. We focus on services that have an ACID component, but manipulate primarily BASE data. Web servers, search/aggregation servers , caching proxies , and transformation proxies are all examples of such services; our framework supports a superset of these services by providing integrated support for the requirements of all four. As we will show,
9096	9111	there is a well-defined average load and that arriving traffic follows a Poisson distribution, burstiness has been demonstrated for Ethernet traffic , file system traffic , and Web requests , and is confirmed by our traces of web traffic (discussed later). In addition, Internet services can experience relatively rare but prolonged bursts of high load: after the recent landing of
9096	9111	1200 800 400 4.2 Burstiness 0 18:00 22:40 02:00 07:30 18:00 time 300 200 100 Burstiness is a fundamental property of a great variety of computing systems, and can be observed across all time scales . Our HTTP traces show that the offered load to our implementation will contain bursts—Figure 6 shows the request rate observed from the user base across a 24 hour, 3.5 hour, and 3.5 minute time
9096	4261	and Availability The technique of constructing robust entities by relying on cached soft state refreshed by periodic messages from peers has been enormously successful in wide-area TCP/IP networks , another arena in which transient component failure is asfact of life. Correspondingly, our SNS components operate in this manner, and monitor one another using process peer fault tolerance 1 :
9096	4261	of soft state to provide improved performance and increase fault tolerance robustness has been well explored in the wide-area Internet, in the context of IP packet routing , multicast routing , and wireless TCP optimizations such as TCP Snoop ; the lessons learned in those areas strongly influenced our design philosophy for the TACC server architecture. Load balancing and scaling:
9096	9113	and cost. Like pioneering systems such as Grapevine  , BASE reduces the complexity of the service implementation, essentially trading consistency for simplicity; like later systems such as Bayou  that allow trading consistency for availability, BASE provides opportunities for better performance. For example, where ACID requires durable and consistent state across partial failures, BASE
9096	9113	web servers using a fault tolerance toolkit called CORDS, but that project is still in progress. BASE: Grapevine  was an important early example of trading consistency for simplicity; Bayou  later explored trading consistency for availability in application-specific ways, providing an operational spectrum between ACID and BASE for a distributed database. The use of soft state to
9096	9116	using 3 pages of Perl code in roughly 2.5 hours, and inherits scalability, fault tolerance, and high availability from the SNS layer. Anonymous Rewebber: Just as anonymous remailer chains  allow email authors to anonymously disseminate their content, an anonymous rewebber network allows web authors to anonymously publish their content. The rewebber described in  was implemented
9096	9117	remailer chains  allow email authors to anonymously disseminate their content, an anonymous rewebber network allows web authors to anonymously publish their content. The rewebber described in  was implemented in one week using our TACC architecture. The rewebber’s workers perform encryption and decryption, its user profile database maintains public key information for anonymous servers,
9096	9118	or a search engine. Pervasive throughout our design and implementation strategies is the observation that much of the data manipulated by a network service can tolerate semantics weaker than ACID . We combine ideas from prior work on tradeoffs between availability and consistency, and the use of soft state for robust fault-tolerance to characterize the data semantics of many network
9096	9118	according to the data semantics that each service demands. At one extreme is the traditional transactional database model with the ACID properties (atomicity, consistency, isolation, durability) , providing the strongest semantics at the highest cost and complexity. ACID makes no guarantees regarding availability; indeed, it is preferable for an ACID service to be unavailable than to
9096	9120	environment with an SDK, and we will encourage our colleagues to author services of their own using our system. Previous research into operating systems support for busy Internet servers  has identified inadequacies in OS implementations and the set of abstractions available to applications. We plan to investigate similar issues related specifically to clusterbased middleware
9096	9121	machine is either up or down. Shared state: Unlike SMPs, clusters have no shared state. Although much work has been done to emulate global shared state through software distributed shared memory , we can improve performance and reduce complexity if we can avoid or minimize the need for shared state across the cluster. These last two concerns, partial failure and shared state, lead us to
9096	9122	machine is either up or down. Shared state: Unlike SMPs, clusters have no shared state. Although much work has been done to emulate global shared state through software distributed shared memory , we can improve performance and reduce complexity if we can avoid or minimize the need for shared state across the cluster. These last two concerns, partial failure and shared state, lead us to
9096	2673	Growth Although we would like to assume that there is a well-defined average load and that arriving traffic follows a Poisson distribution, burstiness has been demonstrated for Ethernet traffic , file system traffic , and Web requests , and is confirmed by our traces of web traffic (discussed later). In addition, Internet services can experience relatively rare but prolonged bursts
9096	2673	1200 800 400 4.2 Burstiness 0 18:00 22:40 02:00 07:30 18:00 time 300 200 100 Burstiness is a fundamental property of a great variety of computing systems, and can be observed across all time scales . Our HTTP traces show that the offered load to our implementation will contain bursts—Figure 6 shows the request rate observed from the user base across a 24 hour, 3.5 hour, and 3.5 minute time
9096	9126	and Availability The technique of constructing robust entities by relying on cached soft state refreshed by periodic messages from peers has been enormously successful in wide-area TCP/IP networks , another arena in which transient component failure is asfact of life. Correspondingly, our SNS components operate in this manner, and monitor one another using process peer fault tolerance 1 :
9096	9126	database. The use of soft state to provide improved performance and increase fault tolerance robustness has been well explored in the wide-area Internet, in the context of IP packet routing , multicast routing , and wireless TCP optimizations such as TCP Snoop ; the lessons learned in those areas strongly influenced our design philosophy for the TACC server architecture. Load
9096	9128	data: • Stale data can be temporarily tolerated as long as all copies of data eventually reach consistency after a short time (e.g., DNS servers do not reach consistency until entry timeouts expire ). ??? Soft state, which can be regenerated at the expense of additional computation or file I/O, is exploited to improve performance; data is not durable. • Approximate answers (based on stale data
9096	9129	environment with an SDK, and we will encourage our colleagues to author services of their own using our system. Previous research into operating systems support for busy Internet servers  has identified inadequacies in OS implementations and the set of abstractions available to applications. We plan to investigate similar issues related specifically to clusterbased middleware
9096	9636	Internet services can experience relatively rare but prolonged bursts of high load: after the recent landing of Pathfinder on Mars, its web site served over 220 million hits in a 4-day period . Often, it is during such bursts that uninterrupted operation is most critical. Our architecture includes the notion of an overflow pool for absorbing these bursts. The overflow machines are not
9096	5400	tolerance guaranteed by the SNS layer means that we don’t have to worry about eliminating all such possible bugs and corner cases from the system. 3.1.7 Graphical Monitor Our extensible Tcl/Tk  graphical monitor presents a unified view of the system as a single virtual entity. Components of the system report state information to the monitor using a multicast group, allowing multiple
9096	9139	varying data semantics. Directories such as Yahoo!  maintain a database of soft state with BASE semantics, but keep user customization profiles in an ACID database. Transformation proxies  interposed between clients and servers transform Internet content on-the-fly; the transformed content is BASE data that can be regenerated by computation, but if the service bills the user per
9096	9146	wireless TCP optimizations such as TCP Snoop ; the lessons learned in those areas strongly influenced our design philosophy for the TACC server architecture. Load balancing and scaling: WebOS  and SWEB++  have exploited the extensibility of client browsers via Java and JavaScript to enhance scalability of network-based services by dividing labor between the client and server. We note
8919844	9656	true in that the effort comes from a computer rather than a human, but for images this represents a great deal of effort. The results reported thus far, however, are encouraging. Harris and Buxton (1996) evolved edge detectors which outperformed Canny’s edge detector, particularly on nonideal step edges of the type included in the training set. They worked in 1D rather than 2D to save on processing
8919844	9656	produce binary output is inefficient. The filter’s output will have to be classified into object and non-object pixels anyway, so selecting a threshold tests the actual filter output. Soule et al. (1996) examined how to use selective pressure to cut down on code growth. The code in GP tends to grow exponentially in size while the fitness grows linearly. Since images require huge amounts of
8919844	9656	a limited number of features rather than the entire gray-scale image reduces the amount of processing required. Other work using GP with images has been done by Koza (1992), Nordin and Banzhaf (1996), Daida et al. (1996) and Swets et al. (1995). 3. Tools TheComp function is a new tool which enhances the capabilities of theMin,Max,Avg, and Logical functions in creating nonlinear filters. TheComp
8919844	9656	not for training runs. If including any tool increases performance, it is in some way fit for the specific task. The scale space interactions with Gabor filters tool, described in Manjunath et al. (1996), implements a feature detection model. Two different Gabor filters are applied to an image, and the absolute value of the difference in their responses is the interaction result.G(XYZ) represents
8919844	9656	mistakes in the training set. The training set should be more representative, and the question arises of how to select non-objects from the training set. In neural network literature, Rowley et al. (1996) suggest a bootstrapping method for selecting non-objects. Recent experiments show that this method does not work in GP. An alternative approach may be the topic for future research. Acknowledgments
11934246	9173	scanning device, accurate registration and data fusion. Image-based registration and fusion can improve the quality of the reconstruction and reduce speckle noise, shadowing and signal dropout . 5.2 Current Research Issues for 3D Doppler Sonoelastography Doppler imaging only measures the component of velocity in the direction of wave propagation i.e. only axial motion can be detected.
11934246	9174	scanning device, accurate registration and data fusion. Image-based registration and fusion can improve the quality of the reconstruction and reduce speckle noise, shadowing and signal dropout . 5.2 Current Research Issues for 3D Doppler Sonoelastography Doppler imaging only measures the component of velocity in the direction of wave propagation i.e. only axial motion can be detected.
8919851	9187	are vital for the reduction of results to a human readable form. XCS (Wilson, 1995, 1998) has been shown to be capable of generating a complete and optimally accurate mapping of a test environment (Kovacs, 1996) and therefore presents a new opportunity for the application of Learning Classifier Systems to Data Mining. As part of a continuing research effort this paper presents some first results in the
8919851	9187	typical amounts of noise found in data after data cleansing without hindering the learning operation. 3.2 THE LEARNING SYSTEM A XCS implementation conforming to the XCS used within (Wilson, 1998; Kovacs, 1996) was obtained (Barry, 1998). In all experiments the parameterization of the XCS was set as follows: p1 =10.0, ?1=0.01, F1=0.01, ?=0.2, ?=0.71, ?=25, ?0=0.01, ?=0.01, ?=0.8, ?=0.04, ?=0.5, P#=0.33
8919851	9187	rises steeply to 1.0 and stays at that level. Occasional fluctuations just below this maximum may occur when the GA deletes a classifier during search for the optimal population (termed ; (Kovacs, 1996)). Macroclassifier population size will typically increase to a peak at around three-quarters of the total population size as the XCS explores as many classifiers as possible. The pressure to
8919851	9188	criteria, XCS is the first LCS to be able to claim to 'reliably generate the most general accurate classifiers' - the so-called 'Generalization Hypothesis' (Wilson, 1998). Further work by Kovacs (Kovacs, 1997) has demonstrated that the provision of further operators can sufficiently focus the population once exploration is complete to reliably produce a minimum rule set consisting of the most general
8919856	9199	hardware. For instance, the JavaCard virtual machine  does not support dynamic class loading or garbage collection due to the typical computing power and memory space available on smart cards . Memory is an especially scarce resource in most embedded systems due to technical constraints which prevent the miniaturization of large memory banks. Thus, reducing the size of the virtual
8919856	9199	its applications, the developer can embed a fully tailored Java Runtime Environment without sacrificing functionalities of the virtual machine, as it is often the case in most embedded environments . Indeed, many other attempts to embed Java on very small devices impose restrictions in their specification that requires using new languages or tools . On the contrary, JITS doesn’t impose
8919856	9200	space consumed by classes obviously means trying to obtain smaller code and smaller data. Previous work has shown that bytecode compression can be used to reduce the memory space used by the code , so it seems interesting to try and compress the data located in the constant pool of each class. A careful analysis of the constant pool shows that lots of its entries are only needed during the
337567	9205	type have changed in this example. In Section 6.1 we formally introduce transformations, and extend the theory from the previous sections. Furthermore, we define a taxonomy of transforma10 See e.g.  to see how lexical chains can be used for text summarization. 26sFigure 8. Transformations are IS-neutral tion functions in Section 6.2. Since this taxonomy is still a topic for further research,
337567	9206	In the following section, this model is formalized, after which (in Section 5) a more detailed example is presented. 9 mime is an acronym for Multipurpose Internet Mail Extensions and is defined in  16sInformation Service Source Destination Relation Relation Type Feature Representation Feature Type Representation Type Figure 5. Typing of Features and Representations The model presented in
337567	9207	techniques such as er , eer , orm , uml Class Diagrams  and their associated query/constraint languages such as ridl , caddy , lisad , ConQuer , can be used to provide a conceptual model of some given application domains. This conceptual model is usually formulated in terms of a set of entity and relationship types, describing the essence
8919858	9209	model novelty is to use a database structure, whose focus was directed to clients, security and cost supervision. The discussion of existent cases studies and current uses are presented in (Borges Gouveia et al., 2000), and a comparison with other well known web based learning platforms is provided by (Gouveia et al., 2000a). In order to fulfil security and cost requirements, some integration mechanisms have
8919858	9209	a framework that bases teacher and students’ interaction on the materials and tasks to be accomplish. In the proposed model content has the same importance than the means for classifying it (Gouveia et al., 2000). The EFTWeb model is implemented with available widespread technology. To support content distribution, World Wide Web becomes the natural solution. It has a lot of information available that
8919858	9209	creation: complementing the thesaurus with additional information by introducing lists of available thesaurus keywords with correspondent weighting factors. The EFTWeb user services are (Borges Gouveia et al., 2000): ? mail: each client must have access an email address to send/receive messages; ? dialog: allow client chat in real time. The service is organised in rooms that groups users by topic; ? personal
8919858	9209	A more detailed discussion of the thesaurus and catalogues use is provided by (Gouveia and Borges Gouveia, 2002). Although a number of successful uses of the system have been reported, (Borges Gouveia et al., 2000) the system lacks the mechanisms to support user orientation concerning the learning process. Even when having specific tasks to be accomplished, the present of someone that provides orientation
8919858	9217	model novelty is to use a database structure, whose focus was directed to clients, security and cost supervision. The discussion of existent cases studies and current uses are presented in (Borges Gouveia et al., 2000), and a comparison with other well known web based learning platforms is provided by (Gouveia et al., 2000a). In order to fulfil security and cost requirements, some integration mechanisms have
8919858	9217	a framework that bases teacher and students’ interaction on the materials and tasks to be accomplish. In the proposed model content has the same importance than the means for classifying it (Gouveia et al., 2000). The EFTWeb model is implemented with available widespread technology. To support content distribution, World Wide Web becomes the natural solution. It has a lot of information available that
8919858	9217	creation: complementing the thesaurus with additional information by introducing lists of available thesaurus keywords with correspondent weighting factors. The EFTWeb user services are (Borges Gouveia et al., 2000): ? mail: each client must have access an email address to send/receive messages; ? dialog: allow client chat in real time. The service is organised in rooms that groups users by topic; ? personal
8919858	9217	A more detailed discussion of the thesaurus and catalogues use is provided by (Gouveia and Borges Gouveia, 2002). Although a number of successful uses of the system have been reported, (Borges Gouveia et al., 2000) the system lacks the mechanisms to support user orientation concerning the learning process. Even when having specific tasks to be accomplished, the present of someone that provides orientation
8874784	9227	need to be developed, especially algorithms more suitable for the highly volatile nodes of desktop grids. In this paper we present an algorithm for the docking process based on MD simulations as in , but characterized by a higher flexibility that makes it adaptable to any computing platform, evensto very challenging desktop grids. Docking many ligands to the same protein followed by scoring
8874784	9227	, ICM , and GOLD . We show that our algorithm is indeed more accurate than all other methods except ICM. We reach a docking success rate of over 70%, confirming the accuracy reported in . The time required for running a complete docking attempt is longer but comparable with the time of the other methods. The fine computational granularity of our algorithm is trivially parallel and
8874784	9227	all atoms of the protein receptor. We have chosen a grid spacing of 1Å based on previous work that showed no significant differences in docking accuracy for grid spacings between 0.25 Å and 1 Å . To facilitate the penetration of small ligands into the protein sites and allow larger configurational changes, van der Waals (vdW) and electrostatic potentials with soft core repulsions  were
8874784	9230	detail in . We validate the accuracy of our algorithm by applying the tests defined in  to our method, and compare to the published results for the following other methods: AutoDock , DOCK , FlexX , ICM , and GOLD . We show that our algorithm is indeed more accurate than all other methods except ICM. We reach a docking success rate of over 70%, confirming the accuracy
8874784	9230	1. Our MD-based protein-ligand docking algorithm. terms, some of which reflect the short-range vdW interaction between protein and ligand as well as the ligand internal energy. The search in DOCK  is driven by the geometry of the ligand in the active site. Different scoring functions can be employed: (1) geometric alignment and shape constraints, (2) the electrostatic potential of the
8874784	9231	We validate the accuracy of our algorithm by applying the tests defined in  to our method, and compare to the published results for the following other methods: AutoDock , DOCK , FlexX , ICM , and GOLD . We show that our algorithm is indeed more accurate than all other methods except ICM. We reach a docking success rate of over 70%, confirming the accuracy reported in .
8874784	9231	and shape constraints, (2) the electrostatic potential of the protein-ligand complex using the program DELPHI, or (3) the energy of the protein-ligand complex under the AMBER force field. FlexX  is also driven by the geometry of the ligand in the active site like DOCK. In FlexX, the scoring uses a variation of the Böhm scoring function with terms for several kinds of interactions and
8874784	9233	of our algorithm by applying the tests defined in  to our method, and compare to the published results for the following other methods: AutoDock , DOCK , FlexX , ICM , and GOLD . We show that our algorithm is indeed more accurate than all other methods except ICM. We reach a docking success rate of over 70%, confirming the accuracy reported in . The time required for
8874784	9233	local search with selection and crossover. The ligands are ranked using an energybased scoring function and, to speedup the score calculation, a grid-based protein-ligand interaction is used. GOLD , like AutoDock, deploys a genetic algorithm and uses a scoring function which is the sum of energy 3 Model Protein Ligand Interactions Alter Ligand Configuration and Orientations Dock Ligand into
8874784	9744	of motion are discretized and solved numerically by an integration procedure such as the Verlet algorithm. The force on the atoms is the negative gradient of the CHARMM potential energy function . 2.2 Modeling Protein-Ligand Interactions Advances in energy calculation techniques make it viable to use a grid-based representation of the proteinligand potential interactions to calculate our
8874784	9237	to complete. Therefore, we conclude that our docking algorithm is well-suited for Intranet desktop grid platforms (e.g., Entropia DCGrid , Infuzion ) and on the Internet (e.g., XtremWeb , BOINC ). The combination of our algorithm with such platforms, which might allow us to perform fast and accurate screening of very large ligand databases, is currently under our development
9245	9246	of balanced multiwavelets, which is now also further investigated by several other authors , , . One of the goals of this concept is to avoid the intricate steps of pre/post filtering ,  that are required with systems based on multiwavelets that do not satisfy the interpolation/approximation properties of balancing. Inspired by some of the results from , , and , we
9245	9249	this product by looking at the eigenvalues of (20) where . If , then the scaling functions cannot have a Sobolev exponent of more than and cannot be more than times continuously differentiable , . Thus, we get an upper bound on the smoothness. Proposition 13: If an orthonormal multiwavelet system has balancing order and the spectral radius of in the factorization (15) verifies , then
9245	9249	for estimating the smoothness to link the smoothness of the scaling functions to a particular factorization of the refinement mask (the counterpart of the zeros at pre-periodic points condition  in the standard wavelet theory). APPENDIX Proof of Theorem 2: • : Assuming ), we have by transposition get . Writing the equations explicitly, we . Therefore, , and since , we have condition ). • :
9245	9251	we will assume that the sequences and are finite and, thus, that and have compact support . Many people worked on the convergence conditions. For more details about these results, see , , , and . Here, we will assume that satisfies the following two basic conditions (following Strang’s notations ). Condition E (Existence and Uniqueness): The transition operator
9245	9253	and their integer translates by introducing by where is a sequence of matrices of real coefficients obtained by completion of (a detailed exposition of the completion scheme is given in ). Introducing in the -domain the refinement masks and , (1) and (2) translate in the Fourier domain into (1) (2) and (3) We can then derive the behavior of the multiscaling function by iterating
9245	9256	the first product above. If this iterated matrix product converges, we get, in the limit (4) In the sequel, we will assume that the sequences and are finite and, thus, that and have compact support . Many people worked on the convergence conditions. For more details about these results, see , , , and . Here, we will assume that satisfies the following two basic conditions
9245	9257	filtering ,  that are required with systems based on multiwavelets that do not satisfy the interpolation/approximation properties of balancing. Inspired by some of the results from , , and , we will clarify the relations between balancing order (discrete-time property) and approximation power (continuous-time property) and prove that the notion of balancing order is truly
9245	9257	of a system based on the DGHM multiwavelet without prefiltering. is a special case of the factorizations (the so-called two-scale similarity transforms) introduced by Plonka and Strela in  and ). Theorem 2: Balancing of order 1 is equivalent (in the case of orthogonal multiwavelet systems) to any of the following conditions. . and . .sLEBRUN AND VETTERLI: HIGH-ORDER BALANCED
9245	9257	and approximation. is the superfunction  associated with . generates a closed linear subspace having the same approximation power as . Proof: • : The part is derived from Lemma 2.1  and orthonormality gives us . The converse is obtained by using Theorem 3.2  and verifying that the can be written in the proper form if we take . • : The part is derived from Theorem 2.2
9245	9257	2), we get that the possible outputs are for . Denoting as the th polyphase component of , we get that Hence, is an eigenvector of the operator . • : This is a direct consequence of Theorem 4.1 in . ??? : Assuming ) so that . . . and this is condition ). . .. . .. . . Proof of Theorem 10: • Balancing : From Lemma 4, for , we have that interpolates the th polyphase component of any polynomial
9245	9257	we get the result. • : satisfies the conditions of Theorem 2.1 in  with for . The balanced vanishing moments of order are just a rewriting of these conditions. • : Applying Corollary 4.3. from , we get the factorization with . . .. . .. . .. . .. . .. . .. and the polynomial matrix verifying where obtained recursively from . Therefore, for , we get and . . . • : First, we give a digest of
9245	9259	will assume that the sequences and are finite and, thus, that and have compact support . Many people worked on the convergence conditions. For more details about these results, see , , , and . Here, we will assume that satisfies the following two basic conditions (following Strang’s notations ). Condition E (Existence and Uniqueness): The transition operator  associated
9245	9260	conditions of order : and , for and . Remark 11: By ), balanced multiwavelets of order behave as bona-fide wavelets up to the order of interpolation and approximation. is the superfunction  associated with . generates a closed linear subspace having the same approximation power as . Proof: • : The part is derived from Lemma 2.1  and orthonormality gives us . The converse is
9245	9260	function has, furthermore, vanishing moments (i.e., for ), we get a multiwavelet generalization of Coiflets . We have then the following properties: • and . • for . • is now the canonical  superfunction, i.e., it verifies the extended Strang–Fix conditions for . Multicoiflets are then constructed as balanced multiwavelets with more stringent conditions on the moments of . A family of
9245	9260	using the transition operator. This method gives the exact Sobolev smoothness of and . An approach giving a good lower bound of the Sobolev smoothness for each scaling function is detailed in . V. CONSTRUCTION OF HIGH-ORDER BALANCED MULTIWAVELETS A. Bat Family Using the results above, we are now able to investigate the construction of orthonormal multiwavelets of arbitrary balancing
9245	9262	signals by the lowpass branch of the filterbank. Consequently, we introduced the concept of balanced multiwavelets, which is now also further investigated by several other authors , , . One of the goals of this concept is to avoid the intricate steps of pre/post filtering ,  that are required with systems based on multiwavelets that do not satisfy the
9245	9262	lowpass synthesis operator (with its intricate time-varying structure) is, in fact, equivalent to a scalar sub1 Condition B3) and its generalization to higher order balancing were first given by . . .. . .. . .. . .. . . Fig. 4. Fundamental condition of high-order balancing. division scheme (on which the classical results from the scalar wavelet theory apply). Lemma 4: Let be the formal
9245	9262	SIGNAL PROCESSING, VOL. 49, NO. 9, SEPTEMBER 2001 Thus with polynomial; hence, we have the result. Now, by the hypothesis Now, we get the result since is polynomial. • Balancing : As mentioned in , condition ) says that the multirate system has zeros of order at the roots of the unity with . Therefore, from the rank wavelet theory , we get that this system preserves discrete
9245	9269	this matrix; for more details, see the proof of Lemma 15 given in the Appendix). 3) Impose a flipping property on . The flipping property enables an easy lossless symmetrization (as seen in ) of finite length input signals both for the lowpass filters and the highpass.sLEBRUN AND VETTERLI: HIGH-ORDER BALANCED MULTIWAVELETS 1925 4) Solve the system of equations using a Gröbner bases
9245	9270	multiwavelets, which is now also further investigated by several other authors , , . One of the goals of this concept is to avoid the intricate steps of pre/post filtering ,  that are required with systems based on multiwavelets that do not satisfy the interpolation/approximation properties of balancing. Inspired by some of the results from , , and , we will
9271	9272	RELATED WORK The evaluation of location-based software in real MANET environments suffers from two problems: Limited repeatability and high resource costs. To alleviate the repeatability problem,  proposes a MANET testbed consisting of several notebooks equipped with wireless network interfaces. According to a predefined choreography, the notebooks display movement commands to be followed by
9271	9275	and decides which transmissions can be successfully delivered, based on the position and the transmission range of the emulated MANET nodes. The existing solutions for centralized MANET emulation  differ in the complexity of the network and signal propagation model that is considered by the central instance, but have in common that this instance constitutes a performance bottleneck limiting
9271	9278	limitations. Both the central model and the local tools have impact on the realism of the emulation. Some approaches simply use the built-in firewall of the operating system as emulation tool . Because firewalls can be configured to drop frames depending on the sender address, this is sufficient to model the changing connectivity in a MANET. The introduction of dedicated emulation tools
9271	9278	threshold, the connection is cut off completely. This model assumes uniform free-space radio propagation, which is the basis for most other approaches modelling MANET communication characteristics . We are currently investigating the integration of more sophisticated wave propagation models that consider spatial information, such as buildings or streets .sIOCTL call config daemon
9271	9281	the communication stack facilitates to control more parameters, such as loss probabilities and delay . Even the impact of collisions on a shared media network can be considered by some tools . While one of the approaches to MANET emulation makes the virtual position information available on the nodes , this information is not accessible for user programs. The position information is
9271	9281	these effects, based on an emulation of the MAC protocol specified in the IEEE 802.11b standard. In prior work, we already proved that the software-based emulation of a MAC protocol is feasible . IV. VIRTUAL POSITIONING DEVICE The emulation testbed described in Section III facilitates the performance analysis of distributed applications and protocols designed to run in MANET environments.
9271	9284	threshold, the connection is cut off completely. This model assumes uniform free-space radio propagation, which is the basis for most other approaches modelling MANET communication characteristics . We are currently investigating the integration of more sophisticated wave propagation models that consider spatial information, such as buildings or streets .sIOCTL call config daemon
9271	9285	communication characteristics . We are currently investigating the integration of more sophisticated wave propagation models that consider spatial information, such as buildings or streets .sIOCTL call config daemon transport layer network layer LLC sublayer emulation layer MAC sublayer physical layer Because the scenario controller does not process the actual traffic in the emulation
9271	2294	it is possible to do MANET routing without using location information, the efficiency in highly mobile scenarios can be improved significantly by using the node location as additional information . The nodes participating in location-based routing determine their own positions through positioning devices. Each node issues local broadcasts with its position (“beacons”) periodically. Based on
3723908	9292	work that considers down-sampling along the temporal dimension. In this case, the matrix A is MNQ×MNP with Q<P. The up-sampling matrix B then maps the lower frame-rate sequence to the higher rate P . 3.2 Closed Loop Design The previous design approach for B assumed a non-proprietary bit-stream, where the up-sampling matrix is not signaled. When a proprietary solution is acceptable, the
8919870	9316	10.s26 To confirm differentiated schooling returns for natives and migrants, column 5 includes the interaction of schooling years and a migrant dummy. Consistent with the previous literature (e.g., Eckstein and Weiss 1999) and columns 3 and 4, schooling returns are higher for natives than for migrants. Since natives are defined as those who stayed in Bangkok more than nine years, they are more experienced, and
8919870	9318	farmers in less-developed countries changed farming methods in response to post-green revolution availability and diffusion of new technologies such as imported high-yielding varieties (e.g., Foster and Rosenzweig 1995). The burst of information technology in the United States during the 1980s and a regime switch to market economies in former communist countries are good examples of structural change to a new
8919870	9318	returns to schooling in developing countries. Prior studies show that schooling facilitates learning speed when farmers face an opportunity to use a new technology, such as high-yielding varieties (Foster and Rosenzweig 1995). Recent evidence from the United States also supports a similar phenomenon: the burst of information technology in the 1980s might have increased returns to schooling and unobserved skills (e.g.,
8919870	9845	distribution of migrants in Bangkok 6 7 In his analysis of potential sources of schooling-experience complementarity in the use of high-yielding varieties in Indian agriculture, Rosenzweig (1995) provides an interesting insight. He shows some evidence that educated farmers (with primary schooling) in a new HYV technology regime learn the optimal use of inputs from new experience more
8919870	9845	action at the initial stage, and (3) be able to accomplish more complex tasks in urban labor markets. However, the problem in this paper differs from those in agricultural settings that Rosenzweig (1995) and Foster and Rozensweig (1995) examine. Even in a mathematically similar setting, this paper examines market wage dynamics, not production efficiency.s7 shows that more-educated migrants (more
8919870	9845	? ? E d + v | s ,? = E d | s ,? + E v | s ,? . z ( h, n) = µ ( s ) k k j0 ? j 12 Mathematically, this construction leads to similar specifications of Bayesian learning used in Foster and Rosenzweig (1995) and Jovanovic and Nyarko (1995). 13 The maximum labor productivity is ? n( s) k= 1 ? k .s13 at the initial stage. Again I do not ask whether workers or firms make the adjustment. Following the
8919870	9845	on experience. In the case of the simplest task, the complementarity decreases as experience increases. ?? ?t 3t j | < 0 ( ) > 0. n 1 since ? v s j ? = 17 Using an input-target model, Rosenzweig (1995) considers the case that ? ? () s 0 ? > , v () 0 s ? ? > in the context of agricultural production. He obtains that 2 ? ? ? ? > 0 if ?v( s) > 0and ??( s) ?0, ?? st where ? is farm profit. However,
9352	9364	Action File under Hockey Feature From Cc Date Subject Value S. Fillmore None October 23 Hockey pool Distance 1 0 0.90 0 (snew;s4) 0.9090 Action File under HockeysIn such a situation, MIT’s Maxims (Metral, 1993) e-mail agent would compute scores for each of the two candidate actions (File under David and File under Hockey), would choose the action with the higher score and would calculate a confidence
9352	9365	agents can be broadly classified into two categories: autonomous agents (e.g., Maes, 1994), which attempt to automate certain actions on behalf of the user, and collaborative agents (e.g., Rich and Sidner, 1997), which are more equal partners with their users, working together on a joint plan and participating in a dialogue in order to determine an appropriate course of action. We argue that there is a
9388	9385	Trajectory-based algorithms are becoming increasingly popular , . Storing and indexing trajectories facilitates not only efficient spatial range queries but time-and-space range queries . See also , .  discusses time-parameterized bounding rectangles and extends trajectory information with expiration information. There is a large body of literature on maintaining a
9388	9386	is a large body of literature on maintaining a specific property of moving objects. For example, a randomized algorithm for maintaining the binary space partition of moving objects is discussed in , and the maintenance of the dynamic Voronoi diagram of a set of moving points in a plane is presented in . For maintaining and querying a database of moving objects, see . Various
9388	9388	changes in the data structure is evaluated as a function of the number of events in the dynamic input data set. This part of our analysis is not shown here due to space constraints, and is found in . In cellular networks (e.g. mobile phone networks), a partial approach is to try to make advantage of the natural cells structure imposed by the network. If R is approximately the radius of a cell,
8919893	9916	government agencies as practice fields for the organizations intent on learning new problem-solving skills and capacities for action. Simulation also provide participates an environment that Schein (1993) indicates is necessary for organizational learning like a “safe” place for learning and “opportunity to try out new things without fear of punishment.” Keys, et al (1996) indicate that “research
8919893	9916	to say that &quot;debriefings must be extensive and performed by experts in the simulation, in the subject matter, and in group processes.” 1.3 Interactive Simulation & the Military Braddock and Thurman (1993) identify various forms of interactivity for simulation systems with associated afteraction-review systems used throughout the military. The extent of the integration of interactive simulation
8919901	9416	c visualization systems, data is mapped to a single abstraction model whose geometry is displayed and analyzed. But complex information systems deal with data of a much higher 2sdimensionality . To present the solution space for exploration, we invented a technique based on 4 abstraction models, each covering an aspect of decision making. Any one of the models allows the selection of the
8919905	9428	In our approach, we will follow the same research line using a more powerful scheduling algorithm that enables the generation of efficient RTL code and consequently more optimized results. Hiasynth  represents a radically different approach to integrate behavioral and RTL synthesis in the same design flow. RTL synthesis is used to execute behavioral synthesis tasks by means of a new design
8919905	9428	tools. In fact, the non-separation between control and data operations makes it easier to combine control and data chaining. This feature, very difficult to implement in behavioral synthesis , is performed very efficiently by FSM and RTL synthesis. The RTL module generator plays an important role when executed in parallel with resource allocation and binding. For example, there are
8919905	9956	are the following: the bandwidth is 155 Mbits/s supporting all types of traffic such as Variable Bit Rate (VBR), Constant Bit Rate (CBR), Unspecified Bit Rate (UBR) and Available Bit Rate (ABR)  and up to 4K connections can be managed simultaneously. The main timing constraint states that the processing of an ATM cell cannot exceed 106 clock cycles at 40 MHz operating frequency. More
10660737	9453	Communication refinement is a crucial design step in System-onChip (SoC) design since it has significant impact on the performance of implemented SoCs in terms of power, runtime, area, etc. . Communication refinement consists of two steps: communication network design and wrapper design. Communication network can be on-chip buses . circuit switch networks, packet switch networks,
10660737	9455	for communication between applications running on different processors, DSPs, IPs, etc. via communication networks. Wrappers are constructed by software (SW) in the form of operating systems (OS’s)  as well as in the form of hardware (HW) . Due to the constraints (in terms of performance, power, area, etc.) given to the embedded SoCs, the design of communication networks and wrappers
10660737	9455	buses, etc.) need to be determined to optimize the performance of SoC design . Wrapper designs can also be optimized. For instance, the size of embedded OS can be scalable or minimized . In terms of design space in such an optimization, there are huge numbers of design alternatives. Thus, to obtain practically optimal designs of communication networks and wrappers, design space
10660737	9455	micro-architecture level (for further details, refer to ). The SW wrapper is an OS that enables the application SW to perform inter/intra-processor communication (for further details, refer to ). To generate HW and SW wrappers, two libraries are used: OS library and HW wrapper library (Figure 1). According to the design decisions made by the designer, the wrapper generation can give
10660737	9455	a bus functional model (BFM) of processor. 4.2 Automatic Generation of OS Simulation Models To generate OS simulation models, we use the same method used to generate/configure real OS codes . The basic idea of this method is to find the OS services that are required by the application SW and then to generate their codes according to the target processor. Figure 3 exemplifies how to
10660737	9455	C codes) or processorindependent (e.g. normal C codes). Based on the dependency of services, the OS generation process performs a composition of the corresponding code sections as explained in . As mentioned in section 4.1, generation of OS simulation models is generating the real OS with the simulation environment as a virtual processor target. Figure 4 shows how to treat simulation
10660737	9457	on different processors, DSPs, IPs, etc. via communication networks. Wrappers are constructed by software (SW) in the form of operating systems (OS’s)  as well as in the form of hardware (HW) . Due to the constraints (in terms of performance, power, area, etc.) given to the embedded SoCs, the design of communication networks and wrappers needs to be optimized. For instance, communication
10660737	9457	level. In terms of implementation, the HW wrapper is a processor interface that connects the processor to the communication network at micro-architecture level (for further details, refer to ). The SW wrapper is an OS that enables the application SW to perform inter/intra-processor communication (for further details, refer to ). To generate HW and SW wrappers, two libraries are used:
10660737	9464	simulation performance. Thus, in our simulation flow, the designer can locate sync int functions by trading off between simulation performance and accuracy. In terms of modeling task preemption,  presents a method to model interrupt handling. In the work, processor modes are not separately modeled and it is assumed that the order of task execution does not change by the interrupt handling.
10660737	9465	4.5 Timing Calculation and Application to Con gurable OS's To calculate the execution delay values used in delay functions, we can use conventional estimation methods of SW execution time . To have processor-dependent delay values, before the OS simulation model is generated, the delay values are calculated for the target processor and then included in the OS simulation model. To
8919914	9474	lacking, but would be dropped from the definition (even if still believed) once the more important information could be observed or inferred. Though monotonic reasoning is preferred where possible , it is at times necessary to withdraw or modify previously held beliefs. Therefore, Cassie has been given an ability to analyze her beliefs, and to select (from among the beliefs supporting a
513578	9478	policies extracted from the optimal solution to the formulations. Our methodology corresponds to the achievable region method in the optimal control of queueing systems (see, e.g., the survey by Bertsimas 1995). The rest of the paper is structured as follows: Section 2 introduces the MQNET model. Section 3 develops a linear set of constraints based on the flow conservation law L ? ? L ? . Section 4
513578	10004	x’s performance region, that involves the matrix of auxiliary variables X, whereas the exact formulation on the original variables x was found to have exponential size in Bertsimas and Niño-Mora (1996). 2. Notice that constraints (5) do not involve changeover time parameters. This is because they are valid under any admissible scheduling policy, regardless of whether it is work-conserving. 3. An
513578	10004	holds with equality when S ? ?. Therefore, for the special case of zero changeover times performance vector x satisfies the generalized work conservation laws introduced by Bertsimas and Niño=Mora (1996), and thus the performance region spanned by the x’s under dynamic nonidling policies is the polyhedron defined by the family of inequalities (39), for S ? ?, together with the ? equation ¥ i?? V i
513578	9494	and 3) projecting back into the original space. Lift and project techniques have proven powerful tools for constructing tight relaxations of hard discrete optimization problems (see, e.g., Lovász and Schrijver 1991). We consider three types of auxiliary performance measures: The first type represents PalmsTABLE 1. Auxiliary performance measures. All performance measures Z in the table are defined immediately
9499	9499	we formulate the Bregman co-clustering problem in terms of the Bregman divergence between a given matrix and an approximation based on the co-clustering. 1 Proofs omitted due to lack of space, see  for details. We start by defining Bregman divergences . Let ? be a real-valued strictly convex function defined on the convex set S = dom(?) ? R, the domain of ?, such that ? is
9499	9499	C as the one in the class MA(?, ?, C) that minimizes the approximation error, i.e., ˆZ = argmin Z ? E. (2.5) 2.1 Minimum Bregman Information Interestingly, it can be shown  that the ???best” matrix approximation ˆ Z turns out to be the minimum Bregman information matrix among the class of random variables MB(?, ?, C) consisting of all Z ? ? S m×n that preserve the
9499	9499	as a special case when the Bregman divergence is squared Euclidean distance. The following theorem characterizes the solution to the minimum Bregman information problem (2.6). For a proof, see . Theorem 1 For a Bregman divergence d?, any random variable Z ? S m×n , a specified co-clustering (?, ?) and a specified constraint set C, the solution ˆ Z to (2.6) is given by ??( ˆ Z) = ? X ? ?
9499	9499	does not require solving the minimum Bregman information problem anew for each possible co-clustering. Further, the matrix approximations of the form ?(?, ?, ?) have a nice separability property  that enables us to decompose the matrix approximation error in terms of either the rows and columns: E = EU] = EV ], where ˜Z =
9499	9499	, but it is not necessarily the best approximation to Z of the form ?(? t+1 , ? t+1 , ?). Hence, we need to now optimize over the Lagrange multipliers keeping the co-clustering fixed. It turns out  that the Lagrange multipliers that result in the best approximation to Z are same as the optimal Lagrange multipliers of the minimum Bregman information problem based on the new co-clustering (?
9499	9500	literature  that illustrate the usefulness of particular instances of our Bregman co-clustering framework. In fact, a large class of parametric partitional clustering algorithms  including kmeans can be shown to be special cases of the proposed framework wherein only rows or only columns are being clustered. In recent years, co-clustering has been successfully applied to
9499	9500	that is both interpretable and applicable to various classes of matrices would be invaluable. The proposed Bregman co-clustering formulation attempts to address this requirement. Recent research  has shown that several results involving the KL-divergence and the squared Euclidean distance 3 http://www.research.compaq.com/src/eachmovie/ are in fact based on certain convexity properties and
9499	9502	Descriptors: I.2.6 : Learning General Terms: Algorithms Keywords: Co-clustering, Matrix Approximation, Bregman divergences 1. INTRODUCTION Co-clustering, or bi-clustering , is the problem of simultaneously clustering rows and columns of a data matrix. The problem of co-clustering arises in diverse data mining applications, such as simultaneous clustering of genes and
9499	9502	, users and movies in recommender systems, etc. In order to design a co-clustering framework, we need to first characterize the “goodness” of a co-clustering. Existing co-clustering techniques  achieve this by quantifying the “goodness” of a co-clustering in terms of the approximation error between the original Permission to make digital or hard copies of all or part of this work for
9499	9502	was used, which is a special case of I-divergence applicable to probability distributions.sany real values. This example with a uniform measure ? corresponds to the setting described in . A k × l co-clustering of a given data matrix Z is a pair of maps: ? : {1, · · · , m} ?? {1, · · · , k}, ? : {1, · · · , n} ?? {1, · · · , l}. A natural way to quantify the “goodness” of a
9499	9502	Bregman information solution (Table 2). Note that for the constraint set C4, this reduces to E?E+E) 2 ], which is identical to the objective function in .s3. A META ALGORITHM In this section, we shall develop an alternating minimization scheme for the general Bregman co-clustering problem (2.8). Our scheme shall serve as a meta algorithm from which
9499	9502	solution and the row and column cluster update steps can then be obtained from the optimal Lagrange multipliers. 4. EXPERIMENTS There are a number of experimental results in existing literature  that illustrate the usefulness of particular instances of our Bregman co-clustering framework. In fact, a large class of parametric partitional clustering algorithms  including kmeans can be
9499	9503	matrix approximation and learning based on Bregman divergences. Co-clustering has been a topic of much interest in the recent years because of its applications to problems in microarray analysis  and text mining . In fact, there exist many formulations of the co-clustering problem such as the hierarchical co-clustering model , the bi-clustering model  that involves finding the best
9499	9505	of co-clustering arises in diverse data mining applications, such as simultaneous clustering of genes and experimental conditions in bioinformatics , documents and words in text mining , users and movies in recommender systems, etc. In order to design a co-clustering framework, we need to first characterize the “goodness” of a co-clustering. Existing co-clustering techniques [5,
9499	9505	San Jose, CA, USA data matrix and a reconstructed matrix based on co-clustering. Of these techniques, the most efficient and scalable algorithms are those based on alternate minimization schemes , but these are restricted to only two distortion measures namely, KL-divergence and the squared Euclidean distance, and a few specific matrix reconstruction schemes. These two limitations restrict
9499	9505	clustering and co-clustering algorithms based on alternate minimization follow as special cases of our methodology. 1.1 Motivation We start by reviewing information-theoretic co-clustering  and motivating the need for a more general co-clustering framework. Let  m 1 denote an index u running over {1, · · · , m} and let X and Y be discrete random variables that take values in the
9499	9505	theoretic formulation of finding the optimal co-clustering is to solve the problem min ˆX, ˆ Y I(X; Y ) ? I( ˆ X; ˆ Y ) , (1.1)swhere I(X; Y ) is the mutual information between X and Y . In , it was shown that I(X; Y ) ? I( ˆ X, ˆ Y ) = D(p(X, Y )||q(X, Y )), (1.2) where q(X, Y ) is a distribution of the form q(X, Y ) = p( ˆ X, ˆ Y )p(X| ˆ X)p(Y | ˆ Y ), (1.3) and D(·||·) denotes the
9499	9505	smaller than the (mn ? 1) parameters that determine a general p(X, Y ). Hence, we call q(X, Y ) a “low complexity” or low parameter matrix approximation. The above is the viewpoint presented in . We now present an alternate viewpoint that highlights the key maximum entropy property that makes q(X, Y ) a “low complexity” or low parameter approximation. 1 Lemma 1 Given a fixed co-clustering
9499	9507	solution and the row and column cluster update steps can then be obtained from the optimal Lagrange multipliers. 4. EXPERIMENTS There are a number of experimental results in existing literature  that illustrate the usefulness of particular instances of our Bregman co-clustering framework. In fact, a large class of parametric partitional clustering algorithms  including kmeans can be
9499	9507	are difficult to interpret, which is necessary for data mining applications. Alternative techniques involving non-negativity constraints  using KLdivergence as the approximation loss function  have been proposed. However, these approaches apply to special types of matrices. A general formulation that is both interpretable and applicable to various classes of matrices would be invaluable.
9499	9508	and contingency tables, since SVD-based decompositions are difficult to interpret, which is necessary for data mining applications. Alternative techniques involving non-negativity constraints  using KLdivergence as the approximation loss function  have been proposed. However, these approaches apply to special types of matrices. A general formulation that is both interpretable and
8919917	9511	4. Data flow in workflows It is clear that discretionaryon control is sufficient for the lowest level of access control. The mandatory or some other type of axiomatic access control (e.g. RBAC ) has to be on the other side used for access control to workflows and their resources (e.g. RBAC) when a common security policy is to be enforced. 3. COOPERATION WITH ACCESS CONTROL Cooperation
8919917	9513	4. Data flow in workflows It is clear that discretionaryon control is sufficient for the lowest level of access control. The mandatory or some other type of axiomatic access control (e.g. RBAC ) has to be on the other side used for access control to workflows and their resources (e.g. RBAC) when a common security policy is to be enforced. 3. COOPERATION WITH ACCESS CONTROL Cooperation
8919917	9515	principles. Ideal seems to be authorization 1s2 Daniel Cvr?ek model that preservers and joins one security state with each particular task. General properties of such a model have been given in . 2. OVERVIEW OF SECURITY IN DISTRIBUTED SYSTEMS The first thing we have to do for solving security issues is to split the distributed system into homogenous (from the security point of view) parts
1096341	9522	standardization. The methods require training data and are adaptable to Asian-types of addresses that are particularly difficult for the rule-based methods. Christen et al.  and Churches et al.  apply hidden Markov models to both names and addresses. They show that it is quite straightforward to quickly generate training data by clerically standardizing a small number ofsrecords. They
1096341	9523	EM methods  that account for interactions of the variables. In the M-step, a general iterative fitting algorithm  that generalizes the iterative scaling algorithm of Della Pietra et al.  is used. The theoretical and computational aspects are fully described in . In particular, convex constraints (possibly based on prior knowledge) can be used to predispose the parameters P(?|M)
1096341	419	The conditional independence assumption corresponds exactly to the naïve Bayes assumption in machine learning . Winkler  showed how to estimate the probabilities using the EM-Algorithm . Winkler  demonstrated that the EM algorithm estimates optimal parameters in some situations. The estimated probabilities are particularly useful because the optimal m- and uprobabilities can
1096341	9528	of duplicates. Probabilistic Relational Models  use sophisticated methods of clustering on different variables to improve the matching using naïve Bayesian networks. Malin et al.  use Reidentification of Data in Trails (REIDIT) algorithms for tracking and identifying individuals visiting a set of web pages. The methods might be used to extend the bridging-file ideas. Torra
1096341	1258	area. Because it is expensive to obtain labeled training data, individuals have used methods for combining a small amount of training data with a large amount of unlabelled data. Nigam et al.  showed how to do the combining in a text classification application in applying naïve Bayes networks. Winkler  extended the use of labeled and unlabeled data to a model where various
1096341	9536	P( ? | M), the u-probabilities P (? | U), and the proportion P(M). The conditional independence assumption corresponds exactly to the naïve Bayes assumption in machine learning . Winkler  showed how to estimate the probabilities using the EM-Algorithm . Winkler  demonstrated that the EM algorithm estimates optimal parameters in some situations. The estimated probabilities
1096341	9539	surname. If a pair of records agrees on last name, then the pair is likely, with probability close to one, to agree of household characteristics such as house number, street name, and phone number . Pairs of records representing two individuals from the same household can be associated with both matches and nonmatches. If ratio (1) is computed under the conditional independence assumption
1096341	9539	combined into thosesassociated with two classes when decision rule (2) is applied. This partially accounts for dependencies of the household variables. Alternatively, we can use general EM methods  that account for interactions of the variables. In the M-step, a general iterative fitting algorithm  that generalizes the iterative scaling algorithm of Della Pietra et al.  is used. The
1096341	9539	subsets of the parameter space. In a narrow range of situations, the parameters obtained by the general fitting procedures yield both good decision rules and accurate estimates of the error rates . The accuracy of the estimates of error rates is partially confirmed in Larsen and Rubin  who extended the EM methods with general MCMC methods. 2.5 Combining Labeled Training Data with
1096341	9539	applied. In record linkage, because training data is seldom available, unsupervised learning under conditional independence is typically used. The theoretical and computational methods of Winkler  extend to situations where a combination of labeled training data and unlabeled data are used. The reason for using labeled training data is that unsupervised learning methods will not always give
1096341	9539	training data had a tendency to significantly reduce the number of computational paths and get the estimates closer to those obtained with large amounts of training data. In earlier work, Winkler  observed that purely unsupervised methods would often yield parameter estimates that were totally unsuitable for accurate decision rules. The unsuitable estimates even occurred in situations where
1096341	9540	methods work poorly for names  when compared to rule-based methods. 4.2 Approximate String Comparison Typographical variations in the spelling of words are prevalent in computer files. Winkler  observed that approximately 25% of first names and 15% of last names for matches could not be compared character-by-character in three sites of a dress rehearsal Census. Using ideas of Pollock and
1096341	9541	classification rules and (3) analytic linking methods for creating extra information during the linkage process to improve linkage and resultant analyses of linked files. 4.1 Preprocessing Winkler  describes methods of preprocessing for business names and general addresses. For a name, we wish to replace titles such as ‘Doctor’ and ‘Dr.’ and words such as ‘Corporation’ and ‘Corp.’ with
1096341	9541	‘CORP’, respectively. When applicable, we wish to identify words such as first name, middle name, and last name so that they can be compared. We also wish do standardization on addresses. Winkler  applies rule-based logic in a business name standardizer that also works well agriculture lists having partnerships and with person lists. He also applies rule-based methods developed by the Census
1096341	9542	error rate estimates were reasonably accurate that Scheuren and Winkler  could use the estimated error rates in a statistical model that adjusts regression analyses for linkage errors. Winkler  observed, however, that the methods only worked well in a narrow range of situations where the curves associated with matches M and nonmatches U were well-separated and had several other desirable
1096341	9544	in applying naïve Bayes networks. Winkler  extended the use of labeled and unlabeled data to a model where various interactions could be accounted for. In a record linkage application, Winkler  demonstrated how the use of small amounts of training data could be combined with unlabeled data in a record linkage application. The main advantage of the record linkage application was that it
1096341	9544	done by the Belin and Rubin  methods. A secondary advantage was that it can be used in datamining experiments to determine what are the suitable interactions between the matching fields. Winkler  further observed that a sufficient amount of training data was needed for combining with the unlabeled data. The training data had a tendency to significantly reduce the number of computational
1096341	9545	P(?) is above 0.03 and for which the EM can yield optimal parameters. Recent verification of the efficacy of this approach and extensions to the methods for choosing the pairs are due to Yancey  and Elfekey et al. . 2.4 Accounting for Dependencies between Fields Individual fields used in matching can have strong dependencies between them. For instance, assume that we are using name,
9547	2312	perform admirably in many settings, they are not scalable when the traffic pattern involves many source-destination pairs. One approach that is potentially very attractive is geographic routing . Here, nodes know, and are identified by, their geographic coordinates. Routing is done greedily; at each step in the routing process, nodes pick as nexthop the neighbor that is closest to the
9547	9548	depends on carefully placed beacons that self-organize themselves into a hierarchical structure. Our beacons are randomly chosen, and we don’t require them to establish any structure. cessing . Without having a scalable pointto-point routing algorithm, we will not even be able to test these ideas. Thus, we see our proposal as an enabler for the next round of exploration of sensornet
9547	9549	depends on carefully placed beacons that self-organize themselves into a hierarchical structure. Our beacons are randomly chosen, and we don’t require them to establish any structure. cessing . Without having a scalable pointto-point routing algorithm, we will not even be able to test these ideas. Thus, we see our proposal as an enabler for the next round of exploration of sensornet
9547	9550	depends on carefully placed beacons that self-organize themselves into a hierarchical structure. Our beacons are randomly chosen, and we don’t require them to establish any structure. cessing . Without having a scalable pointto-point routing algorithm, we will not even be able to test these ideas. Thus, we see our proposal as an enabler for the next round of exploration of sensornet
9552	7943	(Eschenbach et al. 1998), levels of granularity in referring to objects situated in space (Habel 1991), and levels of specification regarding the task to be fulfilled (as observed by Moratz & Fischer 2000 in an experiment on human-robot communication). As communicating with robots navigating in space necessarily involves communicating about space, it is necessary to determine how humans would
9552	7943	or unconscious – hypotheses about a robot’ssperceptual equipment? How does their conceptualization of the robot’s abilities influence their communication strategies? Experimental work (Moratz & Fischer 2000) reveals that even those users who knew only very little about robots, interacted with the robot in a way that reflected their beliefs about robots, of which they were – according to their verbal
9552	7943	that seem to anticipate what their users believe and expect, and behave accordingly, both functionally and linguistically. Human users react strongly to the linguistic output of artificial systems (Fischer 2000), adapting to what the robot's feedback suggests to them about its functionality. Because the users usually have a strong motivation to be understood by their interaction partner, they acknowledge
9552	9558	(Eschenbach et al. 1998), levels of granularity in referring to objects situated in space (Habel 1991), and levels of specification regarding the task to be fulfilled (as observed by Moratz & Fischer 2000 in an experiment on human-robot communication). As communicating with robots navigating in space necessarily involves communicating about space, it is necessary to determine how humans would
9552	9558	or unconscious – hypotheses about a robot’ssperceptual equipment? How does their conceptualization of the robot’s abilities influence their communication strategies? Experimental work (Moratz & Fischer 2000) reveals that even those users who knew only very little about robots, interacted with the robot in a way that reflected their beliefs about robots, of which they were – according to their verbal
9552	9558	that seem to anticipate what their users believe and expect, and behave accordingly, both functionally and linguistically. Human users react strongly to the linguistic output of artificial systems (Fischer 2000), adapting to what the robot's feedback suggests to them about its functionality. Because the users usually have a strong motivation to be understood by their interaction partner, they acknowledge
9552	9562	function according to the user's expectations. Thus, robots may be built for entertainment such as Sony's &quot;dog&quot; that basically behaves like a dog, but that is also being trained to learn language (Kaplan 2000). Other robots may be designed for service, for industry, medical help, or for the assistance of handicapped people (Röfer & Lankenau, 2000). For spatially situated robots, there are limitations
9552	9569	as moving around in space, or talking. Similarly, other robots might be built solely to walk up stairs, or, involving very different kinds of problems, to participate in a robot soccer game (e.g., Weigel et al. 2000). Moreover, there are limitations regarding the kinds of features, or abilities, to be implemented. Generally, the perceptual abilities of robots are on a strictly metric (or quantitative) level.
394740	9571	remaining page content. Collapse-to-zoom uses gestures for collapsing and expanding content (a so-called marquee menu). For a more general taxonomy of selection gestures see paintable interfaces . Unlike gestures in existing systems, such as power browser , marquee menus offer command gestures that simultaneously define a target area and the command to be executed on that area.
394740	9572	a member of the family of generalized fisheye views . Existing research prototypes allow users to collapse text sections (active outlining ) or pan a fisheye lens across the page (fishnet ). Collapse-to-zoom is different from these in that it allows users to collapse tiles in a 2D layout, such as cells in a table. In particular, collapse-to-zoom uses space gained from collapsing
394740	9139	briefly describe our implementation before we conclude with a summary and an outlook on future work. RELATED WORK There are four general approaches to displaying web pages on small screen devices : device-specific authoring; multiple-device authoring; automatic re-authoring, and client-side navigation. The first two allow obtaining highquality results by taking the specifics of the device
394740	9573	a wide range of techniques. Outlining transforms pages into sets of tiles. Tiles may then be presented to the user one at a time (power browser ) or as a layout of cards (flip zooming , WebThumb ). As mentioned above, other browsers combine tiling with zooming, so that users can use an overview to access the individual tiles (e.g., ). The approach of collapsing
394740	9575	category of client-side navigation which encompasses a wide range of techniques. Outlining transforms pages into sets of tiles. Tiles may then be presented to the user one at a time (power browser ) or as a layout of cards (flip zooming , WebThumb ). As mentioned above, other browsers combine tiling with zooming, so that users can use an overview to access the individual tiles (e.g.,
394740	9575	and expanding content (a so-called marquee menu). For a more general taxonomy of selection gestures see paintable interfaces . Unlike gestures in existing systems, such as power browser , marquee menus offer command gestures that simultaneously define a target area and the command to be executed on that area. COLLAPSE-TO-ZOOM: A WALKTHROUGH Figure 2 gives a walkthrough of
394740	9578	This prevents these techniques from being applied to already existing pages, which limits the practical applicability of these approaches. Automatic re-authoring, such as web page summarization  does not require the collaboration of page authors; neither does it require any user effort. However, techniques in this category are limited to what can be extracted from the page, such as its
394740	9581	(Figure 3b), etc. Dragging the pen on the screen creates a rectangular selection enclosing the start and end point of the drag gesture. Marquee menus also share properties known from marking menus , namely the use of a directional gesture for command selection. The main strength of marquee menus is that they combine area selection and command selection into a single gesture. The user performs
8919921	10119	software development and version tracking, it provides facilities for hosting the WWW project pages, and Sourceforge is well known . 3.3 The License: The GNU General Public License (GPL)  was chosen. GPL is one of the most successful and powerful opensource licenses so far. GPL is an open source license, one of its background is the motivation of the “gift culture” . Also we are
8718103	9593	4.1 (Krahl and Lamperti 1997). A previous version of this paper (Schriber and Brunner 1996) covered SIMAN (Pegden, Shannon and Sadowski, 1995), ProModel (ProModel Corporation 1995), and GPSS/H (Crain and Henriksen 1999) in similar detail. These five are among more than forty tools reported in 2001 for discrete-event simulation (Swain 2001). Some other tools might be better suited than any of these for particular
8718103	9603	important issues. The factors that affect system performance can be the number of processing units, memory type and bus structure, arbitration schemes, and workload (Kornecki and Zalewski 1998) (Jonkers 1994). In this paper, we focus on the factor of embedded I/O buffers of the processing units, which has not been explored in detail in prior works. In a shared-memory system, the time to access the data
9606	9607	services and distributed-system technologies, including IP multicast , other rendezvousbased communication services such as the internet indirection infrastructure (i3) , intentional naming , and distributed publish/subscribe systems , . 1 However, as for the specific problem of routing in a content-based network, only a 1 A comparative analysis of the service model and general
9606	9608	predicates using the concrete syntax and semantics embodied in the Siena event notification service . Note that in this regard, Siena is largely consistent with other publish/subscribe systems , ,  and with existing standards for application-level publish/subscribe services , . Thus, a message is a set of typed attributes. Each attribute is uniquely identified within the
9606	9608	rooted at node 6. The propagation path is represented in the figure by thick black arrows. The table attached to (a) (b) 1 2    3 4 i p 6 p6   5 6 p6 1 2 i p 4 p6 3 4  i p 5 2 6 p2 p6 6 Fig. 4. Receiver Advertisement Protocol (RA) p2 (p2 ? p6) node 4 in Figure 4a represents the routing table of node 4 after node 4 has processed the RA. After this first RA gets
9606	9608	IP multicast , other rendezvousbased communication services such as the internet indirection infrastructure (i3) , intentional naming , and distributed publish/subscribe systems , . 1 However, as for the specific problem of routing in a content-based network, only a 1 A comparative analysis of the service model and general architecture of all these systems is available
9606	9609	of the form of messages and predicates. To instantiate this concept, we define messages and predicates using the concrete syntax and semantics embodied in the Siena event notification service . Note that in this regard, Siena is largely consistent with other publish/subscribe systems , ,  and with existing standards for application-level publish/subscribe services , .
9606	9609	Before proceeding with the description of the content-based routing protocol, we briefly review the concept of contentbased address  and that of covering relation between content-based addresses . We define the content-based address of a node as a predicate—a total boolean function— that identifies the messages of interest for that node. In the following, we will use the terms content-based
9606	9609	including IP multicast , other rendezvousbased communication services such as the internet indirection infrastructure (i3) , intentional naming , and distributed publish/subscribe systems , . 1 However, as for the specific problem of routing in a content-based network, only a 1 A comparative analysis of the service model and general architecture of all these systems is available
9606	9610	service whereby the flow of messages from senders to receivers is driven by the content of the messages, rather than by explicit addresses assigned by senders and attached to the messages . Using a content-based communication service, receivers declare their interests by means of selection predicates, while senders simply publish messages. The service consists of delivering to any
9606	9610	protocol produces symmetric routes. B. Preliminary Definitions Before proceeding with the description of the content-based routing protocol, we briefly review the concept of contentbased address  and that of covering relation between content-based addresses . We define the content-based address of a node as a predicate—a total boolean function— that identifies the messages of interest
9606	9611	is therefore forwarded along the set of interfaces defined by the following formula: (B(source(m), incoming if(m)) ?{I0}) ? FC(m) We describe an efficient implementation of this formula elsewhere . D. Routing State The content-based routing module of a CBCB router maintains a routing table that associates a content-based address px to each interface Ix. Consistently with the forwarding
9606	9611	and operators. We do not discuss the numerous parameters that control the generation of messages and predicates in this paper (we do that in our study of the performance of a forwarding algorithm ). Instead, we provide a high-level characterization of messages and predicates by showing, in Figure 10, the distribution of matching predicates for a typical workload. This figure shows that most
9606	9611	However, as for the specific problem of routing in a content-based network, only a 1 A comparative analysis of the service model and general architecture of all these systems is available elsewhere . few of these systems offer solutions that are related to the work presented in this paper. A large body of work has been devoted to advanced network services such as IP multicast. Indeed, IP
9606	9611	traffic. The routing protocol presented in this paper is part of, and builds upon our research work in the area of contentbased networking, complementing our work on content-based forwarding . As a natural progression of this work, we plan to study policy issues and quality-of-service parameters in content-based networking, with the intent of incorporating these aspects into the design
9606	9612	routers, and lightcolored edges represent direct (physical or overlay) links. Initially (Figure 4a) node 6, which is assigned content-based address p6 by its local applications, issues the RA . That RA propagates through the network following the broadcast tree rooted at node 6. The propagation path is represented in the figure by thick black arrows. The table attached to (a) (b) 1 2
9606	9612	a method to synchronize the routing tables as a whole. Recently, Chand and Felber have proposed XRoute, a routing algorithm for a content-based network that uses XML data and XPath expressions . XRoute is designed for unrestricted topologies, and explicitly for content-based routing. Therefore, it is the closest research work to our routing scheme. We believe, however, that XRoute makes
9606	9614	using the concrete syntax and semantics embodied in the Siena event notification service . Note that in this regard, Siena is largely consistent with other publish/subscribe systems , ,  and with existing standards for application-level publish/subscribe services , . Thus, a message is a set of typed attributes. Each attribute is uniquely identified within the message
9606	3026	the protocol produce a reasonable and stable amount of control traffic? A. Experimental Setup The topologies created for our experiments are flat, random, router-level topologies generated by BRITE  using the Waxman edge selection algorithm . Bandwidth is assumed to be unlimited along all links. In our experiments, we simulate the broadcast layer by implementing a global broadcast
9606	8995	of advanced network services and distributed-system technologies, including IP multicast , other rendezvousbased communication services such as the internet indirection infrastructure (i3) , intentional naming , and distributed publish/subscribe systems , . 1 However, as for the specific problem of routing in a content-based network, only a 1 A comparative analysis of the
9606	9615	using the concrete syntax and semantics embodied in the Siena event notification service . Note that in this regard, Siena is largely consistent with other publish/subscribe systems , ,  and with existing standards for application-level publish/subscribe services , . Thus, a message is a set of typed attributes. Each attribute is uniquely identified within the message by a
9616	9617	with adaptive systems theory ; Also, see, the book 'The Knowledge Management Fieldbook' . 2 See, e.g., , , , . 3 See, e.g., , , , , , . 4 See, e.g., , , , , .s“studies or disciplines are distinguishable by the subject matter which they investigate,  appears to me to be a residue from the time when one believed that a theory
9616	1023	vocabularies of basic terms and a precise specification of what those terms mean. Numerous researchers believe that the web ontology can be the useful tools for the following reasons (revised from ): a) The web ontology is more than an agreed vocabulary. It provides a set of wellfounded constructs that can be leveraged to build meaningful higher level knowledge. The terms in ontology are
9616	9625	theory ; Also, see, the book 'The Knowledge Management Fieldbook' . 2 See, e.g., , , , . 3 See, e.g., , , , , , . 4 See, e.g., , , , , .s“studies or disciplines are distinguishable by the subject matter which they investigate,  appears to me to be a residue from the time when one believed that a theory had to proceed from a
9616	9636	with adaptive systems theory ; Also, see, the book 'The Knowledge Management Fieldbook' . 2 See, e.g., , , , . 3 See, e.g., , , , , , . 4 See, e.g., , , , , .s“studies or disciplines are distinguishable by the subject matter which they investigate,  appears to me to be a residue from the time when one believed that a theory had
9616	9645	and Young, Deloitte and Touche, Ernst and Young, KPMG Consulting, McKinsey and Company, and PricewaterhouseCoopers . But, what about KM for solving problems in an engineering domain, e.g., , , , ? Engineering is a human effort to change or facilitate a kind of environment in order to make that environment more suitable or responsive to perceived human needs and wants. Such
9678	9680	applications may use aggregated context, such as location and user activities, to teleport an X Window session , to automatically route a phone call , to guide a user through a space , to proactively remind users of their tasks , to help users interact with nearby objects , to coordinate group interaction , to help occupants of a space control the environment
9678	9680	, and to automate elder care tasks . Extensions of similar applications to a wider area, such as a campus or city, include location-based content delivery (tour guide) and annotation . Schilit and others give a categorization 5 http://architecture.mit.edu/house n/ 6 http://www.gatech.edu/innovations/awarehome/ 4sof typical context-aware applications in such environments .
9678	9607	advertisements and queries. We then discuss the details of our context-sensitive resource discovery framework and its evaluation, using Intentional Naming System (INS) as the core directory service . Finally we briefly present, in Section 4.5, a newly designed distributed directory service that can replace INS for improved scalability. 4.1 Naming representation The name space of the data
9678	9607	and persistent queries in such a name space. Our infrastructure is designed to leverage existing distributed directory services; for our prototype we built the distributed directory using INS , but any other service providing an attribute-based registration and look-up interface would suffice (Section 4.5). Since one of our goals is to reduce load on thin-client devices and on the
9678	9607	existing research on (and implementations of) scalable, distributed, and flexible directory services. In particular, we used the Intentional Naming System (INS) to implement the directory service . To Solar we add INS resolvers, as shown in Figure 4.5. The shaded circle is an operator that may process events from the source to provide context for an application’s query or a source’s
9678	9607	(3) contextual events; (4) actual query based on current context; and (5) actual advertisement based on current context. 4.3.1 Extending INS INS is a resource-discovery and communication system . In conventional networks, a name service resolves names to addresses, then routers route messages to destination addresses. In INS, a distributed collection of resolvers form an overlay network
9678	9607	not enforce the alternative tag-value semantics on the record structure. Solar also supports persistent queries to allow monitoring of the name space while INS/Twine does not. 4.6 Related work INS  unifies resource discovery (naming) and communication (message routing). Applications desiring context-sensitive names, however, must monitor the context themselves and re-advertise their name as
9678	9683	context such as meetings, often with the assistance of other inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by
9678	9684	server while the other two protocols are fully distributed. Several P2P protocols coming out of the research community are designed specifically to handle a large number of participating nodes , such as Pastry , Chord , Tapestry , CAN , and SkipNet . They all provide a self-organized routing substrate in which each peer has a unique numeric key. The interface these
9678	9685	that query key stores all the advertisements containing the corresponding query strand, the query can be resolved appropriately. This naming distribution and query mechanism is similar to INS/Twine . Note that, however, we do not enforce the alternative tag-value semantics on the record structure. Solar also supports persistent queries to allow monitoring of the name space while INS/Twine does
9678	9685	Solar infrastructure, our framework improves responsiveness and scalability. INS/Twine achieves more scalability by partitioning the name space across resolvers by mapping names into numeric keys . Solar could use INS/Twine as its core directory service, but would need a different mechanism to implement persistent queries. Instead, we chose to adapt Pastry and include support for persistent
9678	9686	zone she ever visited in the trace, and the sum of all the prevalence values for that user is 1. This metric is defined and used to characterize user mobility in a corporate wireless LAN study . In their study, a user is categorized as “highly mobile, somewhat mobile, regular, occasionally mobile, and stationary” by segmenting maximum and median prevalence into several bins to see to
9678	9686	We compute the maximum and median prevalence for each user, and Figures 6.7–6.10 show the scatterplots of every user for the four localization systems. The segmentation lines (using the values from ) are drawn in dashed lines. To avoid overplotting of same-valued points, we added randomized jitter in range of  on both x and y axis for every point. We first look at the two plots
9678	9686	indeterministic times also have been investigated in various applications . We compute the percentage of the users in each mobility category (defined using median and maximum prevalence, as in ) for all four location systems and show the results in Table 6.3. Each entry has the following format: (Versus/IR, Versus/RF), (Campus/syslog, Campus/SNMP). Versus/IR and Versus/RF traced over the
9678	9687	source. Recent work using an overlay of event brokers to provide a content-based pub/sub service has focused on routing and matching scalability and has largely ignored end-to-end flow control . Pietzuch and Bhola, however, study the congestion-control issues in the context of the Gryphon network . Congestion in the whole system cannot be solved by simply interconnecting nodes with
9678	9688	session , to automatically route a phone call , to guide a user through a space , to proactively remind users of their tasks , to help users interact with nearby objects , to coordinate group interaction , to help occupants of a space control the environment , to assist with child education , to assist with laboratory experiments , and to automate
9678	9689	of data processing. In particular, there is no direct support for data fusion. PQS is an information-fusion engine that tries to identify a process by observing its output data 17sstream . It currently employs a JMS messaging middleware, which could be replaced with Solar for large-scale deployments. On the other hand, PQS contains several ready-to-use fusion models that can be used
9678	9689	work with our deployed sensors. We are currently expanding our operator library to support advanced fusion algorithms that follow some common models, somewhat similar to PQS’s pluggable processes . We believe that this will be an important step to increase both code-based and instance-based operator reuse. Solar is designed to work in a LAN environment where Planets run inside a single
9678	9690	overlay network for its flexibility of placement and easier deployment. Based on AN, Bhattacharjee and others propose to manage congestion by dropping data units based on source-attached policies . Receiver-driven layered multicast (RLM)  actively detects network congestion and finds the best multicast group (layer) for the multimedia application to join. Pasquale et al. put
9678	9693	inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by researchers . While it may be possible to
9678	9694	, and to automate elder care tasks . Extensions of similar applications to a wider area, such as a campus or city, include location-based content delivery (tour guide) and annotation . Schilit and others give a categorization 5 http://architecture.mit.edu/house n/ 6 http://www.gatech.edu/innovations/awarehome/ 4sof typical context-aware applications in such environments .
9678	9696	recovery time and thus offer higher availability . One ROC technique is recursive restartability (RR), which groups components by their restart dependencies instead of functional dependencies . When error or malfunction is detected, including component failure, the RR system proactively restarts the minimum component group containing the offending one. Our approach has a smaller scope
9678	9697	that as the language gets more powerful and more complex filters are supported, it is more likely that PACK will involve more overhead for filter execution and eventually reduce system scalability . Based on our experience so far, many losstolerant applications desire simple and straight-forward policies. Thus our strategy is to keep the filters simple and efficient, and to expand the filter
9678	9697	during congestion since each overlay node has only limited resources for all the policies. Carzaniga and others discuss the tradeoff between expressiveness and scalability in a similar context . One approach to relieve the situation is to limit the flexibility of PACK policies. For instance, RLM essentially uses a set of hierarchical filtering layers that apply naturally to multimedia
9678	9609	source. Recent work using an overlay of event brokers to provide a content-based pub/sub service has focused on routing and matching scalability and has largely ignored end-to-end flow control . Pietzuch and Bhola, however, study the congestion-control issues in the context of the Gryphon network . Congestion in the whole system cannot be solved by simply interconnecting nodes with
9678	9698	existing autonomous Web services has generated considerable interest in recent years. Researchers have been working on composition languages , specification toolkits , support systems , and load balancing and stability algorithms . If we consider sensors in Solar as output-only services, we also could enhance Solar’s composition model with previous results, such as a more
9678	9699	existing autonomous Web services has generated considerable interest in recent years. Researchers have been working on composition languages , specification toolkits , support systems , and load balancing and stability algorithms . If we consider sensors in Solar as output-only services, we also could enhance Solar’s composition model with previous results, such as a more
9678	8978	top of its peer-to-peer routing substrate. ALM improves the scalability of data dissemination and does not rely on IP multicast, which is often turned off in practice. While ALM is not a new idea , buffer overflow management remains a challenge in ALM. In this chapter, we first present the basics of ALM over Solar in Section 3.1. We motivate the data reduction requirements to handle buffer
9678	8978	network and the soft-state based root, the root becomes a natural entry point for inter-component coordination. This indirection-based technique has been used to manage large-scale event multicast  and host mobility . 5.2 Load balancing In this section, we briefly discuss the load balancing service in Solar. Most of this work is done by Ming Li. When an application requests the
9678	7897	top of its peer-to-peer routing substrate. ALM improves the scalability of data dissemination and does not rely on IP multicast, which is often turned off in practice. While ALM is not a new idea , buffer overflow management remains a challenge in ALM. In this chapter, we first present the basics of ALM over Solar in Section 3.1. We motivate the data reduction requirements to handle buffer
9678	7897	is only duplicated at every parent Planet for each child in the multicast tree. Castro et al. present and compare some protocols that can be used to build an ALM on DHT-based peer-to-peer overlays . Mobile clients may experience temporary disconnection caused by weak links or mobility handoffs. During disconnection, a client may roam and change its network address (network mobility), and it
9678	7897	but PACK uses its own TCP transport service to disseminate events rather than Pastry’s transport library, which has a mixed UDP/TCP mode and its own internal message queues. We 29sused Scribe  to maintain application-level multicast trees for PACK to populate the subscription policies. Since PACK works on the queues accumulated above TCP (namely, after a sender’s TCP buffer is filled),
9678	9700	in the underlying network, is usually below two; and 2) the paths for messages sent to the same key from nearby nodes in the underlying network converge quickly after a small number of hops . 2.2.2 Planet architecture Planets are execution environments for operators and they cooperatively provide several operatormanagement functionalities, such as naming and discovery, routing of
9678	9700	by key. Simulations on realistic network topologies show that the paths for messages sent to the same key from nearby nodes in the underlying network converge quickly after a small number of hops , so this message aggregation may significantly suppress protocol overhead. 5.1.5 Evaluation In this section we present some results on monitoring and migration protocols. We performed the
9678	9701	context such as meetings, often with the assistance of other inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by
9678	9703	language, a rule-based composition engine, and algorithms for achieving certain quality of composed service. Other emergency response and battlefield projects, such as Rescue , EventWeb , and JBI , all have their own middleware systems for distributed data management. In particular, EventWeb and JBI have an event-processor composition model similar to the one used by Solar.
9678	9705	of and adaptive to the situation in which they are running to avoid exposing unnecessary information and complexity to end users. There are many definitions of context in the research community , since context may involve many aspects such as the state of the physical space, of the human users, and of the computational resources. Lieberman and Selker loosely define context to be any input
9678	9705	is an overloaded term. Many researchers have different perspectives on the definition of context, which typically involves the state of physical space, human users, and computational resources . In particular, context is a piece of application-specific and time-sensitive information. Given desired context, applications may automatically adapt to the 1 http://www.cs.cmu.edu/˜aura 2
9678	9705	research directions in our survey of context-aware computing, such as contextsensing techniques, context modeling and representation, supporting system infrastructure, and security and privacy . In this dissertation, we focus on a flexible and scalable context-fusion infrastructure that collects, aggregates, and disseminate contextual information. 1.2 Context-aware applications One
9678	9706	. We implemented the first prototype, with a centralized architecture for simplicity, for two “pervasive-computing” seminar courses in which Solar was used by students to develop applications . Our experience with the first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and
9678	9707	as input and acts as another data source. We have implemented two Solar prototypes, both in Java. Both prototypes adopted an operator composition programming model and similar design choices . We implemented the first prototype, with a centralized architecture for simplicity, for two “pervasive-computing” seminar courses in which Solar was used by students to develop applications [34,
9678	9707	graphs in similar situations, increasing the opportunities for re-use of data streams. Similarly, programmers will be likely to name operators for use by other users or in other applications . The currently available literature about ContextSphere says little about how it manages its composers or other systems issues. The EventHeap used by the iRoom project employs a tuplespace model,
9678	9707	The above temperature sensor might be named sensor camera product . It is arguable whether one approach has clear advantages over the other . In either case the name should be a descriptive handle. In one case the description is a tuple of attributes and values, and in the other case the same attributes may be implicit in the structure
9678	9708	architecture, and the software package consisted of more than 13,000 lines of code. The later version also makes several research contributions on its generalizable operator-management services . In this chapter, we first discuss Solar’s operator composition model and then present the system architecture and services that manage the application-supplied operators. We discuss some related
9678	9709	architecture, and the software package consisted of more than 13,000 lines of code. The later version also makes several research contributions on its generalizable operator-management services . In this chapter, we first discuss Solar’s operator composition model and then present the system architecture and services that manage the application-supplied operators. We discuss some related
9678	9710	“pervasive-computing” seminar courses in which Solar was used by students to develop applications . Our experience with the first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second
9678	9712	interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second version of Solar . The second prototype used a fully distributed and self-organized architecture, and the software package consisted of more than 13,000 lines of code. The later version also makes several research
9678	9712	services: operator hosting and execution, sensor/operator registration and discovery, data dissemination through operator graphs, operator monitoring and recovery, and operator garbage collection . Later in this chapter, we describe a clean system architecture, in which inter-service invocations are local and intra-service communications are hid12sden. S A P S P P Figure 2.6: Solar consists
9678	9713	. We implemented the first prototype, with a centralized architecture for simplicity, for two “pervasive-computing” seminar courses in which Solar was used by students to develop applications . Our experience with the first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and
9678	9714	, and to automate elder care tasks . Extensions of similar applications to a wider area, such as a campus or city, include location-based content delivery (tour guide) and annotation . Schilit and others give a categorization 5 http://architecture.mit.edu/house n/ 6 http://www.gatech.edu/innovations/awarehome/ 4sof typical context-aware applications in such environments .
9678	9714	in the HTTP header. Currently we insert information about the building as a text bar on top of the client requested page. Similarly, a location-prediction service could instruct a Guide application  on a mobile device to prefetch content based on the next likely stop. To provide this kind of service, a locator subscribes to the syslog source and monitors all devices’ associations with the
9678	9714	such as correlation between several APs . Other techniques that explicitly involve user interaction during these indeterministic times also have been investigated in various applications . We compute the percentage of the users in each mobility category (defined using median and maximum prevalence, as in ) for all four location systems and show the results in Table 6.3. Each
9678	9717	proxy interface, matching on the class of the proxy. The Service Location Protocol (SLP) focuses on the protocol for automatic discovery , Czerwinski et al. focus on expressiveness and security , and Castro et al. deal with inter-domain service discovery . DeapSpace  and Heidemann et al.  propose approaches for discovery in ad-hoc sensor networks. None of these systems,
9678	4076	may not be the most efficient, for instance, if the sinks of multiple channels are on the same host. A common solution to improve the efficiency of such data dissemination problem is multicast . The idea is to aggregate the channels and build a minimum spanning tree out of the network topology. Thus an event is only duplicated at a parent node for all its children, instead of being
9678	9719	location and user activities, to teleport an X Window session , to automatically route a phone call , to guide a user through a space , to proactively remind users of their tasks , to help users interact with nearby objects , to coordinate group interaction , to help occupants of a space control the environment , to assist with child education , to assist
9678	9719	lag should be taken into account by any application that requires spontaneous interaction using collocation information. In particular, applications like a memory-aid  or active reminder  may miss some events and fail to act if two people only briefly meet in the hallway. The sighting interval is also a useful metric to measure the freshness or confidence of the location
9678	9719	the sighting interval to degrade the information freshness gracefully . The detection latency also may have impacts on many applications, such as a reminder application that uses collocation , teleporting , and call forwarding application that follows user’s current location . None of our four localization systems provided an explicit location update when the user left the
9678	9720	distributed data-fusion for higher-level contextual understanding. Dey and others propose the Context Toolkit, wrapping sensors with a widget abstraction while computing context using aggregators . Hong and Landy propose the Context Fabric that answers context queries with an automatically constructed data-flow path by selecting appropriate operators from a repository . Both systems,
9678	9720	Load Balance Heartbeat RPC Figure 2.9: Partial service interaction on a Planet. Directory ping sensors with a widget abstraction and by providing pre-defined aggregators for commonly used context . Its data-flow representation of aggregators bears some similarity to Solar’s operator graph. The system structure, however, is established by administrators and becomes static at runtime. Solar
9678	9720	be the high-level specification language the compiler could decompose into a graph specification used by Solar. The Context Toolkit provides several abstractions to construct a context service . It is a distributed architecture supporting context fusion and delivery. It uses a widget to wrap a sensor, through which the sensor can be queried about its state or activated. Applications can
9678	9720	to applications. It can shield the data pre-processing from applications, and share the results with multiple applications. Several major research efforts specifically target this direction . In the case where the data rate still may outrun the capability of the infrastructure, approximation techniques have to be applied to reduce the data stream (Chapter 3). 6.3.3 Load disparity Many
9678	9721	number of hops and thread scheduling effects at the destination Planet. 5.1.6 Related work The concept of data fusion is essential for component-based context-aware systems, such as Context Toolkit  and ContextSphere . Until now, we have not seen a general service, like Solar provides, to manage the dependencies between the distributed data-fusion components. We believe our service
9678	9722	its subscription at RP . Then R asks RP for all the buffered events before requesting data from RP ? . The sequence number in the events is used to prevent duplicated 20 RP Rsdelivery. Fiege et al.  present another approach that we could use to handle host mobility. 3.2 Buffer overflow Consider a receiver R that takes actions on received events. If consuming an event does not block R from
9678	6554	traffic and increases scalability through instance-based reuse. 2.1.1 Filter-and-pipe pattern One popular software architectural pattern for data-stream oriented processing is filter-and-pipe , which supports reuse and composition naturally. In a filter-and-pipe style, as shown in Figure 2.1, each component (filter) has a set of inputs and a set of outputs. A component reads streams of
9678	9725	office buildings or residential homes such as MIT’s House n 5 and Georgia Tech’s Aware Home. 6 These smart spaces typically contain instrumented sensors and actuators, augmented daily objects , and unconventional devices . A CFN can provide a unified approach to connect the sensors and their applications, each of which may use a CFN to customize individual information
9678	9727	, and to automate elder care tasks . Extensions of similar applications to a wider area, such as a campus or city, include location-based content delivery (tour guide) and annotation . Schilit and others give a categorization 5 http://architecture.mit.edu/house n/ 6 http://www.gatech.edu/innovations/awarehome/ 4sof typical context-aware applications in such environments .
9678	9728	allows a client to locate a service and download its proxy interface, matching on the class of the proxy. The Service Location Protocol (SLP) focuses on the protocol for automatic discovery , Czerwinski et al. focus on expressiveness and security , and Castro et al. deal with inter-domain service discovery . DeapSpace  and Heidemann et al.  propose approaches for
9678	9729	telephone that automatically transferred incoming call to voice mail without interrupting ongoing meetings. It is not surprising that many regular building occupants refuse to wear a badge , and visitors in our open academic building, of course, never wear a badge. We designed the meeting detector to detect a meeting’s status by aggregating two kinds of sensors input. We first
9678	8811	context such as meetings, often with the assistance of other inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by
9678	8811	each of which may use a CFN to customize individual information needs. The applications may use aggregated context, such as location and user activities, to teleport an X Window session , to automatically route a phone call , to guide a user through a space , to proactively remind users of their tasks , to help users interact with nearby objects , to
9678	5907	coming out of the research community are designed specifically to handle a large number of participating nodes , such as Pastry , Chord , Tapestry , CAN , and SkipNet . They all provide a self-organized routing substrate in which each peer has a unique numeric key. The interface these protocols expose is sending a message to a numeric key. The message will be
9678	9730	context such as meetings, often with the assistance of other inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by
9678	9732	(implicit input) and can be obtained from one of many commercial or experimental location-provision systems, each of which has different accuracy, precision, and coverage in a particular deployment . It is necessary to combine the output of all available locators to yield the best location estimation. For instance, Hightower and others used a trained Bayesian Network for this purpose . To
9678	9733	. It is necessary to combine the output of all available locators to yield the best location estimation. For instance, Hightower and others used a trained Bayesian Network for this purpose . To deliver the message based on the user’s interruptibility, such as to queue the message temporarily if the user is having a meeting with her boss, the application may have to leverage more
9678	9733	inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by researchers . While it may be possible to
9678	9733	context service, since there is no way for any service provider to foresee all the context needs of diverse applications. Instead, a CFN must allow both deployment of well-known context services , and application-specific customization and user-specific personalization. In addition, a CFN should not limit the expressiveness of the data-fusion algorithms. Instead, it should accommodate
9678	9737	to proactively remind users of their tasks , to help users interact with nearby objects , to coordinate group interaction , to help occupants of a space control the environment , to assist with child education , to assist with laboratory experiments , and to automate elder care tasks . Extensions of similar applications to a wider area, such as a campus or
9678	2243	semantics such as reliable in-order data transport. When computational and network resources are limited, these protocols have to either regulate the sender’s rate or disconnect the slow receivers . The usual alternative, UDP/IP, has no guarantees about delivery or ordering, and forces applications to tolerate any and all loss, end to end. Our goal, on the other hand, is to trade reliability
9678	9739	inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by researchers . While it may be possible to
9678	9740	says little about how it manages its composers or other systems issues. The EventHeap used by the iRoom project employs a tuplespace model, which aims to decouple the data producer and consumer . This loosely-coupled coordination model reduces component interdependency and allows easy recovery from crashes. The simple interface of a tuplespace, tuple retrieval based on pattern matching,
9678	9740	components, and does not directly apply to other coordination models . For instance, Stanford’s Intelligent Room system provides temporally-decoupled communication over a tuple space . Here we can only say a component depends on the tuple-space service while each component may be individually 1 Dell GX260, 2.0 GHz CPU, 512 MB RAM, and running Red Hat Linux 9 64sRecovery time
9678	9741	a phone call , to guide a user through a space , to proactively remind users of their tasks , to help users interact with nearby objects , to coordinate group interaction , to help occupants of a space control the environment , to assist with child education , to assist with laboratory experiments , and to automate elder care tasks . Extensions of
9678	9742	may experience hot paths in the system. The buffer size along these paths need to handle peak traffic not seen in a test phase. Any software infrastructure, such as a lattice-based location service , also will see similar phenomena. A virtual counterpart, 72sFigure 6.3:  Distribution of average sighting intervals in minutes. Cumulative distribution 1 0.8 0.6 0.4 0.2 SNMP syslog 0 0 100
9678	9744	threads on a single processor. Example implementations of filter-and-pipe style in practice include Unix pipes , some Web servers , modern language designs , and a software router . While Solar uses the filter-and-pipe pattern as its basic structure, we have several additional design considerations. First, we need a fan-in and fan-out structure, since a context computation
9678	9745	context such as meetings, often with the assistance of other inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by
9678	9745	with our goal and complicates the task. This suggests that our take-asis approach to these systems has limited usage, if without advanced processing such as correlation between several APs . Other techniques that explicitly involve user interaction during these indeterministic times also have been investigated in various applications . We compute the percentage of the users in
9678	9746	inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by researchers . While it may be possible to
9678	9747	homes such as MIT’s House n 5 and Georgia Tech’s Aware Home. 6 These smart spaces typically contain instrumented sensors and actuators, augmented daily objects , and unconventional devices . A CFN can provide a unified approach to connect the sensors and their applications, each of which may use a CFN to customize individual information needs. The applications may use aggregated
9678	5749	the one used by Solar. Their supporting architectures are not clear from the literature. Data aggregation is also a useful technique inside sensor networks to reduce unnecessary data transmission . Unlike Solar, these systems work at a lower level and are designed for a resource-constrained environment, where the focus is on power consumption and communication costs. They are often designed
9678	5749	also explicitly support disconnect operations caused by end-host mobility. Data aggregation also is a useful technique inside sensor networks to reduce unnecessary transmission, such as TAG . While designed for different purposes, both TAG and PACK try to enforce application-specific policies. The goal of TAG is to apply the aggregation wherever and whenever possible in the sensor
9678	9750	. Our experience with the first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second version of Solar . The second prototype used a fully distributed and self-organized
9678	9751	location and user activities, to teleport an X Window session , to automatically route a phone call , to guide a user through a space , to proactively remind users of their tasks , to help users interact with nearby objects , to coordinate group interaction , to help occupants of a space control the environment , to assist with child education , to assist
9678	9751	first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second version of Solar . The second prototype used a fully distributed and self-organized architecture, and the software package consisted of
9678	9751	(Section 6.2) location-tracking infrastructure. 86sFigure 7.6: An active Web portal shows customized content to approaching user. Earlier, we built a “smart reminder” application using Solar . The goal of this application was to determine the appropriate time to alert its user about the next appointment based on the user’s current location and the location of the next task on the
9678	5363	to relieve the situation is to limit the flexibility of PACK policies. For instance, RLM essentially uses a set of hierarchical filtering layers that apply naturally to multimedia data streams . 3.5.3 Client attach/detach As a mobile client detaches from and re-attaches to its proxy, PACK suspends and resumes its event queue in the RPB buffer (located at RR). To measure how this operation
9678	5363	and easier deployment. Based on AN, Bhattacharjee and others propose to manage congestion by dropping data units based on source-attached policies . Receiver-driven layered multicast (RLM)  actively detects network congestion and finds the best multicast group (layer) for the multimedia application to join. Pasquale et al. put sink-supplied filters as close to the audio/video
9678	5363	From the application’s point of view, their protocols are no different than traditional approaches, and there is no explicit support for mobile clients. Receiver-driven layered multicast (RLM)  leverages the fact that multimedia streams can be encoded in different layers (rates), each of which requires different bandwidth. The receivers then join only the multicast group (corresponding to
9678	9752	several sensor readings. Similarly, an ActiveMap application can adapt to loss of location-change updates by fading the object at its current location as a function of time since the last update . One reason these applications are able to tolerate data delivery loss is that they are designed to cope with unreliable sensors, which also may lead to data loss and inaccuracy. In this chapter,
9678	9752	10 3sconfident it can be of the user’s current location. One active map application uses image fading and frame shading during the sighting interval to degrade the information freshness gracefully . The detection latency also may have impacts on many applications, such as a reminder application that uses collocation , teleporting , and call forwarding application that follows user’s
9678	9753	applications in such environments . Another application area is Emergency Response, dealing with a disaster or crisis, either natural or man-made, in a timely and effective manner . In such situations, diverse sensors, which are either deployed in the environment, carried by victims and responders, or installed on various equipment and vehicles, produce a huge amount of data.
9678	9753	powerful composition language, a rule-based composition engine, and algorithms for achieving certain quality of composed service. Other emergency response and battlefield projects, such as Rescue , EventWeb , and JBI , all have their own middleware systems for distributed data management. In particular, EventWeb and JBI have an event-processor composition model similar to the one
9678	9755	. Our experience with the first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second version of Solar . The second prototype used a fully distributed and self-organized
9678	9755	is not secure and is vulnerable to malicious attacks, and there is no access control enforced to guard information privacy. We plan to integrate an automated access control propagation facility  and provide some degree of security by encrypting the data communications or leveraging existing solutions such as PKI (Public-Key Infrastructure). It is generally not easy to add security
9678	9755	applicable to an infrastructure-free (ad-hoc) environment. Security and privacy issues are beyond the scope of our research focus; interested readers may find more information in another paper . 4.2.1 Name specification Solar provides a light-weight specification language that can be used to specify context-sensitive advertisements and queries. The idea is to define some attribute value,
9678	9756	the audio/video 37ssource as possible to save network bandwidth . The Neem protocol removes the “obsolete” messages, as indicated by the source, or random messages when its queue becomes full . Our work, however, aims at broader categories of applications and must support sink-customized policies since the source typically cannot predict how the sinks want to manipulate the sensor data.
9678	9757	since the aggregated results coming out of a sensor network could supply one event stream to Solar. Recently we have seen many research efforts on continuous query over data streams from a database . A large part of these efforts focus on the algorithmic efficiency of implementing SQL-like queries over data streams with little attention to more general data fusion or to support systems. It may
9678	9758	inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by researchers . While it may be possible to
9678	9761	finds the best multicast group (layer) for the multimedia application to join. Pasquale et al. put sink-supplied filters as close to the audio/video 37ssource as possible to save network bandwidth . The Neem protocol removes the “obsolete” messages, as indicated by the source, or random messages when its queue becomes full . Our work, however, aims at broader categories of applications
9678	9762	on routing and matching scalability and has largely ignored end-to-end flow control . Pietzuch and Bhola, however, study the congestion-control issues in the context of the Gryphon network . Congestion in the whole system cannot be solved by simply interconnecting nodes with TCP because the overlay is constructed in application space above TCP. Their solution is to apply additional
9678	9764	semantics such as reliable in-order data transport. When computational and network resources are limited, these protocols have to either regulate the sender’s rate or disconnect the slow receivers . The usual alternative, UDP/IP, has no guarantees about delivery or ordering, and forces applications to tolerate any and all loss, end to end. Our goal, on the other hand, is to trade reliability
9678	9765	service by composing existing autonomous Web services has generated considerable interest in recent years. Researchers have been working on composition languages , specification toolkits , support systems , and load balancing and stability algorithms . If we consider sensors in Solar as output-only services, we also could enhance Solar’s composition model with
9678	9767	interest in recent years. Researchers have been working on composition languages , specification toolkits , support systems , and load balancing and stability algorithms . If we consider sensors in Solar as output-only services, we also could enhance Solar’s composition model with previous results, such as a more powerful composition language, a rule-based
9678	9768	existing autonomous Web services has generated considerable interest in recent years. Researchers have been working on composition languages , specification toolkits , support systems , and load balancing and stability algorithms . If we consider sensors in Solar as output-only services, we also could enhance Solar’s composition model with previous results, such as a more
9678	9769	KX ? KY if X depends on Y and KX and KY are their keys. One approach to manage the dependencies is for the dependent of a component X to monitor X’s liveness using a soft-state based protocol  and trigger a restart process whenever the dependent detects the failure of X. This approach is conceptually easy but difficult to achieve efficiently and scalably. First, the dependent may be
9678	9770	context such as meetings, often with the assistance of other inputs. The recognition of the importance of location context has resulted in numerous active projects in location-provision systems  and location models . The need to apply data-fusion to multiple incoming sensor streams to improve the quality of computed context has recently been recognized by
9678	939	Several P2P protocols coming out of the research community are designed specifically to handle a large number of participating nodes , such as Pastry , Chord , Tapestry , CAN , and SkipNet . They all provide a self-organized routing substrate in which each peer has a unique numeric key. The interface these protocols expose is sending a message to a numeric key. The
9678	9771	This significant detection lag should be taken into account by any application that requires spontaneous interaction using collocation information. In particular, applications like a memory-aid  or active reminder  may miss some events and fail to act if two people only briefly meet in the hallway. The sighting interval is also a useful metric to measure the freshness or confidence of
9678	5902	equivalent hosts named Planets. The Planets connect with each other to form a service overlay using an application-level distributed hashtable (DHT) based peer-to-peer (P2P) routing protocol , as shown in Figure 2.6. The Planets are denoted as P , and they connect sensors S and applications A, and cooperatively execute data-fusion operators (filled circles). A sensor may connect to any
9678	5902	other two protocols are fully distributed. Several P2P protocols coming out of the research community are designed specifically to handle a large number of participating nodes , such as Pastry , Chord , Tapestry , CAN , and SkipNet . They all provide a self-organized routing substrate in which each peer has a unique numeric key. The interface these protocols expose is
9678	5902	or to move up one level to l+1 if ?l exceeds a negative threshold (?0.1). Otherwise, PACK uses the previous level. 3.5 Evaluation Our implementation is based on Java SDK 1.4.1. We chose Pastry  as the overlay routing protocol, but PACK uses its own TCP transport service to disseminate events rather than Pastry’s transport library, which has a mixed UDP/TCP mode and its own internal
9678	9773	top of its peer-to-peer routing substrate. ALM improves the scalability of data dissemination and does not rely on IP multicast, which is often turned off in practice. While ALM is not a new idea , buffer overflow management remains a challenge in ALM. In this chapter, we first present the basics of ALM over Solar in Section 3.1. We motivate the data reduction requirements to handle buffer
9678	8814	users, portable but resource-constrained hardware platforms, heterogeneous and volatile wireless networks, non-conventional user interfaces, and the need for dependable and adaptive system software . In particular, an information-rich pervasive-computing environment could be overwhelming to the users surrounded by embedded devices. To gracefully integrate a computation and communication
9678	8814	and context-triggered actions. Later Satyanarayanan summarized the challenges for context-aware pervasive computing, such as context representation, middleware support, and adaptation models . Like many other terms, however, “context” is an overloaded term. Many researchers have different perspectives on the definition of context, which typically involves the state of physical space,
9678	9774	integration with human users, since the early research days. Schilit and other researchers at Xerox PARC categorize context-aware applications that they built on top of their ParcTab platform . They identify four context usage patterns: proximate selection, automatic contextual reconfiguration, contextual information and commands, and context-triggered actions. Later Satyanarayanan
9678	9774	58, 19]. Schilit and others give a categorization 5 http://architecture.mit.edu/house n/ 6 http://www.gatech.edu/innovations/awarehome/ 4sof typical context-aware applications in such environments . Another application area is Emergency Response, dealing with a disaster or crisis, either natural or man-made, in a timely and effective manner . In such situations, diverse sensors, which are
9678	9775	context-aware applications. It uses a centralized architecture to manage location information about dozens of active badges in a building, deployed to support daily usage of the ParcTab system . ActiveMap has limited scalability, but is sufficient to meet its specific goals. User agents and device agents selectively publish their representative’s current location to the map server
9678	9775	to applications. It can shield the data pre-processing from applications, and share the results with multiple applications. Several major research efforts specifically target this direction . In the case where the data rate still may outrun the capability of the infrastructure, approximation techniques have to be applied to reduce the data stream (Chapter 3). 6.3.3 Load disparity Many
9678	9778	homes such as MIT’s House n 5 and Georgia Tech’s Aware Home. 6 These smart spaces typically contain instrumented sensors and actuators, augmented daily objects , and unconventional devices . A CFN can provide a unified approach to connect the sensors and their applications, each of which may use a CFN to customize individual information needs. The applications may use aggregated
9678	9779	office buildings or residential homes such as MIT’s House n 5 and Georgia Tech’s Aware Home. 6 These smart spaces typically contain instrumented sensors and actuators, augmented daily objects , and unconventional devices . A CFN can provide a unified approach to connect the sensors and their applications, each of which may use a CFN to customize individual information
9678	9781	are fully distributed. Several P2P protocols coming out of the research community are designed specifically to handle a large number of participating nodes , such as Pastry , Chord , Tapestry , CAN , and SkipNet . They all provide a self-organized routing substrate in which each peer has a unique numeric key. The interface these protocols expose is sending a
9678	9782	application-specific computation, including filtering, inside networks is not a new idea. In particular, it is possible to implement our PACK service using a general Active Network (AN) framework . We, however, chose an overlay network for its flexibility of placement and easier deployment. Based on AN, Bhattacharjee and others propose to manage congestion by dropping data units based on
9678	9784	of context-sensitive names. Solar also uses peer Planets to cooperatively service clients’ requests and to disseminate contextual events, for better responsiveness and scalability. Active Names  is a flexible approach to locate a service and perform customized operations on the returned results by allowing applications to specify a chain of mobile programs through which the result should
9678	9785	communication, either through TCP/IP or DHT transport, is important to ensure low service coupling. The local access to a remote service through a downloaded proxy in Jini shares a similar idea . Our architectural approach also allows us to extract some common functionalities from several services and consider them primitive services. For instance, both the RPC service, which provides
9678	9786	discovery, however, should be similar to the results presented in Section 4.4. We are currently evaluating the performance of the new directory service of Solar in a Masters student’s thesis . Recall that every Solar service, such as the naming service, runs on all Planets in the network. The new Solar distributes the name space (the advertisements) across all the Planets using the
9678	9787	first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second version of Solar . The second prototype used a fully distributed and self-organized architecture, and the software package consisted of
9678	9789	homes such as MIT’s House n 5 and Georgia Tech’s Aware Home. 6 These smart spaces typically contain instrumented sensors and actuators, augmented daily objects , and unconventional devices . A CFN can provide a unified approach to connect the sensors and their applications, each of which may use a CFN to customize individual information needs. The applications may use aggregated
9678	9790	designing wireless networks, and writing new operating systems, middleware infrastructure, and applications. Among the three devices, ParcTab is considered by many to be the most significant effort . The constraints on the hardware capability, however, prevented pervasive computing from thriving at that time. Context-awareness has been a key approach to meet the goal of pervasive computing,
9678	7065	a pipe-and-filter system on parallel processors or in multiple threads on a single processor. Example implementations of filter-and-pipe style in practice include Unix pipes , some Web servers , modern language designs , and a software router . While Solar uses the filter-and-pipe pattern as its basic structure, we have several additional design considerations. First, we need a
9678	9794	in which Solar was used by students to develop applications . Our experience with the first prototype, including an analysis of a sensor environment , performance and interoperability , the security and access control design , and several application studies , contributed to the design and implementation of a second version of Solar . The second prototype
9678	9796	distributed. Several P2P protocols coming out of the research community are designed specifically to handle a large number of participating nodes , such as Pastry , Chord , Tapestry , CAN , and SkipNet . They all provide a self-organized routing substrate in which each peer has a unique numeric key. The interface these protocols expose is sending a message to a numeric
9678	9797	based root, the root becomes a natural entry point for inter-component coordination. This indirection-based technique has been used to manage large-scale event multicast  and host mobility . 5.2 Load balancing In this section, we briefly discuss the load balancing service in Solar. Most of this work is done by Ming Li. When an application requests the deployment of an operator graph,
4493284	9801	error in state estimation is proportional to the number of landmarks used. An explicit solution to the SLAM problem for a onedimensional vehicle called the monobot was presented by Gibbens et al. . It shed some light on the relation between the total number of landmarks and the asymptotic This work was supported by the Spanish Council of Science and Technology under project DPI 2001-2223. A
4493284	9801	we will also introduce their associated measurement noise. It has been argued that the performance of any SLAM algorithm would be enhanced by concentrating on fewer, better landmark observations . That is certainly true, little gain (little reduction in ?) is attained when going from 25 to 125 landmarks compared to the move from 1 to5or5to25. In Fig. 4 we have plotted the results of using
4493284	9802	Pr,0|0 = V = W =1, and various sizes for the measurement vector. ?k i=1 ˜z? i|i?1S?1 i ˜z i|i?1 as small as possible. A measure that is consistent with the spatial compatibility test described in . The Fisher information matrix, a quantification of the maximum existing information in the observations about the state x, is defined in  as the expectation on the dyad of the gradient of ln
4493284	9806	errors do not converge to zero. Their steady state value is subject to the error incurred at the first observation. That is, the filter is marginally stable (the matrix F ? KHF has a pole in one ). A Montecarlo simulation over 100 SLAM runs showed however filter unbiasedness, a property of optimals1 landmark 2 landmarks 20 landmarks x r,k ?x r,k|k (m) 2 1 0 ?1 ?2 ?3 0 20 40 60 Iteration 80
9807	9808	image coder at low bit-rate mostly depends on its non-linear approximation capability. However, good non-linear approximation results do not necessarily imply efficient coding schemes . The contourlet transform is a new geometrical image-based transform (Fig. 1), which is recently introduced by Do and Vetterli . They showed that, when compared to wavelets, the contourlet
9807	9809	results do not necessarily imply efficient coding schemes . The contourlet transform is a new geometrical image-based transform (Fig. 1), which is recently introduced by Do and Vetterli . They showed that, when compared to wavelets, the contourlet transform better approximates images in which textures and oscillatory patterns have a significant presence. However, it is a
9807	9809	results do not necessarily imply efficient coding schemes . The contourlet transform is a new geometrical image-based transform (Fig. 1), which is recently introduced by Do and Vetterli . They showed that, when compared to wavelets, the contourlet transform better approximates images in which textures and oscillatory patterns have a significant presence. However, it is a redundant
3685723	9829	L. Mamatas and V. Tsaoussidis Dept. Of Electrical and Computer Engineering Demokritos University of Thrace, Greece Email: emamatas@ee.duth.gr, vtsaousi@ee.duth.gr applications. Authors in  propose TCPfriendly protocols that satisfy two fundamental goals: (i) To achieve smooth window adjustments. This is done by reducing the window decrease ratio during congestion. (ii) To compete
3685723	6533	metrics. In section 4 we analyze the results of our experiments and in section 5 we highlight our conclusions. 2. Trading ? For ? A throughput equation for standard TCP is first introduced in . GAIMD  extends the equation to include parameters ? and ?: 1 T? , ? ( p, RTT, T0, b) = 2 2b( 1? ? ) ? ( 1 ) b ? 2 RTT p T0 min? ? ? + 1, 3 p ? p( 1+ 32p ) ?( 1+ ? ) ? 2? ? ? ? (1) where p is
3685723	9830	detection and classification that would permit a responsive strategy, oriented by the nature of the error detected (congestion in wired networks versus transient random errors in wireless networks) . As we show, implementation of such strategy requires ocassionally a more responsive TCP. Our approach, however, is dominated by the distinctive characteristics and requirements of wireless
3685723	6534	L. Mamatas and V. Tsaoussidis Dept. Of Electrical and Computer Engineering Demokritos University of Thrace, Greece Email: emamatas@ee.duth.gr, vtsaousi@ee.duth.gr applications. Authors in  propose TCPfriendly protocols that satisfy two fundamental goals: (i) To achieve smooth window adjustments. This is done by reducing the window decrease ratio during congestion. (ii) To compete
9832	9833	data types distributed in the web. How to effectively organize and process such large variety and quantity of musical data to allow efficient indexing, searching and retrieval is a real challenge . There have been many studies on audio content analysis using different features and different methods . In spite of many research efforts, high accuracy audio classification is only
9832	9834	features and different methods . In spite of many research efforts, high accuracy audio classification is only achieved for relative simple problems such as speech/music discrimination . Other works attempt to classify audio records into speech, silence, laughter and non–speech sounds. Relatively few works have dealt with musical genre classification . Most of such
9832	9836	data types distributed in the web. How to effectively organize and process such large variety and quantity of musical data to allow efficient indexing, searching and retrieval is a real challenge . There have been many studies on audio content analysis using different features and different methods . In spite of many research efforts, high accuracy audio classification is only
9832	9837	data types distributed in the web. How to effectively organize and process such large variety and quantity of musical data to allow efficient indexing, searching and retrieval is a real challenge . There have been many studies on audio content analysis using different features and different methods . In spite of many research efforts, high accuracy audio classification is only
9832	9839	information fusion, classifier combination. 1 Introduction The amount of multimedia now available on–line has created a surge for efficient tools to organize and manage such a huge amount of data . At present, multimedia data is usually classified based on textual meta–information. While such information is very useful for indexing, sorting, comparing and retrieval, it is manually generated.
9832	9839	data to allow efficient indexing, searching and retrieval is a real challenge . There have been many studies on audio content analysis using different features and different methods . In spite of many research efforts, high accuracy audio classification is only achieved for relative simple problems such as speech/music discrimination . Other works attempt to classify audio
9832	9843	for a complete music information retrieval system for audio signals. It is extremely more difficult to discriminate musical genres than discriminate music, speech and other sounds. Soltau et al  classified music into rock, pop, techno and classic using hidden Markov models and explicit time modeling with neural networks to extract the temporal structure from the sequence of cepstral
9832	9845	beat, and a number of features concerning the relationship between the first and second beat. The signal is decomposed into a number of frequency bands using the discrete wavelet transform . After this decomposition, a series of steps for the extraction of the time domain amplitude envelope is applied to each band. These steps are full wave rectification, low pass filtering,
9832	9846	discrimination . Other works attempt to classify audio records into speech, silence, laughter and non–speech sounds. Relatively few works have dealt with musical genre classification . Most of such works have focused on relatively few classes of very distinct musical genres. Furthermore, most of the works have used ? 0-7803-8566-7/04/$20.00 c? 2004 IEEE. non–parametric
9832	9846	have dealt with small databases. Musical genre is an important description that has been used to classify and characterize digital music and to organize the large collections available on the web . Genre hierarchies are commonly used to structure the large collections of music available on the web. Furthermore, music genre might be very useful for music indexing and content–based music
9832	9846	Pye  used Mel– frequency cepstral coefficients and Gaussian mixture model to classify music into six types: blues, easy listening, classic, opera, dance, and rock. Tzanetakis and Cook  explored features related to the timbral texture, rhythm and pitch. Gaussian mixture model and k–nearest neighbor classifiers were used to classify the extracted features. Shao et al  used an
9832	9846	region and the last segment from the end region of the music. Figure 2 illustrates the feature extraction process. The feature set used in this paper was originally proposed by Tzanetakis et al  and used in other works . We consider two different types of features: musical surface features and beat–related features. Musical surface features include the mean and average of the spectral
9832	9846	a series of steps for the extraction of the time domain amplitude envelope is applied to each band. These steps are full wave rectification, low pass filtering, downsampling, and mean removal . After the envelope extraction step, the envelopes of each band are summed and the autocorrelation of the resulting envelope is calculated. The result is an autocorrelation function where the
9832	9848	data to allow efficient indexing, searching and retrieval is a real challenge . There have been many studies on audio content analysis using different features and different methods . In spite of many research efforts, high accuracy audio classification is only achieved for relative simple problems such as speech/music discrimination . Other works attempt to classify audio
9849	9850	and by  the maximum in-degree of a node in the network. Among these parameters, only r is known to nodes of the network. 1.1 Related Work In many papers on broadcasting in radio networks (e.g., ), the network is modeled as an undirected graph, which is equivalent to the assumption that the directed graph, which models the network in our scenario, is symmetric. A lot of eort has been
9849	9858	symmetric networks, diameter is of the order of the eccentricity). As for broadcasting in ad hoc symmetric radio networks, an O(n) algorithm assuming spontaneous transmissions was constructed in  and a lower bound (D log n) was shown in , in the case when spontaneous transmissions are precluded. Deterministic broadcasting in arbitrary directed radio networks was studied, e.g., in [6-13,
9849	9858	The best known lower bound on this time is (n log D), proved in . As for the upper bounds, a series of papers presented increasingly faster algorithms, starting with time O(n 11=6 ), in , then O(n 5=3 log 1=3 n) in , O(n 3=2 p log n) in , O(n 3=2 ) in , andsnally, O(n log 2 n) in , which corresponds to the fastest currently known algorithm, working for ad hoc
9849	9859	As for the upper bounds, a series of papers presented increasingly faster algorithms, starting with time O(n 11=6 ), in , then O(n 5=3 log 1=3 n) in , O(n 3=2 p log n) in , O(n 3=2 ) in , andsnally, O(n log 2 n) in , which corresponds to the fastest currently known algorithm, working for ad hoc networks of arbitrary maximum degree. In another approach, broadcasting time is
9849	9860	in ad hoc radio networks was investigated, e.g., in . We use the same denition of running time of a broadcasting algorithm working for ad hoc radio networks, as e.g., in . We say that the algorithm works in time t for networks of a given class, if t is the smallest integer such that the algorithm informs all nodes of any network of this class in at most t steps. We
9849	9860	of papers presented increasingly faster algorithms, starting with time O(n 11=6 ), in , then O(n 5=3 log 1=3 n) in , O(n 3=2 p log n) in , O(n 3=2 ) in , andsnally, O(n log 2 n) in , which corresponds to the fastest currently known algorithm, working for ad hoc networks of arbitrary maximum degree. In another approach, broadcasting time is studied for ad hoc radio networks of
9849	9860	broadcasting algorithm working in time O(n log n log D) for arbitrary n-node ad hoc radio networks of eccentricitysD. This improves the best currently known broadcasting time O(n log 2 n) from , e.g., for networks of eccentricity polylogarithmic in size. Also, for D 2 !(n), this improves the upper bound O(D log n log(n=)) from . The best currently known lower bound on broadcasting
9849	9860	the gap between bounds on deterministic broadcasting time for radio networks of arbitrary eccentricity, to a logarithmic factor. Our algorithm is non-constructive, in the same sense as that from . Using the probabilistic method we prove the existence of a combinatorial object, which all nodes use in the execution of the deterministic broadcasting algorithm. (Since we do not count local
9849	9860	of networks, even assuming that nodes know parameters n and D. The best previous upper bound on broadcasting time in complete layered n-node ad hoc radio networks of eccentricity D was O(n log n) . Hence we obtain a gain for the same range of values of D as before. If nodes do not know any upper bound on the size of the network, the upper bound O(n log 2 n) from  remains valid, using a
9849	9863	and by  the maximum in-degree of a node in the network. Among these parameters, only r is known to nodes of the network. 1.1 Related Work In many papers on broadcasting in radio networks (e.g., ), the network is modeled as an undirected graph, which is equivalent to the assumption that the directed graph, which models the network in our scenario, is symmetric. A lot of eort has been
9849	9863	that nodes have full knowledge of the network. In  the authors proved the existence of a family of n-node networks of radius 2, for which any broadcast requires time (log 2 n), while in  it was proved that broadcasting can be done in time O(D + log 5 n), for any n-node network of diameter D. (Note that for symmetric networks, diameter is of the order of the eccentricity). As for
9849	9866	and by  the maximum in-degree of a node in the network. Among these parameters, only r is known to nodes of the network. 1.1 Related Work In many papers on broadcasting in radio networks (e.g., ), the network is modeled as an undirected graph, which is equivalent to the assumption that the directed graph, which models the network in our scenario, is symmetric. A lot of eort has been
9849	9866	know n but not ). If n is also unknown, the algorithm from  works in time O(D log a n log(n=)), for any a > 1. Finally, randomized broadcasting in ad hoc radio networks was studied, e.g., in . In , the authors give a simple randomized protocol running in expected time O(D log n + log 2 n). In  it was shown that for any randomized broadcast protocol and parameters D and n, there
9868	10452	system put in place after 1994, many features of fiscal contracting continue to exist because the central government made allowances for the vested interest of local governments. 4 Lin and Liu (2000) used econometric methods to show that the decentralized fiscal system contributed to economic growth, but its impact on interregional imbalances is beyond the scope of their analysis. 5 The feature
9868	10455	(Kanemoto 1990). 6 See Krugman (1991) and Fujita, Krugman, and Venables (1999) on a mechanism that generates industrial clustering. With regard to economic development, refer to Otsuka and Sonobe (2001). According to an analysis of Krugman, whether enterprises cluster or not depends on economies of scale at the plant level, the share of manufacturing in the national economy, and transportation
9868	10456	(Kanemoto 1990). 6 See Krugman (1991) and Fujita, Krugman, and Venables (1999) on a mechanism that generates industrial clustering. With regard to economic development, refer to Otsuka and Sonobe (2001). According to an analysis of Krugman, whether enterprises cluster or not depends on economies of scale at the plant level, the share of manufacturing in the national economy, and transportation
9907	9910	regular nor random graphs display long tails in P(k), and the presence of nodes with large k strongly affects the properties of the network , as for instance its response to external factors . In ref. Barabasi and Albert have proposed a simple model (the BA model) to reproduce the P(k) found in real networks by modelling the dynamical growth of the network. The model is based on two
9907	9916	that are somehow in between regular and random networks, have been named small worlds in analogy with the small world phenomenon empirically observed in social systems more than 30 years ago . In the mathematical formalism developed by Watts and Strogatz a generic network is represented as an unweighted graph G with N nodes (vertices) and K edges (links) between nodes. Such a graph is
9907	9918	behavior. The most interesting fact is that neither regular nor random graphs display long tails in P(k), and the presence of nodes with large k strongly affects the properties of the network , as for instance its response to external factors . In ref. Barabasi and Albert have proposed a simple model (the BA model) to reproduce the P(k) found in real networks by modelling the
9937	9939	requirements to support replanning in the case of failures. 6 RELATED WORK The New Millennium Remote Agent (NMRA) architecture is closely related to the 3T (three-tier) architecture described in (Bonasso et al. 1996). The 3T architecture consists of a deliberative component and a realtime control component connected by a reactive conditional sequencer. We and Bonasso both use RAPS (Firby 1978) as our
9937	9944	a wide variety of execution outcomes (Pell et al. 1996b). Our executive can be viewed as a hybrid system that shares execution responsibilities between a classical reactive execution system, RAPS (Firby 1978) and a novel model-based recon guration system, called Livingstone (Williams &Nayak 1996). RAPS provides a specialized representation language for describing context-dependent contingent response
9937	9944	in (Bonasso et al. 1996). The 3T architecture consists of a deliberative component and a realtime control component connected by a reactive conditional sequencer. We and Bonasso both use RAPS (Firby 1978) as our sequencer, although we are developing a new sequencer which is more closely tailored to the demands of the spacecraft environment (Gat 1996). 2 Our deliberator is a traditional AI planner
9937	9946	sequencer. We and Bonasso both use RAPS (Firby 1978) as our sequencer, although we are developing a new sequencer which is more closely tailored to the demands of the spacecraft environment (Gat 1996). 2 Our deliberator is a traditional AI planner based on the HSTS temporal database (Muscettola 1994), and our control component is a traditional spacecraft attitude control system (Hackney,
9937	9946	are added incrementally until full capability isachieved or the fault is unambiguously identi ed. The NMRA architecture uses a model-based fault diagnosis system, adds an on-board 2 The esl system (Gat 1996) has now replaced RAPS as the core engine for the DS-1 Executive. planner, and greatly enhances the capabilities of the onboard sequencer, resulting in a dramatic leap ahead in autonomy capability.
9937	9949	subset of the problem, that of autonomous maneuver planning, which will be incorporated into our work as part of the DS-1 mission. Among the many general-purpose autonomy architectures is Guardian (Hayes-Roth 1995), a two-layer architecture which has been used for medical monitoring of intensive care patients. Like the spacecraft domain, intensive care has hard real-time deadlines imposed by the environment
9937	9952	in the computation of various parameters such as durations or temperature and power levels. Access to such external knowledge is a key requirement for realworld applications of planning systems (Muscettola et al. 1995). 3.2 Hybrid executive The executive is responsible for performing runtime management of all system activities. The executive's functions include process synchronization, process dependency
9937	9956	constraints and sources of complexities of a real mission, making it the most difcult challenge in the context of the most complicated mission phase of the most advanced spacecraft to date (Pell et al. 1996a). The unique requirements of this domain led us to the New Millennium Remote Agent (NMRA) architecture. The architecture integrates traditional real-time monitoring and control with (a)
9937	9956	a strong desire for plan robustness, in which the plans contain enough exibility, and the executive has the capability, to continue execution of the plan under a wide variety of execution outcomes (Pell et al. 1996b). Our executive can be viewed as a hybrid system that shares execution responsibilities between a classical reactive execution system, RAPS (Firby 1978) and a novel model-based recon guration
9937	9957	constraints and sources of complexities of a real mission, making it the most difcult challenge in the context of the most complicated mission phase of the most advanced spacecraft to date (Pell et al. 1996a). The unique requirements of this domain led us to the New Millennium Remote Agent (NMRA) architecture. The architecture integrates traditional real-time monitoring and control with (a)
9937	9957	a strong desire for plan robustness, in which the plans contain enough exibility, and the executive has the capability, to continue execution of the plan under a wide variety of execution outcomes (Pell et al. 1996b). Our executive can be viewed as a hybrid system that shares execution responsibilities between a classical reactive execution system, RAPS (Firby 1978) and a novel model-based recon guration
9937	9959	compiles the results of past searches for fast response in the future. SOAR has been used to control ight simulators, a domain which also has hard real-time constraints and operational criticality (Tambe et al. 1995). CIRCA (Musliner, Durfee, & Shin 1993) is an architecture that uses a slow AI component to provide guidance to a real-time scheduler that guarantees hard real-time response when possible. Noreils
8919945	6648	good, old, small. . . These words determine the fundamental concepts of a particular language, and thus should be incorporated into lexical database as its core components (e.g., EWN Base Concepts ). Representing the most general concepts, these words are associated to most other (more specific) words by means of hyponymy relations. Extracting this set of basic concepts we are to tackle the
8919945	9988	and its potential use (i.e. native speakers’ knowledge of language), that could be examine by means of psycholinguistic techniques. Fig. 1. Overlap between RWAT and the corpus. Several researchers  performed statistical analysis and comparison of such ‘raw’ LRs, namely, text corpora and word associations, in order to confirm the correlation between frequency of XY co-occurrence in a corpus
9994	10003	MC receives the application and executes it. 3 Spatial Queries The query submitted by a mobile client to a query server contains both location-based constraints and non-location-based constraints . For example, in the query “What is the closest Italian restaurant?”, “P(x): x is an Italian restaurant” is a non-location-based constraint, whereas “Q(x): x is the closest to the mobile client”
9994	8805	constrained by real-time deadlines. Although a significant amount of research has been reported in the literature, no one has yet addressed the real-time issue in processing spatial data queries. We first define exactly what kind of spatial data and queries the system will support. To do this, we assume there are two kinds of mobile clients based on their moving 7sspeeds; one is pedestrian
9994	10004	constrained by real-time deadlines. Although a significant amount of research has been reported in the literature, no one has yet addressed the real-time issue in processing spatial data queries. We first define exactly what kind of spatial data and queries the system will support. To do this, we assume there are two kinds of mobile clients based on their moving 7sspeeds; one is pedestrian
9994	10008	one of which is external while the remainder are internal. Each loop is a list of ordered edges or vertices. Recently, old geometric algorithms have been used to solve problems in spatial databases . One such approach, is the winged edge structure . It has been used to successfully represent 3-D geometric models . In this structure, each edge points to two vertices, and to the two
9994	10017	The deadlines in a real-time system can be soft, firm, or hard deadlines . Conventional transactions that have response time requirements can be thought of as soft real-time transactions . We assume that the majority of deadlines for our delivery system are soft deadlines. A soft deadline may be missed, yet the result produced still has some value that monotonically decreases with
9994	10017	such as the deadlines and performance profile of the task, are used to make scheduling decisions. However, an adaptive policy  that considers dynamic information, such as system load  can be considered. The quality of service provided not only depends on the load of the system, but can also depend on the capabilities of the client machines that receive the information. For
9994	10032	application profile would be a simple count of the number of CPU clock cycles needed to run the application to completion. Many such tools exist for gathering information about Java programs . Jones has developed an annotation-aware Java Virtual Machine which does machine-code generation based upon optimization summaries added offline. This system could be modified to gather
9994	10033	for gathering information about Java programs . Jones has developed an annotation-aware Java Virtual Machine which does machine-code generation based upon optimization summaries added offline. This system could be modified to gather performance information and calculate performance estimates. 12sThe mobile client calculates an estimate of the amount of time required to execute the
9994	10033	done based upon the application’s performance requirements. These requirements are stored as annotations to the .class files, in a fashion similar to the “JIT-hints” used by annotation-aware JVMs . The next step is for the application server to produce an application tailored for the requesting device. Program size can be manipulated to affect a tradeoff between ease of use and resource
9994	10034	done based upon the application’s performance requirements. These requirements are stored as annotations to the .class files, in a fashion similar to the “JIT-hints” used by annotation-aware JVMs . The next step is for the application server to produce an application tailored for the requesting device. Program size can be manipulated to affect a tradeoff between ease of use and resource
9994	10035	done based upon the application’s performance requirements. These requirements are stored as annotations to the .class files, in a fashion similar to the “JIT-hints” used by annotation-aware JVMs . The next step is for the application server to produce an application tailored for the requesting device. Program size can be manipulated to affect a tradeoff between ease of use and resource
9994	10032	The first step is the off-line profiling of applications. Since the most pervasive mobile code language is Java, existing tools for gathering processor performance information can be used . This work needs to be augmented to produce information about images and streaming network usage. One possible way of obtaining this additional information is to take advantage of the
8919956	10042	development areas through e-learning. exploring the motivation-8-01-04.doc Page 1 of 7s_____________________________________________________________________ Based upon the reviewed literature  some of these individual-related key variables composing user profile are professional/occupational background, motivation-to-elearn and cognitive styles. Professional/occupational background
8919956	10042	to have some diagnostic tool for organizations to define specific interventions and timely contribute to positive learning outcomes giving the “motives” to e-learn. Based upon reviewed literature ,,,,, some key motivation-related items were identified. Table 1 shows those items, which were classified into two types: individual-related items (6), and work context-related items
8919956	10050	Theoretical framework e-learning is a perceived tool to achieve context-specific, work-related and &quot;just-in-time&quot; training. However, current results have not show up the expected benefits fully . We believe that courseware taking into consideration individual-related key variables and context-related options would enhance interactions with e-learners. Within organizational settings,
8919956	10050	Software Development Lifecycle process Improvement Improvement areas areas exploring the motivation-8-01-04.doc Page 2 of 7s_____________________________________________________________________ . A second relevant aspect of motivation is the direction of effort, which implies individual goalorientation at internally defined level of success. This aspect reveals two motivational dimensions.
8919956	10050	and usefulness as technology attributes. Furthermore, soft-variables such as social influence and psychological attachment have emerged as important theoretical constructs for technology acceptance . At this regard, social and cultural context of organizations might be a variable to account for in assessing learning outcomes as exploring the motivation-8-01-04.doc Page 5 of
8919962	10054	which means that a promise is not merely directed to a future action F but that it is bound up with a chain of physical processes ensuing from it. This brief discussion of promises, based on , already makes explicit many of the sorts of entities that are involved in knitting together that complex whole that is an HCO: the participants of speech acts, the speech acts themselves, the
714570	8516	unrestricted resolution, fares comparably to ordered resolution with a default ordering, and sometimes beats setof-support (SOS) resolution. In Section 5 we show how automatic partitioning  can induce a partition-derived ordering (PDO) for use with ordered resolution. Ordering can be a highly efficient resolution strategy, but its success has previously depended on handcrafted
714570	8516	The PBR framework has two components: graph-based algorithms for automatic partitioning of a theory, and messagepassing algorithms for reasoning with the partitioned theory. For further details see . 2.1 Automatically partitioning a theory We say that {Ai}i?n is a partitioning of a logical theory A if A = ? i Ai. Each Ai is a set of axioms called a partition, L(Ai) is its signature (the set of
714570	8516	where each vertex represents a symbol in the theory, and two vertices are joined by an edge iff the two symbols appear together in an axiom. We then use one of several tree decomposition algorithms  to generate a tree where each node corresponds to a tightly connected cluster of symbols, defining a partition Ai consisting of the axioms in the original theory that contain only those symbols.
714570	8516	of added edges is minimal. The tree decomposition is extracted from this order. The experimental results reported here are based on this partitioning algorithm. The second algorithm is due to  and uses a divide-andconquer approach that is guaranteed to approximate the optimum decomposition by a factor of at most O(log t), where t is the treewidth of the symbols graph. Iteratively, the
714570	8517	Cycorp’s Cyc and the High Performance Knowledge Base (HPKB) . To make headway in large KBs, theorem provers usually require KBspecific tuning and customization. Partition-based reasoning (PBR)  promises to speed up reasoning, without manual tuning, by exploiting the structure implicit in such large commonsense KBs, which typically contain loosely coupled clusters of domain knowledge. Eyal
714570	8517	The PBR framework has two components: graph-based algorithms for automatic partitioning of a theory, and messagepassing algorithms for reasoning with the partitioned theory. For further details see . 2.1 Automatically partitioning a theory We say that {Ai}i?n is a partitioning of a logical theory A if A = ? i Ai. Each Ai is a set of axioms called a partition, L(Ai) is its signature (the set of
714570	8517	than the first algorithm, but yielded similar performance in answering queries. 2.2 Reasoning with message passing (MP) Figure 1 displays Forward-Message-Passing (MP), a PBR algorithm proposed in . It takes as input a partitioned theory A, an associated partition graph G = (V, E, l), and a query formula Q in L(Ak), and returns YES if the query was entailed by A. MP first directs all edges in
714570	10061	first-order provers are commonly used for query answering over large knowledge bases (KBs) containing thousands of axioms, such as Cycorp’s Cyc and the High Performance Knowledge Base (HPKB) . To make headway in large KBs, theorem provers usually require KBspecific tuning and customization. Partition-based reasoning (PBR)  promises to speed up reasoning, without manual tuning, by
714570	10064	entailed by a set of axioms S is also entailed by the set of consequences in L that is generated by R from S. Ordered resolution is an example of a reasoning procedure that satisfies this condition .s3 Experimental setup To evaluate the performance of various PBR strategies, we built an experimental platform around SNARK, a resolutionbased FOL theorem prover developed by Mark Stickel at the
714570	10064	are in the outbound link language. We propose a local resolution strategy, focused support, that takes inspiration from strategies described by Slagle , SOL restriction , and SFK-resolution . Definition 6.1 (Focused Support Restriction). Let T be a clausal theory and let L be a designated subset of L(T ). Initialize S to be the set of clauses in T that non-exhaustively include symbols
714570	10064	is equivalent to using ordered resolution with the order only specifying that literals in L are resolved after literals not in L. Since this is known to be complete for L-consequence finding , we know that ¯ S |= D, and the proof is done. MFS resolution is the combination of MP with the focused support restriction within partitions. As discussed in Section 2.2, MP is sound and complete
714570	10064	propositional methods order nodes in a graph that correspond to propositional symbols and ordering is often dynamic. Our focused support restriction (on which MFS is based) resembles SFK resolution  and SOL resolution  in its computation of resolvents in a target language. However, in contrast to SFK resolution, our target language is not closed under subsumption. Further, in  there is
714570	10066	cut, W , is sent recursively to each of the separated parts, and subsequent iterations are required to find a vertex cut that separates W in a balanced fashion (here, we may use algorithms such as ). The tree decomposition is built from the final parts and the vertex-cuts’ vertex sets recursively. In experiments, this algorithm generated partitions somewhat fewer in number and larger in size
714570	10069	Cycorp’s Cyc and the High Performance Knowledge Base (HPKB) . To make headway in large KBs, theorem provers usually require KBspecific tuning and customization. Partition-based reasoning (PBR)  promises to speed up reasoning, without manual tuning, by exploiting the structure implicit in such large commonsense KBs, which typically contain loosely coupled clusters of domain knowledge. Eyal
714570	10069	The PBR framework has two components: graph-based algorithms for automatic partitioning of a theory, and messagepassing algorithms for reasoning with the partitioned theory. For further details see . 2.1 Automatically partitioning a theory We say that {Ai}i?n is a partitioning of a logical theory A if A = ? i Ai. Each Ai is a set of axioms called a partition, L(Ai) is its signature (the set of
714570	4386	source. We are continuing our efforts to remedy the paucity of experimental data by adding other large KBs. We are currently testing queries against the Suggested Upper Merged Ontology (SUMO) ; experimental results will be reported at the project website 1 . We collected a variety of statistics on each KB partitioning task and each query trial, including runtimes and elapsed times. But
714570	10070	KBs, and none on commonsense KBs. The success rate of leading theorem provers, such as SPASS, Otter, Setheo, Protein and 3TAP, in formal verification problems with hundreds of axioms, is shown in  to depend strongly on how good they are at finding the few relevant axioms needed in the proofs. Our work presents a principled method to successfully elicit such a set of relevant axioms, the
714570	10071	as in ), or generate an arbitrary default ordering (e.g., lexical, as in ). Our work is most significantly distinguished from work on CSPs (e.g., ) and propositional reasoning (e.g., ) in that we partition (and subsequently order) a graph that includes all the nonlogical symbols in the theory, whereas propositional methods order nodes in a graph that correspond to propositional
714570	10074	framework of PBR. In Section 3 we explain how a generic theorem prover may be easily augmented with PBR, and describe the experimental testbed we developed using the SNARK theorem prover . Using this testbed, in Section 4 we compare the performance of the PBR message-passing algorithm (MP) to that of popular resolution strategies . MP far outperforms unrestricted resolution,
714570	10074	setup To evaluate the performance of various PBR strategies, we built an experimental platform around SNARK, a resolutionbased FOL theorem prover developed by Mark Stickel at the SRI AI Center (). Adding PBR capabilities to SNARK was straightforward. Three extensions were required: • Associate a set of partitions with each clause. • Restrict resolution to occur only within partitions. •
714570	10074	of those predicates, such as their arity (e.g., the Knuth-Bendix method  when applied to a uniform weight function, as in ), or generate an arbitrary default ordering (e.g., lexical, as in ). Our work is most significantly distinguished from work on CSPs (e.g., ) and propositional reasoning (e.g., ) in that we partition (and subsequently order) a graph that includes all the
714570	10077	Current automated approaches to orderingspredicates use properties of those predicates, such as their arity (e.g., the Knuth-Bendix method  when applied to a uniform weight function, as in ), or generate an arbitrary default ordering (e.g., lexical, as in ). Our work is most significantly distinguished from work on CSPs (e.g., ) and propositional reasoning (e.g., ) in
10079	10080	College Cork, £ Ireland Department of Computer Science, University of Glasgow, ¤ Scotland @4c.ucc.ie, pat@dcs.gla.ac.uk c.beck,r.wallace¥ Abstract. In ECAI 1998 Smith & Grant performed a study  of the fail-first principle of Haralick & Elliott . The fail-first principle states that “To succeed, try first where you are most likely to fail.” For constraint satisfaction problems (CSPs),
10079	10080	heuristic (choose next to assign a value to the variable with the smallest number of possible values) is a level-one estimate of minimizing branch length. To test this principle, Smith & Grant  created a set of new heuristics designed to aggressively fail early in the search. The hypothesis was that if failing quickly was the explanation for search efficiency, then heuristics that failed
10079	10082	only thing that explains the differences in search cost among heuristics. Our interest in this problem was sparked by the recent development of a new framework for analyzing heuristic performance . This framework incorporates fail-firstness as one of its performance principles. Another, called promise, concerns the selection of alternatives most likely to succeed. (This principle should be
10079	10082	the promise principle. It has also been shown that variable heuristics can be distinguished by their promise (a less obvious relation than the one between promise and value ordering heuristics) . In other words, if the search is on a path to a solution, a heuristic with high promise will select a variable for which there is a high likelihood that a good value selection can be made. That
10079	10082	of checks for the ? 20, 10, 0.5? problem set. The heuristics are ranked as follows: FF ? FF3 ? FF4 ? FF2. likely to lead to a new search state that still has solutions in its subtree. Beck et al.  demonstrated that for problems with many solutions there is a high inverse correlation between the promise of a variable ordering heuristic and the number of constraint checks required to find a
10079	10084	increase in the average branching factor, and the latter effect can be large enough to impair search overall. This occurs with forward checking , but if we use maintained arc-consistency (MAC)  then this tradeoff is mitigated, so that minimizing branch length correlates with reduced search effort. We suggest that the radical fail-first principle can be replaced with an alternative failure
10079	10084	not observe such behavior on larger problems. In this section we report on our experiments with larger problems (up to 70 variables) and a different algorithm (maintaining arc consistency (MAC) ). 7.1 Varying problem size Do we get the same ranking of the heuristics FF ? FF3 ? FF4 ? FF2 when we look at larger problems? We generated random problems of varying size, from §?????? up to
10079	10084	insoluble, therefore each ? value may have a different number of instances up to 1000. In particular, the low values of ? are based on few instances. the maintaining-arc consistency algorithm (MAC) . As the name implies, whenever a variable is instantiated the future sub-problem is made arc-consistent. If this results in a domain wipe-out, a new value is tried, and failing that, backtracking
10079	10086	magnitude better than the FF4 reported by Smith & Grant. Our results were tested on a number of independent implementations of the algorithms and problem generators: a solver written in C extending , one in Java, one based on ILOG Solver, and one written in LISP. All of our attempts failed to reproduce the reported results. Barbara Smith and Stuart Grant kindly provided us with the source code
10079	10086	report the results of repeating the experiments of Smith & Grant. Our results were produced by using two solvers coded independently. The first was written in C and extended the CSP solver used in  while the second was written in Java and extended the solver used in . As noted, we also confirmed these results using the C++ solver used by Smith & Grant with the integer division error in FF4
10079	10087	were produced by using two solvers coded independently. The first was written in C and extended the CSP solver used in  while the second was written in Java and extended the solver used in . As noted, we also confirmed these results using the C++ solver used by Smith & Grant with the integer division error in FF4 corrected. In our implementation of the heuristics, ties (i.e. when more
10079	10088	these probability values; in this case a set of elements is generated repeatedly until the cardinality matches the expected value. This allows it to generate problems in accordance with model B . By specifiying a probability of 1 for domain element inclusion, all domains have a size specified by a maximum domain-size parameter. Sets of soluble or insoluble problems were sometimes used;
10079	10089	each combination of parameters 1000 problem instances are generated. Median checks (the same measure used by Smith & Grant) are plotted against ? (kappa), the measure of constrainedness proposed by . Figure 2 presents the same set of experiments, but for the ¦ 20, 10, 0.5? problem set. In both graphs, FF clearly incurs the highest number of constraint checks, followed by FF3. In Figure 1, FF2
10102	10103	error prone task. From the other side, research within the Semantic Web community proposes a top down unambiguos description of web services capabilities, e.g., in standard languages like DAML-S  and OWL-S , thus enabling the possibility to reason about web services, and to automate web services tasks, like discovery and composition. However, the real taking up of Semantic Web Services
10102	10104	to scale up to large state spaces. As a result, the planning algorithm generates plans that are automata and that can be translated to BPEL4WS code. We implement the proposed techniques in MBP , a planner based on the planning as model cheking approach, and perform an experimental evaluation. Though the results are still preliminary, and deserve further investigation and evaluation, they
10102	10106	observability with EAGLE goals is correct and complete. A potential problem of this approach is that, in most of the cases, knowledge-level domains are exponentially larger than ground domains. In , efficient heuristic techniques are defined to avoid generating the whole (knowledge-level) planning domain. These techniques can be extended to planning with EAGLE goals. We have therefore the
10102	10107	language , a language with a clear semantics which can express complex requirements. We can thus exploit the “Planning as Model Checking” approach based on symbolic model checking techniques , which has been shown to provide a practical solution to the problem of planning with nondeterministic actions, partial observability, and complex goals, and which has been shown experimentally to
10102	10107	consider a set of domain states, each equally plausible given the initial knowledge and the observed behavior of the domain so far. Such a set of states is called a belief state (or simply belief ) . Executing an action a evolves a belief B into another belief B ? which contains all of the possible states that can be reached through a from some state of B. The available sensing is exploited
10102	10107	observability with EAGLE goals is correct and complete. A potential problem of this approach is that, in most of the cases, knowledge-level domains are exponentially larger than ground domains. In , efficient heuristic techniques are defined to avoid generating the whole (knowledge-level) planning domain. These techniques can be extended to planning with EAGLE goals. We have therefore the
10102	10108	for planning under partial observability can be obtained by suitably recasting the algorithms for full observability on the associated knowledge-level domain. Actually, the following result holds : Fact 6 Let ? be a ground-level domain and g be a knowledge-level goal for ? (i.e., a goal expressing conditions on the beliefs reached during plan execution). Let also ?K be the knowledge level
10102	10108	domain, by providing algorithms for natively planning with extended goals under partial observability. Some preliminary results in this directions for a different goal language are presented in . Moreover, we plan to integrate the automated composition task with reasoning techniques for discovery and selection at the level of OWL-S service profiles. Finally, we intend to test our approach
10102	10109	consider a set of domain states, each equally plausible given the initial knowledge and the observed behavior of the domain so far. Such a set of states is called a belief state (or simply belief ) . Executing an action a evolves a belief B into another belief B ? which contains all of the possible states that can be reached through a from some state of B. The available sensing is exploited
10102	10111	observable state transition systems that describe the dynamic interactions with external services. Goals for the service to be automatically generated are represented in the EaGle language , a language with a clear semantics which can express complex requirements. We can thus exploit the “Planning as Model Checking” approach based on symbolic model checking techniques [14, 9, 6, 11,
10102	10111	shows, composition goals need the ability to express conditions on the whole behaviour of a service, conditions of different strengths, and preferences among different subgoals. The EAGLE language  has been designed with the purpose to satisfy such expressiveness requirements. Let propositional formulas p ? Prop define conditions on the states of a state transition system. Composition goals g
10102	10111	Finally, goal g1 And g2 requires the achievement of both subgoals g1 and g2. A formal semantics and a planning algorithm for EaGLe goals in fully observable nondeterministic domains can be found in . Example 5. The EAGLE formalization of the goal in Example 4 is the following. TryReach /* Goal 1 */ (AcceptProductOffer.done & AcceptShippingOffer.done & EvaluateOffer.done &
10102	10111	?. It is the execution structure ?? that must satisfy the composition goal G (see Figure 1). If ?? |= G, we say that ? is a valid plan for G on ?. A formal definition of ?? |= G can be found in . However, notice that when executing a plan, the plan executor cannot in general get to know exactly what is the current state of the domain: the limited available access to the internal state of
10102	10111	and interpreting the goal as a ground goal (rather than as a knowledge-level goal). We pursue this latter approach, so that we can reuse existing EAGLE planning algorithms under full observability . We generate the knowledge level domain by combining the state transition systems defined previously. Similarly to what happens for the ground level domains, this computation consists of a
10102	10113	in the domain, we may have an infinite number of different executions of a plan. We provide a finite presentation of these executions with an execution structure, i.e, a Kripke Structure  with configurations as states. Definition 4 (execution structure). The execution structure corresponding to domain ? and plan ? is the Kripke structure ?? = ?Q, Q0, R?, where: – Q is the set of
10102	10115	applied to related but somehow orthogonal problems in the field of web services. The interactive composition of information gathering services has been tackled in  by using CSP techniques. In , given a specific query of the user, an interleaving of planning and execution is used to search for a solution and to re-plan when the plan turns out to violate some user constraints at run time.
10102	10117	programming control constructs for the generic composition of web service, while we generate automatically plans that encode web service composition through programming control constructs. In , Golog programs are used to encode complex actions that can represent DAML-S process models. However, the planning problem is reduced to classical planning and sequential plans are generated for
10102	10118	time and errors due to manual composition at the programming level. Several works have proposed different automated planning techniques to address the problem of automated composition (see, e.g., ). However, the planning problem is far from trivial, and can be hardly addressed by “classical planning” techniques. In ? The work is partially funded by the FIRB-MIUR project RBNE0195K5,
10102	10118	, but how to deal with nondeterminism, partial observability, and how to generate conditional and iterative behaviors (in the style of BPEL4WS) in these frameworks is still an open issue. In , web service composition is achieved with user defined re-usable, customizable, high level procedures expressed in Golog. The approach is orthogonal to ours: Golog programs can express programming
10102	2570	time and errors due to manual composition at the programming level. Several works have proposed different automated planning techniques to address the problem of automated composition (see, e.g., ). However, the planning problem is far from trivial, and can be hardly addressed by “classical planning” techniques. In ? The work is partially funded by the FIRB-MIUR project RBNE0195K5,
10102	10119	First of all, there is a difference in the “strength” in which we require Goal 1 and Goal 2 to be satisfied. We know that it may be impossible to satisfy Goal 1 The interested reader may refer to  for a detailed discussion of a translation similar to ours. In that case, Petri nets are used as target models. The states of our state transition systems can be seen as the markings in the Petri
10102	10119	used to encode complex actions that can represent DAML-S process models. However, the planning problem is reduced to classical planning and sequential plans are generated for reachability goals. In , the authors propose an approach to the simulation, verification, and automated composition of web services basedson a translation of DAML-S to situation calculus and Petri Nets, so that it is
10102	10120	implementing the three services into state transition systems. The translation is technically different from (but conceptually similar to) the one described for OWL-S models, and is described in . We perform this translation for the same cases considered in the previous experiment. Then we run the same MBP planning algorithm on the resulting planning domains. The results are reported in
10102	10121	time and errors due to manual composition at the programming level. Several works have proposed different automated planning techniques to address the problem of automated composition (see, e.g., ). However, the planning problem is far from trivial, and can be hardly addressed by “classical planning” techniques. In ? The work is partially funded by the FIRB-MIUR project RBNE0195K5,
10102	10121	have been proposed for the composition of web services, from HTNs  to regression planning based on extensions of PDDL , to STRIPS-like planning for composing services described in DAML-S , but how to deal with nondeterminism, partial observability, and how to generate conditional and iterative behaviors (in the style of BPEL4WS) in these frameworks is still an open issue. In ,
10102	10122	planning techniques have been applied to related but somehow orthogonal problems in the field of web services. The interactive composition of information gathering services has been tackled in  by using CSP techniques. In , given a specific query of the user, an interleaving of planning and execution is used to search for a solution and to re-plan when the plan turns out to violate
10102	10123	time and errors due to manual composition at the programming level. Several works have proposed different automated planning techniques to address the problem of automated composition (see, e.g., ). However, the planning problem is far from trivial, and can be hardly addressed by “classical planning” techniques. In ? The work is partially funded by the FIRB-MIUR project RBNE0195K5,
10102	10123	of automated composition at the semantic level w.r.t. the one at the level of executable processes. Different planning approaches have been proposed for the composition of web services, from HTNs  to regression planning based on extensions of PDDL , to STRIPS-like planning for composing services described in DAML-S , but how to deal with nondeterminism, partial observability, and how
3299596	10125	networks is crucial since battery-driven sensor nodes are severely energy-constrained. Considerable research has been recently carried out in an effort to make sensor network energy-efficient. In , a mathematical model is presented to determine a bound on sensor network lifetime, with and without sensing activities. The hardware-based energy model for transmission and reception described in
3299596	10125	in wireless sensor networks for target localization. In general, a sensor network has an almost constant rate of energy consumption if no target activities are detected in the sensor field . The minimization of energy consumption for an active sensor network with target activities is more complicated since target detection involves collaborative sensing and communication involving
3299596	10125	?=0.5, ?=1 ?=1, ?=1 ?=0.5, ?=0.5 0 0 1 2 3 4 5 6 7 8 9 10 Distance d(S i , P) between sensor and grid point Fig. 1. Probabilistic sensor detection model. C. Energy Consumption Model Based on , , , a simplified sensor node energy consumption model is used here as a metric for evaluating energy consumption. Suppose a sensor node has three basic energy consumption types—sensing,
3299596	10125	height]; 23 /* next time instant */ 24 Set t = t + 1; 25 End Fig. 5. Pseudocode of the target localization procedure. ?s ? 1000 nJ/sec. These values are based on the typical values given in , , , assuming the sensing rate for a the sensor is 8 bits/sec. We have no physical data available for Td and Te; however, their values do not affect the target localizationsprocedure, therefore we
3299596	5017	Department of Electrical and Computer Engineering Duke University Durham, NC 27708, USA E-mail: {yz1, krish}@ee.duke.edu example is the TEEN protocol proposed in . Dynamic power management  has also been used for the design of energy-efficient wireless sensor networks. Other related work includes energy-saving strategies for the link layer , data aggregation , and system
3299596	10128	The hardware-based energy model for transmission and reception described in  is widely used as the basic energy consumption model for a wireless sensor network node. Heinzelman et al.  proposed a cluster-based routing algorithm called LEACH as an energy-efficient communication protocol for wireless sensor networks. The selfselected cluster heads collect raw data from the
3299596	10129	?=1 ?=1, ?=1 ?=0.5, ?=0.5 0 0 1 2 3 4 5 6 7 8 9 10 Distance d(S i , P) between sensor and grid point Fig. 1. Probabilistic sensor detection model. C. Energy Consumption Model Based on , , , a simplified sensor node energy consumption model is used here as a metric for evaluating energy consumption. Suppose a sensor node has three basic energy consumption types—sensing, transmitting
3299596	10129	23 /* next time instant */ 24 Set t = t + 1; 25 End Fig. 5. Pseudocode of the target localization procedure. ?s ? 1000 nJ/sec. These values are based on the typical values given in , , , assuming the sensing rate for a the sensor is 8 bits/sec. We have no physical data available for Td and Te; however, their values do not affect the target localizationsprocedure, therefore we only
3299596	10130	power management  has also been used for the design of energy-efficient wireless sensor networks. Other related work includes energy-saving strategies for the link layer , data aggregation , and system partitioning . Sensor deployment for collaborative target detection is discussed in , where path exposure is used as a measure of the effectiveness of the sensor deployment. In
3299596	10133	proposed in . Dynamic power management  has also been used for the design of energy-efficient wireless sensor networks. Other related work includes energy-saving strategies for the link layer , data aggregation , and system partitioning . Sensor deployment for collaborative target detection is discussed in , where path exposure is used as a measure of the effectiveness of the
3299596	10135	Other related work includes energy-saving strategies for the link layer , data aggregation , and system partitioning . Sensor deployment for collaborative target detection is discussed in , where path exposure is used as a measure of the effectiveness of the sensor deployment. In , the authors propose a dual-space approach to event tracking and sensor resource management. We
50272	10156	See Table 1. Content that is customized or specialized typically involves generating a specialized presentation and format that takes into account facts regarding the delivery context of the user . Generation of personalized content involves additional processing. In addition to facts about the delivery context, personalization takes into consideration opinions about the user’s preferences,
10170	10174	query languages have been proposed , but most of these concern descriptive rules. Also, related to our work are algebras to manipulate tree-structured data, especially in XML documents. In  the authors present an algebra for XML data, but it is tuple-based and not tree-based. A navigational algebra is presented 1 www.adorama.com, www.bhphotovideo.comsPatManQL: A language to manipulate
10170	10176	is presented 1 www.adorama.com, www.bhphotovideo.comsPatManQL: A language to manipulate patterns & data in hier. catalogs 2-3 in , but again it treats individual nodes as manipulation units. In , TAX algebra is defined, but as a means for selecting and reconstructing bulk XML data. In our work, we manipulate the descriptions of data as path-like patterns. We capture the notion of
10170	10177	but it is tuple-based and not tree-based. A navigational algebra is presented 1 www.adorama.com, www.bhphotovideo.comsPatManQL: A language to manipulate patterns & data in hier. catalogs 2-3 in , but again it treats individual nodes as manipulation units. In , TAX algebra is defined, but as a means for selecting and reconstructing bulk XML data. In our work, we manipulate the
10170	10178	is one of the different ways to access a resource item. 2 In this work we consider schema matching and semantic mismatch issues to have been resolved using techniques suggested in the literature .s2-4 P. Bouros, T. Dalamagas, T. Sellis and M. Terrovitis Catalog schema lenses point & shoot cameras & lenses 35mm SLR APS cameras X digital printers filters film UV PL slide negative 1 2 SLR
180128	10209	studies in Section 5 examine the effects of different settings of the parameters discussed. 3.2.2 Concurrency Control Concurrent access in R-trees is provided by Dynamic Granular Locking (DGL) . DGL provides low overhead phantom protection in R-trees by utilizing external and leaf granules that can be locked or released. The finest granular level is the leaf MBR. Natively, DGL supports
180128	10209	0 0 2 4 6 8 10 5.4 Throughput Size of Dataset (in millions) (b) Querying Finally, we study the throughput of bottom-up versus topdown approaches. We employ the Dynamic Granular Locking in R-trees  and run the experiments with 50 threads, varying the percentage of updates versus queries. We use window queries within the range of ¢ ? ¨ ? ? ? ¥ ?§£ with updates. As expected, Figure 8 shows that
180128	10210	desired functionality is particularly troublesome in the context of indexing of multidimensional data. The dominant indexing technique for multidimensional data with low dimensionality, the R-tree  (and its close relatives such as the R ¤ -tree ), was conceived for largely static data sets and exhibits poor update performance. Existing R-tree update procedures work in a top-down manner.
180128	10210	update approach, and Section 6 concludes. 2 R-Tree Based Indexing Much research has been conducted on the indexing of spatial data and other multidimensional data. The commercially available R-tree  is an extension of the Bs-tree for multi-dimensional objects, and remains a focus of attention in the research community. It is practical and has shown itself to support the efficient processing of
180128	10211	the past as well as the current and anticipated future positions of moving objects have been proposed that are based on the R-tree, most often the R ¤ -tree, that are efficient for querying . These techniques typically process updates as combinations of separate deletion and insertion operations that operate in a top-down manner. They are related to this paper’s contribution in the
180128	10212	of a query. This is in contrast to the B-tree. A number of variations of the initial R-tree exist, including packed R-trees , the Rs-tree , the R ¤ -tree , and the Hilbert R-tree . Most recently, several extensions of R-trees have been proposed specifically for moving objects, including the TPR-tree , the STAR-tree , and R????? the -tree . These R-tree variants
180128	10213	the past as well as the current and anticipated future positions of moving objects have been proposed that are based on the R-tree, most often the R ¤ -tree, that are efficient for querying . These techniques typically process updates as combinations of separate deletion and insertion operations that operate in a top-down manner. They are related to this paper’s contribution in the
180128	10214	the past as well as the current and anticipated future positions of moving objects have been proposed that are based on the R-tree, most often the R ¤ -tree, that are efficient for querying . These techniques typically process updates as combinations of separate deletion and insertion operations that operate in a top-down manner. They are related to this paper’s contribution in the
180128	10214	one million uniformly distributed points reveal that this simple strategy fails to yield the improvements to be expected, as a large percentage of the updates (82%) remains top-down. Kwon et al.  allow the leaf MBRs to expand by some so that the object can remain in the leaf without the need for an expensive top-down update. In order to preserve the R-tree structure, the expansion of a leaf
180128	10215	XP Professional. We consider a range of tuning parameters, sensitivity and scalability, and throughput. The performance metrics include both disk I/O and CPU time. All experiments utilize a buffer  that is 1% of the database size. A data generator similar to GSTD  is used to generate the initial distribution of the objects, followed by the movement and queries. Each object is a 2D point
180128	10218	to the leaf level. The more the overlap, the worse the branching behavior of a query. This is in contrast to the B-tree. A number of variations of the initial R-tree exist, including packed R-trees , the Rs-tree , the R ¤ -tree , and the Hilbert R-tree . Most recently, several extensions of R-trees have been proposed specifically for moving objects, including the TPR-tree , the
180128	10220	and the Hilbert R-tree . Most recently, several extensions of R-trees have been proposed specifically for moving objects, including the TPR-tree , the STAR-tree , and R????? the -tree . These R-tree variants all process updates as combinations of separate top-down deletion and insertion operations. The techniques proposed in this paper may be applied to each of these. 3 Bottom-Up
180128	10221	the past as well as the current and anticipated future positions of moving objects have been proposed that are based on the R-tree, most often the R ¤ -tree, that are efficient for querying . These techniques typically process updates as combinations of separate deletion and insertion operations that operate in a top-down manner. They are related to this paper’s contribution in the
180128	10221	, the Rs-tree , the R ¤ -tree , and the Hilbert R-tree . Most recently, several extensions of R-trees have been proposed specifically for moving objects, including the TPR-tree , the STAR-tree , and R????? the -tree . These R-tree variants all process updates as combinations of separate top-down deletion and insertion operations. The techniques proposed in this
180128	10222	The more the overlap, the worse the branching behavior of a query. This is in contrast to the B-tree. A number of variations of the initial R-tree exist, including packed R-trees , the Rs-tree , the R ¤ -tree , and the Hilbert R-tree . Most recently, several extensions of R-trees have been proposed specifically for moving objects, including the TPR-tree , the STAR-tree , and
180128	10225	and scalability, and throughput. The performance metrics include both disk I/O and CPU time. All experiments utilize a buffer  that is 1% of the database size. A data generator similar to GSTD  is used to generate the initial distribution of the objects, followed by the movement and queries. Each object is a 2D point in a unit square that can move some distance in ¨ the ¢ ? ?¤£ ¨ ¡¦¥
121307	10227	robot exploration are relevant. In the context of exploration, most of the techniques presented so far focus on generating motion commands that minimize the time needed to cover the whole terrain . Other methods seek to optimize the view-points of the robot to maximize the expected information gain and to minimize the uncertainty of the robot about grid cells . Most of these
121307	10227	bounding box can be obtained efficiently by a principal component analysis. As long as the robot is localized well enough or no loop can be closed, we use a frontier-based exploration strategy  to choose target points for the robot. In our current system we determine frontiers based on the map of the most likely particle s ? . Here a frontier is any known cell that is an immediate
121307	10230	the cost of reaching frontiers with the utility of selected positions with respect to a potential reduction of the pose uncertainty. This approach is similar to the work done by Feder et al.  who consider local decisions to improve the pose estimate during mapping. Both techniques, however, rely on the fact that the environment contains landmarks that can be uniquely determined during
121307	10231	the whole terrain . Other methods seek to optimize the view-points of the robot to maximize the expected information gain and to minimize the uncertainty of the robot about grid cells . Most of these techniques, however, assume that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as
121307	5440	that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as well as belief representation and update . These techniques, however, are passive and only consume incoming sensor data without explicitely generating controls. Recently, some techniques have been proposed which actively control the robot
121307	10232	that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as well as belief representation and update . These techniques, however, are passive and only consume incoming sensor data without explicitely generating controls. Recently, some techniques have been proposed which actively control the robot
121307	10232	this paper computes grid maps. It applies a scan-matching procedure to compute highly accurate odometry data and uses this corrected odometry in the prediction step of the particle filter . This way the number of particles can be reduced so that maps of even large environments can be constructed online. In the following section we describe how the FastSLAM algorithm for grid maps can
121307	10234	Department of Computer Science D-79110 Freiburg, Germany integrated approaches active localization Fig. 1. Sub-tasks that need to be solved by a robot to acquire accurate models of the environment . The overlapping areas represent combinations of these sub-tasks. start start Fig. 2. This figure shows two maps obtained by a real world experiment performed at Sieg Hall, University of
121307	10234	without explicitely generating controls. Recently, some techniques have been proposed which actively control the robot during simultaneous mapping and localization. For example, Makarenko et al.  extract landmarks out of laser range scans and use an Extended Kalman Filter to solve the SLAM problem. They furthermore introduce a utility function which trades-off the cost of reaching frontiers
121307	10235	that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as well as belief representation and update . These techniques, however, are passive and only consume incoming sensor data without explicitely generating controls. Recently, some techniques have been proposed which actively control the robot
121307	10235	robot in its pose. This way the resulting maps become highly accurate. III. GRID-BASED FASTSLAM To estimate the map of the environment we use a highly efficient variant of the FastSLAM algorithm  which itself is an extension of the Rao-Blackwellized particle filter for simultaneous localization and mapping proposed by Murphy et al. . The key idea of the Rao-Blackwellized particle filter
121307	10236	that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as well as belief representation and update . These techniques, however, are passive and only consume incoming sensor data without explicitely generating controls. Recently, some techniques have been proposed which actively control the robot
121307	10237	Pioneer II robot. Both are equipped with a SICK laser range finder. For the simulation experiments we used the real-time simulator of the Carnegie Mellon Robot Navigation Toolkit (CARMEN) . This simulator generates realistic noise in the odometry and laser range sensor data. The experiments described in this section are designed to illustrate that our approach can be used to actively
121307	10238	the whole terrain . Other methods seek to optimize the view-points of the robot to maximize the expected information gain and to minimize the uncertainty of the robot about grid cells . Most of these techniques, however, assume that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as
121307	381	that the location of the robot is known during exploration. In the area of SLAM the vast majority of papers focuses on the aspect of state estimation as well as belief representation and update . These techniques, however, are passive and only consume incoming sensor data without explicitely generating controls. Recently, some techniques have been proposed which actively control the robot
121307	10239	necessary to close the outer loop may vanish. As a result, the filter diverges and the robot fails to build a correct map (see Figure 5). To remedy this so-called particle depletion problem  we introduce a constraint on the uncertainty of the robot. Let H(te) denote the uncertainty of the posterior when the robot visited the entry point last time. Then the new constraint allows the
121307	10240	robot exploration are relevant. In the context of exploration, most of the techniques presented so far focus on generating motion commands that minimize the time needed to cover the whole terrain . Other methods seek to optimize the view-points of the robot to maximize the expected information gain and to minimize the uncertainty of the robot about grid cells . Most of these
121307	10241	robot exploration are relevant. In the context of exploration, most of the techniques presented so far focus on generating motion commands that minimize the time needed to cover the whole terrain . Other methods seek to optimize the view-points of the robot to maximize the expected information gain and to minimize the uncertainty of the robot about grid cells . Most of these
121307	10241	robot. In our current system we determine frontiers based on the map of the most likely particle s ? . Here a frontier is any known cell that is an immediate neighbor of an unknown, unexplored cell . A precise formulation of the loop-closing strategy is given by Algorithm 1. In our implementation this algorithm runs as a background process that is able interrupt the frontier-based exploration
10242	10249	interest in modelling regulatory networks in the evolutionary computation literature . Features of regulatory networks have been previously used in the context of optimization by . However, these models have been explicitly designed for artificial ontogeny. Here we propose the use of a regulatory network framework as a general method for evolving arbitrary time series.
10242	10249	performed for flying , locomotion  and the inference of differential equations . In addition, previous models of ARNs primarily use boolean representations of network dynamics . Here we show that an ARN model using differential equations (approximated as difference equations) can also display complex behaviors which may be selected by evolution. Other ideas relating to
10242	10250	performed for flying , locomotion  and the inference of differential equations . In addition, previous models of ARNs primarily use boolean representations of network dynamics . Here we show that an ARN model using differential equations (approximated as difference equations) can also display complex behaviors which may be selected by evolution. Other ideas relating to
10242	10251	interest in modelling regulatory networks in the evolutionary computation literature . Features of regulatory networks have been previously used in the context of optimization by . However, these models have been explicitly designed for artificial ontogeny. Here we propose the use of a regulatory network framework as a general method for evolving arbitrary time series.
10242	10253	performed for flying , locomotion  and the inference of differential equations . In addition, previous models of ARNs primarily use boolean representations of network dynamics . Here we show that an ARN model using differential equations (approximated as difference equations) can also display complex behaviors which may be selected by evolution. Other ideas relating to
10242	10254	performed for flying , locomotion  and the inference of differential equations . In addition, previous models of ARNs primarily use boolean representations of network dynamics . Here we show that an ARN model using differential equations (approximated as difference equations) can also display complex behaviors which may be selected by evolution. Other ideas relating to
10242	10255	interest in modelling regulatory networks in the evolutionary computation literature . Features of regulatory networks have been previously used in the context of optimization by . However, these models have been explicitly designed for artificial ontogeny. Here we propose the use of a regulatory network framework as a general method for evolving arbitrary time series.
10242	10256	as a general method for evolving arbitrary time series. Obtaining arbitrary functions through evolutionary means for the purpose of model optimization has been previously performed for flying , locomotion  and the inference of differential equations . In addition, previous models of ARNs primarily use boolean representations of network dynamics . Here we show that
10242	10257	method for evolving arbitrary time series. Obtaining arbitrary functions through evolutionary means for the purpose of model optimization has been previously performed for flying , locomotion  and the inference of differential equations . In addition, previous models of ARNs primarily use boolean representations of network dynamics . Here we show that an ARN model using
10242	10260	which may be selected by evolution. Other ideas relating to genetic transcription have also previously been used in function optimization such as genetic–code transformations , gene expression , gene signaling  and diploidity . 2 Artificial Regulatory Network Model The ARN consists of a bit string representing a genome with direction (i.e. 5’ ? 3’ in DNA) and mobile “proteins”
10242	10261	which may be selected by evolution. Other ideas relating to genetic transcription have also previously been used in function optimization such as genetic–code transformations , gene expression , gene signaling  and diploidity . 2 Artificial Regulatory Network Model The ARN consists of a bit string representing a genome with direction (i.e. 5’ ? 3’ in DNA) and mobile “proteins”
10242	10266	plentiful opportunities for individuals in the population to acquire neutral mutations. It has been previously shown that neutral mutation can be extremely beneficial in the context of evolution . Since there may exist extensive non– coding regions of the genome, neutral mutations are free to be collected in such regions with new genes appearing suddenly when a new promotor pattern has been
10267	10268	reduce vmalloc’s usefulness as an extensible infrastructure. Attardi, Flagella, and Iglio created an extensive C++-based system called the Customizable Memory Management (CMM) framework . Unlike our work, the primary focus of the CMM framework is garbage collection. The only non-garbage collected heaps provided by the framework are a single “traditional manual allocation
10267	10268	reduce vmalloc’s usefulness as an extensible infrastructure. Attardi, Flagella, and Iglio created an extensive C++-based system called the Customizable Memory Management (CMM) framework . Unlike our work, the primary focus of the CMM framework is garbage collection. The only non-garbage collected heaps provided by the framework are a single “traditional manual allocation
10267	10269	for a wide range of applications . However, using specialized memory allocators that take advantage of application-specific behavior can dramatically improve application performance . Handcoded custom allocators are widely used: three of the twelve integer benchmarks included in SPEC2000 (parser, gcc, and vpr ) and several server applications, including the Apache web This
10267	10269	based on the range of object sizes and their frequency of usage . Other profile-based allocators use lifetime information to improve performance and reference information to improve locality . Regions, described in Section 1, have received recent attention as a method for improving locality . Aiken and Gay have developed safe regions which delay region deletion when live objects are
10267	10271	Checks for a variety of allocation errors LockedHeap Code-locks a heap for thread safety PerClassHeap Use a heap as a per-class allocator PHOThreadHeap A private heaps with ownership allocator  ProfileHeap Collects and outputs fragmentation statistics ThreadHeap A pure private heaps allocator  ThrowExceptionHeap Throws an exception when the parent heap is out of memory TraceHeap
10267	10271	implementation. Furthermore, because our implementation is based on layers, we can immediately provide an efficient scalable version of our allocator for multithreaded programs comparable to Hoard , whereas the Lea allocator requires significant effort to rewrite for this case. Our results suggest a number of additional research directions. First, because heap layers are so easy to combine
10267	4401	and efficient infrastructure for building custom and general-purpose allocators called heap layers. This infrastructure is based on a combination of C++ templates and inheritance called mixins . Mixins are classes whose superclass may be changed. Using mixins allows the programmer to code allocators as composable layers that a compiler can implement with efficient code. Unlike previous
10267	4401	single inheritance structure, making reuse difficult. To address these concerns, we use mixins to build our heap layers. Mixins are classes whose superclass may be changed (they may be reparented) . The C++ implementation of mixins (first described by VanHilst and Notkin ) consists of a templated class that subclasses its template argument: template <class Super> class Mixin : public
10267	10275	patterns with extremely low-cost operations. For example, a programmer can use a region allocator to allocate a number of small objects with a known lifetime and then free them all at once . This customized allocator returns individual objects from a range of memory (i.e., a region), and then deallocates the entire region. The per-operation cost for a region-based allocator is only a
10267	10275	described in Section 1, have received recent attention as a method for improving locality . Aiken and Gay have developed safe regions which delay region deletion when live objects are present . Techniques for building other application-specific custom allocators have received extensive attention in the popular press . 2.3 Memory Management Infrastructures We know of only two
10267	10277	for a wide range of applications . However, using specialized memory allocators that take advantage of application-specific behavior can dramatically improve application performance . Handcoded custom allocators are widely used: three of the twelve integer benchmarks included in SPEC2000 (parser, gcc, and vpr ) and several server applications, including the Apache web This
10267	10277	general-purpose allocation. Grunwald and Zorn’s CustoMalloc builds memory allocators from allocation traces, optimizing the allocator based on the range of object sizes and their frequency of usage . Other profile-based allocators use lifetime information to improve performance and reference information to improve locality . Regions, described in Section 1, have received recent
10267	10278	and LeaHeap. To evaluate allocator runtime performance and fragmentation, we use a number of memory-intensive programs, most of which ObstackHeap+FreelistHeap were described by Zorn and Wilson  and shown in Table 3: cfrac factors arbitrary-length integers, espresso is an optimizer for programmable logic arrays, lindsay is a hypercube simulator, LRUsim analyzes locality in reference
10267	10279	allocators within a single application. 1. Introduction Many general-purpose memory allocators implemented for C and C++ provide good runtime and low fragmentation for a wide range of applications . However, using specialized memory allocators that take advantage of application-specific behavior can dramatically improve application performance . Handcoded custom allocators are
10267	10279	into smaller ones) or coalescing (combining adjacent free objects). This algorithm is well known to be among the fastest memory allocators although it is among the worst in terms of fragmentation . The Lea allocator is an approximate best-fit allocator that provides both high speed and low memory consumption. It forms the basis of the memory allocator included in the GNU C library . The
10267	10279	best-fit. Large objects are allocated and freed using mmap. The Lea allocator is the best overall allocator (in terms of the combination of speed and memory usage) of which we are aware . 2.2 Special-Purpose Allocation Most research on special-purpose (custom) allocation has focused on profile-based optimization of general-purpose allocation. Grunwald and Zorn’s CustoMalloc builds
10267	10279	and LeaHeap. To evaluate allocator runtime performance and fragmentation, we use a number of memory-intensive programs, most of which ObstackHeap+FreelistHeap were described by Zorn and Wilson  and shown in Table 3: cfrac factors arbitrary-length integers, espresso is an optimizer for programmable logic arrays, lindsay is a hypercube simulator, LRUsim analyzes locality in reference
10267	10279	from excessive memory consumption. Wilson and Johnstone attribute this effect to the Kingsley allocator’s lack of coalescing or splitting that precludes reuse of objects for differentsized requests . A natural question is to what extent adding coalescing remedies this problem and what impact it has on performance. Using heap layers, we just add coalescing and splitting with the layers we
10267	10284	based on the range of object sizes and their frequency of usage . Other profile-based allocators use lifetime information to improve performance and reference information to improve locality . Regions, described in Section 1, have received recent attention as a method for improving locality . Aiken and Gay have developed safe regions which delay region deletion when live objects are
10267	10285	patterns with extremely low-cost operations. For example, a programmer can use a region allocator to allocate a number of small objects with a known lifetime and then free them all at once . This customized allocator returns individual objects from a range of memory (i.e., a region), and then deallocates the entire region. The per-operation cost for a region-based allocator is only a
10267	10285	lifetime information to improve performance and reference information to improve locality . Regions, described in Section 1, have received recent attention as a method for improving locality . Aiken and Gay have developed safe regions which delay region deletion when live objects are present . Techniques for building other application-specific custom allocators have received
10267	10286	we use mixins to build our heap layers. Mixins are classes whose superclass may be changed (they may be reparented) . The C++ implementation of mixins (first described by VanHilst and Notkin ) consists of a templated class that subclasses its template argument: template <class Super> class Mixin : public Super {}; Mixins overcome the limitation of a single class hierarchy, enabling the
10267	10287	systems and ours, focusing on the performance and flexibility advantages that heap layers provide. The most successful customizable memory manager of which we are aware is the vmalloc allocator . Vmalloc lets the programmer define multiple regions (distinct heaps) with different disciplines for each. The programmer performs customization by supplying user-defined functions andstructs that
10267	10288	for a wide range of applications . However, using specialized memory allocators that take advantage of application-specific behavior can dramatically improve application performance . Handcoded custom allocators are widely used: three of the twelve integer benchmarks included in SPEC2000 (parser, gcc, and vpr ) and several server applications, including the Apache web This
10267	10288	allocation. We then compare heap layers to previous infrastructures for building memory managers. 2.1 General-Purpose Allocation The literature on general-purpose memory allocators is extensive . Here we describe two memory allocators, the Kingsley allocator used in BSD 4.2  and the Lea allocator . In Section 6, we describe the implementation of these two allocators in heap layers.
10267	10288	and thus achieved a better trade-off between program size and optimization opportunities to yield improved performance. 5.2 176.gcc Gcc uses obstacks, a well-known custom memory allocation library . Obstacks also are designed to take advantage of stack-like behavior, but in a more radical way than xalloc. Obstacks consist of a number of large memory “chunks” that are linked together.
8832677	10292	are tested individually. This method was first proposed for deterministic computer simulations by Bettonvil and Kleijnen (1997). Later the method was extended to cover stochastic simulations (Cheng 1997). The sequential property of the method makes it well suited for simulation experiments. Examples have shown that the method is highly efficient when important factors are sparse and clustered
8832677	10292	EMPIRICAL EVALUATION In this section, we discuss the numerical results of simulation experiments to compare the following two procedures: 1. The CSB method proposed in Section 3. 2. Cheng’s method (Cheng 1997), an enhancement of the SB procedure for stochastic responses that assumes equal variances. The idea behind Cheng’s method is to determine whether a group of two or more factors are unimportant by
8832677	10293	gone in the opposite direction by combiningsthe screening experiments and a follow-up response exploration into one design to screen out the important factors and build the model simultaneously (Cheng and Wu 2001). Group-screening methods have been widely used for situations with large numbers of factors. The fundamental idea is to identify the important/unimportant factors as a group to save experimental
8832677	10297	it is difficult to control more than 20 factors, while in simulation experiments it is easy to control and simulate many combinations of decision variables because the experiment can be automated (Trocine and Malone 2000, 2001; Bettonvil and Kleijnen 1997; Kleijnen, Bettonvil and Persson 2003). Hong Wan Bruce Ankenman Barry L. Nelson Department of Industrial Engineering and Management Sciences Northwestern
8832677	10297	will be different from those for physical experiments. Many screening strategies have been developed to identify important factors with an economical number of design points and samples (Trocine and Malone 2000, 2001). For instance, the first stage of response surface methodology is usually factor screening, which is often based on a firstorder design such as a 2 k?p fractional factorial design or an
10300	10301	described. The first experiment compared the identification of concurrently presented earcons based on the guidelines for individual earcon design and presentation of Brewster, Wright and Edwards  which were presented in spatially distinct locations, to the identification of non-spatially presented earcons which incorporated guidelines for concurrent presentation from McGookin and Brewster
10300	10301	identification when earcons were spatially presented. The second experiment compared the identification of concurrently presented earcons based on the guidelines of Brewster, Wright and Edwards  which were presented in spatially distinct locations, to the identification of spatially presented earcons which incorporated guidelines for the presentation of concurrent earcons from McGookin and
10300	10301	earcon presentation. The CEG have already been shown to significantly improve earcon identification over a set of earcons which were formed from the guidelines of Brewster, Wright and Edwards  (BWE). The purpose of this paper is therefore to identify if presenting BWE earcons in spatial locations is superior to non-spatially presented earcons incorporating the CEG, and if so, can the
10300	10301	Motivation 3. SPATIAL VS. NON-SPATIAL EARCONS Previous research  has shown that the identification of concurrently presented earcons based on the guidelines of Brewster, Wright and Edwards (BWE)  can be significantly improved by presenting each earcon with a unique timbre as well as staggering the starts of each earcon by at least 300ms. It is believed that presenting each earcon in a
10300	10301	presented earcons. In order to determine if this is the case, the experiment in this section attempts to determine if participants can identify earcons complying only with the guidelines of BWE  but with each presented in a different spatial location around a participant’s head, better than earcons based on the guidelines on earcon design from BWE  which also incorporate the guidelines
10300	10302	earcons complying only with the guidelines of BWE in a spatial audio environment would be significantly easier to identify than earcons which also incorporated the CEG from McGookin and Brewster  on concurrent earcon presentation, but were presented in a common spatial location. The independent variable (IV) was the earcon set and presentation method, the dependant variables (DV’s) were the
10300	10302	Used The earcons used in this experiment were of the transformational type  and were the same as those used in previous work investigating non-spatialised concurrent earcon identification . Each earcon represented a ride in an imaginary theme park. Each earcon encoded three parameters of a theme park ride. These attributes and their possible values are described in Table 2. The
10300	10302	Non-Spatial Condition The non-spatial condition used earcons which were based on those from Section 3.2.1, but also incorporated the concurrent earcon guidelines from McGookin and Brewster (CEG) . Hence, although earcons were presented concurrently, each started 300ms after the previous one. In addition, instead of having one timbre for each ride type, each ride type had three distinct but
10300	10302	attributes as whole earcons more easily than when all earcons are located in the same spatial location. If both spatial presentation and the concurrent earcon guidelines from McGookin and Brewster  were combined, would a further increase in concurrent earcons be observed? 4. SPATIAL VS. REVISED SPATIAL EARCONS 4.1. Motivation The previous experiment has shown the importance of spatialisation
10300	10302	audio environment, significantly outperformed non-spatially presented earcons which had been revised based on the guidelines for concurrent earcon presentation (CEG) from McGookin and Brewster . This result indicates a possible divergence of guidance for designers to improve the identification of concurrently presented earcons. One possibility, is that if spatial audio presentation is
10300	10304	The concurrent presentation of auditory sources in an auditory display can allow both increased data presentation rates to users as well as allow for comparisons to be made between multiple data . However, such concurrent presentation can be problematic as auditory sources may interfere with each other, making it difficult to identify each individual source. As Norman  has pointed out on
10300	10307	to reduce such unwanted interactions between concurrently presented sounds, making it difficult for designers to exploit the advantages of concurrent presentation. Previously undertaken work  has investigated how AuICAD04-1 ditory Scene Analysis  principles can be applied to the design of earcons to improve their identification. However, this work failed to take account of the impact
10300	10307	This paper investigates the relationship between the previously determined guidelines of concurrent earcon design and presentation (hence forth referred to as CEG) from McGookin and Brewster , and spatialised earcon presentation. The CEG have already been shown to significantly improve earcon identification over a set of earcons which were formed from the guidelines of Brewster, Wright
10300	10307	concurrent presentation guidelines, this may remove the requirement for designers to have multiple similar timbres (one for each concurrently presented earcon), and the restrictions this imposes . 3.2. Methodology Sixteen participants undertook the experiment described in this section which was of a within groups design and involved two conICAD04-2 First Training Session Group 1 Spatial
10300	10307	earcons complying only with the guidelines of BWE in a spatial audio environment would be significantly easier to identify than earcons which also incorporated the CEG from McGookin and Brewster  on concurrent earcon presentation, but were presented in a common spatial location. The independent variable (IV) was the earcon set and presentation method, the dependant variables (DV’s) were the
10300	10307	separation of sounds existed, as was the case in the revised spatial condition. From these experiments, the following guidelines, which extend those previously developed by McGookin and Brewster  have been identified. These guidelines can be used for future designers of interfaces which use concurrent earcon presentation. • The use of spatialised presentation with headtracking significantly
10300	10309	the two conditions, modified NASA task load index (TLX) questionnaires  were used to evaluate this. 3.2.1. Earcons Used The earcons used in this experiment were of the transformational type  and were the same as those used in previous work investigating non-spatialised concurrent earcon identification . Each earcon represented a ride in an imaginary theme park. Each earcon encoded
4009766	10884	to the originals. Authentication of imprinted features via electronic devices is complex and most importantly, expensive . In all-digital environments such as smart cards or lasercards , authenticating the source of a personal ID is an easy task using off-the-shelf public cryptography  and one-way authentication protocols . Typically, the stored photograph as well as other
4009766	10320	importantly, expensive . In all-digital environments such as smart cards or lasercards , authenticating the source of a personal ID is an easy task using off-the-shelf public cryptography  and one-way authentication protocols . Typically, the stored photograph as well as other biometric features are concatenated to the textual message and hashed. The resulting hash is then signed
4009766	10320	successful verification of a FaceCert, the text and the barcode need to be read without errors. Next, after verifying/encrypting the signature with the corresponding public RSA key of the issuer , the verifier obtains the signed message m. After the verifier hashes the text to obtain t, it computes f||i = m? ?1 t. Then, the verifier decompresses f into a subimage of the original photo that
4009766	10888	this figure has been taken from the CASIA iris database. Portions of the research in this paper use the CASIA iris image database collected by Institute of Automation, Chinese Academy of Sciences . f’ i’ t f istypically compressed down to 700-1.5Kbits  depending upon the desired balance of error rates vs. barcode size. Messages f||i 2 and t are merged into a message m =(f||i)? t using a
4009766	10323	can be of arbitrary length and is also printed on the ID. The ID is certified in the following way. First, the textual data is hashed using a cryptographically secure hashing algorithm such as SHA1 . The resulting 160-bit hash is denoted as t. Next, the facial features on the photo are identified and compressed using an algorithm partially described in this manuscript in Section 3. The
4009766	10323	=w and b =0then return f||i else return REJECT. The signed message m has bit-length |(f||i)| +2hLen + 16, where hLen is the length of the output of the hash function h() in bits (160 bits for SHA1 ). Under the assumption that the cryptographic functions are signing the biometric properties and the associated text in a provably secure manner, the security of FaceCerts stems from the fact that
4009766	10324	can be also certified and verified using a FaceCert ID as illustrated in Figure 1. For example, details of a FaceCert system that encompasses person authentication via iris patterns is described in . We omit the details of the feature extraction and compression process for iris patterns in this manuscript; instead, we recognize a string of bits, i, as an output of this process. The iris digest
4009766	10324	Portions of the research in this paper use the CASIA iris image database collected by Institute of Automation, Chinese Academy of Sciences . f’ i’ t f istypically compressed down to 700-1.5Kbits  depending upon the desired balance of error rates vs. barcode size. Messages f||i 2 and t are merged into a message m =(f||i)? t using a reversible noncommutative operator ? such that (?? ?1 )f||i
4009766	10324	error-prone iris recognition procedure), the detection threshold in the FaceCert system can be set to adjust for much lower false positive error rates than “classic” iris recognition systems . In general, two verification procedures are recognized in the FaceCert system: (a) low-cost, where only facial features are verified, and (b) high-security, where both facial and iris (possibly,
4009766	10325	during FaceCert verification. Also it is safe to assume that message t can be recovered error-free from a printed FaceCert because of deployed high-performance optical character recognition engines , . Shortly, we review this operator in more detail. In the next step, message m is signed with the private key of the FaceCert issuer. We use an RSA private key of |m| + 1 bits to sign/decrypt
4009766	10329	of the operator ?. Its purpose is to prevent adaptive existential forgery on the signing primitive, e.g., RSA, where the adversary creates a valid signature with no control over the message , , . This problem is well known to the cryptography community and has been addressed in several protocols including the probabilistic signature scheme with message recovery (PSS-R) ,
4009766	10332	, , . This problem is well known to the cryptography community and has been addressed in several protocols including the probabilistic signature scheme with message recovery (PSS-R) , which is based upon optimal asymmetric encryption padding (OAEP) . Although several integrity check mechanisms for RSA signatures can be used with different security properties, the exemplary
4009766	10332	f||i and hashing M||t||r to obtain w = h(M||t||r), 2 Operator || denotes concatenation.swhere h() is a hash function and M, w, andr refer to the corresponding variables in Figure 2 in Section 5 of . PSS-Rderives m = b||w||r ? ||M ? where b is a single bit set to 0 and variables r ? and M ? are created as in Figure 2 in Section 5 of . Signature’s integrity check in this case is performed
4009766	4941	and has been addressed in several protocols including the probabilistic signature scheme with message recovery (PSS-R) , which is based upon optimal asymmetric encryption padding (OAEP) . Although several integrity check mechanisms for RSA signatures can be used with different security properties, the exemplary PSS-Rachieves provable security with near-optimal redundancy used in
4009766	10905	distinguishing traits. Several biometric solutions have been proposed via face, speech, fingerprint, handwriting, iris, and retina recognition. Solid survey of these techniques can be found at . Just as FaceCerts, a person identification system that relies on biometric solutions must involve a human verifier who must ensure the identification system is not fooled. For instance, an
4009766	10338	in particular in face and speaker recognition, does not stay constant as the system scales up, which commonly renders these systems highly prone to false alarms and false positives , , and ??? centralized decision making – the verifier needs to be connected to a central trusted server which actually performs the identification, which in a sense implies: • high cost – the equipment
4009766	10339	has been demonstrated so far that smart cards cannot be considered a secure storage because it is relatively easy to extract the hidden information even without reverse engineering the smart card . Exemplary attacks that have successfully identified encryption keys (both symmetric and private keys), have been based on analyzing smart card’s I/O behavior via differential power analysis
4009766	10340	. Exemplary attacks that have successfully identified encryption keys (both symmetric and private keys), have been based on analyzing smart card’s I/O behavior via differential power analysis  or timing analysis . Finally, there are several differences that strongly favor FaceCerts to smart cards. • A smart card based system must display the photo, whereas FaceCerts only scans the ID
4009766	10342	present in the verifier. Hence, a single broken verifying device renders the entire system broken. A public-key watermarking system has been developed, however, with a different target application . This system requires significantly longer host signals than a single photo to reliably detect the existence of a given secret. Also, such a system would require that the secret used to mark a
4009766	10343	for the design of the optimal encoding strategy for the factors y. A realted method, principal component analysis, was used by Moghaddam and t ytsPentland for face recognition and compression . By limiting their representation to the central part of the face they were able to represent each image in a carefully manually preprocessed database, with only 85 bytes describing 100 face
4009766	10344	the subspace encoding is illustrated in Figure 2. First, given an ID photograph, we identify the facial structure to be modelled x = N(?y+µ, ?) with eigenfaces using a face detection algorithm , . Vector µsdenotes the first order statistics of the input image x. As the posterior p(y|x) can be computed using the Bayesian rule, hence we compute: log p(y|x) = ? log p(x) ? 1 2 yy? ? 1 2
13709956	10352	hard real-time task required strict deadlines with zero time fault tolerance. Thanks to the RCCL/RCI package we conveniently achieve synchronous robot control employing a standard SUN SparcStation . One standard SORMA client-server CO asynchronously connected the vision analysis and the robot host. All other communication used the SORMA time-optimal intraprocess invocation (about ¨ time) and
13709956	10353	interprocess communications services via shared memory ?s? ?¢¥¤?? (about s).s© £¢¥¤?? s elapsed call overhead In contrast to other robot control software frameworks like Chimera  or Psyche , SORMA doessnot attempt to be a real-time operating system (OS) itself (e.g. no scheduler). Chimera and Smart  focus on multi-processor VME-bus systems which are hosted by a Unix workstation.
13709956	10355	¨ time) and further interprocess communications services via shared memory ?s? ?¢¥¤?? (about s).s© £¢¥¤?? s elapsed call overhead In contrast to other robot control software frameworks like Chimera  or Psyche , SORMA doessnot attempt to be a real-time operating system (OS) itself (e.g. no scheduler). Chimera and Smart  focus on multi-processor VME-bus systems which are hosted by a Unix
13709956	10357	section introduces the Service Object Request Management Architecture (“SORMA”), and describes some of the concepts which contribute to solve the above stated goals. For a more detailed account see . 4 SORMA Concepts Central to SORMA is the notion of a “service”, which offers a certain functionality, e.g. abstract computing, providing low- or high-level access to hardware devices including
13709956	10357	present, it indicates a remote service which is then resolved via the proxy mechanism to the SO name “foo.a” – then local at the daemon side, see below (for options on opaque network addressing see ). At creation time, a local name (like “foo.a” without “@” part) gets expanded into a list of CTRL arguments – text tokens, which are interpreted by the service's CTRL method (see Fig. 4). This
13709956	10357	5 Experiences with SORMA The first implementation example was a checkers player system, developed with a team of students. Due to the limited space, we must refer any details to the description in . In this hybrid application the distributed object infrastructure was in the foreground. Complying to the international rule of Fig. 6: Example of a graphical user interface (GUI, by R. Rae),
13709956	10357	with a daemon; across our computer park). The real-time capabilities were demonstrated in a 3 D robot tracking application, combining vision and force senses and a rapid learning PSOM network . This hard real-time task required strict deadlines with zero time fault tolerance. Thanks to the RCCL/RCI package we conveniently achieve synchronous robot control employing a standard SUN
13709956	10358	resources; economy of reusing components; communication cost; built-in interactivity; time-optimal and protected invocation. 1 Introduction The experience of building and operating robotvision labs  shows, that a substantial amount of effort easily dissipates in adaption of software components to application specific needs. A lot of interesting ideas and application code is developed, but too
10359	10362	as part of the user-provided input. Others view information about the set of components and the possible compositional structures as part of the input specification. Following Chandrasekaran  one could argue that a specification of the technology to be applied in the design process is also part of the input. In addition to the problem specification-- knowledge is needed about the design
10359	10362	more details on the case-based design method in general. 3.2 Propose, Critique and Modify (PCM) methods The class of methods based on the PCM approach, was extensively discussed by Chandrasekaran . The general principle is that an initial configuration is proposed, which is subsequently tested against constraints and requirements. If violations are found, modifications are used to remove the
10359	10363	the verification process. The selection process can be simple or can be based on a more complex matching process where requirements are matched against properties of the given arrangements. Clancey  proposes heuristic classification as a way of selecting and verifying enumerated solutions to configuration problems. The select-and-verify PSM assumes that a solution can be found among the given
8919995	10950	of source address verification (SAV) in routers. This led to network operators and the IETF publishing Best Common Practice #38  in May 2000, following growing concerns after the Teardrop  and Smurf attacks began in late 1997. This document recommends that network operators implement SAV mechanisms on the edge of the network using traditional ‘access list’ methods of filtering
8919998	10977	and empty ribosomes). This is a surprise. Hydrodynamic and scattering data suggest that there are small conformational differences between pre-translocational and post-translocational ribosomes , and one might have thought a change that appears modest hydrodynamically would be easy to see at 20 Å resolution. However, van Heel’s observation is consistent with the chemical modification
10421	10421	which is consistent with pure functional programming. Space considerations preclude a rigorous presentation of type safety for AFL. A complete proof is given in the companion technical report . 6 Change Propagation is Sound We formalize the notion of an input change and present a formal version of the change-propagation algorithm. Using this formal framework, we prove that the
10421	10421	?, denoted B and B by replacing each location l in T or ? with its image B whenever the image is defined. The formal definitions for these are given in the companion technical report . The theorem below states that the change-propagation algorithm is correct, the proof of the theorem is given in the companion technical report . In the theorem, the reason that the store ? ? m
10421	10421	change propagation can subsequently be used as further inputs. The proof requires that store modification operation respect the typing of the store being modified and given in the companion report . 258 7 Discussion Variants. In the process of developing the mechanisms presented in this paper we considered several variants. Here we mention a few of them. One variant is to replace the explicit
10421	10422	side-effects in its implementation. Applications. The work in this paper was motivated by the desire to make it easier to define kinetic data structuressfor problems in computational geometry . Consider the problem of maintaining some property of a set of objects in space as they move, such as the nearest neighbors or convex hull of a set of points. Kinetic data structures are designed
10421	1748	could be made to work in an imperative setting as long as relevant data structures are persistent. There has been significant research on persistent data-structures under an imperative setting . We further note that certain “benign” side effects are not harmful. For example, side effects to objects that are not examined by the adaptive code itself are harmless. This includes print
10421	10429	methods we have seen to decide when and what to evict seem ad-hoc, although Liu and Teitelbaum have made some progress using automatic program transformation techniques to decide what to cache . In spite of these problems, function caching might have some advantages over our method for “shallow” modifications. We expect that these techniques can be integrated to further improve
10421	10430	methods we have seen to decide when and what to evict seem ad-hoc, although Liu and Teitelbaum have made some progress using automatic program transformation techniques to decide what to cache . In spite of these problems, function caching might have some advantages over our method for “shallow” modifications. We expect that these techniques can be integrated to further improve
10421	7106	mode. Finally, conditionals with changeable branches are themselves changeable. Static Semantics. The AFL type system is inspired by the type theory of modal logic given by Pfenning and Davies??? . We distinguish two modes, the stable and the changeable, corresponding to the distinction between terms and expressions, respectively, in Pfenning and Davies’ work. However, they have no analogue
8920007	10442	facilitate user understanding”. As introduced by Ishida, people spend their incomes within a close range from home, as the case of the US, where 80% of incomes are spent within 20 miles from home (Ishida, 2000). The same author defends that networks of people with the same problem are more valuable and that digital cities provide the infrastructure for networking local communities. Although this approach
8920007	10442	according to its goals, architecture, technology and organisation. It concludes that digital cities provide an opportunity to people to create a new information space for their everyday life (Ishida, 2000). Gurstein defends that digital cities must provide resources to fulfill a number of requirements such as community Internet Access, community information, community service delivery on-line,
72840	10452	integers in any reasonable amount of time. In our case, we assume the adversary cannot solve an artificial intelligence problem with higher accuracy than what’s currently known to the AI community . This approach has the beneficial side effect of inducing security researchers, as well as otherwise malicious programmers, to advance the field of AI (much like computational number theory has
72840	10454	A good example of this process is the recent progress in reading distorted text images motivated by the CAPTCHA in use at Yahoo. In response to the challenge provided by this test, Malik and Mori  have developed a program that can pass the test with over 80% accuracy. Malik and Mori’s algorithm represents significant progress in the general area of text recognition, and it is extremely
72840	10455	integers in any reasonable amount of time. In our case, we assume the adversary cannot solve an artificial intelligence problem with higher accuracy than what’s currently known to the AI community . This approach has the beneficial side effect of inducing security researchers, as well as otherwise malicious programmers, to advance the field of AI (much like computational number theory has
72840	10456	worms and spam: only accept an email message if you know there is a human behind the other computer. A few companies, such as www.spamarrest. com are already marketing this idea. Pinkas and Sander  have also suggested using CAPTCHAs to prevent dictionary attacks in password systems. The idea is simple: prevent a computer from being able to iterate through the entire space of passwords by
4207	5098	in . A different priority-index heuristic, obtained from Whittle’s linear programming relaxation, together with improved performance bounds, has been developed and tested computationally in . A polyhedral framework for analysis and computation of the Whittle index and extensions, based on the notion of partial conservation laws, has been recently developed in . 3 Queueing
4207	4201	with larger current index. The optimality of Gittins rule, for the original model and extensions, has a rich history of proofs, based on different techniques, including interchange arguments , dynamic programming , intuitive arguments , induction arguments , and conservation laws/linear programming . For more complex model extensions, the Gittins rule is no longer
4207	5109	level of generality). The performance of Klimov’s rule, when used as a heuristic for the model extension that incorporates identical parallel servers, has been analyzed using this approach in : a relaxed linear programming formulation of the performance region is shown to yield closed-form suboptimality bounds, which imply asymptotic optimality in the heavy-traffic limit. More general
4207	4206	computationally in . A polyhedral framework for analysis and computation of the Whittle index and extensions, based on the notion of partial conservation laws, has been recently developed in . 3 Queueing scheduling control models Models in this class are concerned with the problem of designing optimal service disciplines in queueing systems, where the set of jobs to be completed,
19494	499	functions and compute the exact probability of a transition occurring between the boundary of ? C ? and ? C ? CI. Instead of enumerating disjoint covers, ordered binary decision diagrams (BDD’s)  can be used for the calculation of signal probability. It has been shown in  and  that exact signal probability calculation for a given function can be performed by a linear traversal of a
19494	10491	estimation are attractive because statistical estimates can be obtained without recourse to time-consuming exhaustive simulation. In the past, probabilistic peak current estimation methods (e.g., ) that compute probabilistic current waveforms for combinational circuits have been developed. Estimation of worst case power dissipation (e.g.,  and ) is a difficult problem requiring a
19494	7133	the delay of the individual modules is lost. For an excellent review of power estimation methods, the reader is referred to . Our work on switching activity estimation, originally presented in , has the following important characteristics. We use a variable delay model for combinational logic in our symbolic simulation method, which correctly computes the Boolean conditions that cause
19494	7130	is embedded in a larger sequential circuit. The techniques described have to be augmented to handle sequential circuits and primary input correlation, and efforts in this direction are presented in  and .sIEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 1, JANUARY 1997 127 ACKNOWLEDGMENT The authors thank M. Bershetyn, J. Rabaey, and S. Murai for
19494	10496	signal probabilities 1 of a multilevel circuit derived from the original circuit by a process of symbolic simulation. The work closest to ours is the transition density calculation method by Najm . Transition densities correspond to average switching rates for gates in the circuit. In , an interconnection of combinational logic modules, each with a certain delay, makes up a circuit.
19494	10496	g??—? is the load capacitance, †?? is the supply voltage, „™?™ is the global clock period, and i@??—????????A is the expected value of the number of gate output transitions per global clock cycle  or equivalently the average number of gate output transitions per clock cycle. All of the parameters in (1) can be determined from technology or circuit layout information except i@??—????????A,
19494	10496	of ? C ? and ? C ? CI. Instead of enumerating disjoint covers, ordered binary decision diagrams (BDD’s)  can be used for the calculation of signal probability. It has been shown in  and  that exact signal probability calculation for a given function can be performed by a linear traversal of a BDD representation of a logic function. We have implemented methods for signal probability
19494	10497	modules into one large module, but in this case the information regarding the delay of the individual modules is lost. For an excellent review of power estimation methods, the reader is referred to . Our work on switching activity estimation, originally presented in , has the following important characteristics. We use a variable delay model for combinational logic in our symbolic
19494	7130	function that represents the total switching activity over any possible input vector pair, assuming a variable delay model for the gates in the circuit. 2 This assumption is relaxed in the work of . Fig. 1. Glitching in a static CMOS circuit. Fig. 2. A multilevel combinational circuit. A. Symbolic Simulation 1) An Example: Consider the multilevel combinational circuit shown in Fig. 2. It has
19494	7130	in a larger sequential circuit. The techniques described have to be augmented to handle sequential circuits and primary input correlation, and efforts in this direction are presented in  and .sIEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 1, JANUARY 1997 127 ACKNOWLEDGMENT The authors thank M. Bershetyn, J. Rabaey, and S. Murai for helpful
10498	10499	to be rapid and accurate. By optimizing the subspaces in AAM, we can get more accurate and faster results. What’s more, it can be used to optimize other alignment algorithms, such as ASM and AAA . The rest of the paper is arranged as follows. An overview of AAM is described in Section 2. Section 3 discusses the problem of optimizing the subspaces in AAM. In Section 4, we present the
10498	10500	However, we find that these two parts are closely interrelated and the performance depends on both of them. Unfortunately, this relationship is often neglected by previous works. Most efforts  only attempted to improve the search procedure without considering the subspace model. Stegmann et al  attempted to optimize the subspace model, but he did not consider the search procedure. In
10498	1102	for extraction of good facial features for success of applications such as face recognition, expression analysis and face animation. Active Shape Models (ASM)  and Active Appearance Models (AAM)  are two successful methods for face alignment. ASM uses the local appearance model, which represents the local statistics around each landmark to efficiently find the target landmarks. And the
10498	1102	shape, the corresponding texture T0 is warped and normalized to T . Then all of them are aligned to the tangent space {T } of the mean texture ¯ T by using an iterative approach as described in . The AAM texture subspace is obtained by PCA analysis T = ¯ T + Ptt (2) where Pt is the matrix consisting of l principal orthogonal modes of variation in {T }. Any texture in the AAM texture
10498	4369	However, we find that these two parts are closely interrelated and the performance depends on both of them. Unfortunately, this relationship is often neglected by previous works. Most efforts  only attempted to improve the search procedure without considering the subspace model. Stegmann et al  attempted to optimize the subspace model, but he did not consider the search procedure. In
10498	1102	However, we find that these two parts are closely interrelated and the performance depends on both of them. Unfortunately, this relationship is often neglected by previous works. Most efforts  only attempted to improve the search procedure without considering the subspace model. Stegmann et al  attempted to optimize the subspace model, but he did not consider the search procedure. In
10498	10501	for extraction of good facial features for success of applications such as face recognition, expression analysis and face animation. Active Shape Models (ASM)  and Active Appearance Models (AAM)  are two successful methods for face alignment. ASM uses the local appearance model, which represents the local statistics around each landmark to efficiently find the target landmarks. And the
10498	10503	for extraction of good facial features for success of applications such as face recognition, expression analysis and face animation. Active Shape Models (ASM)  and Active Appearance Models (AAM)  are two successful methods for face alignment. ASM uses the local appearance model, which represents the local statistics around each landmark to efficiently find the target landmarks. And the
10498	10505	However, we find that these two parts are closely interrelated and the performance depends on both of them. Unfortunately, this relationship is often neglected by previous works. Most efforts  only attempted to improve the search procedure without considering the subspace model. Stegmann et al  attempted to optimize the subspace model, but he did not consider the search procedure. In
10498	10506	likely to be poor due to (5)sover-fitting. There are two basic categories of techniques for model selection: complexity penalization and hold-out testing (e.g. cross-validation and bootstrapping) . However, there is a little difference between model selection and the estimation of explanation proportions in AAM. As for traditional model selection problem, it is usually the case that the more
10498	10508	use the above method to find the optimal appearance subspace constructed from the optimal subspaces of shape and texture. 6. EXPERIMENTS The database used consists of 200 face images from the FERET , the AR  databases and other collections. 87 landmarks are labelled on each face. We randomly select 100 images as the training and the other 100 images as the testing images. Our AAM
10510	10511	suggests the application of the more general partially ordered case (sections 3.4 and 4) to produce a poset-valued-possibilistic logic, of a different form to that produced by Benferhat et al., . 3.4 The General Partially Ordered Set Case Here we consider the general case: where A = (A, 0, 1, ?) is an arbitrary partially ordered set that has a unique minimal element 0 and a unique maximal
10510	10513	in idempotent semiring-based CSPs. This semantics differs in a fundamental way from the model-theoretic semantics for semiring-based Constraint Logic Programming given in (Bistarelli et al, 2001) . In the semantics described above (both in the hard constraints and soft constraints cases), a constraint is taken to restrict allowable tuples, and so conveys only negative information: a tuple
10510	10513	in the constraint may or may not be adequate. Consequently, there exists a unique maximal model of a set of constraints C, i.e., ( ? C) ?V . In contrast, in the logic programming-based semantics of , constraints are expressed using a set of facts expressing known instances of the constraints, which is not assumed to be an exhaustive set of instances; these therefore express positive
10510	5566	follows because combination and projection obey the Shenoy-Shafer axioms (Shenoy and Shafer, 90) ; this is an instance of the fusion algorithm  (c.f. also Dechter’s Bucket Elimination ), which for this case corresponds to the adaptive consistency algorithm . Therefore the deletion inference rule gives a sound and complete proof procedure. This approach is efficient if one
10540	8642	asymptotic form ?(k) ? c?k ?? , for some positive constant c? and some real ? ? (0, 1). In this case, the parameter ? is related to the Hurst parameter by H = 1 ? ?/2. This definition is used in       . LRD can also be defined in terms of the spectral density of a process. This defines LRD in terms of a spectral density which has a pole at zero.s1.1. INTRODUCTION TO
10540	8642	density obeys f(?) ? cf|?| ?? , as ? ? 0, for some positive constant cf and some real ? ? (0, 1). The parameter ? is related to the Hurst parameter by H = (1+?)/2. This definition is found in     . A more general frequency domain definition is also occasionally used which allows the existence of LRD when the spectral density has a pole at a frequency ?0 ? . This
10540	10543	arrival times at the destination show LRD”. This, obviously indicates that round trip times in networks are LRD processes. This work is extended to multifractal measures by . Recent work  shows that LRD can arise in a relatively simple simulation where Poisson sources randomly situated in a grid network are aggregated as they route via shortest paths to randomly situated sinks.
10540	10543	does not knows2.11. DISCUSSION 98 in advance how many points of data will be wanted. In addition these generation mechanisms are typically slower. The widely-used iterated map based approach  has issues related to double precision arithmetic. While generally, the precision is good enough for most purposes, the correlations in that model necessarily fall off eventually due to the finite
10540	10550	signs and in-car systems for route guidance have impact only because they provide information to the driver. This information provision may significantly impact route and departure time choice . Citing ,  and ,  argues that, “...the notion of a simple optimised decision-making rule is unrealistic for understanding fully the impact of ATIS on travel behavior.”
10540	10550	gaps in knowledge exist. One such gap is knowledge of the attribute-set relevant to route and departure time choices, and the appropriate representation of these attributes in choice utilities. In  it is suggested that travel time is perhaps the most important attribute influencing route choice, but recognised that difficulties exist in taking account of how individuals perceive travel time.
10540	10556	only by a vanishingly small amount of traffic). Multinomial models, either multinomial logit(MNL) or probit(MNP), are used to implement SUE models. MNL is characterised by the following assumptions : (1) The utilities are independent and identically distributed (i.i.d.) with a Gumbel distribution. (2) There is a response homogeneity across individuals. (3) There is error variance-covariance
10540	10558	dependence” (that is, LRD up to a certain time scale). (4) Finally, there remains the distinct possibility that LRD is an emergent property of the networks themselves. Measurements made in  show that, even when “packet inter-departure times are independent, arrival times at the destination show LRD”. This, obviously indicates that round trip times in networks are LRD processes. This
10540	10562	packet networks are consistent with statistical self-similarity or fractal characteristics... measured packet traffic data are consistent with long-range dependence...” A different view is put in  which reports measurements made on some high speed networks. The paper does not clearly describe when the measurements examined were taken but claims that in high speed networks then the merging of
10540	8656	is proved in . There are good reasons to believe that source traffic on the Internet is heavy-tailed — file sizes and sizes of accessed web documents have been shown to have heavy tails (see ).s1.4. LRD IN THE INTERNET 52 (3) Another potential cause of LRD is the feedback mechanisms in the Transmission Control Protocol (TCP). Markov chains were used in  to model TCP timeout and
10540	10583	time plots of the bytes per unit time, the authors concluded that the traffic was long-range dependent. In addition to the above studies, in a 1996 review of research in the area,  listed      as having “provided convincing evidence that actual traffic data from working packet networks are consistent with statistical self-similarity or fractal characteristics...
10540	8662	In general it has been found that a higher Hurst parameter often increases delays in a network, the probability of packet loss and affects a number of measures of engineering importance. In fact  claims that the Hurst parameter is “...a dominant characteristic for a number of packet traffic engineering problems...”. Some of the effects on queuing performance are given by  .
10540	10588	to have heavy tails (see ).s1.4. LRD IN THE INTERNET 52 (3) Another potential cause of LRD is the feedback mechanisms in the Transmission Control Protocol (TCP). Markov chains were used in  to model TCP timeout and congestion window behaviour and the authors prove that these can cause what the authors refer to as “local long-range dependence” (that is, LRD up to a certain time scale).
10540	8668	A non-technical introduction and review of research up to 1999 is provided by . An introduction to the difficulties of modelling and measuring Internet behaviour in general is provided by . A more recent summary of work in the area is found in . While this thesis does not actually make any measurements using telecoms data, a brief survey of the most relevant work in the area
10540	10591	plots of the bytes per unit time, the authors concluded that the traffic was long-range dependent. In addition to the above studies, in a 1996 review of research in the area,  listed      as having “provided convincing evidence that actual traffic data from working packet networks are consistent with statistical self-similarity or fractal characteristics... measured
10540	10591	Each might make a contribution to the packet traffic behaviour of the network. (1) The evidence that LRD arises directly in the source of data comes mainly from studies of video traffic (see ,  and ). The claim in these papers is that variable-bit-rate (VBR) video traffic by its nature exhibits LRD at source. The LRD, in this case, arises from an encoding mechanism whereby video is
10540	10594	?(k) ? c?k ?? , for some positive constant c? and some real ? ? (0, 1). In this case, the parameter ? is related to the Hurst parameter by H = 1 ? ?/2. This definition is used in       . LRD can also be defined in terms of the spectral density of a process. This defines LRD in terms of a spectral density which has a pole at zero.s1.1. INTRODUCTION TO LRD 29 Definition
10540	10594	obeys f(?) ? cf|?| ?? , as ? ? 0, for some positive constant cf and some real ? ? (0, 1). The parameter ? is related to the Hurst parameter by H = (1+?)/2. This definition is found in     . A more general frequency domain definition is also occasionally used which allows the existence of LRD when the spectral density has a pole at a frequency ?0 ? . This definition was
10540	10610	? c?k ?? , for some positive constant c? and some real ? ? (0, 1). In this case, the parameter ? is related to the Hurst parameter by H = 1 ? ?/2. This definition is used in       . LRD can also be defined in terms of the spectral density of a process. This defines LRD in terms of a spectral density which has a pole at zero.s1.1. INTRODUCTION TO LRD 29 Definition 1.22.
10540	10610	f(?) ? cf|?| ?? , as ? ? 0, for some positive constant cf and some real ? ? (0, 1). The parameter ? is related to the Hurst parameter by H = (1+?)/2. This definition is found in     . A more general frequency domain definition is also occasionally used which allows the existence of LRD when the spectral density has a pole at a frequency ?0 ? . This definition was first
10540	10614	introduce the extra parameters required by multifractal modelling. However, other authors disagree. For example, the scalings1.4. LRD IN THE INTERNET 50 properties of teletraffic are discussed by  which suggests that LRD in teletraffic happens in two separate regimes: a scaling behaviour at time scales above one second and a less clear scaling behaviour at time scales below one second. (The
10540	10615	of the bytes per unit time, the authors concluded that the traffic was long-range dependent. In addition to the above studies, in a 1996 review of research in the area,  listed      as having “provided convincing evidence that actual traffic data from working packet networks are consistent with statistical self-similarity or fractal characteristics... measured packet
10540	10621	Citing ,  and ,  argues that, “...the notion of a simple optimised decision-making rule is unrealistic for understanding fully the impact of ATIS on travel behavior.” In  a Bayesian updating model is developed to analyse the mechanism by which drivers update their travel time perceptions from one day to the next, on the basis of ATIS and previous experience. In
10540	2673	in Appendix A.2) in place of c? and cf. With this replacement, the two definitions become and ?(k) ? L(k)k ?? , f(?) ? L(?)|?| ?? , as ? ? 0, respectively. These definitions are used in  . Of the definitions listed in this section, Definition 1.20 (the non summability of the modulus of the ACF) encompasses the widest class of processes and is implied by all the other definitions. It
10540	2673	The global log periodogram and Local Whittle techniques are considered to be the most effective. 1.4. LRD in the Internet In 1993, Leland, Taqqu, Willinger and Wilson published their classic paper  which identified the presence of LRD in data sets captured on Ethernet Local Area Network (LAN) traffic. Since its publication, this paper has been cited more than five hundred times. The paper
10540	2673	not actually make any measurements using telecoms data, a brief survey of the most relevant work in the area will provide context for the research undertaken. 1.4.1. Traffic Measurements. The paper  used R/S analysis, aggregated variance and Whittle’s estimator to investigate a large number of Ethernet measurements made between 1989 and 1992. The paper examined busy times, “normal” traffic
10540	10637	be long-range dependent if the absolute value of its ACF ?(k) sums to infinity. ?? k=?? |?(k)| = ?. This definition (or the equivalent definition with autocovariance instead of ACF) is used by   . A more restrictive definition often used is given by putting conditions on how ?(k) decays as k ? ?. Definition 1.21. A stationary process Xt is said to be long-range dependent if its ACF
10540	10638	various protocols. The protocol used is critical with some connections being Poisson and others presenting distributions of connection times which are completely at odds with Poisson modelling. In  two hour traffic traces collected for the paper, each lasting twenty four hours and made on 100Mb links at Harvard University, were analysed. Using variance time plots of the bytes per unit time,
10540	10639	addition, the techniques known as Absolute Moments and Ratio of Variance of Residuals are described on Taqqu’s website . A method known as the global log-periodogram estimator is described in  and is a frequency domain technique which uses the entire frequency spectrum to estimate H. A number of authors have compared the different estimation techniques for H. Several techniques are
10540	10642	that the Hurst parameter is “...a dominant characteristic for a number of packet traffic engineering problems...”. Some of the effects on queuing performance are given by  . However,  shows that while the Hurst parameter is important to queueing, the relationship is not a simple one — in some cases a high Hurst parameter may improve performance or have no effect. (A commonly
10540	8679	In fact  claims that the Hurst parameter is “...a dominant characteristic for a number of packet traffic engineering problems...”. Some of the effects on queuing performance are given by  . However,  shows that while the Hurst parameter is important to queueing, the relationship is not a simple one — in some cases a high Hurst parameter may improve performance or have no
10540	11222	in many cases. 2.10. Simulation Results on a Simple Network The simulation results in this section were obtained with the help of Dr. Yann Golanski. The software used was the ns-2 simulation  which models individual packets in a network using approximations of the protocols used in the Internet. The topology chosen for testing in ns was to represent an aggregation of traffic from
10540	2679	exhibit LRD. For example,  makes measurements on videos1.4. LRD IN THE INTERNET 49 traffic. The traffic is shown to be long-range dependent at source due to the encoding of the video stream. In  a number of WAN traces collected from 1989 to 1994 are analysed. The general hypothesis of long-range dependence is confirmed. In addition to this, statistical models are given for how users
10540	2679	back as 1989). Of course, if VBR video traffic contains LRD at source then it is likely that other applications might have traffic distributions with unexpected statistical effects. For example,  shows that telnet packets are not well modelled by a Poisson distribution. (2) The proposal that LRD in Internet traffic arises from the aggregation of heavy-tailed data streams is similar to the
10540	10651	Definition 1.19. A stationary process Xt is said to be long-range dependent if its ACF ?(k) sums to infinity. ?? k=?? ?(k) = ?. This definition is sometimes used in the literature, for example in . (Note that the summation is usually given in the literature as being between ?? and ? although the assumption of weak stationarity means that ?(k) = ?(?k).) Often a slightly less restrictive
10540	10651	(which will be discussed in Section 1.4) that Internet traffic exhibits different scaling behaviour at different timescales. A general description of multifractal processes and wavelets is found in  and a description of how wavelets can be used to create models with the same multifractal spectrum as a given data set can be found in . Self-similar processes can be simulated (and, indeed,
10540	10651	Comparison of Methods. Wavelet analysis has been used for the estimation of the Hurst parameter. In addition this has the benefit of providing an estimate of the multifractal spectrum of the data  . Crossing trees (analysis of where a process crosses certain preset levels) can be used to estimate H in self-similar data sets  . Higuchi’s method estimates H by estimating fractal
10540	10652	of multifractal processes and wavelets is found in  and a description of how wavelets can be used to create models with the same multifractal spectrum as a given data set can be found in . Self-similar processes can be simulated (and, indeed, measured) using Embedded Branching Processes. Details can be found in  and . An aggregation of ON-OFF sources with heavy-tails can be
10540	10652	of Methods. Wavelet analysis has been used for the estimation of the Hurst parameter. In addition this has the benefit of providing an estimate of the multifractal spectrum of the data  . Crossing trees (analysis of where a process crosses certain preset levels) can be used to estimate H in self-similar data sets  . Higuchi’s method estimates H by estimating fractal
10540	10656	in the area up to 1996 is found in  which references more than four hundred papers related to the subject area. A non-technical introduction and review of research up to 1999 is provided by . An introduction to the difficulties of modelling and measuring Internet behaviour in general is provided by . A more recent summary of work in the area is found in . While this thesis
10540	10656	In fact  claims that the Hurst parameter is “...a dominant characteristic for a number of packet traffic engineering problems...”. Some of the effects on queuing performance are given by  . However,  shows that while the Hurst parameter is important to queueing, the relationship is not a simple one — in some cases a high Hurst parameter may improve performance or have no effect.
10540	10670	computer networks follow a heavy tailed distribution — the lengths of files stored on computers and the amount of data which is transferred by a given connection to the Internet. It has been shown  that a superposition of ON/OFF sources (in Internet traffic this could be visualised as packet trains and inter-train pauses) will give rise to a time series exhibiting LRD if the lengths of the
10540	8690	domain technique which uses the entire frequency spectrum to estimate H. A number of authors have compared the different estimation techniques for H. Several techniques are compared empirically in  by testing methods against time series which are FARIMA (0, d, 0) or FGN with known H.s1.4. LRD IN THE INTERNET 47 The methods tested included R/S, Whittle, Aggregated Variance, Higuchi’s Method
10540	8698	?? , for some positive constant c? and some real ? ? (0, 1). In this case, the parameter ? is related to the Hurst parameter by H = 1 ? ?/2. This definition is used in       . LRD can also be defined in terms of the spectral density of a process. This defines LRD in terms of a spectral density which has a pole at zero.s1.1. INTRODUCTION TO LRD 29 Definition 1.22. The
10540	8698	to 1999 is provided by . An introduction to the difficulties of modelling and measuring Internet behaviour in general is provided by . A more recent summary of work in the area is found in . While this thesis does not actually make any measurements using telecoms data, a brief survey of the most relevant work in the area will provide context for the research undertaken. 1.4.1. Traffic
10540	8700	parameter). Resolving the traffic into separate components (breaking it up by destination or protocol used) showed that the traffic shared a characteristic H value for all major components. In  some of these same traces are analysed to show that heavy tails are present in the data sources. That is, if the distribution of the lengths of data sent is plotted then its distribution is heavy
8920020	4753	graph models proposed in  in order to model real world distortions in attributed graphs. Training set expansion has been recently applied also in the domain of handwritten character recognition . The paper is organized as follows: in Section 2 we describe the proposed method for training set expansion, that in turn contains a description of the proposed tree grammar (Sec. 2.1) and a
8920020	10688	retrieval, routing, and understanding. An efficient initial classification can be achieved by representing the layout with XY-trees , and their extension dealing with ruling lines (MXY-tree ). In XY-trees the root brings information about the entire page and each child contains a portion of the image related to its father. Every portion is recursively obtained by XY-cuts. An XY-cut is
8920020	10690	of the tree that is classified by means of artificial neural networks . MXYtree descriptions of document images have been used as well for the retrieval of relevant pages in the image domain . Unfortunately, the segmentation algorithms which build the XY-tree, do not produce similar trees starting from similar pages (e.g. Fig. 1). In some cases the related trees are very different each
8920020	10696	one tree into another. We can define the distance between two trees as the cost of the minimum-cost set of operations that are required to transform the first tree into the second one. Zhang  proposed an efficient algorithm to compute the tree edit distance. Using tree edit distance we can build a K ? nn classifier were each tree is ascribed in the most common class among the K trees of
9270328	5967	making it therefore applicable for data modelling techniques such as ER (), NIAM () and PSM (), and action modelling techniques such as Task Structures (), DFD () and ExSpect (). Version modelling in engineering databases can be seen as a restricted form of evolving information systems (, ). An important requirement for evolving
9270328	5620	of application models. The central part of this theory will make weak assumptions on the underlying modelling technique, making it therefore applicable for data modelling techniques such as ER (), NIAM () and PSM (), and action modelling techniques such as Task Structures (), DFD () and ExSpect (). Version modelling in engineering databases can be seen as a
9270328	5969	, F ?, where T is a (discrete) time axis, and F a set of functions over T . In this paper, the time axis is regarded as an abstract data type. Several ways of defining a time axis exist, see e.g. ,  or . The time axis is the axis along which the application model evolves (through the application model universe). An application model element history is a partial function, that
9270328	5974	and specification of dynamic aspects). In case of snapshot databases structure modifications lead to costly data conversions and reprogramming. The intention of an evolving information system () is to be able to handle updates of all components of the so-called application model, containing the information structure, the constraints on this structure, the population conforming to this
9270328	5974	Model ?? ? ????? ? ? ?? World Action Model Model ? ? ? ? ????????? ? ? ? ? ? ?? Information ? Structure Constraints Population Figure 1: A hierarchy of models model, is called an application model (, ). The resulting hierarchy of models is depicted in figure 1. 2.2. An example of evolution Traditionally, a world model consists of a fixed data model according to some data modelling
9270328	5975	such evolution. In this theory, the underlying data model is a parameter, making the theory applicable for a wide range of modelling techniques. 1. Introduction As has been argued in  and , there is a growing demand for information systems, not only allowing for changes of their information base, but also for modifications in their underlying structure (conceptual schema and
9270328	5975	????? ? ? ?? World Action Model Model ? ? ? ? ????????? ? ? ? ? ? ?? Information ? Structure Constraints Population Figure 1: A hierarchy of models model, is called an application model (, ). The resulting hierarchy of models is depicted in figure 1. 2.2. An example of evolution Traditionally, a world model consists of a fixed data model according to some data modelling technique, and
9270328	5978	part of this theory will make weak assumptions on the underlying modelling technique, making it therefore applicable for data modelling techniques such as ER (), NIAM () and PSM (), and action modelling techniques such as Task Structures (), DFD () and ExSpect (). Version modelling in engineering databases can be seen as a restricted form of evolving
9270328	10702	# Author Frequency # Times Figure 2: The Data Model of an LP rental store WHEN ADD Lp: x DO ADD Lp: x has Lending-frequency of Frequency: 0 This action specification is in the style of Lisa-D (). Note that the keyword ‘has’ connects object types to relation types, and the keyword ‘of’ just the other way around. After the introduction of the compact disc, and its conquest of a sizeable
9270328	10702	is: Mt. 4.1.3. Domains The separation between concrete and abstract world is provided by the distinction between the information structure I, and the set of underlying (concrete) domains in D (). Therefore, label types in an information structure version will have to be related to domains. An application model version contains a mapping Domt providing the relation between label types and
9270328	11287	system, and its evolving counterpart, will become clear. This is followed by a discussion on how the evolution of an information system is modelled. 2.1. A hierarchy of models According to , a conceptual (i.e. complete and minimal) specification of a universe of discourse consists of the following components: 1. an information structure, a set of constraints and a population
9270328	10705	such as Task Structures (), DFD () and ExSpect (). Version modelling in engineering databases can be seen as a restricted form of evolving information systems (, ). An important requirement for evolving information systems, not covered by version modelling systems, is that changes to the structure can be made on-line. In version modelling, a structural
9270328	10707	can be made on-line. In version modelling, a structural change requires the replacement of the old system by a new system. Other research regarding evolving information systems can be found in , in which an algebra is presented that allows relational tables to evolve by changing their arity. The structure of the paper is as follows. In section 2 we describe the approach that has been
9270328	10708	rules about, and query the evolution of distinct objects. This alternative also allows for the formulation of rules concerning the well formedness of the evolution of application model elements (). The first alternative clearly does not offer this oppertunity, as it does not provide relations between successive versions of the application model. Furthermore, the snapshot view from the first
9270328	10708	relatedness of roots implies type relatedness of object types: Theorem 3.1 (type relatedness propagation) ?z1,z2 ?? x ? y These properties have been proven in . 4. Secondary elements of Application Models The hierarchy of models (see figure 1) describes how an application model is constructed from other (sub)models. However, this hierarchy disregards
9270328	10708	model version must adhere to some rules of wellformedness. Some of these rules are modelling technique dependent. Nonetheless, some general rules about application model versions can be stated. In  a formalisation of these rules is provided. An object type x is called alive at a certain point of time t, if it is part of the application model version at that point of time (x ? Ot).
9270328	10708	related. Finally, due to inheritance, if a constraint is defined for a parent object type, it must be defined for its children as well. For populations some interesting properties have been proven . An first example states: every instance of an object type is also an instance of one of its roots. Lemma 4.1 Pop t(x) ? ? y RootOf x Pop t(y) From the nature of type relatedness it follows that
9270328	10708	not type related are mutually exclusive. This rule can be generalised to all object types, leading to: Lemma 4.2 x ?? y ? ?t,u  For the proofs of these properties, refer to . 5. Evolution of Application Models The basis of the theory for evolving application models is formed by the concept of evolution continuum, capturing both dimensions of evolution of application
9270328	5638	be taken to the modelling of this evolution. A first approach is to model the history of application model elements by adding birth-death relations to all object types in the information structure (). This approach, however, is very limited, as it only enables the modelling of evolution of the population of an information system. For example, thesMedium # Title # Artist ? ??? ? ?? ? ? Medium?
10713	10721	similar problems. Our research in the collaborative research center on “Media and cultural communication” aims at analyzing and evaluating the impact of difficulties in new media applications  (http://www.uni-koeln.de/inter-fak/fk-427/). In this paper we present a cooperative web-learning environment called SOCRATES (Simulation of Oral Communication Research Analysis and Transcription
10713	10722	similar problems. Our research in the collaborative research center on “Media and cultural communication” aims at analyzing and evaluating the impact of difficulties in new media applications  (http://www.uni-koeln.de/inter-fak/fk-427/). In this paper we present a cooperative web-learning environment called SOCRATES (Simulation of Oral Communication Research Analysis and Transcription
10713	10725	since input is not supported. That is what predictive and abbreviation systems try to cover, but as well as auditory user interfaces, those systems now lack up detection reliability as well . Nevertheless, this work focuses on individuals only. It neglects the observation that many communication barriers are best overcome in cooperative learning communities with people facing similar
553044	10727	because 50sof the number of word-pairs that the application will need to compare. The word-similarity function that was used here uses a dictionary-based approach developed by Banerjee & Pedersen . WordNet, in addition to representing the interconnections between words, provides a gloss (definition) for each of them. In this algorithm, similarity between a pair of words is represented by the
553044	10728	Torch , with some modifications detailed below. 2.3.3 From Maximum Entropy to Multi-Layer Perceptrons In this section we note some similarities between the popular Maximum Entropy technique , the Multi-Layer Perceptron and a particular case of MLP, the Single-Layer Perceptron (SLP), when used for probability estimation. 23sAll these models involve a log-linear combination of feature
553044	10730	for developing a CE infrastructure common to more than one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although
553044	1818	stochastic gradient descent seems an attractive learning strategy. To implement MLP training, we used the neural network on-line algorithm provided by the free machine learning library Torch , with some modifications detailed below. 2.3.3 From Maximum Entropy to Multi-Layer Perceptrons In this section we note some similarities between the popular Maximum Entropy technique , the
553044	7696	? S and therefore better characterise the distribution of ( ? S?S). However, we usually can’t do that, for the same reason that prevents us from calculating S directly. 36sThe bootstrap principle  consists in replacing the unknown ( ? S ? S) by the empirically obtained (S ? ? ? S). Having a possibly large collection of estimates S ? from many bootstrap samples, we can derive statistics on (S
553044	7696	bar for ? S. Our final estimate of the performance ?S, with error bars, is therefore ? S ± ?. More general details about the bootstrap may be found in several textbooks about the bootstrap, eg . For all experiments reported in subsequent chapters for which confidence intervals are given, the boostrap technique was used with 20 resampling runs and a confidence interval of 95%. 37sChapter 3
553044	10734	one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of work on CE concerns speech recognition, there are a
553044	10736	pair can be determined dynamically from the probability of correctness using standard decision theory. Note that it is straightforward to convert a weak CE score into a probability estimate (see  for an example of this). About half of the CE work reported in the speech recognition literature is devoted to each of these approaches. Both are typically evaluated according to how well they
553044	10737	and malapropism detection, approaches based on information content using corpus statistics  and approaches involving some kind of weighted path length in a lexical taxonomy such as WordNet . Vector-space techniques are also often employed in information retrieval . Using these metrics in a machine translation context places certain restrictions on the type of metric that is
553044	10740	one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of work on CE concerns speech recognition, there are a
553044	10741	same scores used as search criteria to establish the output word sequence in the first place, but rather posterior probabilities p(y|x) calculated from an nbest list or from a word lattice as in . 1 A major advan1 Note that p(y|x) is not necessarily the same as p(correct|x, y). For problems like speech recognition, in which there is only one right answer, the two expressions are equivalent.
553044	10742	have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of work on CE concerns speech recognition, there are a few recent efforts in other fields, such as
553044	10743	CE is additionally evaluated according to the accuracy of its probability estimates. Section 2.5 gives details about the standard evaluation techniques, and further reading can be found in  and . Another axis along which CE techniques differ is whether or not there is a separate “CE layer” distinct from the base NLP system. Many speech recognition approaches, eg , simply derive
553044	10744	, and decision trees . Although by far the bulk of work on CE concerns speech recognition, there are a few recent efforts in other fields, such as information retrieval . For MT, the only previous work is by members of our workshop. Ueffing et al  describe several methods, including posterior probabilities, for estimating the correctness of individual words in
553044	10745	mean of hypothesis n-gram precisions, weighted by n-gram frequencies in a fixed corpus (effectively, less common n-grams receive greater emphasis). Also uses a length penalty. • F-Measure (): The harmonic mean of precision and recall, where the size of the match between hypothesis and reference is the maximum 100sFigure 5.5: The F-measure error metric. Matching runs are grayed. When k
553044	10746	common to more than one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of work on CE concerns
553044	10748	systems to be statistical themselves. CE can also be used for active learning approaches, in which human annotation effort is targeted to those examples from which the model stands to benefit most . Finally, confidence estimates can be used to calibrate the scores of partial hypotheses during search, leading to more accurate algorithms . preferable formulation for CE, because it allows
553044	10750	. . , fm is the source sentence, and f0 is the empty word. 4.2.4 SMT Model Based Features Two features that are based directly on the bast Statistical MT model, namely the Alignment Template model (), were applied. • Alignment Template containing this word: The Statistical Machine Translation system segments source and target sentence into bilingual phrases, the so-called Alignment Templates.
553044	1823	Position-independent error rate; treats both hypotheses and references as unordered bags of words and counts the necessary operations to make them equal. Normalized by reference length. • BLEU (): The geometric mean of hypothesis n-gram precision for 1 ? n ? 4, multiplied by an exponentially decaying length penalty, to compensate for short, high-precision translations (“the”). – Smoothed
553044	10751	Torch , with some modifications detailed below. 2.3.3 From Maximum Entropy to Multi-Layer Perceptrons In this section we note some similarities between the popular Maximum Entropy technique , the Multi-Layer Perceptron and a particular case of MLP, the Single-Layer Perceptron (SLP), when used for probability estimation. 23sAll these models involve a log-linear combination of feature
553044	10754	a CE infrastructure common to more than one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of
553044	10756	on CE concerns speech recognition, there are a few recent efforts in other fields, such as information retrieval . For MT, the only previous work is by members of our workshop. Ueffing et al  describe several methods, including posterior probabilities, for estimating the correctness of individual words in MT output. Gandrabur and Foster  describe the use of a neural-net CE layer to
553044	10756	is widely and successfully used in speech recognition—note that the components of x can themselves be sophisticated quantities such as posterior probabilities extracted from the base system, as in . The second and third methods are standard supervised machine learning problems, requiring a labelled data set for training as described in the next paragraph. The first two methods are applicable
553044	10756	sentence. Variant 2 results in a probability distribution over pairs of target words e and their aligned set of source positions B. For a detailed description of these confidence measures, see . 4.2.3 IBM Model 1 As described in section 3.1.11, IBM1 translation models were trained separately from the base translation model, and their probabilities were used as features at the sentence
553044	10757	one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of work on CE concerns speech recognition, there are a
553044	10758	be found in  and . Another axis along which CE techniques differ is whether or not there is a separate “CE layer” distinct from the base NLP system. Many speech recognition approaches, eg , simply derive confidence scores directly from quantities in the base system. When the latter is probabilistic, these scores can be the probability estimates required for strong CE. Typically these
553044	10758	same scores used as search criteria to establish the output word sequence in the first place, but rather posterior probabilities p(y|x) calculated from an nbest list or from a word lattice as in . 1 A major advan1 Note that p(y|x) is not necessarily the same as p(correct|x, y). For problems like speech recognition, in which there is only one right answer, the two expressions are equivalent.
553044	10759	on the resulting translations. These combination schemes are evidently sub-optimal. A lot of work has been done in statistical model combination and a more sophisticated approach, such as stacking  could yield much better results than simple maximum score voting. We expected maximum base-model score voting scheme to degrade the translation accuracy compared to the output of the single best
553044	10760	one base system or application. Systems using this approach have employed a wide range of machine learning algorithms, including Adaboost , naive Bayes , Bayesian nets , neural networks , boosting , support vector machines , linear models , and decision trees . Although by far the bulk of work on CE concerns speech recognition, there are a
3532452	5620	in an application model. The one part is the conceptual schema, determining the data base structure. Such a conceptual schema is a schema conform a modelling technique like NIAM () or ER (), or for more complex applications a schema conform IFO (), or PSM (, ). The other part is the information base, or population, conform the conceptual schema. Besides the above
3532452	5974	as well. Furthermore, the notion of evolution dependency is discussed, taking the dependencies of changes within the universe of discourse into consideration. 1 Introduction As has been argued in ,  and , there is a substantial demand for information systems which are able to evolve to the same extent and at the same pace at which the supported organisation system evolves due
3532452	5974	Organization for Scientific Research (NWO). 1sfor these evolving information systems is that no update of any sort should result in the interruption of activities of the evolving organisation (, ), i.e. changes should be performed on line. Directly related research regarding evolving information systems can be found in  and . Indirectly related research can be found
3532452	5974	at a certain point of time, but lack the ability to preserve the history of information. An evolving information system, on the contrary, must be conservative or temporal (, , , , ) in the sense that it should not forget anything ever fed to the system, unless explicitly asked for, thus allowing for the formulation of queries about the history of the information
3532452	5974	systems can thus be regarded as ‘degenerations’ of these evolving information systems. For a more elaborate discussion on the difference between traditional and evolving information systems, see  and . An important concept in evolving information systems are updates. In traditional information systems, updates are a non-trivial aspect. In the field of evolving information systems,
3532452	5974	of updates in evolving information systems is based on the possible causes for update requests. On this basis, three kinds of update are distinguished: recording, correction and forgetting (, ). Since the application model should be a reflection of the universe of discourse, every event, being a change of state at a point of time, in the universe of discourse, (for instance the
3532452	5975	transitions are determined: a birth-transition creating an application model element, a death-transition terminating an element, and a change-transition replacing one element with another (). The necessity of birth- and death-transitions should be clear. Change-transitions are needed in order to maintain the evolution of an application model element. Such an application model element
3532452	5975	sequence of consecutive change-transitions, and can alternatively be seen as a partial function assigning application model elements to points of time (see also ). As has been argued in e.g.  and , the time of recording of the events is different from its occurence time. The time at which an event occurs in a universe of discourse is called the event time, and the time at which
3532452	5975	Model for Update Requests 3 A Conceptual Framework for Update Processing ? ? ? ? Based on the notion of update as discussed in the previous section, a conceptual framework for update is presented (). This framework distinguishes and relates different types of state transitions. Each type of state transition corresponds to a different level of abstraction in the context of update in evolving
3532452	5975	and a population conforming to these requirements. The complete specification of a universe of discourse containing all these components is referred to as the application model (, ). These definitions, result in the hierarchy of models, as denoted in figure 9. There the distinct models, their interrelationships, and their respective components are depicted. Most updates of
3532452	5978	structure. Such a conceptual schema is a schema conform a modelling technique like NIAM () or ER (), or for more complex applications a schema conform IFO (), or PSM (, ). The other part is the information base, or population, conform the conceptual schema. Besides the above two traditional parts, an application model may also contain descriptions of activities and
3532452	5978	defined concepts is provided in the conceptual schema (meta schema) of the framework of the processing of the user update requests, in figure 8. The schema depicted there is in the style of PSM () which is an extension of the modelling technique from NIAM (). The extension of NIAM used in figure 8 deals with schema objectifications. In a schema objectification, a population of the
3532452	10702	’Information Systems’ e2: DELETE Person works-for Department: ’Information Systems’ The event specifications in the above example are denoted in the semi-natural language LISA-D as defined in . The first event denotation (e1) represents the adding of person ‘Erik’ as a coworker of the department of ‘Information Systems’, whereas the second event denotation (e2) represents the deletion of
3532452	10702	working for departments at a university, the set of elementary transitions implied by the statement DELETE Person works-for Department: ’Information Systems’ depends on the semantics of LISA=D (). The signature of the Concr function, nonetheless, can indeed be given: Definition 3.3 Concr : EO × (INI ? ?(T R)) ? ?(T R) Concr(e, T ) must be interpreted as the set of elementary transitions
3532452	11350	systems which are able to evolve to the same extent and at the same pace at which the supported organisation system evolves due to changes in the universe of discourse. A universe of discourse () is that part of the organisation which is reflected in the information system. Whenever the term information system is used in this paper, we refer to information systems in a narrower sense, i.e.
3532452	11350	evolution dependencies. Before doing this, we define the contents of an application model in more detail. An application model, being a complete specification of a universe of discourse typically (, ) contains the following components: 1. An intentional description of the set of populations of the universe of discourse. This is referred to as the underlying information structure. 2. A
3532452	10705	regarding evolving information systems can be found in  and . Indirectly related research can be found in the area of version modelling in engineering databases: , , . In  a relational algebra is presented in which relational tables are allowed to evolve, e.g. a change of their arity. In this paper, we take a more conceptual approach to evolution of
3532452	10705	old system by a new system. In these systems, the support of evolution is restricted to version managament. This latter notion of evolution is the approach to evolving information as taken in e.g. . The main difference between a traditional information system and an evolving information system, is best explained in more detail by means of the general architecture for an evolving information
3532452	10707	of activities of the evolving organisation (, ), i.e. changes should be performed on line. Directly related research regarding evolving information systems can be found in  and . Indirectly related research can be found in the area of version modelling in engineering databases: , , . In  a relational algebra is presented in which
3532452	10767	intention of an evolving information system is, to be able to change all parts of the application model, and not just the information base as is the case in most traditional information systems. In  the evolvability of the composing parts of an information system is taken even further. There the evolution of meta models is considered. Another limitation of many traditional systems is that they
3532452	10768	and is the most primitive form of correction. Read User Update Requests ? Determine Primitive Update Requests ? Process Primitive Update Requests Figure 2: Processing of User Update Requests In  a formal definition is given of how the above discussed three kinds of corrections can be mapped onto roll-backs and (re)-recordings. It is therefore sufficient to consider roll-backs as the only
3532452	10768	section. A recorded event may have to be replaced by another one, a recording may have to be inserted in the sequence of already recorded events, and a recording may have to be removed. As shown in , these three kinds of recordings can be implemented by means of a roll-back and a series of re-recordings of already recorded (correct) events. In all cases which need a correction, a roll-back
3532452	10708	element evolution consists of a sequence of consecutive change-transitions, and can alternatively be seen as a partial function assigning application model elements to points of time (see also ). As has been argued in e.g.  and , the time of recording of the events is different from its occurence time. The time at which an event occurs in a universe of discourse is called
3532452	5638	reflect information valid at a certain point of time, but lack the ability to preserve the history of information. An evolving information system, on the contrary, must be conservative or temporal (, , , , ) in the sense that it should not forget anything ever fed to the system, unless explicitly asked for, thus allowing for the formulation of queries about the
3532452	5638	at which an event occurs in the universe of discourse - and recording time - the time at which the event is recorded in the information systemand recording time - is of great importance (, , , ). Traditional systems can thus be regarded as ‘degenerations’ of these evolving information systems. For a more elaborate discussion on the difference between traditional and
3532452	5638	consecutive change-transitions, and can alternatively be seen as a partial function assigning application model elements to points of time (see also ). As has been argued in e.g.  and , the time of recording of the events is different from its occurence time. The time at which an event occurs in a universe of discourse is called the event time, and the time at which the event is
3532452	1755	information valid at a certain point of time, but lack the ability to preserve the history of information. An evolving information system, on the contrary, must be conservative or temporal (, , , , ) in the sense that it should not forget anything ever fed to the system, unless explicitly asked for, thus allowing for the formulation of queries about the history of
3532452	1755	an event occurs in the universe of discourse - and recording time - the time at which the event is recorded in the information systemand recording time - is of great importance (, , , ). Traditional systems can thus be regarded as ‘degenerations’ of these evolving information systems. For a more elaborate discussion on the difference between traditional and evolving
3532452	1755	state in 3sthe information system due to an update (addition, deletion or modification), the former state cannot be ‘remembered’ by the information system. Such systems are called snapshot systems (), due to the fact that these information systems can only model and remember (single) ‘snapshots’ of an organisation’s evolution. For evolving information systems, as well as for temporal or
3532452	10769	occurs in the universe of discourse - and recording time - the time at which the event is recorded in the information systemand recording time - is of great importance (, , , ). Traditional systems can thus be regarded as ‘degenerations’ of these evolving information systems. For a more elaborate discussion on the difference between traditional and evolving information
6634765	10776	genes or samples, according to the liability. Second, we analyze the data statistically. Generally many researchers use hierarchical clustering analysis  and principal component analysis (PCA)  as statistical method. They check the visualized result and grasp patterns of the gene expression data. A result of clustering analysis is visualized as a dendrogram and it enables us to grasp
10777	10778	leads to power dissipation equal or greater than? . Thus the first of the two requirements for NP-completeness is met. To prove that it is NP-hard, we will transform a known NP-complete problem , SAT, to PEAKPOWER. Consider an instance of SAT, and build the equivalent circuit, where an OR gate corresponds to each clause in the SAT instance, and an AND gate computes the conjunction of all
10777	10779	assume simplified models of temporal and spatial correlations. One common simplification is the assumption that the primary inputs are uncorrelated in time and space. Both exact  and approximate  algorithms, for this problem have been proposed. Methods that use a more accurate modeling of spatial and temporalscorrelations have also been presented. One such method  models the pairwise
10777	10780	primary inputs and propagates them through the circuit. A different approach that is able to take into account the full set of temporal and spatial correlations at the inputs has also been proposed . Experimentally, it has been observed that all statistical power estimation methods become computationally very expensive as the size of the circuit grows. In sections 4 and 5 we show that the
10777	10780	long description if input traces were to be used. 2sMany other exponentially long input sequences can be specified using appropriate descriptions languages. One such language was proposed in , but, in general, we will not be concerned with the particular description language used to specify the input sequence. All the results in the following sections are independent of the particular
10777	7133	methods usually assume simplified models of temporal and spatial correlations. One common simplification is the assumption that the primary inputs are uncorrelated in time and space. Both exact  and approximate  algorithms, for this problem have been proposed. Methods that use a more accurate modeling of spatial and temporalscorrelations have also been presented. One such method
10777	10782	on the size of the circuit. To avoid this exponential blowup without incurring in a significant loss of precision, approximate methods like sequence compaction  and sequence synthesis  have been proposed. The alternative to this approach are probabilistic power estimation methods. They do not explicitly require the existence of a trace and, in principle, can be used to compute
10777	7129	these methods effectively exponential on the size of the circuit. To avoid this exponential blowup without incurring in a significant loss of precision, approximate methods like sequence compaction  and sequence synthesis  have been proposed. The alternative to this approach are probabilistic power estimation methods. They do not explicitly require the existence of a trace and, in
10777	7134	and approximate  algorithms, for this problem have been proposed. Methods that use a more accurate modeling of spatial and temporalscorrelations have also been presented. One such method  models the pairwise spatial correlations of the primary inputs and propagates them through the circuit. A different approach that is able to take into account the full set of temporal and spatial
10777	10783	since proving that a particular input transition causes a transition in node? can be verified in polynomial time. To prove that #BIN-TRANSITIONS is #P-hard, we will transform #SAT, a known #P-hard  problem to #BIN-TRANSITIONS. For that, we build the circuit of figure 2. In this circuit, we added an extra input? , that forces the output of the AND gate to become 0. i 1 i 2 i n-1 i n z ...
36968	902	flow and interaction among multiple agents in a group plays an important role in understanding the coordinated movements of these agents. Research efforts toward this direction are reported in , , , , , to name a few. Some applications of coordinated control require information to be shared among multiple agents in a group (c.f. , , , , , ), which in turn
36968	902	Our work relies on two infrastructures including graph theory and nonnegative matrices. Graph theory has been used to effectively model the interaction between agents (c.f. , , , , , , ). In , using graph theory, a team of nonholonomic mobile robots is controlled to navigate in a terrain with obstacles while maintaining a desired formation and changing
36968	10786	and interaction among multiple agents in a group plays an important role in understanding the coordinated movements of these agents. Research efforts toward this direction are reported in , , , , , to name a few. Some applications of coordinated control require information to be shared among multiple agents in a group (c.f. , , , , , ), which in turn requires
36968	10786	Our work relies on two infrastructures including graph theory and nonnegative matrices. Graph theory has been used to effectively model the interaction between agents (c.f. , , , , , , ). In , using graph theory, a team of nonholonomic mobile robots is controlled to navigate in a terrain with obstacles while maintaining a desired formation and changing formations
36968	904	among multiple agents in a group plays an important role in understanding the coordinated movements of these agents. Research efforts toward this direction are reported in , , , , , to name a few. Some applications of coordinated control require information to be shared among multiple agents in a group (c.f. , , , , , ), which in turn requires
36968	904	work relies on two infrastructures including graph theory and nonnegative matrices. Graph theory has been used to effectively model the interaction between agents (c.f. , , , , , , ). In , using graph theory, a team of nonholonomic mobile robots is controlled to navigate in a terrain with obstacles while maintaining a desired formation and changing formations when
36968	904	and Computer Engineering Brigham Young University Provo, Utah 84602 {weiren,beard}@ee.byu.edu of nonnegative matrices, which is proved to be useful in studying certain switched linear systems (c.f. ). In this paper, directed graphs will be used to represent the interaction (information exchange) topology among multiple agents, where information can be exchanged between agents via communication
36968	904	subject to communication range limit. In the case of interaction via sensing, the agents that can be sensed by a certain agent may change over time. In the ground breaking work by Jadbabaie et al. , a theoretical explanation is provided for the observed behavior of Vicsek model . Ref.  explicitly takes into account possible changes of each agent’s nearest neighbors over time, which can
36968	904	of each agent in their context) can reach consensus provided that the union of a collection of graphs for all agents is connected frequently enough as the system evolves. However, the approaches in  are based on undirected graphs, which assume bidirectional information exchanges. It might be the case that information exchange is unidirectional, that is, consensus may need to be achieved in the
36968	914	main purpose of this paper is to extend the work of Jadbabaie et al.  to the case of directed graphs and explore the minimum requirements to reach global consensus. As a comparison, Ref.  solves the average-consensus problems with directed graphs, which requires the graph to be strongly connected and balanced. We show that under certain assumptions consensus 1 can be achieved
36968	911	direction are reported in , , , , , to name a few. Some applications of coordinated control require information to be shared among multiple agents in a group (c.f. , , , , , ), which in turn requires information consensus. In this paper, we extend some of the results of  to the case of directed graphs and show a necessary and sufficient condition for
36968	910	are reported in , , , , , to name a few. Some applications of coordinated control require information to be shared among multiple agents in a group (c.f. , , , , , ), which in turn requires information consensus. In this paper, we extend some of the results of  to the case of directed graphs and show a necessary and sufficient condition for consensus
36968	10789	are reported in , , , , , to name a few. Some applications of coordinated control require information to be shared among multiple agents in a group (c.f. , , , , , ), which in turn requires information consensus. In this paper, we extend some of the results of  to the case of directed graphs and show a necessary and sufficient condition for consensus of
36968	10789	(information exchange) topology among multiple agents, where information can be exchanged between agents via communication or sensing. A preliminary result for information consensus is addressed in , where a linear update scheme is proposed but no complete answers are given for the issue of whether the linear update scheme achieves global consensus asymptotically when accounting for all known
36968	10789	information variable associated with the ith agent. The set of agents A is said to achieve global consensus asymptotically if for any ?i(0), i ? I, ??i(t) ? ?j(t)? ? 0 as t ? ? for each (i, j) ? I . Given T as the sampling period, a discrete time consensus scheme is given by 1 ?i(k + 1) = ?n j=1 ?ij(k)Gij(k) n? ?ij(k)Gij(k)?j(k), j=1 (1) where k ? {0, 1, 2, · · · } is the discrete time index,
36968	10789	(1), we obtain the Vicsek model. Also the simplified Vicsek model can be acquired if we let ?ij(k) ? = 1 g , ?j ?= i, and ?ii(k) ? = 1 ? ? j?=i 1 g Gij(k), where g > n is a constant. Compared to , where the interaction graph is assumed to be time-invariant and weighting factors ?ij are specified a priori to be constant and equal to each other, we study continuous time consensus scheme with
36968	10794	studied extensively in the mathematics community. The well-known Perron-Frobenius theory for nonnegative matrices provides a useful tool in analyzing the properties of the graph Laplacian (c.f. ). The classical result in  demonstrates the property of the infinite products of certain categories *Corresponding author. Wei Ren Randal W. Beard* Department of Electrical and Computer
36968	10794	is a directed graph, where every node, except the root, has exactly one parent. A spanning tree of a directed graph is a tree formed by graph arcs that connect all the vertices of the graph (c.f. ). Let Mn(IR) represent the set of all n × n real matrices. Given a matrix A =  ? Mn(IR), the directed graph of A, denoted by ?(A), is the directed graph on n vertices Vi, i ? I, such that
36968	10796	matrix product is also stochastic with positive diagonal entries. Combining Corollary 3.2 and Lemma 3.4, we know that the above matrix product is SIA. In this paper, we also apply dwell time (c.f. , ) to the continuous time update scheme (4), which implies that the interaction graph and weighting factors are constrained to change only at discrete times, that is, matrix C(t) is piecewise
10879	6648	French, German, Czech and Estonian, have been developed. The Interlingual Index (ILI), Top Ontology, set of Base Concepts and set of Internal Language Relations have been introduced as well . These changes also led to the design and development of the new database engine for EuroWordNet and it resulted in the editing and browsing tool called Polaris . In 2001 the EU project Balkanet
9079920	10901	will result from incorporating a wider set of execution phenomenon as join points. There are similar concerns for reasoning about implicit invocation and there has been some work in this direction , , , and . We aim to exploit the mapping from implicit invocation space to aspect-oriented space and existing body of knowledge on reasoning about implicit invocation to enable reasoning
9079920	10902	compilation, link, test, use, etc.. Integration concern is scattered and tangled across the components resulting in code complexity and non-modularity in design. Implicit invocation techniques , e.g. subjectobserver pattern, allow better management of names relationship. In this design technique, observers register with subjects that in turn implicitly invoke them without naming them.
9079920	10903	result from incorporating a wider set of execution phenomenon as join points. There are similar concerns for reasoning about implicit invocation and there has been some work in this direction , , , and . We aim to exploit the mapping from implicit invocation space to aspect-oriented space and existing body of knowledge on reasoning about implicit invocation to enable reasoning about
9079920	10904	from incorporating a wider set of execution phenomenon as join points. There are similar concerns for reasoning about implicit invocation and there has been some work in this direction , , , and . We aim to exploit the mapping from implicit invocation space to aspect-oriented space and existing body of knowledge on reasoning about implicit invocation to enable reasoning about
9079920	10905	a wider set of execution phenomenon as join points. There are similar concerns for reasoning about implicit invocation and there has been some work in this direction , , , and . We aim to exploit the mapping from implicit invocation space to aspect-oriented space and existing body of knowledge on reasoning about implicit invocation to enable reasoning about aspectoriented
9079920	3362	concern, is still scattered across the component. Further, mediator requirements dictate the need for events declared and announced by a component. 4. Research hypothesis Recent aspect-oriented  techniques seek modular representation of requirements that otherwise map to tangled and scattered code, and so to poorly modularized and unnecessarily costly designs. An aspect in such techniques
9079920	10910	the subject. In addition, the integration concern is still scattered and tangled across the components. Hridesh Rajan University of Virginia hr2j@cs.virginia.edu The mediator-based design approach  was developed to enable the modular representation of behavioral relationships to ease component integration. The Behavioral relationship is defined as a protocol for coordinating the control,
9079920	10912	automatically. 5. Solution approach To ease the design and evolution of integrated systems, mapping of the mediator approach into the design space of AspectJ  was attempted. The results  were encouraging but mixed and revealed some shortcomings of the AspectJ design with respect to its usability in this context. The language does not provide first-class aspect instances or
9079920	10913	For example, a mediator might have to respond if one branch of an if statement is taken but not the other (e.g., representing successful insertion of an element into a collection). In Prism , an integrated environment for radiation treatment planning—itself a major test of the mediator approach, such events were routine. AspectJlike languages do not expose such events as join points.
8701336	10915	XML data and metadata, cf. the Resource Description Framework (RDF) . This paper presents a query language that combines XML data and metadata. There are many proposed query languages for XML . None of these proposals provide support for combining data with metadata. A data model and query language for metadata must address several concerns. First, metadata and data reside in different
8701336	10918	to restrict access to data. The authors have previously described and implemented an SQL-like query language called AUCQL for a semistructured database that addresses all four issues raised above . In this paper we extend XPath  with concepts borrowed from AUCQL. XPath is a specification language for locations in an XML document. It serves as the basis for XML query languages like XSLT
8701336	10918	is presented. Finally, the paper concludes with plans for future research. The presentation throughout the paper is informal. A reader interested in formal details should refer to the AUCQL paper . 2 Motivating Example To exemplify our data model, consider the XML document for a person given below. <?xml version=&quot;1.0&quot;> <person ssn=&quot;234&quot;> <name>Ichiro</name> </person> Figure 1 shows the
8701336	1028	and semantics, for instance the metadata might describe the type and schema of data to be exchanged. Proposals exist for relating XML data and metadata, cf. the Resource Description Framework (RDF) . This paper presents a query language that combines XML data and metadata. There are many proposed query languages for XML . None of these proposals provide support for
491473	11512	and ?one and ?all the respective response threshold values. The probabilities for picking up one item and picking up all the items are given by sone Ppickup one = · Tm1 sone + sall (sone; ?one) (2) sall Ppickup all = · Tm2 sone + sall (sall; ?all) (3) where m1 and m2 are positive integers. The values of the stimuli are calculated by evaluating fuzzy if-then rules as explained below. First we
491473	11512	of more than two elements, the stimuli for picking up a single element and for picking up all elements are inferred using the fuzzy rule bases and the corresponding probabilities are given by Eqn. (2)-(3). If the ant is loaded with a heap L, a new heap containing the load L is added to the list of heaps with a fixed probability. Else, if H consists of a single element a, and L consists of a
491473	11512	the stimulus for dropping the load is calculated and the probability that H and L are merged is given by Eq. (1). The most important parameters of the algorithm are n1, n2, m1, m2 in Eqn. (1),(2) and (3). Good results were found within a wide range of values, satisfying m1 = m2 < n1 < n2. Moreover, the values of the parameters seem to be independent of the dataset, but are dependent on the
11048	11077	computing in the form of small, integrated computational appliances supporting multiple collocated users throughout the home, is a more appropriate domestic technology than the monolithic PC&quot;, (Mateas et al 1996, p284). Similar sentiments are echoed by O'Brien and colleagues from a series of home visits to 10 PC-owning families in the North West of England. They observed an 'overloading' of the space
11089	7216	Agent. Initial work on Intentional Agents led to the design of a number of agent architectures that define the data structures and algorithms that are required to implement such an agent  . For a review of some of the more prominent Intentional Agent architectures see . Many of these architecture were based upon earlier theoretical work on Intentional Agents that employed three
11089	11093	Agent. Initial work on Intentional Agents led to the design of a number of agent architectures that define the data structures and algorithms that are required to implement such an agent  . For a review of some of the more prominent Intentional Agent architectures see . Many of these architecture were based upon earlier theoretical work on Intentional Agents that employed
11089	11095	not without fault, and hybrid architectures to date suffer from the deficiencies of static and monolithic design. Often the top layers of these systems are built around one all powerful agent . This rigid methodology provides not only problems in initial integration, but also leeds to a lack of system robustness, since the failure of any one component can lead to a cascading failure of
11089	7231	upon the computationally intractable Possible Worlds semantics, and the syntax and semantics of the associated language. Some of the more prominent AOP languages include Agent-0 , AgentSpeak(L) , 3APL , and AF-APL . At a high-level, these languages provide constructs for representing mental attitudes such as beliefs, goals, and commitments, together with sophisticated reasoning
11089	7232	notions, or variant of them, have become known as Belief-Desire-Intention (BDI) architectures. Similarly, theories based upon beliefs, desires and intentions have become known as BDI theories   . A chief concern underlying the area of Intentional Agents is the identification of a clear link between the various BDI theories and the associated BDI architectures. This issuesFigure 1. Two
11089	7232	Probably the best known variants on commitment management are concerned with the maintenence condition for a commitment i.e. under what conditions a commitment is adopted, maintained and dropped . Although these commitment management approaches and their successors, recognise external agents as those who make requests for commitments, the external agents are ignored in the process of
8920078	11108	concerns models of computation, e.g. process models, SR, FSMs, event-driven models, etc. For MEMS design, academic research has created specialized tools for MEM modeling and simulation , , and has bridged the gap between CAD and foundry facilities. Chatoyant  is also a CAD tool for optical MEMS. 3. Conceptual framework This section presents the conceptual framework of our
8920079	11110	and accuracy are usually higher then lexicon-driven word recognizers. 1. Introduction Most of the conventional handwritten word recognition methods found in the literature are lexicon-driven methods in which one is given a handwritten word image with a list of possible target words - the lexicon. The recognition of the word image is basically a matching process. Each algorithm gives a way of
11117	11130	code and a WSDL description. Extensive documentation on the use of the gSOAP toolkit is available, making it relatively easy to use. Independent sources report that the performance of gSOAP is good . The gSOAP toolkit supports compression of the messages by using either the ZLIB  or deflate  algorithm. Compression is not supported by the other toolkits. Because we would like to
11117	9306	available, making it relatively easy to use. Independent sources report that the performance of gSOAP is good . The gSOAP toolkit supports compression of the messages by using either the ZLIB  or deflate  algorithm. Compression is not supported by the other toolkits. Because we would like to investigate the influence of data compression on our Web services, this is an advantage of
11117	9306	it relatively easy to use. Independent sources report that the performance of gSOAP is good . The gSOAP toolkit supports compression of the messages by using either the ZLIB  or deflate  algorithm. Compression is not supported by the other toolkits. Because we would like to investigate the influence of data compression on our Web services, this is an advantage of gSOAP over the
8920082	11135	agricultural trade, with South Africa ranking at the first with about 34 percent. With geographical constraints and other economic reasons, intra-SSA trade is mainly intra sub-region’s trade (Diao, et al., 2003). For this reason, we further look at which countries are major traders in intra sub-region’s agricultural trade. We assume that a country whose agricultural exports to its region account for more
8920082	11140	region. These results contradict the findings of the many other studies that utilize quite aggregate data and look at total trade instead of agricultural trade only. Most of these studies (e.g., Yeats, 1998; and Foroutan and Pritchett, 1993) conclude that African exports are highly concentrated in a very few products and hence significant increase in intraregional trade will not occur in the
11141	11143	crop rotation may reduce short term productivity but enhance long term productivity). Uninsured production and price risks (limited insurance markets) may also be another important explanation (Barrett 1996). Conservation technologies have been introduced in the area through projects. In addition, there exist several traditional conservation techniques. On any plot one coulds12 find a mix of
11141	11146	and provides consistent parameter estimates. It is considered a desirable alternative due to its robustness to conditional heteroskedasticity and distributional misspecification of the error term (Chen and Khan 2000). We may write the model as follows: (15) ? = 1( x '? + ?) = max( 0, x'? + ?) and take the median conditional on x to get:s(16) q ( ? | x) = max( 0, x'?) 50 18 where q ( ? | x) denotes the median
11141	11159	(Davidson and MacKinnon 1993, p.554). We also tried a third method for robust estimation of the censored model. This method is called Powell’s censored least absolute deviations (CLAD) estimator (Powell 1984) and provides consistent parameter estimates. It is considered a desirable alternative due to its robustness to conditional heteroskedasticity and distributional misspecification of the error term
11141	11165	The resource distribution is likely to be important for the existence of and participation in rural factor markets. There have been few studies of the efficiency of factor markets in Africa (Udry 1996). Barrett (1996), Gavian and Fafchamps (1996) and Collier (1983) found an inverse relationship between farm size and efficiency in Madagascar, Niger and Kenya. Gavian 1 Senior Author. Department of
11141	11165	reject the perfect markets hypothesis when it is correct. If this problem is worse on large plots (poorer quality) than on small plots, plot quality will be negatively correlated with plot size (Udry 1996). We have included plot size to attempt to control for this possible bias. Following Udry (1996) we have also subtracted the plot size from the farm size and included the area of other household
11141	11770	equalized across households. Udry (1996) found evidence of imperfections in 2 labor and land markets in Kenya and of imperfections in capital and insurance markets in Burkina Faso. Gavian and Ehui (1999) found in a study in Ethiopia that total factor productivity was lower on rented land but that input intensity was not different on rented land. Other studies of efficiency in agriculture in
11166	11167	properties that characterize temporal locality in memory access patterns. The presence of temporal locality has been documented for many other types of data streams, including web reference streams . 2.1 Sources and Characteristics There are two significantly different sources of temporal locality in data streams in general: (1) popularity over long time scales, often captured by Zipf’s law,
11166	11167	the performance of memory management schemes. Definition 1 For a given data stream, stack distance refers to the nubmer of unique references separating consecutive requests to the same object. In , Almeida et al used the marginal distribution of stack distance strings to characterize temporal locality. While the stack distance model provides means for characterizing the degree of temporal
11166	11168	are applied over these streams. However, since the data streams are potentially unbounded, the storage that is required to evaluate complex relational operations, such as joins, is also unbounded . There are two approaches to address the above issue. One is to allow for approximations that can guarantee high-quality results. The other, is to define new versions of the relational operations
11166	11170	online data management on very fast changing data. Example applications that motivate the need for stream database systems include network monitoring, sensor networks, financial applications, etc. . The main difference between a traditional data base management system and a system that manages data streams (DSMS) is the assumption of the former that each relation is stored on disk and that
11166	11170	sliding windows on data streams is a natural method to get good quality approximate results and in many applications, (e.g., monitoring), it makes more sense to consider only the most recent data . 1sOne very important operation in a DSMS is the sliding window join, which can be defined as follows: given two streams R and S and a sliding window W , a new tuple r in R that arrives at time t,
11166	11170	and comparable to the other methods. 6 Other Related Work Stream Databases have received a lot of interest recently. General issues and architectures for stream processing systems are discussed in  (Stanford’s STREAM),  (TelegraphCQ), and  (AURORA). Continuous queries are studied in detail in , where the authors proposed methods to provide the best possible query performance in
11166	5680	queries are studied in detail in , where the authors proposed methods to provide the best possible query performance in continuously changing environments. Load shedding is used in . In  a utility based load shedding scheme is introduced for a general data stream manager that executes a query plan. The utility is defined for processing each tuple from a data stream that is
11166	11171	and comparable to the other methods. 6 Other Related Work Stream Databases have received a lot of interest recently. General issues and architectures for stream processing systems are discussed in  (Stanford’s STREAM),  (TelegraphCQ), and  (AURORA). Continuous queries are studied in detail in , where the authors proposed methods to provide the best possible query performance in
11166	1930	properties that characterize temporal locality in memory access patterns. The presence of temporal locality has been documented for many other types of data streams, including web reference streams . 2.1 Sources and Characteristics There are two significantly different sources of temporal locality in data streams in general: (1) popularity over long time scales, often captured by Zipf’s law,
11166	1930	of interrequest times. In , Jin and Bestavros showed that this distribution is predominantly determined by the large skew (e.g., power law) governing the long-term popularity of objects . This inherent relationship tends to disguise the existence of short-term temporal correlations (i.e., popularity over shorter time scales). One way of quantifying the causes of reference locality
11166	1930	otherwise. The initial value and the increase/decrease rules we use are detailed later. GDJ is in fact an extension of the GreedyDual (GD) algorithm  in the same way that GreedyDual-Size (GDS)  and GD*  are extensions, in the sense that it allows the value attached to a cache entry to depend on its potential for producing join results (as opposed to its “size” as with GDS, for
11166	11174	manner. In either of these cases, it is desirable that the use of the available buffer space be optimized so as to produce the best possible approximate result of the sliding window join operation . Clearly, what constitutes a “good approximation” depends on the application at hand. Examples may include (1) producing the largest subset of the exact answer, or (2) producing an unbiased sample
11166	11174	our disposal is M, then M< ?n i=1 Wi. Thus, it is impossible to guarantee exact results, and we must rely on approximate solutions instead. To do so, we adopt the MAX-subset criterion proposed in , which aims to maximize the number of tuples produced by the system. Furthermore, we discuss the problem of producing a representative (unbiased) random sample, and we show how our technique can be
11166	11174	of temporal locality (such as GDJ, the technique we propose later in this paper) to one which only leverages long-term popularity (such as the frequency-based technique considered in prior work ). As explained earlier, the marginal distribution of the stack distance for a request stream captures the temporal locality present in that stream. That is, if D is a random variable corre1 IRM
11166	11174	proportional to the rate of the stream over the size of the buffer . However, this approach does not optimize the Max-subset criterion and has been shown to perform poorly in almost all cases . A better approach is to assume a model for the stream and drop tuples selectively using that model. This approach is called semantic or informed load shedding. Next we discuss two model-based
11166	11174	the arrival time of s and is always f(v). This is equivalent to the IRM model  mentioned in the previous section. Based on this model, an online algorithm called PROB Heuristic was proposed in . This algorithm could be viewed as an adaptation of the LFU for sliding window 6sstream join. PROB Heuristic assumes a single join between two streams S and R, and assumes that each stream has a
11166	11177	smallest credit Assign initial credit to s Insert s into Bs Decrease credits in Bs and Br Return join results When a new tuple s comes in stream S, we use one of the existing join algorithms (e.g., ) to perform the join of this tuple with the buffer of the other stream R. For every tuple in BR that produces a join with s, we update its credit. Next, we remove the tuples from BS that have
11166	11177	stream join queries are studied in . The problem addressed there is mainly the disk-memory coordination for evaluating these joins. Join over multiple streams also has been considered in , where they analyze the cost of various join algorithms. However, in both of these works, they did not consider approximate results and load shedding issues. The most related work to ours except
11166	11178	Furthermore, it assumes that the probability of value f(v), ?v ?Dis known for both streams. A simple approach to approximate that is to build a histogram of observed (past) tuple values  and use that as a model for future arrivals. When the buffer for S is full and a new tuple arrives, the tuple that has the value with the smallest probability to appear in the other stream, e.g. R,
11166	5686	over the data. Also, the main goal is to estimate the selectivity of the joins and not maximizing the output. Also, a number of stream join processing methods have been presented recently. In  scheduling issues for shared window joins are discussed. By carefully scheduling the join sequence for a set of joins with different window sizes over the same stream data, the response time of the
11166	11179	The initial value and the increase/decrease rules we use are detailed later. GDJ is in fact an extension of the GreedyDual (GD) algorithm  in the same way that GreedyDual-Size (GDS)  and GD*  are extensions, in the sense that it allows the value attached to a cache entry to depend on its potential for producing join results (as opposed to its “size” as with GDS, for example). Classic
11166	11180	locality—namely whether it is due to popularity over long or short time scales . The second method in characterizing temporal locality is the use of the distribution of interrequest times. In , Jin and Bestavros showed that this distribution is predominantly determined by the large skew (e.g., power law) governing the long-term popularity of objects . This inherent relationship
11166	11180	but will not affect the popularity profile of the constituent references in the trace: A significant change in either distributions would indicate the prevelance of short-term correlations. In , Jin and Bestavros used this methodology to quantify (using newly proposed metrics) the relative strengths of both sources of temporal locality in a variety of web traces. Their results revealed
11166	11180	it is evicted (unless its value is replenished as a result of being accessed). In , Jin and Bestavros proposed a generalization of GD called GreedyDual* (GD*). GD* uses the metrics proposed in  to tune GD’s aging algorithm (possibly in an on-line fashion) to capitalize optimally to the relative strengths of the two sources of temporal locality discussed earlier. 3 Model-Based Approaches
11166	11181	expensive. 5.1.1 Synthetic Data Streams To generate the synthetic data sets, we created a data stream generator 6 with adjustable parameters based on the temporal locality model developed in GISMO  for Internet streaming media traffic. The distribution of the join attribute values is skewed and follows a Zipf distribution so that the frequency of a value is inversely proportional to its
11166	11179	original value. Effectively, GDimplements an aging technique whereby the value of an object slowly decreases until it is evicted (unless its value is replenished as a result of being accessed). In , Jin and Bestavros proposed a generalization of GD called GreedyDual* (GD*). GD* uses the metrics proposed in  to tune GD’s aging algorithm (possibly in an on-line fashion) to capitalize
11166	6000	Randomly Permuted Stock Trading Trace. Another example of popularity over long versus short time scales concerns Origin-Destination pairs (a.k.a., ODflows) observed in network packet streams. In , large traces of ODflows in two major networks (US Abilene and Sprint-Europe) revealed that traffic intensity at any router is merely the superposition of ODflows with three different
11166	11184	indirectly captures frequency as we will see below) and was shown to be superior to other policies, e.g. FIFO and Random. Several studies have considered both recency and frequency information . For example, the LRU-K  algorithm maintains the last K reference times to each object to compute the average reference rate. In the remainder of this section, we contrast the benefits of using
11166	11185	General issues and architectures for stream processing systems are discussed in  (Stanford’s STREAM),  (TelegraphCQ), and  (AURORA). Continuous queries are studied in detail in , where the authors proposed methods to provide the best possible query performance in continuously changing environments. Load shedding is used in . In  a utility based load shedding
11166	11187	indirectly captures frequency as we will see below) and was shown to be superior to other policies, e.g. FIFO and Random. Several studies have considered both recency and frequency information . For example, the LRU-K  algorithm maintains the last K reference times to each object to compute the average reference rate. In the remainder of this section, we contrast the benefits of using
11166	7060	with periodically changing intensities, where the period is over a fairly long (diurnal) time scale, those with bursting intensities over a fairly short time scale (e.g., subseconds as shown in ), and those with random intensities. Yet another example of data stream systems with data exhibiting locality over multiple time scales include sensor networks. Here, one would expect that data
11166	11189	join sequence for a set of joins with different window sizes over the same stream data, the response time of the system can be significantly improved. Multi-way stream join queries are studied in . The problem addressed there is mainly the disk-memory coordination for evaluating these joins. Join over multiple streams also has been considered in , where they analyze the cost of various
11166	11190	at hand. Examples may include (1) producing the largest subset of the exact answer, or (2) producing an unbiased sample of the join results. Existing work focused mainly on the first problem , with the exception of the work in  which addressed both problems. In this paper, while we concern ourselves mostly with the first of these approximation goals, we also discuss and evaluate the
11166	11190	join algorithms. However, in both of these works, they did not consider approximate results and load shedding issues. The most related work to ours except the ones discussed in section 3, are . In , a technique based on a unit-time cost model is proposed. The scheme selects the join implementation and memory allocation for the two input streams according to their arrival rates. Load
11166	11191	on) such convergence; we present an alternative buffer management technique that does. Our approach—which could be thought of as a generalization of the well-known Greedy-Dual (GD) algorithm ???enables the reference stream of one cache to affect the cache-ability of entries in other caches, thus engendering said convergent behavior. Among its many features, our approach capitalizes
11166	11191	hybrid techniques mentioned earlier do not consider variable-cost objects. For instance, in LRU the most recently accessed object has (by definition) the highest value. The GreedyDual(GD) algorithm  can be viewed as a generalization of LRU which takes into consideration both the “cost” (or value) of an object as well as recency information. GDwas shown to be online optimal in terms of its
11166	11191	in producing a join result, and is decremented otherwise. The initial value and the increase/decrease rules we use are detailed later. GDJ is in fact an extension of the GreedyDual (GD) algorithm  in the same way that GreedyDual-Size (GDS)  and GD*  are extensions, in the sense that it allows the value attached to a cache entry to depend on its potential for producing join results (as
11166	11191	approximate join processing as a generalized cache replacement problem. Our GreedyDual-Join (GDJ) approach—which could be thought of as a generalization of the well-known Greedy-Dual (GD) algorithm ???enables the reference stream of one cache to affect the cache-ability of entries in other caches. Among its many features, our approach capitalizes effectively on temporal locality properties in
11166	5688	mostly on web request streams, it should be evident that the same phenomena apply to many other data stream systems. For instance, consider streams of stock market data used to answer queries . Temporal locality in such a stream could be a reflection of the market capitalization of various companies, or a reflection of breaking news that impact stock prices of specific companies (which
8920085	11194	correlation to any prior request made by the same user. Cookies are a mechanism to preserve state between HTTP requests and responses. Typically, cookies are small files approximately 1KB in size . A web server generates a cookie containing client state information and delivers it to a browser. If the browser is configured to accept cookies, the browser will store the cookie on the client’s
8920085	11198	are commonly contained in a digital credential certified by an authority (a trusted third party). Roles typically represent various functions in an organization and are granted certain permissions . Permissions are the ability to access a resource or a service. In trust negotiation, attributes are mapped to roles, and each role is associated with a set of permissions. The design decision of
8920085	11199	negotiation, a new authentication paradigm, enables strangers on the Internet to establish trust or proof of qualification through the gradual disclosure of credentials and access control policies . In open systems like the Internet, the majority of transactions are between strangers—entities that are not in the same security domain and do not have a pre-existing relationship. The identity of
11201	11205	fRC. We take the same rules and add to A ? w as forbidden context all left hand sides of productions greater than A ? w. By definition, fRC ? ?fRC ? ?RC and fRC ? RC. The strictnesses are shown in  and . ? Now we consider a type of grammars where with any nonterminal in a sentential form we associate (partially) its derivation. Definition 12 i) An indexed grammar is a quintuple G = (N, T,
11201	11206	take the same rules and add to A ? w as forbidden context all left hand sides of productions greater than A ? w. By definition, fRC ? ?fRC ? ?RC and fRC ? RC. The strictnesses are shown in  and . ? Now we consider a type of grammars where with any nonterminal in a sentential form we associate (partially) its derivation. Definition 12 i) An indexed grammar is a quintuple G = (N, T, S, I, P
11201	11207	saying that any recursively enumerable language has a succint description by matrix and programmed grammars whereas this does not hold for random context grammars. For a proof we refer to ,  and . Theorem 16 i) For any recursively enumerable language L, we have V ar?Mac(L) ? 3 and V ar?P rac(L) ? 3. ii) V ar?Mac({a n b n c m d m e p f p | n, m, p ? 1}) = 3 iii) There is a sequence
11201	11208	valence grammar where each production is associated with 0. This implies the first inclusion, and its strictness follows by Example 9. We omit the proofs of the other relations and refer to . ? 12s2. Control by Context Conditions In this section we consider some grammars where the applicability of a rule depends on the current sentential form. With any rule we associate some
11201	11211	of ?rC over a unary alphabet are regular. ii) CF ? rC ? rCac ? CS iii) CF ? rC ? ?rC ? ?rCac = RE Proof. Since the known proofs for statement i) use deep results from the theory of Petri nets (see ) we omit a proof. CF ? rC. Obviously, the context-free grammar G = (N, T, S, P ) (which can be assumed to have no erasing rules) and the regularly controlled grammar G ? = (N, T, S, P, P ? )
11201	11214	on syntactic complexity. 0. Introduction The regular and context-free grammars/languages are the most investigated types of formal languages which, in addition, have a lot of nice properties (see  and the corresponding chapters of this volume). However, these types of grammars/languages are not able to cover all aspects which occur in modelling of phenomena by means of formal languages. Here
11215	11216	find a feature that is shared by words such as “must”, “can” and “may”. In earlier studies, independent component analysis has been used for document level analysis of texts (see, e.g., , , ). A. Data collection II. DATA AND METHODS The data used in the experiments consists of collection of e-mails sent to the connectionists mailing list 1 . The texts were concatenated into one file.
11215	11217	based on independent component analysis. A. Analysis of Words in Contexts Contextual information has widely been used in statistical analysis of natural language corpora (consider, e.g., , , , ). Handling computerized form of written language rests on processing of discrete symbols. How can a symbolic input such as a word be given to a numeric algorithm? Similarity in the
11215	11219	methods. A classic method for reducing the dimension in a vector space model is latent semantic analysis that will be described next. B. Latent Semantic Analysis In latent semantic analysis , a technique known as singular value decomposition (SVD) is used to create a latent semantic space. First, a term-by-document matrix A is generated. Every term is represented by a row in matrix A,
11215	11222	claim is that the LSA acquired knowledge about the full vocabulary of English at a comparable rate to school-children. The development of the LSA has also been motivated by practical applications . One problem with the LSA is that the concept space is difficult to understand by humans. The self-organizing map, that will be introduced in the next section, creates a visual display of the
11215	11223	words, or a word category map. Earlier, the name self-organizing semantic map has also been used. Similar results have also been presented by Miikkulainen , , . Consider ,  and  for more thorough analysis and explanation of the methodology. Areas or local regions on a word category map can be considered as implicit categories or classes that have emerged during the
11215	11225	up to a rotation, which is quite insufficient for our purposes. For our ICA analyses we applied FastICA 2 software package for Matlab. We fed the word-context matrix C to the FastICA algorithm  so that each column was considered one data point, and each row one random variable. We used the standard maximum-likelihood estimation by setting the nonlinearity g to the tanh function, and using
11215	11226	these features are given by hand, and the membership is crisp. D. Data Collection and Analysis by ICA In the following, we propose the use of independent component analysis (ICA) , ,  for the extraction of linguistic features from text corpora and present a detailed methodological description. ICA learns features in an unsupervised manner. Several such features can be present in
11215	11226	in order a reduce the effect of the very most common words in the analysis. B. Independent component analysis We will give a brief outline of the basic theory of independent component analysis . The classic version of the ICA model can be expressed as x = As (1) where x = (x1, x2, . . ., xn) T is the vector of observed random variables, the vector of the independent latent variables is
11215	11226	the order of the terms in Eq. (2) and call any of the components the first one. The third important property of ICA is that the independent components must be nongaussian for ICA to be possible . Then, the mixing matrix can be estimated up to the indeterminacies of order and sign discussed above. This is in stark contrast to such techniques as principal component analysis and factor
11215	11226	one data point, and each row one random variable. We used the standard maximum-likelihood estimation by setting the nonlinearity g to the tanh function, and using symmetric orthogonalization  (p. 212). The dimension of the data was reduced to 10 by principal component analysis (this is implemented as part of the software) 3 . Reduction of the dimension is often used to reduce noise and
11215	11227	be able to find a feature that is shared by words such as “must”, “can” and “may”. In earlier studies, independent component analysis has been used for document level analysis of texts (see, e.g., , , ). A. Data collection II. DATA AND METHODS The data used in the experiments consists of collection of e-mails sent to the connectionists mailing list 1 . The texts were concatenated into
11215	11228	to find a feature that is shared by words such as “must”, “can” and “may”. In earlier studies, independent component analysis has been used for document level analysis of texts (see, e.g., , , ). A. Data collection II. DATA AND METHODS The data used in the experiments consists of collection of e-mails sent to the connectionists mailing list 1 . The texts were concatenated into one
11215	11229	by concept matrix C. The third matrix is a concept by document matrix D. This is a special case of the coding of contexts explained in above: the context is one whole document in the LSA. In  the LSA is described in terms of learning and cognitive science. The claim is that the LSA acquired knowledge about the full vocabulary of English at a comparable rate to school-children. The
11215	11233	based on independent component analysis. A. Analysis of Words in Contexts Contextual information has widely been used in statistical analysis of natural language corpora (consider, e.g., , , , ). Handling computerized form of written language rests on processing of discrete symbols. How can a symbolic input such as a word be given to a numeric algorithm? Similarity in the appearance
11277	11279	R2,s with image . The algorithm is an improved version of the RecBFN algorithm of Huber and Berthold  which in turn is based on radial basis functions  with dynamic decay adjustment . During the learning phase the input data is passed unmodified to layer 1. Then all neurons are adapted, i.e. the sides of the smaller rectangles (= core rules) and the sides of the larger
11277	11280	R2,s with image . The algorithm is an improved version of the RecBFN algorithm of Huber and Berthold  which in turn is based on radial basis functions  with dynamic decay adjustment . During the learning phase the input data is passed unmodified to layer 1. Then all neurons are adapted, i.e. the sides of the smaller rectangles (= core rules) and the sides of the larger
11277	11283	classification rules which can be inspected by the physician. Actual approaches to rule generation consider supervised learning neuro-fuzzy-methods , especially for medical applications . Usually, medical data contain both metric and categorical variables. Here, our data is substantially based on metric variables, so in the following we consider the process of rule generation
11277	11287	cases where there is no probability distribution information available as in our case this is very hard to do, see . There are some attempts to introduce confidence intervals in neural networks , but with moderate success. Therefore, we decided to vary the context of testing as much as possible and give as result the deviation, maximum and minimum values additionally to the mean
11277	11288	also detect linear separability if it exists. 3.1 The Network The neural network chosen for our classification task is a modified version of the supervised growing neural gas (abbr. SGNG, see ) 1 . Compared to the classical multilayer perceptron trained with backpropagation (see ) which has reached a wide public, this network achieved similar results on classification tasks,
11277	11289	also detect linear separability if it exists. 3.1 The Network The neural network chosen for our classification task is a modified version of the supervised growing neural gas (abbr. SGNG, see ) 1 . Compared to the classical multilayer perceptron trained with backpropagation (see ) which has reached a wide public, this network achieved similar results on classification tasks, see
11277	11290	becomes also arbitrary, depending on the particularities of the training set composition. Here, special strategies are necessary. One of the most used methods is the p-fold cross validation  . Here, the whole data set is divided into p parts of equal size. The training is done in cycles or epochs where in each epoch one part (subset) of the data set is chosen as test set and the
11277	11290	we consider a classification by learning classification rules which can be inspected by the physician. Actual approaches to rule generation consider supervised learning neuro-fuzzy-methods , especially for medical applications . Usually, medical data contain both metric and categorical variables. Here, our data is substantially based on metric variables, so in the following we
11277	11298	where there is no probability distribution information available as in our case this is very hard to do, see . There are some attempts to introduce confidence intervals in neural networks , but with moderate success. Therefore, we decided to vary the context of testing as much as possible and give as result the deviation, maximum and minimum values additionally to the mean
11277	11307	there is no probability distribution information available as in our case this is very hard to do, see . There are some attempts to introduce confidence intervals in neural networks , but with moderate success. Therefore, we decided to vary the context of testing as much as possible and give as result the deviation, maximum and minimum values additionally to the mean
11316	11317	restricted to the length ? is considered. The empirical loss of a given training set is the number of misclassified points. To derive a large margin bound, this loss is modified in analogy to  in the following way: we fix a margin parameter ?? . The ‘security’ or margin of the classification of a point Ü ? is given by the quantitys?Ö ?Ö , the distance of the point from the closest
11316	6356	dimensional data and complex underlying regularities. The support vector machine constitutes a prime example of a machine learner which directly aims at structural risk minimization during training . The structural risk of the SVM is given by the classification margin. A large margin ensures good mathematical generalization bounds and, in practice, excellent generalization ability of SVMs. Two
11316	11318	optimum. Input data are implicitly mapped to a high dimensional feature space by a kernel, which offers a natural interface to adapt the model to specific settings and to integrate prior knowledge . However, one drawback of the SVM is the expansion of a solution in terms of the dual Lagrangian variables: given training data Ü ? ?Ý? Ê Ò ¢?s? ? and a kernel ? ? Ê Ò ¢ Ê Ò ? Ê, the final SVM
11316	11319	descent method on a cost function, thus it might converge to a local optimum of the cost function. This fact can be prevented introducing neighborhood cooperation of the prototypes as proposed in . In the article , the cost function of GRLVQ is merged with the cost function of Neural Gas (NG), which constitutes an unsupervised and very reliable clustering algorithm . NG introduces a
11316	11319	cooperation of the prototypes such that initialization of the prototypes has almost no effect on the training result and the prototypes spread faithfully among the data points. As demonstrated in , the combination of NG with GRLVQ allows to apply GRLVQ also to highly multimodal settings, and the modification reliably converges to global optima of the GRLVQ cost function. 2.2 Kernelization
11316	11320	for example, that ? ? is symmetric with ?? ? ? , ands? ? being conditionally positive definite. Examples for alternative kernels especially suited e.g. for time series data have been proposed in .s2.3 Large margin bounds Generalization bounds for standard LVQ have recently been presented in the article . GRLVQ differs from LVQ in the essential property that the metric is also adapted
11316	11321	chosen independent and identically distributed according to È , one can show that the error ? deviates from the empirical error ? ? Ä Ñ by a term of order Ô ? ¡ Ô ¡ ? Ñ Ô ÐÒ ?Æ with confidence Æ . This bound does not depend on the input dimensionality, thus GRLVQ is well suited also for high dimensional input data. Rather, the larger the margin ?, the better the generalization bound.
11316	11322	effort is constant. In addition, we demonstrate that the method is competitive to SVM in one experiment. 2 GRLVQ Generalized relevance learning vector quantization (GRLVQ) has been proposed in  as an extension of simple LVQ. Assume a finite set of training data Ü ? ?Ý? Ê Ò ¢? ??????? is given, ? being the number of different classes. A GRLVQ network represents every class by a finite set
11316	11322	Û ÖsÜ ? , being another factor, and the relevance terms are adapted by ?? Ð ?s? ¡ ¡ Û Ö ÐsÜ ? Ðs? ¡ Û ÖsÐsÜ ? Ð , followed by normalization, and ? being additional factors. It has been discussed in  that this update scheme can be interpreted as a straightforward application of Hebbian learning, as used for basic LVQ. As pointed out in , where also the precise update formulas can be found,
11316	11323	with respect to the training set size, and classification is linear. Alternatives to the SVM which expand solutions in terms of typical vectors have been proposed, such as Bayes-point machines . Here, we focus on another very intuitive learning model which is based on a different training paradigm: prototype based learning vector quantization (LVQ) as proposed by Kohonen . We discuss
11316	11326	This new version with similarity measure ? ? ? instead of ? ? can be interpreted as a kernelized of standard GRLVQ iff a function ¨ exists such that ?? Ü? Ý ? ? ? ¨ Ü ?¨ Ý holds. The article  discusses conditions under which such ¨ can be found. A sufficient condition is, for example, that ? ? is symmetric with ?? ? ? , ands? ? being conditionally positive definite. Examples for
11316	11327	1. Accuracy (in ) of various methods achieved on the IPsplice dataset, the classification accuracy on the test set is reported for the models. The results for alternatives to GRLVQ are taken from . of the similarity measure, we add an experiment where we compare several recent classification results achieved by the SVM with GRLVQ networks. The task is to distinguish pieces of human DNA
11316	11327	the classification accuracy achieved by hidden Markov models (HMM), and the SVM with different kernels, the locality improved kernel, and two kernels derived from a statistical model (TOP and FK) . Note that the accuracy of our method is competitive to the solutions found by SVM, whereby the achieved classifiers are very sparse for our setting (only ? prototypes per class) and the training
11341	3468	one of two extremes: robust, partial parsing with the goal of broad data coverage versus more traditional parsers that aim at complete analysis for a narrowly defined set of data. Chunk parsing  offers a particularly promising and by now widely used example of the former kind. The main insight that underlies the chunk parsing strategy is to isolate the (finite-state) analysis of
11341	11342	one of two extremes: robust, partial parsing with the goal of broad data coverage versus more traditional parsers that aim at complete analysis for a narrowly defined set of data. Chunk parsing  offers a particularly promising and by now widely used example of the former kind. The main insight that underlies the chunk parsing strategy is to isolate the (finite-state) analysis of
11341	11342	help of the bigram tagger LIKELY . 1 The parts of speech serve as pre-terminal elements for the next step, i.e. the chunk analysis. Chunk parsing is carried out by an adapted version of Abney’s  scol parser, which is realized as a cascade of finite-state transducers. The chunks, which extend if possible to the simplex clause level, are then remodeled into complete trees in the tree
11341	11344	The chunks, which extend if possible to the simplex clause level, are then remodeled into complete trees in the tree construction level. The tree construction is similar to the DOP approach  in that it uses complete tree structures instead of rules. Contrary to Bod, we do not make use of probabilities and do not allow tree cuts, instead we only use the complete trees and minimal tree
11341	11345	follow the data format for trees defined by the NEGRA project of the Sonderforschungsbereich 378 at the University of the Saarland, Saarbrücken. They were printed by the NEGRA annotation tool . ¡ Memory-based learning has recently been applied to a variety of NLP classification tasks, including part-of-speech tagging, noun phrase chunking, grapheme-phoneme conversion, word sense
11341	11346	for a specific grammatical function or constituents with several possible functions may be found so that an additional classifier is needed for selecting the most appropriate assignment (cf. ). The second approach, which we have chosen, is to regard the complete parse trees as classes so that the task is defined as the selection of the most similar tree from the instance base. Since in
11341	11347	three PARSEVAL measures: labeled precision, labeled recall and crossing accuracy, with the results shown in Table 1. While these results do not reach the performance reported for other parsers (cf. , ), it is important to note that the task carried out here is more difficult in a number of respects: 1. The set of labels does not only include phrasal categories, but also functional labels
11356	11349	that the patching tree has considerably smaller size and shorter lifetime than the original tree. • Bandwidth overhead: although the MDC codec used in CoopNet introduces some bandwidth overhead , , our solution is more bandwidth intensive. Since we need to reserve same amount of bandwidth for each original stream allocated. But the adoption of the MDC approach brings extra overhead such
11356	7172	at them rejoins the application layer multicast tree independently. IV. PERFORMANCE EVALUATION A. Simulation Environment Setup We use the GT-ITM transit-stub model to generate a network topology  of about 1400 routers. The average degree of the graph is 2.5, and core routers have a much higher degree than edge routers. The media server and the end hosts are connected with the edge routers.
11356	7895	a smaller number of overlay hops from a client to the server, thus a smaller average stretch. Stretch is the ratio of the latency along the overlay to the latency along the direct unicast path ,  . The stretch of the application layer multicast tree is the average stretch value over all the clients in the tree. Furthermore , by 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM
11356	11351	the patching tree has considerably smaller size and shorter lifetime than the original tree. • Bandwidth overhead: although the MDC codec used in CoopNet introduces some bandwidth overhead , , our solution is more bandwidth intensive. Since we need to reserve same amount of bandwidth for each original stream allocated. But the adoption of the MDC approach brings extra overhead such as
11356	11356	?i i=1? d ?×d. If video patching is introduced, the initial delay can be reduced to D ? max(? ?i d ?) × d under non overlapping failures. For detailed analysis, please refer to our technical report . In Figure 3, at time t0, the client is disconnected. Since the client buffer stores video , it can still play from the buffer while reconnecting to the server. If the initial delay
11356	11356	In this section, we consider several aspects of complexity in our system design. We only give qualitative analysis in this paper, for quantitaive analysis, please refer to our technical report . • Message processing overhead: in our system design, the server has to process four kinds of messages: join, leave, rejoin and patching end. We assume the weight for processing these messages is
11356	8991	applications, but it is not widely deployed. One recently proposed approach relies on the cooperation of the video clients in forming an overlay network over which the video is propagated , , . In this approach, a client currently in the overlay network forwards the content it is receiving, and serves other client’s request as a server. By distributing the transmission load evenly
11356	8991	worse, unsatisfied clients leaving the group can become a positive feedback process, causing more clients to leave, which ultimately makes the streaming media service unacceptable. In CoopNet , video continuity is maintained using multiple description coded (MDC) streams multicast over several application layer trees. CoopNet employs a serverbased centralized control protocol, the server
11356	11357	continuity, it can cause video quality fluctuation as clients depart and trees are reconstructed around them. Recent work in CoopNet shows the PSNR variation with different number of descriptions . Our solution tries to provide continuous streaming service without video quality fluctuation. In this paper we develop a scheme using the transmission of a single description coded video over an
11368	11369	then consider the WA problem, in which our goal is to derive a feasible wavelength assignment solution. We note that the WA problem has been studied by several researchers before (e.g., see , , , , , ,  and references therein). However, the objective in all these studies has been to minimize the number of wavelengths required in a network, in some cases by using
11368	11370	in which our goal is to derive a feasible wavelength assignment solution. We note that the WA problem has been studied by several researchers before (e.g., see , , , , , ,  and references therein). However, the objective in all these studies has been to minimize the number of wavelengths required in a network, in some cases by using wavelength converters. In general,
11368	11372	are typically required to carry a large number of lower rate (sub-wavelength) traffic demands. The traffic grooming problem has been considered by several researchers for ring networks (e.g. see, , , , , , , , , , , , ), and is only considered recently in  for mesh networks. The objective considered in  is either to maximize the network
11368	11377	required to carry a large number of lower rate (sub-wavelength) traffic demands. The traffic grooming problem has been considered by several researchers for ring networks (e.g. see, , , , , , , , , , , , ), and is only considered recently in  for mesh networks. The objective considered in  is either to maximize the network throughput or to
11368	11379	have a compelling cost advantage over ring networks. Mesh networks are more resilient to various network failures and also more flexible in accommodating changes in traffic demands (e.g., see , ,  and references therein). In order to capitalize on these advantages, effective design methodologies are required. In the design of an optical mesh network, traffic grooming, routing, and
11368	11382	consider the WA problem, in which our goal is to derive a feasible wavelength assignment solution. We note that the WA problem has been studied by several researchers before (e.g., see , , , , , ,  and references therein). However, the objective in all these studies has been to minimize the number of wavelengths required in a network, in some cases by using wavelength
11368	11389	rate (sub-wavelength) traffic demands. The traffic grooming problem has been considered by several researchers for ring networks (e.g. see, , , , , , , , , , , , ), and is only considered recently in  for mesh networks. The objective considered in  is either to maximize the network throughput or to minimize the connection-blocking probability,
11368	11390	grooming problem has been considered by several researchers for ring networks (e.g. see, , , , , , , , , , , , ), and is only considered recently in  for mesh networks. The objective considered in  is either to maximize the network throughput or to minimize the connection-blocking probability, which are operational network-design problems.
11392	11425	of requirements and behavior related to structuring problems , and analyst and user values . Whereas these studies employ empirical means to document these assumptions, Bostrom and Heinen  have relied on an analysis of the literature to document seven implicit theories and views of designers as causes @1989ACM0001-0782/89/1000-1199 $1.50 The article is organized as follows. We begin
11392	11425	be largely outside the domain of the systems developer. Moreover, a large number of systems have been successfully completed by foll.owing the tenets of this story. However, as Bostrom and Heinen  have pointed out, the systems designer’s assumptions associated with this story can lead to a number of conditions that contribute to system failure. The story, therefore, has a number of potential
11392	11427	(2) Kind of information flow was derived from the language action view of information systems  which focuses on the purposes of information flows. (3) Control of users was derived from Kling  who notes that it “is often assumed that when automated information systems become available, managers and line supervisors exploit them to enhance their own control over different resources,
11471	12093	Related research can be found in Sect. 6. Finally, Sect. 7 offers a general discussion. 2 Structure of Thesauri Many thesauri are historically based on the ISO 2788 and ANSI/NISO Z39.19 standards . The main structuring concepts are terms and three relations between terms: Broader Term (BT), Narrower Term (NT) and Related Term (RT). Preferred terms should be used for indexing, non-preferred
11471	12093	hierarchy in either fields or facets. The former divides terms into areas of interest such as “injuries” and “diseases”, the latter into more abstract categories such as “living” and ???non-living” . The standards advocate a term-based approach, in which terms are related directly to one another. This is contrasted by the concept-based approach . In the latter, concepts are interrelated,
11471	11473	Van Assem, Menken, Schreiber, Wielemaker and Wielinga In the last step, it is identified that users may have the need to automatically define new concept compositions. Goldbeck et al.  have described a conversion for the NCI Thesaurus from its native XML format to OWL. OWL Lite is chosen over the DL and Full versions of OWL, as the needed relationships were only those in Lite.
11471	11483	a preparational step, (1) a syntactic conversion step, (2) a semantic conversion step, and (3) a standardization step. The division of the method into four steps is an extension of previous work . 3.1 Step 0: Preparation An analysis of the thesaurus is made using its documentation and digital form, containing the following elements: – Conceptual model (the model behind the thesaurus is used
11471	11484	will now be able to visualize the Synset tree as a subclass tree. The repercussions of this type of metamodelling for RDF storage and retrieval are discussed in . 6 Related Research Wroe et al.  have described a methodology to “migrate” the Gene Ontology (GO) from XML to DAML+OIL. In the first of five steps, go:term is mapped onto a daml:Class and go:isa onto rdfs:subClassOf. Also, three
563565	11486	lie in a 3D space. The resulting energy can now be minimised by deriving equations of motion from Equation 1, adding damping terms, and running a dynamics solver until a steady state is achieved . Suppose we have a model that is topologically equivalent to a disk. We can add springs to the boundary of this disk with the opposing ends of these springs attached to a surrounding fixed frame.
563565	11495	correctly. The final task of the artist is then to paint the small region of the patch in the immediate vicinity of the cut. Implementing the texture blending is easily achieved with modern shaders  or with multipass texturing. Although we have concentrated on the implementation details for subdivision surfaces a simplified scheme can be implemented for use with polygon models using linear
563565	11497	define the head, torso, arms, and legs makes skeletal binding and weighting a much simpler and more effective process. However, in order to apply surface detail with 2 dimensional texture maps , the surface must be parametrised. Spline patches come with a natural parametrisation, but there is no such natural Dan Piponi and George Borshukov MVFX, a division of Manex Entertainment
563565	1850	of such texture coordinates. This makes the problem of choosing texture coordinates for subdivision surfaces similar to that for implicit surfaces or for surfaces derived from point coulds , ,,. One useful characteristic of any technique for assigning texture coordinates to a surface is to ensure that distances between points on the surface are represented accurately in the
563565	11499	such texture coordinates. This makes the problem of choosing texture coordinates for subdivision surfaces similar to that for implicit surfaces or for surfaces derived from point coulds , ,,. One useful characteristic of any technique for assigning texture coordinates to a surface is to ensure that distances between points on the surface are represented accurately in the distances
563565	11502	along the edges of the model. approximation to a distortion measure on a grid. There are a number of different candidates for a measure of distortion such as the Green-Lagrange deformation tensor , . An elementary approach is to consider the lengths of the edges of the polygon mesh in texture space and make the functional the sum of the squares of the deviations of the lengths of the
563565	11502	to minimise this function. Unfortunately, this is not necessarily adequate. This is a complex search problem with many local minima - especially for complex surfaces. Sometimes ‘buckling’  can occur during minimisation. In addition, minima do not necessarily preserve approximate symmetries in the original model that artists involved in a production desire. Sometimes there are other
563565	11502	a way to deal with all of these issues. There are a number of different types of terms that can be added to Equation 1 in order to eliminate problems, for example, terms to prevent faces ‘flipping’  or angular variation. We use a different approach. Equation 1 is formally identical to the total energy of a collection of springs that obey Hooke’s law except that the dynamics is described only
563565	11502	we can find texture ? ? coordinates ? ? and on ? ? each so that any ? ? function ? on can be written as ? ? ??? ? ¦?? ? ? (Each? ? is called a chart and the whole collection is ? called an atlas , ? .) Suppose we have a set of ? ??? ¡???? continuous functions such ? ? that each is zero outside ? of and ? ? ???§? ? 2 An open set is one where for every point there is a real ? such that it
563565	11506	clarity of exposition we consider mainly Catmull-Clark surfaces derived from quadrilateral meshes. After the first Catmull-Clark refinement any polygonal mesh becomes a quadrilateral mesh anyway .) Unfortunately, assigning texture coordinates to a subdivision surface can be a difficult problem. Firstly, there is a purely topological problem. There is no way to assign texture coordinates to
563565	11506	assigning values to control vertices on the zeroth refinement of the mesh and a technique for interpolating these values over the surface. Therefore, we need to consider interpolation schemes. Stam  describes a set of basis functions that parametrise patches on the subdivision surface corresponding to each quadrilateral in the zeroth refinement. This parametrisation defines a pair of
8917370	6653	probabilistic models and statistical processing. In particular, it is well known that in a Bayesian context the Hidden Markov Field model based segmentation methods may be of exceptional efficiency . Otherwise, fusing information supplied by different sensors can be performed by exploiting the theory of evidence . A piece of information is attached to each sensor via a
8917370	6653	is to estimate different parameters from the observations alone. In the classical hidden Markov field case, different solutions to the difficult parameter estimation problem have been proposed  and, although it is very difficult to advance any theoretical results, the methods generally perform well. Here we propose an original method, inspired by the &quot;generalized&quot; mixture estimation
8917370	11512	it is very difficult to advance any theoretical results, the methods generally perform well. Here we propose an original method, inspired by the &quot;generalized&quot; mixture estimation methods proposed in , to solve this problem. The organization of the paper is as follows. In the next section we briefly recall the classical multisensor hidden Markov field model, and section 3 is devoted to a brief
8917370	11512	gives D(yi ) ?{ fi , fi , fi }. (d) Update ga ,gb ,gc by putting q+1 q+1 q+1 q q q (ga ,gb ,gc ) = (D(ya ), D(yb ), D(yc )). The novelty of our method is situated at the decision rule D level. In  the rule D is based on the use of the Pearson system in which one calculates the skewness and the kurtosis, and in  the rule D is the minimization of the Kolmogorov distance. The decision rule
8917370	6657	it is very difficult to advance any theoretical results, the methods generally perform well. Here we propose an original method, inspired by the &quot;generalized&quot; mixture estimation methods proposed in , to solve this problem. The organization of the paper is as follows. In the next section we briefly recall the classical multisensor hidden Markov field model, and section 3 is devoted to a brief
8917370	6657	as a generalised mixture, although in simulations below, we consider that it is a Gaussian mixture and we estimate it with the classical ICE . The general mixture estimation method proposed in , called the ICE-GEMI algorithm, is an iterative method: at step q, let ? q q q q and ga ,gb ,gc be current prior parameters and current densities ga , gb , and gc . The updating is as follows: (a)
8917370	6657	)). The novelty of our method is situated at the decision rule D level. In  the rule D is based on the use of the Pearson system in which one calculates the skewness and the kurtosis, and in  the rule D is the minimization of the Kolmogorov distance. The decision rule we propose is based on kernel estimation; the step (c) becomes: (i) For i = a,b,c, calculate ˆf i (y) = 1 K nhn y ? y n
8917370	6657	parameter estimation problem as a classical mixture estimation problem. We then proposed an original generalized mixture estimation method, which belongs to the wide family of methods proposed in , and we have applied it to solve the parameter estimation problem in the context of the multisensor evidential Markovian field model considered. First simulations show favourable behavior of the
8917370	11517	segmentation methods may be of exceptional efficiency . Otherwise, fusing information supplied by different sensors can be performed by exploiting the theory of evidence . A piece of information is attached to each sensor via a &quot;fuzzy&quot; measure, which gives, in a particular case, a classical probability measure. Then the fusion is performed by the so-called
11529	11530	generates a pitched sound with a time evolving spectrum by scanning some slow moving dynamic Ireland. Brian.Sheehan@ul.ie system . In recent implementations of scanned synthesis instruments , it is a virtual model of a physical system that is scanned. In this paper it is contended that by using some suitable sensor as the dynamic system to be scanned, the mapping problem can be
11529	11531	generates a pitched sound with a time evolving spectrum by scanning some slow moving dynamic Ireland. Brian.Sheehan@ul.ie system . In recent implementations of scanned synthesis instruments , it is a virtual model of a physical system that is scanned. In this paper it is contended that by using some suitable sensor as the dynamic system to be scanned, the mapping problem can be
11529	8951	sound synthesis. And now that real time control is possible, the problems of creating an expressive and engaging instrument are only just coming to light, in explorations of the mapping problem . Work on the mapping problem has provided many useful guidelines that help to construct an effective mapping between the output of a control interface and the input of a synthesis algorithm, but
8920144	12161	obtained by TDI. MODEL We assume a linear response of the mass spectrometer and model the mass signal vector d j of measurement j as d j ? Cx j ? j? (1) x j is the vector of species concentrations and ? j the error vector associated with d j. C is the cracking matrix which results from the fragmentation of species in the ionization source. The cracking column vectors and the data vector are
400354	11543	is outlined. After discussing briefly current solutions for mobility support in Section 5, our lightweight approach is presented in Section 6. 2 Private Host Addresses Private IP addresses  have been introduced in order to have a clear separation between public and private hosts, i. e. those that are not supposed to connect to external hosts on the public Internet. However today, even
400354	943	above is caused by a number of P2P protocols and applications, like JXTA  offering a P2P framework, Gnutella 0.6  using dynamic hierarchies or hash table based approaches like Chord , Pastry  or Tapestry . All of them have the same characteristic, that the signaling messages are routed in the overlay network and that the content is transmitted out-band, i. e. via
400354	5902	is caused by a number of P2P protocols and applications, like JXTA  offering a P2P framework, Gnutella 0.6  using dynamic hierarchies or hash table based approaches like Chord , Pastry  or Tapestry . All of them have the same characteristic, that the signaling messages are routed in the overlay network and that the content is transmitted out-band, i. e. via additional TCP
400354	5903	of P2P protocols and applications, like JXTA  offering a P2P framework, Gnutella 0.6  using dynamic hierarchies or hash table based approaches like Chord , Pastry  or Tapestry . All of them have the same characteristic, that the signaling messages are routed in the overlay network and that the content is transmitted out-band, i. e. via additional TCP connections, as
400354	12173	and data transport in a P2P network. 3.1 Signaling In the following, we will explain the basic routing functionalities in P2P overlay networks in more detail based on the Gnutella 0.4 protocol . Depending on the bandwidth of the node’s network connection, each node in the Gnutella network is connected dynamically to an average of seven nodes . These connections are used to route
400354	188	detail based on the Gnutella 0.4 protocol . Depending on the bandwidth of the node’s network connection, each node in the Gnutella network is connected dynamically to an average of seven nodes . These connections are used to route signaling messages in the network. The messages can be divided into two categories, query and respond messages. The query messages are used to explore the
400354	11550	of new network services leading to networks that explicitly support dynamic service creation, deployment and management in the network infrastructure . Programmable network systems like AMnet  support the provisioning of netcentric services to end systems unaware of the additional functionality inside the network. These services may analyze and modify the data passing through e. g. for
8920148	11561	approaches to controlling animal position: a physical agent such as a sheepdog or robot, and a stimulation device worn by the animal. In the first category there is the pioneering work of Vaughan who demonstrated a mobile robot that was able to herd a flock of ducks to a desired location within a circular pen. In the second category there are a number of commercial products used to control
8920148	11561	virtual fence techniques, we developed a Matlab simulator that models the behavior of a herd of cows both with and without the virtual fence stimulus. We were inspired by Vaughan’s duck simulator, but extended the animalsDynamic Virtual Fences for Controlling Cows 7 model to account for the differences between the species as well as their environments. Most importantly, while we also use
985968	11564	from one agent to another then we can serialize the graph in a message and then compute a digest and signature for that message using conventional means. This has been used successfully in the past  for serialized RDF messages and works well for direct communication where it is sufficient to sign a particular message rather than the graph itself. The difficulty with this approach is that the
985968	11565	without doing any intermediate canonical serialization. The key to this approach is recognizing the applicability of incremental cryptographic functions. These were introduced by Bellare et. al.  and are similar to the Set Hash described much earlier by Zobrist . The idea is to compute a digest not by working on an entire message at once, but instead break the message into pieces, pass
985968	11566	without doing any intermediate canonical serialization. The key to this approach is recognizing the applicability of incremental cryptographic functions. These were introduced by Bellare et. al.  and are similar to the Set Hash described much earlier by Zobrist . The idea is to compute a digest not by working on an entire message at once, but instead break the message into pieces, pass
985968	11566	any duplicate statements present and we require it to still maintain all the other required properties (one-way, collision-resistant, incremental) even in that case. 5s3 Incremental Cryptography In  they suggest using a function defined as follows: Given a message X, consisting of N ordered message blocks: x1 . . . xN, they compute a digest of that message: Where: N? D(X) = h(<i> .xi) i=1 • ?
985968	11566	More sophisticated randomizing functions, such as those which require a key or those which generate much longer outputs, may be constructed using the SHA-1 algorithm as a building block (see ). We note that the graph digest algorithm does not depend on any particular randomizing function and as better hash functions are developed they may naturally be incorporated. 10s6.2 The combining
985968	11566	it is not an appropriate choice. Even if we could guarantee there were no duplicate statements, XOR would still be a poor choice since there are known efficient algorithms for finding collisions . 6.2.2 Multiplication If the combining function, ? is multiplication mod some suitably-large prime P then the security is expected to be very good (see ) but the operations are relatively
985968	11566	a very high level of security is not required, then addition modulo a suitably-large number M offers a good compromise between speed and security. This is based on the “AdHash” algorithm of Bellare  and is similar to the “MSET VADD HASH” algorithm recently analyzed by Clarke et. al. . Note that M must be at least as large as 2 n , where n is the number of bits in the randomizing function.
985968	1028	By carefully avoiding the need for an intermediate canonical serialization we are able to efficiently compute a digest for such graphs. 1.1 An RDF Graph The Resource Description Framework (RDF)  provides a means to make statements about resources. An RDF graph is a set of such statements. Figure 1 shows an example graph. http://example.com/title “John Smith Autobiography”
985968	10320	• It is “collision-resistant”. That is, given a digest and a document it is very difficult to find a different document that has the same digest. A common use of digests is for signing documents . Computing a digital signature is usually expensive. So it is common to compute a message digest and then sign that instead of the original message . The message digest has the property that it
985968	11576	interesting uses as well. For example, you can compare digests computed at different times to see if a document has changed and the digest can also serve as a convenient content-based identifier . In addition, if the digest may be computed incrementally then you can add information to a document and generate an updated digest even if you don’t know all the content of the original document.
985968	11578	purposes. 11sHowever, it should be noted that the use of addition and a 160-bit randomizing function is likely insufficient to guarantee security against a determined adversary. Recently Wagner  has shown an efficient algorithm for finding a set of numbers which sum to a particular value. That algorithm is most efficient when the adversary is allowed to choose the number of things to sum.
11580	11583	back. This is because we found evidence that developing standards for annotation may not be utopic after all, and that training across corpora may be highly beneficial for system development (Chen et al. 2002). More details in what follows. Data collection / analysis. The only answer seems to be to develop repositories of tagged corpora. Special interest groups such as ACL-SigDIAL
11580	11583	seem to be the answer. However, we go back to the original problem: how to build the annotated corpora necessary to train them on. Recent work opens some exciting possibility in this regard. (Chen et al. 2002) provides some evidence that a system can be successfully trained on a corpus which was developed and annotated for a different application in a different domain. (Chen et al. 2002) shows that a
11580	11593	student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis 1996; Huang & Fiedler 1996; Shaw 1998; Reape & Mellish 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part
11580	11593	rapidly improve DIAG-orig’s output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis 1996; Huang & Fiedler 1996; Shaw 1998), in which aggregation rules and heuristics are plausible, but are not based on any hard evidence. To understand how a human tutor may verbalize a collection of facts, we collected 23
11580	11596	a third prototype that mirrors the findings from the corpus. The NLG architecture has been completely redesigned, so as to make it more flexible. Moreover, the NLG system is now coupled to RealPro (Lavoie & Rambow 1997) that performs syntactic and lexical realization. RealPro is a text generation ”engine” that performs syntactic realization. RealPro provides a grammar rule engine that can generate text from
11580	11599	describing patented systems. For a reader with little or no knowledge of the domain, TAILOR was designed to plan a description of an object in terms of its function and the function of its parts (Paris 1988). On the one hand, these results lend support to the rule that groups parts according to the system hierarchical structure that we implemented in DIAGNLP1 and DIAG-NLP2. However, the aggregations
11580	11600	Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis 1996; Huang & Fiedler 1996; Shaw 1998; Reape & Mellish 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised
11580	11602	in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis 1996; Huang & Fiedler 1996; Shaw 1998; Reape & Mellish 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2
11580	11602	output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis 1996; Huang & Fiedler 1996; Shaw 1998), in which aggregation rules and heuristics are plausible, but are not based on any hard evidence. To understand how a human tutor may verbalize a collection of facts, we collected 23 tutoring
11580	11604	we return to our discussion of the development cycle, and based on our experiences with this project, we offer some suggestions on howsit might be made more efficient. The DIAG-NLP project DIAG (Towne 1997) is a shell to build ITSs that teach students to troubleshoot complex systems such as home heating and circuitry. Authors build interactive graphical models of systems, and build lessons based on
11580	11606	dimension values. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we use EXEMPLARS (White & Caldwell 1998), an object-oriented, rule based generator. It mixes template-style text planning with a more sophisticated type of text planning based on dynamic dispatch. The rules (called exemplars) are meant
11607	11612	of the human body tracking rather than on the development of a general and comprehensive methodology for tracking and data association problems capable to deal with general form of coordination. In  the coordination of point features placed on a human body is described by a a convenient probability density function of positions and velocities. The authors assume that the trajectories are
11607	4878	data. There is an extensive literature on this problem and standard methods can be found for instance in . These includes the popular JPDAF (joint probabilistic data association filter). In  the MHT (multi hypothesis trackers) have been introduced. In such techniques a target is described through a mixture of gaussian densities. Monte Carlo approaches have been recently adopted ,
11607	11615	The solution to the problem of missing data is also one of the main innovations brought to our previous work . Some analogies with this work can be found in motion capture literature , . The focus is, however, on specific and technical aspects of the human body tracking rather than on the development of a general and comprehensive methodology for tracking and data association
11607	11616	In  the MHT (multi hypothesis trackers) have been introduced. In such techniques a target is described through a mixture of gaussian densities. Monte Carlo approaches have been recently adopted ,  as well. Adaptive algorithms  have been proposed in order to cope with particular uncertainties such as the unknown inputs which typifies maneuvering targets. In multiple models based
11607	11617	the MHT (multi hypothesis trackers) have been introduced. In such techniques a target is described through a mixture of gaussian densities. Monte Carlo approaches have been recently adopted ,  as well. Adaptive algorithms  have been proposed in order to cope with particular uncertainties such as the unknown inputs which typifies maneuvering targets. In multiple models based
8920158	11627	different time scale, of course, is tos272 T. Yamamoto, K. Kaneko / Physica D 181 (2003) 252–273 include the compartmentization of a cell, and that cell division occurs in a much slower time scale . This approach will be relevant to the study of cell differentiation and development, but here we are interested in the origin of a cellular system itself. In our study, we did not impose an
8920158	11629	rate is faster than its host. Then slow reaction cannot be added to the existing network. Also, other models of reaction networks such as the algorithmic chemistry  or tape-machine network  assume a single reaction time scale for reproduction of each molecule. Although the reaction network is topologically complex, there appears no temporally differentiation in reaction pathways. In
11637	11644	this group has described fast and effective model-based algorithms for automatic tracing of linear branched structures, such as neurons, in three-dimensional volumetric images , and vasculature . These methods are based on the modeling assumption that the structures of interest are bounded by nearly parallel edges. The present work was motivated by the failure of these otherwise robust and
11637	11644	they tend to scale poorly with image size. The third approach, exemplified by this paper and others, is referred to variously as vectorization, vectorial tracking, or exploratory tracing , , . These methods work by first locating an initial point, and then exploiting local image properties to trace the structures recursively , . Broadly, three categories of exploratory
11637	11644	having several processes efferent from it, and when neurons are large enough that only partial views are feasible. The third category, including this work, consists of fully automated methods  that overcome the limitations of the first two. The core algorithm presented here builds upon the prior work of Sun , and our prior work , . In particular, we employ an adaptive
11637	11644	of interest. For the results presented here, we used yielding an angular precision of 11.25 and a total of 64 left and right templates. For a detailed description of the tracing algorithm, see , . As illustrated in the next section, the tracing algorithm described above has demonstrated excellent performance when applied to images with good contrast and high SNR. However, it failed to
11637	11644	where the curvature is higher. Note that, other than replacing the average response with the median response, the tracing algorithm introduced in the previous section and discussed in detail in ,  remains the same. The next section illustrates the superiority of the median template response over the average response, especially in images with low SNR. V. MEDIAN VS. AVERAGE; AN
11637	11649	. In earlier work, this group has described fast and effective model-based algorithms for automatic tracing of linear branched structures, such as neurons, in three-dimensional volumetric images , and vasculature . These methods are based on the modeling assumption that the structures of interest are bounded by nearly parallel edges. The present work was motivated by the failure of these
11637	11649	this work, consists of fully automated methods  that overcome the limitations of the first two. The core algorithm presented here builds upon the prior work of Sun , and our prior work , . In particular, we employ an adaptive two-dimensional (2-D) kernel as opposed tosAL-KOFAHI et al.: MEDIAN-BASED ROBUST ALGORITHMS FOR TRACING NEURONS 305 the fixed 2-D kernel used by Can et al.
11637	11649	this section we present those aspects of the tracing algorithms necessary to illustrate the differences between the two kernel types. We do not discuss other parts of the system described elsewhere , such as the soma detection and the seed point selection algorithms, both of which are necessary for the system’s operation. A. Exploratory Tracing The 5 K kernel given in (1) and shown in Fig. 3
11637	11649	of interest. For the results presented here, we used yielding an angular precision of 11.25 and a total of 64 left and right templates. For a detailed description of the tracing algorithm, see , . As illustrated in the next section, the tracing algorithm described above has demonstrated excellent performance when applied to images with good contrast and high SNR. However, it failed to trace
11637	11649	(1) by computing a median rather than mean for the LPD kernels along the edge direction as follows: The length is estimated analogous to the average response based tracing algorithm described in . The robustness of the median template response for different situations is illustrated by an example in Fig. 4. Note that the median response (4) measure holds while up to 50% of the LPD responses
11637	11664	vectorial tracking, or exploratory tracing , , . These methods work by first locating an initial point, and then exploiting local image properties to trace the structures recursively , . Broadly, three categories of exploratory processing techniques are described in the literature. In the first category, the initial and end points of a vessel are entered manually (sometimes a
11637	11674	apparent discontinuities in the dendritic/axonal structures. Further robustness is achieved by simultaneously detecting both edges of the dendrite/axon structures. Canny’s edge detection algorithm , considered to be the standard for edge detection, involves smoothing, directional filtering, gradient computation, nonmaximum suppression, thresholding and edge-linking applied to all the pixels
11637	11674	as well as dynamically linking broken edges. Correlating a template with an image computes all of the above operations at once rather than applying them in a sequential manner such as in Canny . This contributes to the robustness and effectiveness of our tracing algorithms. The algorithms compute the templates on a locally relevant portion of the image, which makes them computationally
11681	11681	agents receives for its actions (and to which it adapts to optimize) in private utility functions so as to optimize the reward received by the collective. As a case study, originally published in , we investigate the performance of COIN extended for sequences of actions for representative token retrieval problems found to be difficult for agents using classical Reinforcement Learning (RL).
11681	11681	actions that add to the global utility and also to avoid actions that deduct from the utility of other agents as otherwise the second term in Equation 1 will increase. In Figure 1(a),as taken from , differently valued tokens are placed on the edge of a 11 × 11 grid and eight agents take five steps in one epoch of learning; they are able to pick up all tokens only if they cooperate perfectly
11681	11682	form of learning. Unless special care is taken as to how this reward is shared, there is a risk that agents in the collective work at cross-purposes. The COllective INtelligence (COIN) framework , as introduced by Wolpert et al., suggests how to engineer (or modify) the rewards an individual agents receives for its actions (and to which it adapts to optimize) in private utility functions so
11683	1046	a practical approach to detect gene regions and to identify alternative splicing on a genomic scale. In previous studies EST-genome alignments were made using 90-93% sequence identity as threshold . However, their low thresholds allow ESTs to incorrectly align with paralogous genes or pseudogenes. Moreover, EST sequences have low-quality sequencing region or contaminated region . We
11683	11687	of gene prediction. Because there is no guarantee that the predicted events occur in vivo. It has become important to discriminate experimentally verified data from computationally predicted data . Aligning expressed sequence tags (ESTs)/mRNAs to the genomic sequences has been a practical approach to detect gene regions and to identify alternative splicing on a genomic scale. In previous
11683	11690	to retain splice variants that have identical sequences but distinct exon-intron structures. We searched ESTs identical to the remained FL-pre-mRNA sequences by a homology search program, megablast  with ???97% identity and ?50 nt alignment length. The detected ESTs were precisely re-aligned to the corresponding FL-pre-mRNAs by a dynamic programming alignment tool, ALN  with ?97% identity,
11691	11694	unless special care is taken, simple pointerbased traversals suffer from a nonlocal pattern of memory references. This is one of the principal motivating factors behind I/O efficient algorithms  and cache sensitive and cache oblivious data structures and algorithms . In contrast with pointer-based implementations, regular spatial subdivisions support pointerless implementations.
11691	11695	geometric modeling. There has been considerable amount of work in simplicial mesh refinement, particularly in 2- and 3-dimensions, and a number of different refinement techniques have been proposed . Because of ? This material is based upon work supported by the National Science Foundation under Grant No. 0098151. 1sthe need to handle data sets with a temporal component, there is growing
11691	11697	rays in a fast data structure and associate a number of continuous geometric attributes with each sample. We can then interpolate among these samples to reconstruct the value at intermediate rays . Because of variations in the field values, it is necessary to sample 2sadaptively, with denser sampling in regions of high variation and sparser sampling in regions of low variation. An adaptively
11691	11697	of interpolation using simplicial complexes, consider the images generated from our ray-tracing application in Fig. 2. Images (a) and (c) show the result of an interpolation based on kd-trees , which is not a cell complex, and images (b) and (d) show the results of using the hierarchical simplicial decomposition described in this paper. (These results will be reported in a separate
11691	11699	that they are much simpler in the sense that the interpolations are performed with a minimal number of samples. It is possible to further subdivide an octree cell to produce a simplicial complex , but this approach does not scale well with dimension due to the exponential increase in the number of vertices and explosion of cases that need to be considered. To illustrate the advantage of
11691	11700	geometric modeling. There has been considerable amount of work in simplicial mesh refinement, particularly in 2- and 3-dimensions, and a number of different refinement techniques have been proposed . Because of ? This material is based upon work supported by the National Science Foundation under Grant No. 0098151. 1sthe need to handle data sets with a temporal component, there is growing
11691	11701	There are often many orders of magnitude of difference between the time needed to access local data (which may be stored in registers or cache) versus global data (which may reside on disk) . Large dynamic pointerbased data structures are particularly problematic from this perspective, because node storage is typically allocated and deallocated dynamically and, unless special care is
11691	11701	nonlocal pattern of memory references. This is one of the principal motivating factors behind I/O efficient algorithms  and cache sensitive and cache oblivious data structures and algorithms . In contrast with pointer-based implementations, regular spatial subdivisions support pointerless implementations. Pointerless versions of quadtree and its variants have been known for many years
11691	11702	nonlocal pattern of memory references. This is one of the principal motivating factors behind I/O efficient algorithms  and cache sensitive and cache oblivious data structures and algorithms . In contrast with pointer-based implementations, regular spatial subdivisions support pointerless implementations. Pointerless versions of quadtree and its variants have been known for many years
11691	11705	simplicial mesh in the plane. In computer graphics, adaptively refined regular meshes in 2- or 3-dimensions have been of more interest for their use in realistic surface and volume rendering . In many such applications, efficiency of various operations such as traversal and neighbor finding on the mesh is most desired. Based on the 3-dimensional version of Maubach’s method, Hebert
11691	11708	to compute neighbors efficiently in 3-space . Lee, et al. developed an alternative location code for this same tetrahedral mesh, and presented algorithms for efficient neighbor computation . Both approaches are quite specific to 3-space, and are not readily generalizable to higher dimensions. We introduce a new location code, which provides unique encoding of the simplices generated
11691	11708	to compute basic properties of the simplex such as neighbors from this code. Nonetheless, Lee, De Floriani, and Samet showed how to compute neighbors from the path code in the 3-dimensional case . Instead we modify an approach presented by Hebert  for the 3-dimensional case, by defining a location code that more directly encodes the geometric relationship between the each simplex and
11691	11708	neighbors within the complex. Two simplices are neighbors if they share a common (d ? 1)-dimensional face. Hebert  presented rules for computing neighbors in the 3-dimensional case. Lee, et al.  presented an efficient neighbor finding algorithm for the 3-dimensional case based on the path codes. Maubach  considered the case of arbitrary dimensions and presented a recursive algorithm
11691	11708	a single arithmetic operation by subtracting (or adding) 1 from the resulting binary number. (This trick has been applied elsewhere in the context of neighbor finding in quadtree-like structures .) Under the assumption that the machine’s word size is ?(D/d), where D is the maximum depth of any simplex, it follows that the orthant list for the neighbor can be computed in O(1) time. 7
11691	11709	than representing the hierarchy explicitly as a tree using child pointers, we use a pointerless representation in which we access nodes through an index called a location code. Location codes  have arisen as a popular alternative to standard pointer-based representations, because they separate the hierarchy from its representation, and so allow the application of very efficient access
11691	11709	regular simplicial meshes has principally been in 2- and 3-dimensions. Lee and Samet presented a pointerless hierarchical triangulation based on a four-way decomposition of equilateral triangles . Hebert presented a location code for longestedge hierarchical tetrahedral meshes and a set of rules to compute neighbors efficiently in 3-space . Lee, et al. developed an alternative location
11691	11710	geometric modeling. There has been considerable amount of work in simplicial mesh refinement, particularly in 2- and 3-dimensions, and a number of different refinement techniques have been proposed . Because of ? This material is based upon work supported by the National Science Foundation under Grant No. 0098151. 1sthe need to handle data sets with a temporal component, there is growing
11691	11710	geometric modeling. There has been considerable amount of work in simplicial mesh refinement, particularly in 2- and 3-dimensions, and a number of different refinement techniques have been proposed . Because of ? This material is based upon work supported by the National Science Foundation under Grant No. 0098151. 1sthe need to handle data sets with a temporal component, there is growing
11691	11712	presented rules for computing neighbors in the 3-dimensional case. Lee, et al.  presented an efficient neighbor finding algorithm for the 3-dimensional case based on the path codes. Maubach  considered the case of arbitrary dimensions and presented a recursive algorithm for computing neighbors, assuming that the neighbors of the parent simplex are known. This leads to a neighbor
11691	11715	simplicial mesh in the plane. In computer graphics, adaptively refined regular meshes in 2- or 3-dimensions have been of more interest for their use in realistic surface and volume rendering . In many such applications, efficiency of various operations such as traversal and neighbor finding on the mesh is most desired. Based on the 3-dimensional version of Maubach’s method, Hebert
11691	11716	that they are much simpler in the sense that the interpolations are performed with a minimal number of samples. It is possible to further subdivide an octree cell to produce a simplicial complex , but this approach does not scale well with dimension due to the exponential increase in the number of vertices and explosion of cases that need to be considered. To illustrate the advantage of
3779703	11751	but small enough so that the tail conditionsP (X > r)=Cr?? remains a useful approximation. Finding the best value of m is a practical challenge, and creates a certain amount of controversy . Jansen and de Vries  use Hill’s estimator with a fixed value of m = 100 for several different assets. Loretan and Phillips  tabulate several different values of m for each asset. Hill’s
3779703	11752	to computer science, finance, and signal processing appear in Adler, Feldman, and Taqqu . More applications to signal processing can be found in Nikias and Shao . Mandelbrot  and Fama  pioneered the use of heavy tail distributions in finance. Mandelbrot  presents graphical evidence that historical daily price changes in cotton have heavy tails with ? ? 1.7, so that the mean
3779703	11770	D1, and to Vpswhen j > Dp?1. This shows that the eigenvectors estimate the coordinate vectors in the spectral decomposition, at least for the lightest and heaviest tails. Meerschaert and Scheffler  show that the same results hold for moving averages. If p ? 3, this gives a practical method for determining the right coordinate system for modeling heavy tail data: Simply use the eigenvalues of
125048	11785	This policy is harder to undermine, but also likely to exclude relevant information, published by unknown information providers. Trust policies can be based on the following types of information : First-hand information published by the actual information provider together with a graph, e.g. information about the intended assertional status of the graph or about the role of the information
125048	11567	. Access control A triple store may wish to allow fine-grain access control, which appears as metadata concerning the graphs in the store . Signing RDF graphs As discussed in , it is necessary to keep the graph that has been signed distinct from the signature, and other metadata concerning the signing, which may be kept in a second graph. Expressing propositional
125048	11567	retrieve a document. The document describes the method of forming the signature in detail. Such a method could specify, for example, a variation of the graph canonicalization algorithms provided in  8 , and choosing one of the XML canonicalization methods and one of the signature methods supported by XML Signatures . Rather than make a set of decisions about these methods, we permit the
125048	11788	assertions and logic where logical relationships between graphs have to be captured . RDF reification has well-known problems in addressing these use cases as previously discussed in . To avoid these problems several authors propose quads , consisting of an RDF triple and a further URIref or blank node or ID. The proposals vary widely in the semantic of the fourth
125048	11788	Graphs has to exhibit the name, the graph and the association between them. We offer three concrete syntaxes: TriX and RDF/XML both based on XML; and TriG as a compact plain text format. The TriX serialization is an XML format which corresponds fairly directly with the abstract syntax, allowing the effective use of generic XML tools such as XSLT, XQuery, while providing syntax extensibility
125048	11791	or about the information provider (e.g. ratings about his trustworthiness within a specific application domain). Most trust architectures proposed for the Semantic Web fall into this category . These approaches assume explicit and domainspecific trust ratings. Providing such ratings and keeping them up-to-date puts an unrealistically heavy burden on information consumers. The content of
11802	11803	inspect the results of their work. The particular formalism supported is Multimodal Functional Unification Grammar (MUG, ), which is similar to Functional Unification Grammars (FUG: , ), but supports several coordinated modes, such as voice prompts or structural and/or language-based screen displays. For each input description, the grammar can generate a range of coherent
11802	11806	unification grammars, which is designed to help grammar developers inspect the results of their work. The particular formalism supported is Multimodal Functional Unification Grammar (MUG, ), which is similar to Functional Unification Grammars (FUG: , ), but supports several coordinated modes, such as voice prompts or structural and/or language-based screen displays. For each
11807	11810	also clearly shows the need for full life-cycle support for aspects. 1 Introduction Engineering access control in distributed applications is a challenging problem for many application domains . It should be possible to set and manage one organization-wide access control policy that must then be enforced reliably in a multitude of applications running within the organization. This is
11807	11811	Woo and Lam , and Bertino et al. . The decoupling of authorization logic from application logic has been the prime objective of architectures like Resource Access Decision Service (RAD)  and the IBM Tivoli Access Manager  With respect to the binding of access control functionality to applications, mainstream commercial systems, such as for example J2EE  and .NET  use
11807	11816	an application are essentially ? Tine Verhanneman is supported by a grant from the Institute for the Promotion of Innovation through Science and Technology in Flanders (IWT-Vlaanderen) 1 pointcuts , this paper proposes a new approach for engineering access control into applications. Our approach is based on the concepts of access interface and view connector, presented in figure 1: The Access
11807	11817	is related with several research domains,which are discussed below. In the domain of access control engines extensive research has been carried out, e.g. the Flexible Authorization Framework (FAF) , Woo and Lam , and Bertino et al. . The decoupling of authorization logic from application logic has been the prime objective of architectures like Resource Access Decision Service (RAD)
11807	11818	can be fine-grained and dependent on application state, and hence its enforcement can crosscut an application in an intricate way. State-of-the-art commercial systems, such as Tivoli Access Manager , provide for centralized management of access control, but as soon as policies are dependent on application state, explicit calls to the authorization engine must be inserted in the application
11807	11818	al. . The decoupling of authorization logic from application logic has been the prime objective of architectures like Resource Access Decision Service (RAD)  and the IBM Tivoli Access Manager  With respect to the binding of access control functionality to applications, mainstream commercial systems, such as for example J2EE  and .NET  use fairly simple method-level interception
11807	11820	earlier by B. De Win . He uses the AspectJ tool to modularize this binding. Our view connectors can be seen as a special case of Collaboration Interfaces, proposed by K. Ostermann and M. Mezini , tuned to the access control scenario. 7 Conclusion The definition of an access control view on an application, provides abstractions that can be reused throughout applications based on the same
11807	11822	logic is suggested. First, the overall architecture is described. Subsequently, we describe into further detail how a dynamic wrapper based AOSD system, such as for instance JAC  and Lasagne , can be used for the implementation of view connectors to connect the access control module at deployment time to the application. The access module itself, will not be dealt with in great depth,
11807	11824	research domains,which are discussed below. In the domain of access control engines extensive research has been carried out, e.g. the Flexible Authorization Framework (FAF) , Woo and Lam , and Bertino et al. . The decoupling of authorization logic from application logic has been the prime objective of architectures like Resource Access Decision Service (RAD)  and the IBM
11825	11826	18 Figure 1-7 Example Temporal Plan Network................................................................... 21 Figure 2-1 Allen's Interval Relationships  ............................................................. 26 Figure 2-2 A Plan Graph................................................................................................... 29
11825	11826	allow the expression of mutual exclusion relationships as well as preconditions that must hold before, during, or after a particular action interval .sFigure 2-1 Allen's Interval Relationships  Intervals within a constraint-based interval planner are often ordered using Allen’s basic interval relationships: before, meets, overlaps, starts, contains, equals, and ends  (see Figure
11825	11826	one can easily determine temporal consistency by using a negative cycle detection algorithm, such as the Floyd-Warshall all-pairs shortest path algorithm  or the FIFO label-correcting algorithm . 4.5.2 TPN Tell Consistency To be consistent, a TPN must not only be temporally consistent, but also ensure that its assignments to state variables do not conflict. This is to ensure that each
11825	11826	destination  } Fly-to ( destination ) Tell ( location UAV = unknown ) Tell ( location UAV = destination ) Move Activity RMPL Code Move Activity TPN Drop-water (waypoint) { do { open-doors() ; drop-water() ; close-doors()  } maintaining location UAV = waypoint; waypoint = suppressed  }  open-doors ( )   Drop Water
11825	11826	cost using activities from the activity library, if it exists (see Figure 5-6). Fly-to ( dest ) Tell ( location UAV = unknown ) Tell ( loc UAV = dest )  open-doors ( )  Move ( Base, fire1 )  drop-water ( )   Ask ( location UAV = waypoint )  Drop-Water ( fire1 )   close-doors ( ) Tell ( wpt = spprsd )
11825	11828	cost metrics, however, they are also inherently slower than HTN or graph-based planners. One way of optimizing forward chaining planners is to use expansion rules, as demonstrated by TLPlan . Expansion rules inform the planner such that it avoids searching redundant or wasteful candidate solutions, thus reducing the search branching factor and increasing planning speed. Unfortunately,
11825	11830	the planning goals. This section will discuss graph-based planning, which is one of today’s leading architectures for solving generative planning problems. Graph-based planners, such as Graphplan , Blackbox , and LPGP , all utilize a structure called a plan-graph. Plan-graphs compactly represent the plan-space for a given planning problem, allowing graph-based planners to solve
11825	11830	it out, and depending on the scenario, also return to base. return to base execute extinguish-fire activity forest fire Forest base fly to forest fire UFFAV 1 Ask (at $start) Tell (at $end)    Ask (have retardant)  UFFAV Move Activity Ask (at forest fire)  Tell (fire extinguished)  UFFAV Extinguish-Fire Activity Figure 6-1 Test Scenario with Activity Library
11825	11830	95sPlan Candidate Activity Library Activity A Activity B f-value = 10 1 2 0  10 Tell ( X = v ) A1 A2 A3 Tell ( Y = z )  3 5 Ask ( X = v ) Ask ( Y = k ) B1 B2 B3  Tell ( Y = k )   Figure 6-2 Example Candidate Graph before Heuristic Cost Estimation Relaxed Candidate Graph conflicting Tells are ignored Original Candidate 1 2  0 10 0 Tell ( X = v ) Tell ( Y = z )
11825	11831	of efficient graph-based algorithms for determining plan consistency and cost. The selection of a forward progression planning architecture is motivated by existing planners such as FF  and HSP . These planners have achieved fast generative planning by coupling forward progression planning with a relaxed plan-graph heuristic cost estimate. Furthermore, forward progression planners support
11825	11831	22sprogression planning architecture, as forward progression search combined with informative heuristic cost estimates has recently been shown to be an effective way to achieve fast planning . In the style of FF  and HSP , Spock has the capability of including a relaxed plan heuristic cost estimate that will upgrade its search algorithm from a uniform-cost search to a
11825	11831	speed. Unfortunately, these expansion rules are inappropriate for Spock as they violate the spirit of model-based programming. Recently, some forward progression planners, such as FF  and HSP , have shown dramatic performance improvements by using relaxed plan-graphs to calculate admissible heuristic cost estimates. A relaxed plan-graph is constructed in a manner similar to a plan-graph,
11825	11831	While HTN planners such as HSTS  have been developed for real-world systems in the past, forward progression heuristic search has recently been demonstrated by such planners as FF , HSP , and LPG  to be a novel way to achieve even faster planning speeds. Spock applies this fast search algorithm to the Temporal Plan Network structure, which provides temporal flexibility like the
11825	11831	Thus Spock needs an admissible, yet close estimate of the remaining cost of a plan candidate to the goal. 93sRecently, advances in planner efficiency have been demonstrated by the FF  and HSP  planners through the use of an admissible heuristic cost estimate called a relaxed plan graph. Relaxed plan graphs are similar in structure to standard plan graphs, however they do not prohibit
11825	11832	and plan candidates with a uniform representation called a Temporal Plan Network (TPN) . TPNs aressignificant in that they support temporal flexibility using simple temporal constraints , which enable dynamic scheduling and improve mission robustness. Third, Spock is implemented as a forward progression planner. When combined with a relaxed plangraph heuristic cost estimate, this
11825	11832	terms of intended state evolutions as opposed to explicit sequences of specific activity operators. TPNs are significant in that they support temporal flexibility using simple temporal constraints , which enable dynamic scheduling and improve mission robustness. This representation supports fast planning as it enables the use of efficient graph-based algorithms for determining plan
11825	11832	with variables that can be constrained using the interval durations embedded in the plan . These constraints are represented using a constraint network, such as a Simple Temporal Network  or distance graph , which allows consistency to be checked using efficient graph-based algorithms . Spock uses a similar temporal representation in terms of Simple Temporal Networks .
11825	11832	occur without violating one of the vehicles’ temporal requirements. Thus we say that the plan is temporally inconsistent. Because TPNs have temporal constraints similar to Simple Temporal Networks , graph algorithms for determining STN consistency can also be applied in order to determine TPN temporal consistency. As shown by Dechter and Meiri, the temporal constraints from both STNs and TPNs
11825	11832	as weight 7. return d Figure 4-5 TPN to Distance Graph Pseudo Code As mentioned above, temporal consistency in a TPN or STN corresponds to negative cycle detection in the associated distance graph . Once the distance graph for a 0 51sgiven TPN has been constructed, one can easily determine temporal consistency by using a negative cycle detection algorithm, such as the Floyd-Warshall
11825	11836	within Spock. 3.4 RMPL Subsumption of PDDL+ Operators The planning community has established the Planning Domain Description Language (PDDL+) as a standardized format for encoding planning problems . PDDL+ was developed to be a flexible format for encoding primitive operators. PDDL+ supports durative actions, start pre-conditions and effects, invariant conditions and effects, and end
11825	11836	of PDDL+ Operators As described in Chapter 3, the planning community has established the Planning Domain Description Language (PDDL+) as a standardized format for encoding planning problems . PDDL+ supports durative actions, start pre-conditions and effects, invariant conditions and effects, and end pre-conditions and effects. PDDL+: Start preconditions A Start effects B Invariant
11825	9946	robots typically have hundreds or thousands of interacting components that must be controlled and monitored. To encode the relationships between system components, languages such as RAPS , ESL , and TDL  allow mission designers to program autonomous robots with redundant methods and goal monitoring while simultaneously expressing any necessary constraints between system components.
11825	9946	or controlled at all times. To help manage the inherent complexity of autonomous systems control, mission programmers have traditionally relied on programming languages such as RAPS , ESL , and TDL . These languages help model the relationships between various robot states by incorporating features such as concurrency, metric constraints and durations, functionally redundant
11825	9946	all of the Ask constraints in a TPN are closed by Tell constraints and any conflicting Tell constraints are ordered so as to not co-occur, we say that the TPN is complete. 1 Ask ( lights = on )  2   3 Tell ( lights = on )  4 Figure 4-7 Example of Complete TPN Consider the example in Figure 4-7. In this example, the TPN has an Ask constraint that requires the lights to
11825	11837	22sprogression planning architecture, as forward progression search combined with informative heuristic cost estimates has recently been shown to be an effective way to achieve fast planning . In the style of FF  and HSP , Spock has the capability of including a relaxed plan heuristic cost estimate that will upgrade its search algorithm from a uniform-cost search to a much
11825	11837	not use a forward progression planning algorithm, they generally operate on plan representations similar to those used in forward progression planning. An example of a local-search planner is LPG . LPG plans by using a randomized local search algorithm similar to WalkSAT , called WalkPlan. LPG is quite fast, however, its randomized search means that it is not optimal, often returning
11825	11837	planners such as HSTS  have been developed for real-world systems in the past, forward progression heuristic search has recently been demonstrated by such planners as FF , HSP , and LPG  to be a novel way to achieve even faster planning speeds. Spock applies this fast search algorithm to the Temporal Plan Network structure, which provides temporal flexibility like the
11825	11837	to a relaxed graph’s cost estimate. A key feature of relaxed graphs is that they can be constructed quickly to determine a cost estimate. Constructing a relaxed graph require polynomial time , which implies an algorithm that avoids decision making and expensive search procedures. While the elimination of decision making causes a relaxed graph to become inconsistent, this is not a
11825	11840	Furthermore, Allen’s relationships are used when a programmer writes an activity model to describe complex interactions within system processes. Constraint-based interval planners, such as HSTS , usually plan using a goal-directed search. Planning begins with an initial plan that contains open conditions. The planner closes those open conditions by adding actions from its action library.
11825	11840	relevant concepts. 5.1 Overview Spock is designed to integrate forward progression heuristic search, temporal flexibility, and the composition of complex processes. While HTN planners such as HSTS  have been developed for real-world systems in the past, forward progression heuristic search has recently been demonstrated by such planners as FF , HSP , and LPG  to be a novel way to
11825	11840	planning speeds. Spock applies this fast search algorithm to the Temporal Plan Network structure, which provides temporal flexibility like the constraint-based interval planners HSTS and Europa . Finally, Spock’s inputs are expressed in the Reactive Model-based Programming Language, which allows mission designers to specify the evolution of state variables within complex processes by using
11825	11841	goals. This section will discuss graph-based planning, which is one of today’s leading architectures for solving generative planning problems. Graph-based planners, such as Graphplan , Blackbox , and LPGP , all utilize a structure called a plan-graph. Plan-graphs compactly represent the plan-space for a given planning problem, allowing graph-based planners to solve planning problems
11825	11842	process combinators within an object-oriented framework. Second, Spock represents goal plans, plan operators, and plan candidates with a uniform representation called a Temporal Plan Network (TPN) . TPNs aressignificant in that they support temporal flexibility using simple temporal constraints , which enable dynamic scheduling and improve mission robustness. Third, Spock is implemented as
11825	11842	which an autonomous robot may perform its mission. Figure 1-2 Titan Model-based Executive Architecture The contributions of this thesis are part of a mission-level model-based executive called Kirk  (see Figure 1-3). Kirk is designed to control mobile autonomous robots in rich environments, such as rovers exploring the surface of Mars or unmanned aerial vehicles flying search and rescue
11825	11842	generative planning with complex processes through its input language, RMPL , a temporally-flexible representation for control programs, operators, and plans called a Temporal Plan Network , and an optimal forward progression planning algorithm. RMPL is an innovative way for mission programmers to specify control programs and activity operators, because it supports a rich set of
11825	11842	dynamically, eliminating the need for plan slack or excessive replanning. Spock’s internal plan representation that supports temporal flexibility is called the Temporal Plan Network (TPN) , and is a central contribution of this thesis. A Temporal Plan Network  is a graphical depiction of a process representing plans in plan-space. When a mission programmer finishes writing an
11825	11842	their flexibility, it also makes them fast by eliminating a large portion of the search space. Examples of HTN planners include SHOP2 , Aspen , and Kirk’s strategy selection algorithm . When using an HTN planner, a programmer uses a library of macro operators, which can be decomposed into other macros, primitive operators, or some combination of the two. Additionally, there may
11825	11844	changed consequences are small relative to that of the overall problem, a significant performance gain is achieved. Examples of incremental algorithms include a truth maintenance systems  and Incremental A* . Candidate TPNs are modified only slightly during each child expansion. Thus, it makes sense to check temporal consistency with an incremental temporal consistency checking
11825	11844	within Spock’s iterative planning process. A better approach would be to track active Tells, enabled events, and enabled episodes using support links in the style of truth maintenance systems , in order to efficiently determine which pending events and episodes become enabled or un-enabled during the expansion process. Such an approach could theoretically result in a linear time
11825	11845	executive for mobile autonomous systems. The primary components of this system include the RMPL compiler , the TPN Sequencer , the Spock generative TPN planner, and the plan runner . The RMPL compiler takes input RMPL files and converts them into Temporal Plan Networks suitable for mission planning. The TPN Sequencer identifies a consistent goal / strategy plan that
11825	11848	representations similar to those used in forward progression planning. An example of a local-search planner is LPG . LPG plans by using a randomized local search algorithm similar to WalkSAT , called WalkPlan. LPG is quite fast, however, its randomized search means that it is not optimal, often returning plans with obviously wasteful sub-sequences. Spock’s planning algorithm is
11825	11849	using an incremental algorithm that reuses work from past iterations. An algorithm for performing incremental temporal consistency checking, ITC, has recently been introduced by I-hsiang Shu . While not integrated in the current implementation of Spock, using ITC to perform temporal consistency checking is discussed in future work Section 6.3.1. 5.5 Continuation: Combining Equivalent
11825	11849	As described in Chapter 1, Spock is part of the Kirk model-based executive for mobile autonomous systems. The primary components of this system include the RMPL compiler , the TPN Sequencer , the Spock generative TPN planner, and the plan runner . The RMPL compiler takes input RMPL files and converts them into Temporal Plan Networks suitable for mission planning. The TPN Sequencer
11825	11849	it makes sense to check temporal consistency with an incremental temporal consistency checking algorithm. Recently, I-hsiang Shu has developed an incremental consistency checking algorithm, ITC . ITC verifies temporal consistency in minimal time by using an 92sefficient verification algorithm that reuses existing work from previous iterations. It uses the idea of a set of support from
11825	11850	have hundreds or thousands of interacting components that must be controlled and monitored. To encode the relationships between system components, languages such as RAPS , ESL , and TDL  allow mission designers to program autonomous robots with redundant methods and goal monitoring while simultaneously expressing any necessary constraints between system components. While these
11825	11850	at all times. To help manage the inherent complexity of autonomous systems control, mission programmers have traditionally relied on programming languages such as RAPS , ESL , and TDL . These languages help model the relationships between various robot states by incorporating features such as concurrency, metric constraints and durations, functionally redundant choice,
11825	11851	18 Figure 1-7 Example Temporal Plan Network................................................................... 21 Figure 2-1 Allen's Interval Relationships  ............................................................. 26 Figure 2-2 A Plan Graph................................................................................................... 29 Figure
11825	11851	progression design. 2.1 Constraint-based Interval Planning Spock’s internal plan representation, the Temporal Plan Network (TPN), inherits from constraint-based interval plan representations . Similar to constraint-based interval plans, a TPN contains episodes of state assignments that have interval durations with flexible time-bounds. However, TPNs differ with regard to how these
11825	11851	add constraints between action intervals that allow the expression of mutual exclusion relationships as well as preconditions that must hold before, during, or after a particular action interval .sFigure 2-1 Allen's Interval Relationships  Intervals within a constraint-based interval planner are often ordered using Allen’s basic interval relationships: before, meets, overlaps,
11825	11851	interval plan’s temporal constraints, the start and end-points for each interval in the plan are represented with variables that can be constrained using the interval durations embedded in the plan . These constraints are represented using a constraint network, such as a Simple Temporal Network  or distance graph , which allows consistency to be checked using efficient graph-based
11825	11851	are inspired by the history-based process representations used in qualitative physics  and concise histories , and by interval representations from constraint-based interval planning . As such, the episodes (or arcs) in a TPN represent state variable assertions and requests that hold for a given interval of time. The end-points of these episodes are called events, which are
8920176	9187	derived from work by Booker (1982) XCS achieves the ability to discover and maintain a full State × Action × Payoff mapping for the test environment, with optimal levels of generalization (Kovacs, 1996). XCS thus represents a quantum leap forward in the reliability and utility of Learning Classifier Systems. The interested reader is directed to Kovacs (1996) which provides a detailed explanation
8920176	11889	used. &quot;Most important the system fails to converge to optimal performance when, due to the structure of the environment, the agent is not able to visit all the areas of the environment uniformly&quot; (Lanzi, 1997, section 7.3). The results obtained from the non-uniform run without learning in the simple FSW-5A environment, depicted in figure 7, shed some light on Lanzi's findings. Non-uniform exploration
8920176	11890	combined exploration/exploitation would suggest some degree of prediction inaccuracy, and would support Lanzi's choice of a hybrid mechanism utilizing exploitation only for the memory mechanism (Lanzi, 1998). Unfortunately Lanzi does not provide any error measures or population details that would allow more specific hypotheses to be generated without repeating his experiments. 5 CONCLUSIONS This work
11892	11896	f0 and duration. 1. Introduction The research reported on is undertaken within the Swedish GROG project “Boundaries and groupings – the structuring of speech in different communicative situations” (Carlson et al., 2002) 1 . An extended version of the present paper will appear in Strangert (2003). Focal accent is the highest level of prominence in the Swedish intonation model (Bruce, 1977). It is signalled
11892	11900	of emphasis, and Ericson & Lehiste (1995), reported on longer word durations in emphasized words. To emphasize, and for contrastive purposes, speakers also tend to insert pauses (Strangert, 1991; Selkirk 2002). Thus, speakers apparently adjust the prosodic means to the degree of emphasis required. The interaction dimension is central here. Borrowing from the H&H theory (Lindblom, 1990) communication
11913	11915	a system to be built). Under certain conditions, however, they can also be used for the analysis of problems related to actual systems in operation. Building on the Jackson/Zave reference model  for requirements and specifications, this paper presents a framework useful for the prevention, analysis and communication of designer and operator errors and, importantly, their subtle
11913	11915	. In this paper we propose an analytical framework for socio-technical systems. The framework is based upon a dynamic view of the reference model (RM) for requirements and specifications . The formal approach of the RM, complemented with the semiformal approach of Jackson’s Problem Frames , provides a clear distinction between descriptions of the world (or environment of the
11913	11918	and prevention of accidents in sociotechnical systems focus on only one or two of these dimensions but not on all three at the same time. However, it is well known that accidents combine all three . In this paper we propose an analytical framework for socio-technical systems. The framework is based upon a dynamic view of the reference model (RM) for requirements and specifications . The
11913	11918	of mixed causes (intermingled design and operator errors) can be done, like when even subtle design errors lead to wrong information in a display that lead the operator into taking a bad decision . 4. Conclusions and Further Work We have presented a framework into which many aspects of socio-technical systems fit, and within 2 The reader can check this by chasing round Figure 1 the sequence
8824055	919	whose narrowest hop in the chain gives better expected TCP throughput than the default IP path. This performance improvement is much in the spirit of alternative detour routes described in , ; these papers observe that IP does not provide the “best” path, measured in terms of delay or loss rates. We find that the best ROMA path is often a multihop path in which the minimum expected TCP
8824055	919	rate). These results enabled the authors to identify detour paths over which the expected TCP throughput was higher than the default path (validated with actual TCP transfers). The designers of RON  employed the idea of alternative paths in an overlay context, and used paths similar to detour paths both to improve performance and to route around faults in their overlay. In our work, we
8824055	919	our overlay so as to optimize the layout of the set of TCP connections in our delivery tree. Our analysis goes beyond the conservative model used to estimate TCP performance common to both  and  ??? we find that their methods underestimate the actual throughput that a chain of TCPs is likely to see. III. CANDIDATE ARCHITECTURES We first develop a basic model for an overlay network and
8824055	919	to B to C. It is worth noting that in previous work, a different, and more conservative formula was used to estimate the throughput of a chain of TCP connections. Following the methodology used in , , the aggregate RTT is defined as the sum of rtti along the path and the aggregate loss rate is defined as 1 ? ? (1 ? pi) (assuming uncorrelated losses). Instantiating these values into the
8824055	11922	and the aggregate RTT across this chain are larger than the values of the original long loop. Breaking long TCP control loops in the context of overlay networks has a similar effect as split TCP , which shortens the TCP feedback loop and separates lossy components. Ideas from split TCP are commonly used in satellite communication and in various terrestrial wireless contexts to improve TCP
8824055	8974	live streaming to reliable delivery of popular content, a recent research trend has proposed to serve these applications using endsystem, or application-level, multicast , , , , , . There is ample motivation for such an approach: multicast-based delivery provides excellent scalability in terms of bandwidth consumption and server load, while an endsystem approach avoids
8824055	8974	and conclude our paper in Section VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two
8824055	8974	the tree layout so as to minimize network costs such as average latency; or to minimize overlay costs, such as link stress; or to perform load balancing, such as by bounding the maximum fanout , , , , , . Results from the measurement community have also been used in designing and optimizing overlay layouts. Savage et al  showed that the default IP path between two
8824055	8976	and conclude our paper in Section VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas
8824055	8976	the tree layout so as to minimize network costs such as average latency; or to minimize overlay costs, such as link stress; or to perform load balancing, such as by bounding the maximum fanout , , , , , . Results from the measurement community have also been used in designing and optimizing overlay layouts. Savage et al  showed that the default IP path between two hosts
8824055	11923	conclude our paper in Section VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas that
8824055	11923	layout so as to minimize network costs such as average latency; or to minimize overlay costs, such as link stress; or to perform load balancing, such as by bounding the maximum fanout , , , , , . Results from the measurement community have also been used in designing and optimizing overlay layouts. Savage et al  showed that the default IP path between two hosts often
8824055	6323	streaming to reliable delivery of popular content, a recent research trend has proposed to serve these applications using endsystem, or application-level, multicast , , , , , . There is ample motivation for such an approach: multicast-based delivery provides excellent scalability in terms of bandwidth consumption and server load, while an endsystem approach avoids the
8824055	6323	the ability to tolerate asynchronous joins, the ability to adaptively reconfigure the topology, and the ability to speed up downloads with collaborative peer-to-peer transfers as described in . B. Transmitting Encoding Symbols with TCP One nuance of using codes is that the encoding symbols must be treated atomically, i.e. receipt of a fraction of an encoding symbol is not useful. For
8824055	11924	handling rate mismatches. FEC codes, applying well-known techniques developed for reliable (IP) multicast. The central component that enables our methods is the use of the digital fountain approach , a paradigm which is ideally capable of encoding n packets of original content into an unbounded set of encoding packets; and where receiving any n distinct encoding packets allows the complete,
8824055	11925	our paper in Section VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas that are
8824055	11350	paper in Section VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas that are relevant
8824055	11350	so as to minimize network costs such as average latency; or to minimize overlay costs, such as link stress; or to perform load balancing, such as by bounding the maximum fanout , , , , , . Results from the measurement community have also been used in designing and optimizing overlay layouts. Savage et al  showed that the default IP path between two hosts often is
8824055	7895	applications ranging from live streaming to reliable delivery of popular content, a recent research trend has proposed to serve these applications using endsystem, or application-level, multicast , , , , , . There is ample motivation for such an approach: multicast-based delivery provides excellent scalability in terms of bandwidth consumption and server load, while an
8824055	7895	tree edges simultaneously. The throughput degradation comes from the effect of link stress, which is defined as the number of identical copies of a packet carried by a physical link in an overlay . In our example, downstream and upstream links from a single node often share some physical links. When these shared links are a bottleneck, the contention at these resources negatively impact the
8824055	9829	without root access, we used equation (1) to compute the approximate loss rate based on the measured throughput and the measured RTT. To doublecheck our measurements, we also concurrently ran TFRC  on the same path and measured the average loss rate from TFRC. Details of our methodology and comparisons of measured loss rate and computed loss rate are in the full version of this paper .
8824055	11927	One could certainly adapt our methods to address dynamic changes in available bandwidth, for example, by using non-intrusive methods to monitor available bandwidth such as pathload  or PTR . In the following sections, we use values from the table as input to our algorithms to construct overlay multicast trees. Next, we describe additional details involving experimentation with ROMA.
8824055	11928	bandwidth. One could certainly adapt our methods to address dynamic changes in available bandwidth, for example, by using non-intrusive methods to monitor available bandwidth such as pathload  or PTR . In the following sections, we use values from the table as input to our algorithms to construct overlay multicast trees. Next, we describe additional details involving experimentation
8824055	6493	ranging from live streaming to reliable delivery of popular content, a recent research trend has proposed to serve these applications using endsystem, or application-level, multicast , , , , , . There is ample motivation for such an approach: multicast-based delivery provides excellent scalability in terms of bandwidth consumption and server load, while an endsystem
8824055	6493	Section VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas that are relevant to our
8824055	7032	an unbounded set of encoding packets; and where receiving any n distinct encoding packets allows the complete, efficient reconstruction of the source data. Using the best codes currently available , a very close approximation to an idealized digital fountain can now be realized. This method has been widely used to enable receivers to recover from packet losses in the network; we apply it here
8824055	7032	Buffer Incoming TCP Connection Buffer in Application layer Packet Flow Outgoing TCP Connection Fig. 4. Overlay Node Implementation. Overflow Buffer Send Buffer of ratelessness, may be found in , . Both of these codes also have have strong probabilistic decoding guarantees, along with low decoding overheads and average degrees. In our experiments, we simulate use of LT codes , and
8824055	11930	from the encoding symbols. For a given symbol, we refer to the number of input symbols used to produce the symbol as its degree, i.e. y3 = x3 ? x4 has degree 2. Using the methods described in , the time to produce an encoding symbol from a set of input symbols is proportional to the degree of the encoding symbol, while decoding from a sequence of symbols takes time proportional to the
8824055	11930	which input symbols were combined, which is typically represented by a 64-bit random seed.) Provably good degree distributions for sparse parity check codes were first developed and analyzed in . However, these codes are fixed-rate, meaning that only a pre-determined number of encoding symbols are generated, for example only c?, where c is a small constant > 1. In our application, this can
8824055	11931	Incoming TCP Connection Buffer in Application layer Packet Flow Outgoing TCP Connection Fig. 4. Overlay Node Implementation. Overflow Buffer Send Buffer of ratelessness, may be found in , . Both of these codes also have have strong probabilistic decoding guarantees, along with low decoding overheads and average degrees. In our experiments, we simulate use of LT codes , and assume
8824055	11932	may or may not affect the performance of downstream TCP connections, but a downstream connection never affects the performance of upstream connections. Applying standard equation-based methods , we examine the expected throughput across a chain of TCPs given per-hop RTTs and per-hop loss rates, where hop refers to a hop in the overlay. Conventional wisdom indicates that overlay multicast
8824055	11932	C is limited to that of the upstream rate into B. To develop formulas for the expected TCP throughput as a function of the per-hop loss rates and RTTs, we employ the following equation derived in : s T = ?? ? 2p 3p rtt 3 + (12 8 )p(1 + 32p2 ?. (1) ) This provides an estimate of the expected throughput T of a TCP connection in bytes/sec as a function of the packet size s, the measured
8824055	7896	from live streaming to reliable delivery of popular content, a recent research trend has proposed to serve these applications using endsystem, or application-level, multicast , , , , , . There is ample motivation for such an approach: multicast-based delivery provides excellent scalability in terms of bandwidth consumption and server load, while an endsystem approach
8824055	7896	VI. II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas that are relevant to our proposed
8824055	7896	paradigm in the next section. A. Limited buffer space solution If the host has finite buffer space in application layer, the push-back flow control or back-pressure mechanism ,  can be used to avoid buffer overflow. The basic operation of this approach is to dequeue the packet from the incoming B 10Mbps A 10Mbps 5Mbps D C 5Mbps Fig. 3. Adaptive reconfiguration of the
8824055	11933	nevertheless be forced to slow to the rate of the slowest receiver. In this sense, this method has performance which closely resembles TCP-friendly single-rate multicast congestion control , . On the other hand, it is not clear how to devise a TCP-based solution which provides an effective, multiple-rate remedy. Our main contribution in this paper is the design and evaluation of ROMA
8824055	11934	for all receivers. If the fastest receiver exhausts all data in the buffer, the buffer is filled with new data from disk. The sender’s functionality is similar to the Cyclone webserver architecture , which is optimized for delivery of content in situations in which a group of clients is concurrently downloading a small set of large, popular files. In particular, the sender can be optimized to
8824055	6499	path whose narrowest hop in the chain gives better expected TCP throughput than the default IP path. This performance improvement is much in the spirit of alternative detour routes described in , ; these papers observe that IP does not provide the “best” path, measured in terms of delay or loss rates. We find that the best ROMA path is often a multihop path in which the minimum expected
8824055	6499	such as by bounding the maximum fanout , , , , , . Results from the measurement community have also been used in designing and optimizing overlay layouts. Savage et al  showed that the default IP path between two hosts often is quantitatively inferior to a “detour” route taken through an intermediate end-system. Using a large set of Internet path measurements
8824055	6499	in our overlay so as to optimize the layout of the set of TCP connections in our delivery tree. Our analysis goes beyond the conservative model used to estimate TCP performance common to both  and  — we find that their methods underestimate the actual throughput that a chain of TCPs is likely to see. III. CANDIDATE ARCHITECTURES We first develop a basic model for an overlay network
8824055	6499	to C. It is worth noting that in previous work, a different, and more conservative formula was used to estimate the throughput of a chain of TCP connections. Following the methodology used in , , the aggregate RTT is defined as the sum of rtti along the path and the aggregate loss rate is defined as 1 ? ? (1 ? pi) (assuming uncorrelated losses). Instantiating these values into the simple
8824055	11935	buffer for additional buffering at intermediate hosts avoids known pitfalls associated with bursty packet arrivals when high bandwidth connections with large window sizes use small socket buffers . As described earlier, each intermediate host dequeues arriving packets and copies them to an application buffer. If the buffer is full, then the host writes the packets in the buffer to disk and
8824055	8996	II. RELATED WORK IN OVERLAY DESIGN A large body of work has recently been proposed to support multicast functionality at the application layer, including , , , , , , , , . The design of overlay network layout has also been impacted by work initiated in the measurement community. We review and critique work in these two areas that are relevant to our proposed
8824055	8996	to minimize network costs such as average latency; or to minimize overlay costs, such as link stress; or to perform load balancing, such as by bounding the maximum fanout , , , , , . Results from the measurement community have also been used in designing and optimizing overlay layouts. Savage et al  showed that the default IP path between two hosts often is quantitatively
8824055	11936	in-flight packets will soon exceed the finite application level buffers available for relaying data at the intermediate endsystem, and then there is a problem to solve. One solution, as proposed in , is to use push-back flow control to rate-limit the TCP connection of the upstream sender. But it is easy to see that push-back flow control will recursively propagate all the way back to the
8824055	11936	the rate of data delivery while maintaining low end-to-end latencies. ALMI and Overcast employ TCP to provide reliable file transfers between any set of hosts. However, like the methods of , ALMI uses a back-pressure mechanism to rate-limit the sender, resulting in a single rate control. Overcast was explicitly designed with the goal of building distribution trees that maximize each
8824055	11936	the store-and-forward paradigm in the next section. A. Limited buffer space solution If the host has finite buffer space in application layer, the push-back flow control or back-pressure mechanism ,  can be used to avoid buffer overflow. The basic operation of this approach is to dequeue the packet from the incoming B 10Mbps A 10Mbps 5Mbps D C 5Mbps Fig. 3. Adaptive reconfiguration of the
8824055	11937	will nevertheless be forced to slow to the rate of the slowest receiver. In this sense, this method has performance which closely resembles TCP-friendly single-rate multicast congestion control , . On the other hand, it is not clear how to devise a TCP-based solution which provides an effective, multiple-rate remedy. Our main contribution in this paper is the design and evaluation of
8521970	11940	has been increasingly used to model various aspects of call center operations, with recent applications to call center and workforce management (Chokshi 1999, Klungle 1999) and call routing (Miller and Bapat 1999). This paper uses simulation to model the call center process to compare the performance of three different scheduling approaches. The paper begins with a brief description of call scheduling
11946	11948	high level communication primitives (services) that are used by the processes to communicate. Access to a channel is controlled by a xed set of primitives and relies on remote procedures call ,  of these communication primitives. A process that is willing to communicate through a channel makes a remote procedure call to a communication primitive (send, receive) of that channel. Once the
11946	11950	a new processing element and a bus when it is not possible to assign a process to an already existing processing element or a communication on a bus without violating real time constraints. In  , Chou and Srivastava use a set of prede ned interconnection models during communication synthesis. Several works on protocol selection are reported in the software synthesis for distributed
11946	11950	the speci cation of complex synchronous/asynchronous interface is proposed by Vanbekbergen in . Approaches where communication is done through shared memory are detailed in   and . In  the problem of interface between a memory and a coprocessor or I/O processor is addressed. In  Gupta also address the problem of communicationbetween a processor (software) and a
11946	11960	selection/allocation, Interface generation I. INTRODUCTION Recently the synthesis community has moved toward the highest level of abstraction commonly known as the system level      . This move vas motivated by the increasing complexity of systems and by the need for a uni ed approach toallow the development of systems containing both hardware and software. As the
11946	11962	These features may bepacked into a cost function to be reduced by the allocation algorithm. This is similar to the binding/allocation of functional units in classic high-level synthesis tools  .sMost of the allocation algorithms used in high level synthesis may be used to solve this problem . P1 P2 P3 P4 RPCall svc3 c_f1 RPCall svc1 c_h1 RPCall svc1 c_h1 RPCall svc1 c_h1 RPCall svc3
11946	11965	communication synthesis. Several works on protocol selection are reported in the software synthesis for distributed systems . Lots of previous work has focused on interface synthesis        and . In , Ecker presents a method for transforming and optimising protocols. In , Narayan addresses the problem of bus interface generation between two di erent
11946	11966	synthesis. Several works on protocol selection are reported in the software synthesis for distributed systems . Lots of previous work has focused on interface synthesis        and . In , Ecker presents a method for transforming and optimising protocols. In , Narayan addresses the problem of bus interface generation between two di erent hardware
11946	11966	in  , Lin and Nayaran consider the problem of interface synthesis with automatic protocol conversion with one or both sides having a xed interface. Madsen interface synthesis approach  consider the problem of interface adaptation between a xed interface and a communication medium chosen during partitioning. A state based model that describes both functional and timing properties
11946	11978	by the user. It is not possible to use a protocol that is not described in the library. The need for a realistic cost function for the algorithm and the need for communication estimator   to lead network synthesis and protocol selection. In the following sections we present our proposed communication synthesis method. The next section introduces the communication model. Section III
11946	11981	Most of the work in communication synthesis for codesign has focussed on interface synthesis assuming a xed network structure  . Only few works in codesign handle network synthesis   . In  Gong's network synthesis is guided by the mapping of variables (shared or private) to memory (local or global). In , Yen create a new processing element and a bus when it is not
11946	11981	be provided by the user. It is not possible to use a protocol that is not described in the library. The need for a realistic cost function for the algorithm and the need for communication estimator   to lead network synthesis and protocol selection. In the following sections we present our proposed communication synthesis method. The next section introduces the communication model. Section
11946	11982	and by the need for a uni ed approach toallow the development of systems containing both hardware and software. As the level of abstraction rise some problems heretofore non existing appear  . At the system level, some of the main concepts are behaviour and communication . These two concepts have brought new problems konwn as partitionning and communication synthesis. The goal of
11946	11982	via high level primitives 3 into a set of interconnected processors that communicate via signals and share communication control. At thislevel the system is represented as a process graph . The nodes represent the processes and the edges the communication. Communication through abstract channels is based on the remote procedure call of communication primitives ( gure 1). Starting
11985	11988	tool and the second is the investigation and improvement of input vectors used for the simulation of the design. 4.1 Power Estimation Tools The group has already developed a tool, PowerCount , capable of estimating the power consumption at the netlist level. This tool has several advantages over existing tools. It is fully incorporated into the normal IC design cycle. This means that
8920201	11992	by W is denoted by |~ W and is defined by: ? |~ W ? iff (i) =?, and (ii) every state minimal in  satisfies ?. Similar forms of reasoning have been considered in the literature before (e.g. Benferhat et al., 1992). The system CP is included here mainly for the sake of argument; however, we will briefly pause to comment on one of its possible applications. Consistent preferential reasoning was studied in
8920201	11993	out every state which represents an exception to a preferential argument (the preference order becoming obsolete in the process). 4 4 Similar results have been obtained by (Stachniak, 1993; Benferhat et al., 1996). THEOREM 7 (Semantic characterisation of extension of P to M). Let |~ be a preferential relation characterised by the preferential structure ?S,l,<?, and let |~? be characterised by the monotonic
677931	12020	the strategic issue is to consider alternative markets that are at least myopically incentive compatible. One example is a market mechanism called a market scoring rule, suggested by Hanson . These markets have the property that a risk-neutral agent’s best myopic strategy is to truthfully bid her current expected value of the security. Additionally, the number of securities involved in
677931	12027	outcomes given all information, even if that information is distributed across many sources. Supporting evidence can be found in empirical studies of options markets , political stock markets , sports betting markets , horse racing markets , market games , and laboratory investigations of experimental markets . The process of information incorporation is,
677931	12028	Supporting evidence can be found in empirical studies of options markets , political stock markets , sports betting markets , horse racing markets , market games , and laboratory investigations of experimental markets . The process of information incorporation is, at its essence, a distributed computation. Each trader begins with his or her own
677931	12029	Supporting evidence can be found in empirical studies of options markets , political stock markets , sports betting markets , horse racing markets , market games , and laboratory investigations of experimental markets . The process of information incorporation is, at its essence, a distributed computation. Each trader begins with his or her own
677931	12032	if that information is distributed across many sources. Supporting evidence can be found in empirical studies of options markets , political stock markets , sports betting markets , horse racing markets , market games , and laboratory investigations of experimental markets . The process of information incorporation is, at its essence, a distributed
8920208	12036	each candidate term as returned by Google and removes low frequency candidates. This is done as a simple threshold set at one standard deviation from the mean of the Zipf distribution ( m - s ).  THE STUDY We designed a study testing if Google’s document frequency was an adequate measure of human judgment of familiarity and obscurity. We hypothesize that terms and phrases will be judged
8920208	12036	to determine the most salient phrases to be included in a broader search. The inclusion of a phrase based obscurity checker is currently under implementation for the next version of Watson.  Another piece of software currently under development uses Google's phrase query ability to determine the readability of a document, given the document's domain. The software works by parsing a
8920208	12038	to determine the readability of a document, given the document's domain. The software works by parsing a document into n, k-word phrases and retrieving the Google hit count for each parsed phrase.  The Overall Variance of Familiarity 0%-14% 15%-29% 30%-44% 45%-59% 60%-75% 75%+ Google Term Percentile Group 4 Raw Variance Normalized Variance Figure 4: Variance of Participant’s Term Familiarity.
312868	12043	Bertalmio et al in  where the authors’ motivation comes from the professional artists who restore damaged ancient paintings by hand. There are many inpainting models and we refer the reader to  for some recent works and references therein. The problem we address and wish to solve in this paper is the restoration of blurred images with occluded or missing regions arising from multiple
312868	12044	0 otherwise; • Av is a matrix representation of the blur operator with PSF v; • Lv is an (m1m2)-by-(m1m2) matrix representation of the differential operator ?? · (?/|?v|). We refer the readers to  for details on the construction of the matrices Av and Lv. We remark that the above forward Euler schemes (8) and (9) are implemented for simplicity; other finite difference schemes may be used as
312868	12044	for regular deconvolution problems. Fig. 9(a) and (b) show the original and the blurred images respectively. We compare the inpainting boundary conditions to the Neumann boundary conditions . To focus on the effects of the boundary conditions, we consider the nonblind deconvolution case where the PSF is known. In this way, the effects due to the errors in the recovery of the PSF are
312868	12046	Bertalmio et al in  where the authors’ motivation comes from the professional artists who restore damaged ancient paintings by hand. There are many inpainting models and we refer the reader to  for some recent works and references therein. The problem we address and wish to solve in this paper is the restoration of blurred images with occluded or missing regions arising from multiple
312868	12046	is a well-known phenomenon with the TV inpainting model where the model chooses to connect the strips if the length of the gap is less than the width of the gap and chooses not to connect otherwise . If the completion of the strips on the shirt is desired, one may replace the TV inpainting method with some higher order inpainting methods such as curvature driven diffusion or Euler’s elastica
312868	12047	Bertalmio et al in  where the authors’ motivation comes from the professional artists who restore damaged ancient paintings by hand. There are many inpainting models and we refer the reader to  for some recent works and references therein. The problem we address and wish to solve in this paper is the restoration of blurred images with occluded or missing regions arising from multiple
312868	12047	The parameters ? and ? control the amount of regularization which is given by the TV norm in both k and u. 2 ? ?s2.2 TV Image Inpainting In this paper we will focus on the TV inpainting model  which is variational and fits well with the TV blind deconvolution model (3). It has several benefits including an easy implementation, a tendency towards preserving edges, and a robustness to
312868	12047	is a well-known phenomenon with the TV inpainting model where the model chooses to connect the strips if the length of the gap is less than the width of the gap and chooses not to connect otherwise . If the completion of the strips on the shirt is desired, one may replace the TV inpainting method with some higher order inpainting methods such as curvature driven diffusion or Euler’s elastica
312868	12048	Bertalmio et al in  where the authors’ motivation comes from the professional artists who restore damaged ancient paintings by hand. There are many inpainting models and we refer the reader to  for some recent works and references therein. The problem we address and wish to solve in this paper is the restoration of blurred images with occluded or missing regions arising from multiple
312868	12049	For statistical methods, see . For Tikhonov regularization methods, in particular for H 1 regularization we direct the reader to  and for total variation (TV) regularization to . A survey on blind image deconvolution may also be found in . Often times images may also have regions with missing data. Examples may include scratches on film frames, scratches on images, the
312868	12049	One potential drawback of using such a regularization is that the minimization of the H1- norm tends to smear edges in both the recovered image and PSF. In view of this, Chan and Wong, in , utilize total variation (TV) regularization R1(u) = ? ? |?u|dx and R2(k) = ? |?k|dx which favors ? the preservation of edge information (discontinuities) . Hence, restored image and PSF
312868	12049	Discrete Cosine/Sine Transforms to solve the deconvolution problem . 7s5 Numerical Implementations The minimization of the energy in (5) is carried out by alternating minimization , i.e., given an initial guess u 0 , we alternate between the following two steps until the iterates u n and k n become stable: 1. Solve k n+1 by mink E(u n , k). 2. Solve u n+1 by minu E(u, k n+1
312868	12049	For example, one may easily implement the lagged diffusivity fixed point iteration method  and solve the arising linear systems via the preconditioned conjugate gradient method as outlined in . Indeed, the schemes in (8) and (9) have already been expressed in matrix form which are very similar to the ones in . Thus, it is very easy to adapt the fast methods in  to solve the current
312868	12049	we may encounter the problem of solving (non-symmetric) linear systems of the form (L + ??A T A)x = b which is slightly different from the kind of linear systems (L + ?A T A)x = b encountered in  and may require new techniques to optimize the performance when solving them. 6 Simulation Results In this section, we use numerical simulations to illustrate the benefits of using the proposed
312868	12054	the deblurred image u and the PSF k in the above model (1) which may include inverse filtering, statistical, and Tikhonov regularization methods. For inverse filtering methods, we refer the read to . For statistical methods, see . For Tikhonov regularization methods, in particular for H 1 regularization we direct the reader to  and for total variation (TV) regularization to
312868	12061	effects during the deconvolution process, regardless of the particular deconvolution method used. For more examples of the ringing phenomenon due to boundary conditions we refer the reader to . We will show that by coupling the deconvolution and inpainting problems, it is possible to construct boundary conditions based on smooth extension of the level sets. This leads to a more natural
312868	12061	of these boundary conditions allow fast transform based deblurring). The commonly used boundary conditions are homogenous Dirichlet/zero (see Fig. 4(b)) and periodic  (see Fig. 4(c)). Neumann  (see Fig. 4(d)) and anti-reflective  (not shown because it contains negative values) boundary conditions are more recent ones that have been shown to outperform the former two. (a) Original
312868	12061	operators such as the H 1 -norm, other boundary conditions such as the Neumann and anti-reflective ones allow the use of fast Discrete Cosine/Sine Transforms to solve the deconvolution problem . 7s5 Numerical Implementations The minimization of the energy in (5) is carried out by alternating minimization , i.e., given an initial guess u 0 , we alternate between the following two steps
312868	12061	for regular deconvolution problems. Fig. 9(a) and (b) show the original and the blurred images respectively. We compare the inpainting boundary conditions to the Neumann boundary conditions . To focus on the effects of the boundary conditions, we consider the nonblind deconvolution case where the PSF is known. In this way, the effects due to the errors in the recovery of the PSF are
312868	12062	effects during the deconvolution process, regardless of the particular deconvolution method used. For more examples of the ringing phenomenon due to boundary conditions we refer the reader to . We will show that by coupling the deconvolution and inpainting problems, it is possible to construct boundary conditions based on smooth extension of the level sets. This leads to a more natural
312868	12062	also gives rise to another useful application in image reconstruction — generation of boundary conditions. In image restoration problems such as deblurring and super-resolution reconstruction , it is necessary to impose some boundary conditions (image intensity value right outside the scene) to make the problem solvable because the convolution k ? u in (3) evaluated near the boundary of
312868	12062	intensity outside the scene which is unknown. Improper assumptions made on the boundary conditions may deteriorate the quality of the reconstructed images, e.g. ringing effects and loss of contrast . Over the years, there have been a number of research articles addressing the problem of imposing proper boundary conditions for image restoration. The main considerations in these papers have been
312868	12062	operators such as the H 1 -norm, other boundary conditions such as the Neumann and anti-reflective ones allow the use of fast Discrete Cosine/Sine Transforms to solve the deconvolution problem . 7s5 Numerical Implementations The minimization of the energy in (5) is carried out by alternating minimization , i.e., given an initial guess u 0 , we alternate between the following two steps
312868	12065	above model (1) which may include inverse filtering, statistical, and Tikhonov regularization methods. For inverse filtering methods, we refer the read to . For statistical methods, see . For Tikhonov regularization methods, in particular for H 1 regularization we direct the reader to  and for total variation (TV) regularization to . A survey on blind image
312868	12065	is known, then one may use the Morozov Principle  to obtain a good choice of ?. If the noise level is unknown, then one may use methods such as L-curve  or generalized cross-validation  instead. The parameter ? controls the spread of the recovered PSF. When ? increases, the spread of the recovered PSF increases. For example, in the particular case where k is an out-of-focus blur
312868	12070	(which are static problems) may be solved directly without the introduction of artificial time variables. For example, one may easily implement the lagged diffusivity fixed point iteration method  and solve the arising linear systems via the preconditioned conjugate gradient method as outlined in . Indeed, the schemes in (8) and (9) have already been expressed in matrix form which are
13641913	12705	is presented. Experimental results have confirmed that this approach is much more efficient than previous solutions and results in equally good video quality. 1. INTRODUCTION MPEG2 standard  has become the most widely used format for video transmission and storage in various video applications, such as Standard Definition TV and High Definition Digital TV (HDTV, ) Broadcast,
13641913	12708	Definition Digital TV (HDTV, ) Broadcast, Internet VideoOn-Demand, interactive TV and video conferencing. On the other hand, broadband networking technologies such as cable modem  and xDSL  are bringing to our home and office buildings an Internet connection with up to 10Mbps or even higher bandwidth at acceptable and reducing prices. More advanced technologies of even higher
13641913	9782	request. Even the TV station could utilize this Internet channel for more interactivity between the TV viewers. This service model fits well into many existing architectures, such as Active Network  and Content Service Network , and the video composition can be done wherever most appropriate in terms of efficiency, cost and convenience. Secondly, a “Content Preparation” component will
13641913	12716	5. EXPERIMENTAL RESULTS We have implemented the first version of the realtime VOE filter with functions such as video Picture-in-Picture and image embedding, as part of the Active Space project . HDTV quality MPEG2 streams are multicast through Gigabit LAN, and client PCs receive the stream and output it to dedicated plasma displays. Our VOE service gateway intercepts the stream and
8920214	1249	5. Do you have suggestions for the yearly follow-up system architecting event? 6. Do you have interest in a 1 week course zooming in on more specific system design methods, based on my Thesis. 7. Any other feedback? The response was 40 filled in forms, 21 participants could not longer be reached because they moved and their new address was unknown. The average time between following the
12089	12091	in an OWL ontology for automatic composition of Web services. It has not yet developed formalisms for optimization on the basis of QoS. An effort that comes closest to our research is Self-Serv , which provides an environment for creation of processes. They have, however, not considered issues like handling dependencies between Web services in a process. Another relevant work  proposed
12089	12093	as key to increasing automation in applying Web services and managing Web processes that take care of interactions between Web services to support business processes within and across enterprises . Academic approaches like WSMO, OWL-S and METEOR-S have tried to approach this solution by using ontologies to describe Web services. This approach is consistent with the ideas of the Semantic Web,
12089	12093	as well as the available Web services. Thus, Web services need semantic annotation and process requirements need to be specified at a high level. These requirements may be specified as goals , application logic (e.g. using extended Golog ) or hierarchal planning constructs . None of the above approaches for automated composition have considered a comprehensive framework for
12089	12094	high level specifications as abstract processes. We use Semantic Web  technologies to represent the requirements for each service in the process. We build on earlier work on automated discovery  of Semantic Web Services based on the users requirements. After discovery, the candidate services must be selected on the basis of process and business constraints. We present a multi-phase
12089	12094	to understand each others QoS terms, a common understanding must be reached on the meaning of the terms. Ontologies can be used to represent and explicate the semantics of these parameters.  have described generic QoS metrics based on time, cost, availability and reliability. We have created an ontology to represent the generic metrics, as well as domain specific QoS metrics. We have
12089	12094	estimated values. We model process constraints as constraints on Quality of Service specifications which were discussed in section 2.3. The process level QoS is calculated as the aggregation of QoS  of all the services in the process. In this implementation, the user has to specify the aggregation operators for QoS parameters. QoS(p) = <T(p), C(p), R(p), A(p), DS1(p), DS2(p),...... DSN (p)>
12089	12094	QoS specifications. Highly intertwined with semantics (and considered in this proposal as part of semantic specification) is the issue of Quality of Service (QoS), pursued from academic setting in , and in industry setting under the Web Service Policy framework . Use of automation in composing Web processes is predicated on having sufficient machine processable information about the
12089	12106	), presented semantic representation of Web services using an ontology based mark-up language. In an effort to be closely aligned to industry standards, we proposed semantic annotation of WSDL . The service advertisements which include the specifications developed in Section 2 are annotated WSDL files and published in our enhanced UDDI registry.sSA = <SLP(SA), OP(SA,o1), …, OP(SA,on)> To
12089	10123	and process requirements need to be specified at a high level. These requirements may be specified as goals , application logic (e.g. using extended Golog ) or hierarchal planning constructs . None of the above approaches for automated composition have considered a comprehensive framework for composition that would optimize selection of Web services on the basis domain specific QoS in
12089	12111	Automated discovery of Web services requires accurate descriptions of the functionality of Web services, as well as an approach for finding Web services based on the functionality they provide.  has discussed classification of services based on their functionality. Another approach tries to define the functionality of a Web service as the transformation of inputs to outputs . Creating
12113	12120	As a result, instead of a more complicated system, we will get a simpler one which happens to also be more powerful. While the foundational intuitions behind Glue have changed very little since (Dalrymple et al., 1993) and have remained intact since (Dalrymple et al., 1995a, 1995b), the formal system for Glue has been evolving. The original Glue system (G0) of Dalrymple et al.(1993) was quickly superseded by the
12113	12120	projections of f-structures were directly given a meaning in G0: “The semantic projection of an f-structure, written with a subscript ?, is a representation of the meaning of that f-structure.”(Dalrymple et al., 1993). But in G1 ‘semantic projections’ lost their initial meaning (excuse the pun). Semantic projections in G1 were not the meaning of their corresponding f-structures but attribute-value structures
12113	12122	constraints of meaning assembly. As we will see soon, the propositional intuitionistic type system of the simply-typed ?-calculus does not, but a type-system based on linear predicate logic(Girard, 1987) does. The first constraint that intuitionistic logic does not capture is that each semantic contribution is to be used exactly once in the forming of the composite sentence meaning. The Weakening
12113	12122	any logic. It only allows an existing assumption to be used an arbitrary number of times. FirstOrderGlue.tex; 18/05/2004; 20:59; p.8 : t : t W eakening ContractionsFirst-Order Glue 9 Linear logic (Girard, 1987) rejects the Weakening and Contraction rules. This guarantees that each semantic contribution (assumption) is used exactly once in the composite semantic representation (conclusion). Special
8604580	12137	the ANOVA model approach. Wolfinger et al.  proposed a mixed effect model for normalization. Schadt et al.  proposed smoothing splines with generalized cross-validation (GCVSS). Kepler et al.  used a local polynomial regression to estimate the normalized expression levels as well as the expression level dependent error variance. Yang et al.  summarized a number of normalization
8604580	12138	cross-validation (GCVSS). Kepler et al.  used a local polynomial regression to estimate the normalized expression levels as well as the expression level dependent error variance. Yang et al.  summarized a number of normalization methods for dual labeled microarrays such as global normalization and robust locally weighted scatter plot smoothing (LOWESS, Cleveland ). Workman et al.
8604580	12138	of intensities: (g fi , g bi ) and (r fi , r bi ), i = 1,..., p, where p is the number of spots in a slide. (For simplicity, we omit the subscript and define A and M using notation of Yang et al.  as follows: A = ? rf ? rb + gf ?gb ? ? ? = ( R)+ ( G) M =  = , (2) ? 1 1 log( ) log( ) ?log log ? 2 2 ? , ( 1) In cDNA microarray experiments, there
8604580	12141	scatter plot smoothing (LOWESS, Cleveland ). Workman et al.  proposed a robust nonlinear method for normalization using array signal distribution analysis and cubic splines. Wang et al.  suggested iterative normalization of cDNA microarray data to estimate normalization coefficients and to identify control gene set. Chen et al.  presented subset normalization to adjust for
8604580	12148	the proposed two-stage normalization methods. Results section describes the results from NCI 60 cDNA microarray experiment, which illustrates the effects of background intensities (Zhou et al. ). In addition, some comparative results are presented from cDNA microarrays of cortical stem cells of rat (Park et al. ) and those from kidney, liver, and testis cells from mice (Pritchard et
8604580	12148	log(background) (a) (b) 6 8 10 12 14 Example Figure 1of correlations between background intensity and signal intensity from one of the NCI 60 data of Zhuo et al.  Example of correlations between background intensity and signal intensity from one of the NCI 60 data of Zhuo et al. 16 (a) log(background) vs. log(R), correlation coefficient: 0.353, (b)
8604580	12148	reanalyzed as an experimental model due to the inaccessibility to human tumor tissues for various studies on cancer. Using HCT116, one of the colon cell lines in the NCI 60 panel, Zhuo et al.  performed gene expression profile of dose- and time-dependent effects by the topoisomerase inhibitor I camptothecin compound (CPT). We here use a subset of the array data set consisting of ten
8604580	12148	better the normalization method, the smaller the variation among the replicated observations. Here, we use three different sets of microarray data: colorectal cancer data of NCI 60 (Zhou, et al. ), cortical stem cells data (Park, et al. ), and mouse gene expression data (Pritchard et al. ). The goal of cortical stem cells study is to identify genes that are associated with neuronal
12157	12158	operates on types instead of values, and an augmented dominator tree verifier (ADTVerifier) that makes sure that control flows (b)s3 CERTIFICATE ABSTRACT MACHINE 10 5 3 4 6 ADT 0 2 7 1 10 (a) 8 9 12 11 block 0 result int param int block 1 decl int loadr 2 loadr 1 loadc int &quot;2&quot; apply irem loadr 2 apply ifne block 2 loadr 1 apply ifne block
12157	12158	3 loadc int &quot;2&quot; apply move loadr 1 loadc int &quot;?1&quot; apply iinc block 10 loadr 1 loadc int &quot;?1&quot; apply iinc loadr 3 loadr 3 loadc int &quot;2&quot; apply imul block 9 loadr 1 apply ifgt block 11 loadr 1 return R:int R:int R:long R:int block 0 result int param int block 1 decl int block 2 ... ifne block 3 decl long ... block 4 ... if_icmpgt block 5 ... R:long decl long ... popfa block 6 ...
12157	12158	the type-check sequence is given below in section 3.4.4. The TypeInterpreter completely ignores control flow instructions. It interprets (b)s3 CERTIFICATE ABSTRACT MACHINE 11 B1 1 rem R R 2 2 ifne R 12 B2 3 ifne R 5 B7 4 return 1 B3 5 i2l R R 6 goto 10 B5 7 iadd R R -1 8 i2l R R 9 imul R R R B4 10 if_icmpgt R 1 7 B6 11 return R B8 12 mov R 2
12157	12158	to leaves are traversed. 3. We then form the union of the results of traversing all paths. Given the dominator tree in Figure 10, the register allocation becomes: F rom B0 down to B5 a : int ? R b : int ? R c : long ? R d : long ? R ... F rome B0 down to B10 a : int ? R b : int ? R e : int ? R Forming the union of the results of traversing all paths, we get a : int ?
12157	12158	classic LengauerTarjan algorithm , which has a timebound of O(E ? log(V)). This means that improvements  in asymptotic complexity may not help on realisticallysized examples. Stephen  has given a practical O(E + V) dominance algorithm limited to reducible graphs. In previous work , we gave a simple O(V + E) algorithm, from a program certification viewpoint, to verify whether
12157	12160	of the proof) effort on the code consumer. An early concern that the proofs were often larger than the programs themselves has been put to rest by proof compaction techniques . Amme et al.  have proposed a mobile-code format based on Static Singles1 INTRODUCTION 4 Assignment Form that is both type-safe and control-safe. This format, SafeTSA, retains the high-level control structures
12157	12160	us recover instruction sequences that have non-overlapping scopes for each register:type pair. Figure 5(b) illustrates this point. The x : int and x : long in the original Java code correspond to R : int and R : long respectively. 3.2 The Verifier The CAM Verifier has two logically indepedent parts: a type-level abstract interpreter (TypeInterpreter) that operates on types instead of
12157	12160	move loadr 1 loadc int &quot;?1&quot; apply iinc block 10 loadr 1 loadc int &quot;?1&quot; apply iinc loadr 3 loadr 3 loadc int &quot;2&quot; apply imul block 9 loadr 1 apply ifgt block 11 loadr 1 return R:int R:int R:long R:int block 0 result int param int block 1 decl int block 2 ... ifne block 3 decl long ... block 4 ... if_icmpgt block 5 ... R:long decl long ... popfa block 6 ... return popfa popfa
12157	12160	The TypeInterpreter completely ignores control flow instructions. It interprets (b)s3 CERTIFICATE ABSTRACT MACHINE 11 B1 1 rem R R 2 2 ifne R 12 B2 3 ifne R 5 B7 4 return 1 B3 5 i2l R R 6 goto 10 B5 7 iadd R R -1 8 i2l R R 9 imul R R R B4 10 if_icmpgt R 1 7 B6 11 return R B8 12 mov R 2 13 iadd R R -1 14 goto 17 B10 15 iadd R R -1 16
12157	12160	We then form the union of the results of traversing all paths. Given the dominator tree in Figure 10, the register allocation becomes: F rom B0 down to B5 a : int ? R b : int ? R c : long ? R d : long ? R ... F rome B0 down to B10 a : int ? R b : int ? R e : int ? R Forming the union of the results of traversing all paths, we get a : int ? R, b : int ? R, c : long ?
12157	12161	et al.  published a theoretical linear-time complexity dominance algorithm, but the actual complexity of the algorithm, using practical data structures, is O(E + VlogloglogV). Cooper’s work  on fast dominance algorithms has demonstrated that a well-engineereds3 CERTIFICATE ABSTRACT MACHINE 21 O(V 2 ) dominance algorithm runs faster, in practice, than the classic LengauerTarjan
12157	12162	that operates on types instead of values, and an augmented dominator tree verifier (ADTVerifier) that makes sure that control flows (b)s3 CERTIFICATE ABSTRACT MACHINE 10 5 3 4 6 ADT 0 2 7 1 10 (a) 8 9 12 11 block 0 result int param int block 1 decl int loadr 2 loadr 1 loadc int &quot;2&quot; apply irem loadr 2 apply ifne block 2 loadr
12157	12162	of functional languages. ECC limits its inputs tofunctional source code. Therefore, it does not need to deal with arbitrary jumps (even irreducible control flow graphs) as our approach does. Davis  experiments with converting the stack-based JVML into a register basedinstruction set. His experiment shows that the transformation reduces the number of executed instructions by 34.88% and
12157	12163	that operates on types instead of values, and an augmented dominator tree verifier (ADTVerifier) that makes sure that control flows (b)s3 CERTIFICATE ABSTRACT MACHINE 10 5 3 4 6 ADT 0 2 7 1 10 (a) 8 9 12 11 block 0 result int param int block 1 decl int loadr 2 loadr 1 loadc int &quot;2&quot; apply irem loadr 2 apply ifne block 2 loadr
12157	12163	type checking is sufficient to check the well-typedness of source code in these languages. Type-checkers have been widely implemented in compilers for strongly typed languages. Abstract machines  simulate real hardware machines by allowing step-bystep execution. They bridge the semantic gap between high level languages and low level real machines. The Java Virtual Machine is the abstract
12157	12167	augmented dominator tree verifier (ADTVerifier) that makes sure that control flows (b)s3 CERTIFICATE ABSTRACT MACHINE 10 5 3 4 6 ADT 0 2 7 1 10 (a) 8 9 12 11 block 0 result int param int block 1 decl int loadr 2 loadr 1 loadc int &quot;2&quot; apply irem loadr 2 apply ifne block 2 loadr 1 apply ifne block 7 loadc long &quot;1&quot; return block 3 decl long
12157	12167	relatively complex in both time and space consumption. KVM  for resource limited devices adopts a lightweight verification algorithm  instead of the full Bytecode Verifier. Kozen’s  work on Efficient Code Certification (ECC) has a nature contextfree structure which mirrors the structure of high-level functional languages. ECCissimilar to SafeTSA (introduced in the
12157	12168	augmented dominator tree verifier (ADTVerifier) that makes sure that control flows (b)s3 CERTIFICATE ABSTRACT MACHINE 10 5 3 4 6 ADT 0 2 7 1 10 (a) 8 9 12 11 block 0 result int param int block 1 decl int loadr 2 loadr 1 loadc int &quot;2&quot; apply irem loadr 2 apply ifne block 2 loadr 1 apply ifne block 7 loadc long &quot;1&quot; return block 3 decl long
12157	12168	fast dominance algorithms has demonstrated that a well-engineereds3 CERTIFICATE ABSTRACT MACHINE 21 O(V 2 ) dominance algorithm runs faster, in practice, than the classic LengauerTarjan algorithm , which has a timebound of O(E ? log(V)). This means that improvements  in asymptotic complexity may not help on realisticallysized examples. Stephen  has given a practical O(E + V)
12157	12169	uses first-order logic to represent correctness properties and a safety proof is constructed based on a general system such as Edinburgh LF and carried along with code. • Typed Assembly Language  preserves typing information from a highlevel program written in a strongly-typed language and includes it with the compiled program. It can then be checked by an ordinary type checker. The JVM is
12157	882	code generation. In the meantime, alternative schemes for mobile-code distribution have appeared that differ from the model exemplified by the JVM and the CLR. Proofcarrying code (PCC) approaches  involve the code producer in the verification process by obliging it to construct a proof of safety. The code consumer computes a verification condition from the received program and checks whether
12157	882	RELATED WORK 25 code conforming to the semantics of high level source code is not trivial and less efficient. However, encouraging progress has been made in recent years: • Proof Carrying Code  uses first-order logic to represent correctness properties and a safety proof is constructed based on a general system such as Edinburgh LF and carried along with code. • Typed Assembly Language
12157	12170	code generation. In the meantime, alternative schemes for mobile-code distribution have appeared that differ from the model exemplified by the JVM and the CLR. Proofcarrying code (PCC) approaches  involve the code producer in the verification process by obliging it to construct a proof of safety. The code consumer computes a verification condition from the received program and checks whether
12157	12171	linear (in the length of the proof) effort on the code consumer. An early concern that the proofs were often larger than the programs themselves has been put to rest by proof compaction techniques . Amme et al.  have proposed a mobile-code format based on Static Singles1 INTRODUCTION 4 Assignment Form that is both type-safe and control-safe. This format, SafeTSA, retains the high-level
12157	12172	flow and data flow (Figure 1), the JVM bytecode verifier may need to iterate until it reaches a stable state in which no new variable or new type is produced. This fixed-point iteration algorithm  has a worst-case performance that is quadratic. In a recent paper, Gal et al.  have shown how one can systematically construct JVML programs that exhibit worst-case verification behavior and use
12157	12173	the main costs of an interpreter is that for instruction dispatch. Davis’ work has demonstrated that virtual register machines are an attractive alternative to virtual stack machines. Rose and Rose  propsed a (sparse) annotation of JVM code with types to enable a one-pass verification of well-typedness. Roughly speaking, this transforms a type reconstruction problem into a type checking
12157	12175	time: it is sufficient to store only the state type for the entry point to each basic block because the remaining state types in that block can be computed in linear time.s6 CONCLUSION 26 Sreedhar  proved a weaker version of lemma 1. D-J Graphs are an alternative program representation used in loop identification, elimination-based data flow analysis and for placing ? nodes in linear time. 6
12157	12177	the applicability of representations such as SafeTSA. Moreover, SSA-based representations are meant to be compiled on-the-fly rather than interpreted. A recent attempt at interpreting SSA directly  has resulted in success, but at the expected low performance point. Hence, for the foreseeable future, SSA-based mobile code formats are likely to remain restricted to workstation-class devices
12179	12180	increasingly important in acoustical applications for spatial filtering. In the absence of any known source signals (blind case), a blind update equation similar to the natural gradient method  is presented, a derivative of which can be used in the case of known references (non-blind case). If some, but not all, source signals are known, blind-only algorithms are suboptimal, since some
12179	12180	to be separated. Lately, these tasks have been approached using information-theoretic methods  or, more generally, higher-order statistics . Fast convergence can be achieved using the natural  or relative  gradient. In this paper we will deal with the separation problem only (no deconvolution), i.e., the signals x =  T to be processed are linear mixtures of the original
12179	12180	scaled permutation matrix. A possible update equation for the separation matrix W results from the minimization of the mutual information of the output signals  and applying the natural gradient  Wt+1 = Wt + µ ? I ? g(u)u T ? Wt, (3) where µ is the step-size, I the identity matrix. g(u) is known as the Bussgang nonlinearity orthescore function and depends on the pdf of the source signals
12179	12180	the separation progress during convergence, a scalar number is needed that describes the average degree of the residual mixing of the separated ub sources at the output of the network. Amari et al.  define the performance index as a ratio of the sum of absolute values of unwanted sources leaking through and the absolute value of the desired source. We define our performance measure based on
12179	12181	as a single-layer neural network, whose training algorithms depend partially on the kind of signals to be separated. Lately, these tasks have been approached using information-theoretic methods  or, more generally, higher-order statistics . Fast convergence can be achieved using the natural  or relative  gradient. In this paper we will deal with the separation problem only (no
12179	12181	well, WA = P should become close to a scaled permutation matrix. A possible update equation for the separation matrix W results from the minimization of the mutual information of the output signals  and applying the natural gradient  Wt+1 = Wt + µ ? I ? g(u)u T ? Wt, (3) where µ is the step-size, I the identity matrix. g(u) is known as the Bussgang nonlinearity orthescore function and
12179	12182	algorithms depend partially on the kind of signals to be separated. Lately, these tasks have been approached using information-theoretic methods  or, more generally, higher-order statistics . Fast convergence can be achieved using the natural  or relative  gradient. In this paper we will deal with the separation problem only (no deconvolution), i.e., the signals x =
12179	12183	Lately, these tasks have been approached using information-theoretic methods  or, more generally, higher-order statistics . Fast convergence can be achieved using the natural  or relative  gradient. In this paper we will deal with the separation problem only (no deconvolution), i.e., the signals x =  T to be processed are linear mixtures of the original source signals s =
12179	12186	unknown sources considerably. Both approaches can be extended to the multichannel blind deconvolution case where the elements of matrix A are filter polynomials by using FIR-matrix algebra . In this case the mixing matrix A(z) aswellasthe separation matrix W(z) contain filter polynomials as their matrix elements. ACKNOWLEDGEMENT The authors would like to thank Russell Lambert and
12187	12187	a vertex that is adjacent to all the other vertices, then such levelling exists, by making the edges of the appropriate star flat. A comprehensive introduction to random lifts can be found in . 1.2 Minors and topological minors Definition 1.3. We recall that a minor H of G (denoted H ? G) is a graph that is obtained from G by a series of edge contractions. Thus, each vertex v ? V (H)
12187	12189	exists a real c > 0 such that for every h ? N, every graph of average degree d ? ch ? log h has a K h minor. This bound is tight up to the value of c. Theorem 1.7 (Bollobás, Catlin, Erd??s 1980 ). For almost every graph G with average degree d > 2, ?(G) < n ? log n?d log log n Theorem 1.8 (Thomason 1999 ). Let c(t) be the minimum number such that for every graph G with average degree d
12198	12207	itself essentially in changing the distribution of fading. Tendency to Gaussianity with the increase of diversity is mentioned also in  and others. Some additional results are reported in , where capacity for Nakagami channels with or without diversity for different power strategies, as optimal power/rate adaptation, constant rate, and constant power, is evaluated. See also ,
12198	12850	in , where capacity for Nakagami channels with or without diversity for different power strategies, as optimal power/rate adaptation, constant rate, and constant power, is evaluated. See also , where capacity is evaluated for three strategies: optimal power and rate adaptation, optimal rate adaptation, and channel inversion or constrained channel inversion. Maximal-ratio and selection
12198	12212	mathematically but conceptually, calling for a serious unification between information and network theory ,  and so far only very rare pioneering efforts have been reported , , . We have not yet touched upon signaling constraints, imposed on each user which transmits several signals through the available transmitting antennas. The standard constraints are as follows. a)
12198	12212	combinination of information and network (including queuing) theories, essential for a deep understanding of practical communications systems, is at best in its infancy stages , , , , . Decisive results on the joint source-channel coding in time-varying channels are still needed, which would incorporate various decoding constraints (as delay, etc.) motivated by practical
12198	12214	in the fading-channel realm, has been very thoroughly investigated for several decades. See  for example, and the reference list , –, , , , , , , , , , , , , , ,  which forms just a small unrepresentative sample of the available literature. As already said, cutoff rates were denied in recent years
12198	12215	to time-varying channels. Some additional relevant references not cited in the text are , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . By no means is this list complete or even close to complete:
12198	12217	metric irrespective of its suboptimality. The full extent of the information-theoretic problem, as reflected by the ultimate mismatched achievable rates, is not yet solved, but for binary inputs . However, numerous bounds and fundamental insights into that problem have been reported. See  for a selected list of references and for further details. One of the more interesting and
12198	12221	of information and network (including queuing) theories, essential for a deep understanding of practical communications systems, is at best in its infancy stages , , , , . Decisive results on the joint source-channel coding in time-varying channels are still needed, which would incorporate various decoding constraints (as delay, etc.) motivated by practical
12198	12225	Many other models for cellular communication were studied via information-theoretic tools, and the reference list provided here includes dozens of relevant entries. See for example , , , , , , , , , , , , , and . Some of the most interesting results concern the -out-of- model , which captures the fact that although there are
12198	12232	models for cellular communication were studied via information-theoretic tools, and the reference list provided here includes dozens of relevant entries. See for example , , , , , , , , , , , , , and . Some of the most interesting results concern the -out-of- model , which captures the fact that although there are many
12198	12248	in both time and frequency. See also some results by  on this matter, and for the first extension (of water filling) to the multiple-user case over fixed intersymbol-affected Gaussian channels . While ,  considered the throughput (sum rate), in a remarkable work  the polymatroidal structure of the multiaccess Gaussian capacity region was exploited so as to provide an elegant
12198	12253	Some information-theoretic notions to be treated in the following, as capacity-versus-outage, delay-limited capacity, and expected capacity, are intimately related with compound  and composite  channels, and they apply directly to the case where CSI is available to the transmitter only. This setting, when the fading CSI is available to the transmitter only, poses some interesting
12198	12253	rate. The above notion is strictly connected to the classical compound channel with a priori associated with its transitionprobability-characterizing parameter This is a standard approach: see ???; in  this channel is called a composite channel. The capacity-versus-outage approach has the simple interpretation that follows. With any given rate we associate a set That set is the
12198	12253	since is constant, its rate for long codes, , goes to zero, and therefore it can be accurately estimated at the receiver site. Transmit, for example, a training sequence with length proportional to  to facilitate the accurate estimation of , at no cost of rate as In fact, the value of is not at all required at the receiver, which employs universal decoders , , [164, and references
12198	12253	is calculated for flat fading with strict coherence time (the block-fading model). The usefulness and relevance of the capacity-versus-outage results, as well as variants to the expected capacity  to be discussed later in the context of fading , are usually not emphasized explicitly in the literature in the context of unavailable CSI, in spite of their considerable theoretical
12198	12253	approach: The broadcast approach for a compound channel is introduced in Cover’s original paper ons2634 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 the broadcast channel . The maximization of the expected capacity is advocated, attaching a prior to the unknown state that governs the compound channel transition probability. This class of channels with a prior was
12198	12900	references not cited in the text are , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . By no means is this list complete or even close to complete: hundreds of directly relevant references were
12198	12274	a fixed number of users transmitting to a receiver. In common models for communication (network) systems, a user accesses the channel randomly, as it gets a message to be transmitted , , . The random access of users is a fundamental issue which is not yet satisfactorily treated in terms of information-theoretic concepts , ,. Indeed, the -out-of- model discussed is
12198	12274	channels in general and fading MAC in particular (for more examples see ). These interesting observations motivate serious study into this yet immature branch of information theory , . Another possibility is to consider the average rate per a specific user, where this rate is measured only while that user is active. Under the assumption of all individual users independently
12198	12274	as an intimate combinination of information and network (including queuing) theories, essential for a deep understanding of practical communications systems, is at best in its infancy stages , , , , . Decisive results on the joint source-channel coding in time-varying channels are still needed, which would incorporate various decoding constraints (as delay, etc.) motivated by
12198	12282	examples are the space–time codes, which attempt to benefit from the dramatic increase in capacity of spatial diversity in transmission and reception, i.e., multiple transmit and receive antennas , , , , . The recently introduced efficient turbo-coded multilevel modulation schemes  and the bit interleaved coded modulation (BICM) , as a special case, were
12198	12282	antennas is considered, shows the substantial benefit of this diversity, which in fact may yield information rates that increase linearly with the number of (transmit/receive) antennas. See also , where capacity calculations of systems with transmitter and receiver antennas, with the receiver equipped with full CSI, is evaluated. A nice lower bound is presented, which gives rise to a
12198	12929	have assumed a fixed number of users transmitting to a receiver. In common models for communication (network) systems, a user accesses the channel randomly, as it gets a message to be transmitted , , . The random access of users is a fundamental issue which is not yet satisfactorily treated in terms of information-theoretic concepts , ,. Indeed, the -out-of- model
12198	12929	channels in general and fading MAC in particular (for more examples see ). These interesting observations motivate serious study into this yet immature branch of information theory , . Another possibility is to consider the average rate per a specific user, where this rate is measured only while that user is active. Under the assumption of all individual users
12198	12929	as an intimate combinination of information and network (including queuing) theories, essential for a deep understanding of practical communications systems, is at best in its infancy stages , , , , . Decisive results on the joint source-channel coding in time-varying channels are still needed, which would incorporate various decoding constraints (as delay, etc.)
12198	12931	,  that is a nondecreasing function of for i.i.d. nonnegative random variables thus establishing the advantage of CDMA over TDMA and FDMA under this fading model. For further details, see , , , , , and . A natural question that arises here is how orthogonal CDMA compares to the optimum (3.4.3) and to orthogonal TDMA and FDMA (3.4.5). In orthogonal CDMA, all
12198	12931	realm, the optimal power control yields a substantial growth in capacity, increasing with the number of users , with respect to the fixed-transmitted-power case with the optimal CDMA strategy  also over the Gaussian classical unfaded maximum sum rate . The intuition for this result  is that if is large, then with high probability at least one of the i.i.d. fading powers will be
12198	12931	for some additional results including error exponents). This model has been investigated in , , , , in combination with CDMA, where  focuses on the fading effect. Parallel to , it is demonstrated that CDMA is inherently advantageous over FDMA in the presence of fading. Up to now we have assumed a fixed number of users transmitting to a receiver. In common models for
12198	12931	input distribution is discrete. It is instructive to learn that, while for available CSI at the receiver the CDMA channel accessing technique is advantageous in a fading environment , TDMA prevails in the case of no CSI. If CSI is available both to transmitters and receiver (in a centralized manner), again a TDMA-like (i.e., randomized TDMA) becomes optimal , where the
12198	12931	capacity limit, under average-power constraints , as we have seen in the previous section, this is no more so when fading is present. It was demonstrated, for example, that CDMA is advantageous  for known CSI at the receiver, whilesBIGLIERI et al.: FADING CHANNELS: INFORMATION-THEORETIC AND COMMUNICATIONS ASPECTS 2651 TDMA is preferred when no CSI is available . Under the standard
12198	12942	and receiver receiver is given by (3.4.29) at the top of this page, where denotes the indicator function. The case of CSI available to both receiver and transmitter is treated in , , , and then and may depend on the CSI ( and here) as indicated explicitly in (3.4.29). The region given by the union in (3.4.29) is then maximized over all assignments of satisfying (3.4.30) In
12198	12942	(TDMA) and frequency (FDMA) division techniques for the fading broadcast channel was examined and compared to the optimal code-division (CDMA) approach. Note, however, that taking , as in  and , is a suboptimal selection. In fact, the optimality of time division for the broadband broadcast channel stems directly by . The rate region of the broadcast channel with CSI available to the
12198	12942	result on the capacity of spectrally shaped broadcast channels see , , and references therein. The broadcast channel is used to model downlinks in cellular communication (see , , and ), and this will further be addressed in the following. The fading-interference channel: The interference channel ,  is a very important model which accounts for the case where
12198	12942	realization that affects the respective user. In the following these interesting broadcast and interference channels models will be mentioned in reference to cellular communications (see , , and ). 7) Cellular Fading Models: The rapidly emerging cellular communications spurred much theoretical research into fading channels, as the time-varying fading response is the basic
12198	12942	for FDMA using the well-known time-frequency duality. In this work we have restricted our attention to the uplink channel. Yet, some conclusions can be drawn regarding the downlink channel as well , . The results of  for a TDMA intracell accessing, yielding a single active transmission per cell, are relevant here. This is because all users are equivalent with respect to the downlink
12198	12302	constraints. This example also demonstrates an interesting new features of this informationtheoretic problem, as for example, the state-constrained AVC capacity region is nonconvex in general . The possibly different results for deterministic versus random codes as well as average and maximum error probability criteria (see  for details) implies here operational practical insight of
12198	12306	Some additional relevant references not cited in the text are , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . By no means is this list complete or even close to complete: hundreds of directly
12198	12312	ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 but has enriched information theory itself, and introduced interesting notions. This is illustrated by the notion of delaylimited capacity , , the polymatroidal property of the multiple-user capacity region , and the like. It is the practical constraints to which various communications systems are subjected which gave rise to
12198	12312	provide the correct intuition into the specific expressions, as will be detailed in the following, resorting first to a very simple single-user flat-fading channel model , , , , , . We notice that the assumption of joint ergodicity of plays a fundamental role: in fact, without it the Shannon sense capacity, where the decoded error probability can be driven to zero by
12198	12312	where the channel-state information is made available also to the receiver. This capacity (where CI stands for Channel Inversion), is, in this case, also what is known as the delay-limited capacity . This is a special case of the capacity-versus-outage framework in case of CSI available at the transmitter and receiver , as it will be briefly described later on. The difference between and
12198	12312	information-theoretic problems. Concluding remarks: Although in this subsection we have only used simple channel models which cannot accommodate multipath, intersymbol interference, and the like, (, , , ), the basic structure of these capacity results is maintained also when they are extended to more general settings, as will be demonstrated succinctly in the following
12198	12312	depends on the fading statistics only through a threshold value, below which the transmission is turned off. The rate which corresponds to zero outage is associated with the delay-limited capacity : in fact, the power-control strategy which gives rise to a zero outage probability gives also rise to the standard (Shannonsense) capacity. For the case of (single block) the optimal strategy is
12198	12990	on the capacity of spectrally shaped broadcast channels see , , and references therein. The broadcast channel is used to model downlinks in cellular communication (see , , and ), and this will further be addressed in the following. The fading-interference channel: The interference channel ,  is a very important model which accounts for the case where the
12198	12990	that affects the respective user. In the following these interesting broadcast and interference channels models will be mentioned in reference to cellular communications (see , , and ). 7) Cellular Fading Models: The rapidly emerging cellular communications spurred much theoretical research into fading channels, as the time-varying fading response is the basic ingredient in
12198	12990	using the well-known time-frequency duality. In this work we have restricted our attention to the uplink channel. Yet, some conclusions can be drawn regarding the downlink channel as well , . The results of  for a TDMA intracell accessing, yielding a single active transmission per cell, are relevant here. This is because all users are equivalent with respect to the downlink
12198	12990	the cell site, transmits to many users (the mobiles). Usually a single-cell scenario is considered, while intercell interference, when accounted for, is added to the ambient Gaussian noise (see , , , , and ). In fact, a better model accounting in a more elementary way for the intercell interference is the broadcast/interference model. That is, since the cell site (the
12198	12341	of statistical time-varying channels is new in information theory, and in fact by now this topic is considered as classic , with Shannon himself contributing to some of its aspects  (see  for a recent tutorial exposition, and references therein). Fading phenomena were also carefully studied by informationtheoretic tools for a long time. However, it is only relatively recently that
12198	12341	on the behavior of is not always available. This gives rise to the use of mismatched metrics and universal decoders , and makes classical notions of compound and arbitrarily variable channels , along with the large body of associated results, relevant to our setting. Central notions as random-versus-deterministic code books and maximum- versus average-error probabilities emerge naturally
12198	12341	case gives rise to interesting information-theoretic settings as capacity versus outage, broadcast interpretation, and delay-limited capacities, all relying on notions of compound channels , . The fact that a specific channel is underspread in the terminology of Section II, i.e., , implies that it can be treated as a flat slow-fading process, but nevertheless the total transmission
12198	12341	by eliminating the fading absolutely, which gives rise to the delaylimited capacity notion. This will be further addressed within the notions of compound and arbitrarily varying channels , , with constrained input and state spaces. We shall demonstrate the general expression in the case of flat fading with inputs subjected to an average-power constraint, that is, (3.3.8) where , the
12198	12341	we use the natural extensions of the finite-state expressions, leaving the details to the references (in case these are available, which unfortunately might not always be so). See reference list in  and . We shall demonstrate the general setting for the most simple model of a single-user, flat fading case where the signaling is subjected to an average-power constraint. The discretetime
12198	12344	within the framework of , as the transition probabilities for the code block transmitted are explicitly given and fully characterized statistically. See  for further discussion. See also  for coding theorems of compound channels with memory. Delay-limited capacities: The notion of a delay-limited capacity has already been referred to before, in the context of capacity versus outage
12198	13009	rate , determining both an achievable rate and the magnitude of the random-coding error exponent, serves as another interesting information-theoretic notion. Although, contrary to past belief , it is no more considered as an upper bound on practically achievable rates, yet it provides a useful bound to the rates where sequential decoding can be practically used. In any case, it is a most
12198	13019	to be referred to in the following. Some practical implication of successive interference cancellation as the effect of imperfect cancellation and residual noise is addressed in , , , , , and . The effect of successive cancellation on other information-theoretic measures, such as the cutoff rate is discussed in , where this measure does not seem to be a natural
12198	12360	We shall further refer to similar results in the context of imperfect CSI at the receiver. The Gaussian-based mismatched metric has been applied for a variety of cases; see examples in  and . In a variety of cases with special practical implications, the receiver has at its disposal imperfect channel-state information, which it uses to devise the associated decoding metric. Many works
12198	12366	cases of long-signature sequences spreading many coded symbols are stressed. Other nonlinear front-end detectors, as decision-feedback decorrelator and MMSE processors, were also addressed (see  and ) and shown to be very efficient. The literature on the information-theoretic aspects of this issue is so vast that the references here, along with the reference lists therein,
12198	12366	studied via information-theoretic tools, and the reference list provided here includes dozens of relevant entries. See for example , , , , , , , , , , , , , and . Some of the most interesting results concern the -out-of- model , which captures the fact that although there are many potential users per cell, only a relatively
12198	12367	max is taken over the users and the min over respective powers. The algorithm in  for the optimal resource allocating solves simultaneously also the call admission problem. Related results in  address another criterion, the minimum transmit energy for a given rate vector and given realizations of the fading (referred to in  as channel attenuations). It is shown similarly to
12198	12367	Successive cancellation plays a major role in network information theory from both theoretical and practical viewpoints. So far we have addressed some works , , , , , , , , , , ,  which demonstrate the theoretical optimality of this method in a rather general multiple-access framework, which also accounts for flat as well as
12198	12367	It is shown that the optimal power strategy with a minmax power allocation criterion manifests itself so that the rates are amenable to successive decoding. A similar result is reported in , where the minimum transmit energy for a given rate vector with different and constant attenuation is considered. While for a single receiver the best energy assignment gives rise to successive
12198	12367	power distribution is used, motivated by its optimality under average-power constraints, in the singlecell delay-limited capacity problem as well as under average transmit power constraints . First, the proper ordering of the closest user to the cell-site receiver, operating at high power in the successive cancellation procedure, is decided upon. Those in the vicinity of the cell
12198	12367	undermines the arguments in  and  claiming limited incentive of employing optimal multiuser processing in case where intercell interference is present. Similar results can be deduced from  by reinterpreting the different attenuations to which different users are subjected and accounting also for the multiple-cell interference. The reference to Wyner’s model in  is inappropriate,
12198	13050	account more closely for classical constraints such as the inherent lack of synchronization, presence of memory, and the associated information-theoretic implications (see, for example, , , , ) in the fading regime are yet to be studied.s2662 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 Arbitrarily varying channels and compound multiuser channels
12198	12391	are the space–time codes, which attempt to benefit from the dramatic increase in capacity of spatial diversity in transmission and reception, i.e., multiple transmit and receive antennas , , , , . The recently introduced efficient turbo-coded multilevel modulation schemes  and the bit interleaved coded modulation (BICM) , as a special case, were motivated by
12198	12391	while many others can be traced by scanning the information-theoretic literature. The classical orthogonalization which decomposes the original dispersive channel into parallel channels , , ,  is fundamental not only for a conceptually rigorous derivation of capacity, but carries over basic insight into the very implementation of information-theoretic inspired signaling
12198	12391	antennas case, thus facilitating the comparison of the performance of specific coding approaches (as the recently introduced space/time coding technique ), to the ultimate optimum. In , a discrete model for the time-invariant multipath fading, with paths and, respectively, and transmit and receive antennas is considered. The information capacity is studied and forms of
12198	12408	in the text are , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . By no means is this list complete or even close to complete: hundreds of directly relevant references were not included, but rather
12198	12416	capacity when CSI is available either to the receiver or to both receiver and transmitter. This is evident from the fact that even the first-order statistics do not match the Gaussian statistics . It is interesting to observe that the lower the SNR, the larger the amplitude tends to be . The intuition behind this result, already contained in , follows the observation that For SNR ,
12198	12416	Inspired Signaling: Optimal Parameter Selection: In general, the capacity as well as the capacity-achieving distribution imply some underlying structure of optimal coding/signaling . This is valid also in the realm of fading, and even more so, as information theory dictates some parameters of close-to-optimal coding systems. This is best demonstrated by the results for the
12198	13081	can never destroy the inherent orthogonality of orthogonal FDMA or orthogonal TDMA, but can do so in the case of orthogonal CDMA. For further details on random CDMA in a flat-fading regime, see . Another model, where orthogonal CDMA, TDMA, and FDMA are interpreted as different ways of using degrees of freedom in a fading regime, is considered in , where it is shown that, as far as
12198	13081	which is to account for the presence of multiple users, and the standard decoder for the code, is the issue of , where the linear minimum mean-squareerror demodulator is advocated (see also , , and references therein). Multiuser information-theoretic aspects of DS-SS coded systems has been thoroughly examined  (see also ). Many studies examine exactly the approach,
12198	13081	along with the reference lists therein, provide hardly more than a glimpse on this issue (see  and references therein for more details). Much less work has been done on the fading regime. In , two fading models were addressed: the homogeneous model affects each chip independently, while the slow-fading model operates on the coded symbols, where that fading process is either correlative
12198	13081	aspects of coded DS-SS, random versus deterministic CDMA, the role of multiuser detection in this setting, and the like. Some of these misconceptions are dispelled in  (see also , , and references therein). The struggle to achieve the full promise of information theory in the network multiple-user systems by adhering to the more familiar single-user techniques gave
12198	12428	where is the average transmitted power (all Rayleigh fading coefficients are normalized to unit power), and is the associated Laguerre polynomial. Using the asymptotic eigenvalue distribution of  yields, for example, for , the result (3.3.59) which demonstrates the substantial linear (in ) increase in the reliable rate. In fact , the result in (3.3.59) is invariant to the actual fading
12198	12429	been well established for decades with new touches motivated by practical technological achievements, reported systematically over the years (see , , , , , , , ,  for some recent developments). Neither the treatment of statistical time-varying channels is new in information theory, and in fact by now this topic is considered as classic , with Shannon
12198	12432	thoroughly investigated for several decades. See  for example, and the reference list , –, , , , , , , , , , , , , , ,  which forms just a small unrepresentative sample of the available literature. As already said, cutoff rates were denied in recent years (especially with the advent of turbo codes and
12198	12443	codes, which attempt to benefit from the dramatic increase in capacity of spatial diversity in transmission and reception, i.e., multiple transmit and receive antennas , , , , . The recently introduced efficient turbo-coded multilevel modulation schemes  and the bit interleaved coded modulation (BICM) , as a special case, were motivated by
12198	12443	apply to the multiple transmitting/receiving antennas case, thus facilitating the comparison of the performance of specific coding approaches (as the recently introduced space/time coding technique ), to the ultimate optimum. In , a discrete model for the time-invariant multipath fading, with paths and, respectively, and transmit and receive antennas is considered. The information
12198	12443	increases linearly with the minimum diversity supplied by the multiple transmit/receive antennas as is well known. The coding scheme advocated in the paper should be compared with that in  and . Elegant rigorous analytical results are provided in , where the multi-input multi-output single-user fading Gaussian channel is investigated with CSI available to the receiver. Equations for
12198	12443	with Transmit- and Receive-Antenna Diversity: Space–Time Codes: As of today, the most promising coding schemes with transmit- and receive-antenna diversity seem to be offered by “space–time codes” . These can be seen as a generalization of a coding scheme advocated in , where the same data are transmitted by two antennas with a delay of one-symbol interval introduced in the second path.
12198	12443	The diversity gain provided by space–time codes equals the rank of certain matrices, which translates the code design task into an elegant mathematical problem. Explicit designs are presented in , based on 4-PSK, 8-PSK, and 16-QAM. They exhibit excellent performance, and can operate within 2–3 dB of the theoretical limits. E. Coding with CSI at Transmitter and Receiver An efficient coding
12198	13110	codes, which attempt to benefit from the dramatic increase in capacity of spatial diversity in transmission and reception, i.e., multiple transmit and receive antennas , , , , . The recently introduced efficient turbo-coded multilevel modulation schemes  and the bit interleaved coded modulation (BICM) , as a special case, were motivated by informationtheoretic
12198	13110	multiple transmit/receive antennas as is well known. The coding scheme advocated in the paper should be compared with that in  and . Elegant rigorous analytical results are provided in , where the multi-input multi-output single-user fading Gaussian channel is investigated with CSI available to the receiver. Equations for the capacity, capacity distribution (i.e., capacity versus
12198	13110	are provided. The methodology is based on the distribution of the eigenvalues of random matrices. The exact nonasymptotic distribution of the unordered eigenvalue is known  and used in  to compute the capacity of a single-user Gaussian channel with transmitters and receivers, where each transmitter reaches each receiver via an independent and identically distributed Rayleigh
12198	13110	far as practical applications and implications are concerned. The asymptotic result is easily extendible to the case where while is a fixed number, not necessarily unity, as in in the example above , sBIGLIERI et al.: FADING CHANNELS: INFORMATION-THEORETIC AND COMMUNICATIONS ASPECTS 2643 While the results of  apply to the case where perfect CSI is available at the receiver,
12198	13110	systems experiencing relatively fast fading. Clearly, if is large, the substantial gain of transmit diversity is attainable, as the ideal assumption of perfectly known CSI (say, at receiver only ) is realistic and can be closely approached in practice. Also, this striking outcome can be understood within the framework of the relation (3.3.27), which yields here (3.3.60) where is the
12198	12450	parallel broadcast channels first addressed in  can be used to model the presence of memory (intersymbol interference); optimal power allocation, under average power constraint is addressed in  and . For classical result on the capacity of spectrally shaped broadcast channels see , , and references therein. The broadcast channel is used to model downlinks in cellular
12198	12451	theory itself, and introduced interesting notions. This is illustrated by the notion of delaylimited capacity , , the polymatroidal property of the multiple-user capacity region , and the like. It is the practical constraints to which various communications systems are subjected which gave rise to new notions, as the “delaylimited capacity region” , capacity versus
12198	12451	problems. Concluding remarks: Although in this subsection we have only used simple channel models which cannot accommodate multipath, intersymbol interference, and the like, (, , , ), the basic structure of these capacity results is maintained also when they are extended to more general settings, as will be demonstrated succinctly in the following sections. We have also
12198	12451	channels play a central part in many other applications. Also there, including, for example, satellite communications , , underwater channels, which exhibit particularly harsh conditions , , ,  16 informationtheoretic analysis is providing insight into the limitations and potentials of efficient communications. With this in mind, we have constructed a rather extensive
12198	13123	Another example, where there is interplay between standard DFE procedures and the multiple-user regime, is the model of , where again it suits the standard vector single-user Gaussian channel ,  with i.i.d. inputs of the multipleuser regime . In all those examples, which were originally developed for Gaussian nonfading channels, the fading effect can be straightforwardly
12198	12457	Extensive study of this issue has been undertaken, as evidenced in the small sample of references , , , , , , , , , , , , , , , and references therein. Characterization of the properties of these schemes, which still maintain optimality in terms of throughput on the AWGN channel, was addressed in . It was shown
12198	12460	diversity, the channel inversion works very well and is almost optimal. As expected, capacity of the AWGN channel is approached with the increase of the number of diversity branches. See also , , , , , and many other references provided in the reference list here and reference lists of the cited papers. With no CSI, it has been demonstrated in  that for the Rayleigh fast
12198	12460	much easier notion to evaluate in the fading-channel realm, has been very thoroughly investigated for several decades. See  for example, and the reference list , –, , , , , , , , , , , , , , ,  which forms just a small unrepresentative sample of the available literature. As already said, cutoff rates were
12198	12460	additional reference list, focusing on information-theoretic approaches to time-varying channels. Some additional relevant references not cited in the text are , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
12198	12460	that, with several diversity branches, the probability that the signal will be simultaneously faded on all branches can be made small. Another approach, which was investigated by the authors in , , and , is philosophically different, as it is based upon the observation that, under fairly general conditions, a channel affected by fading can be turned into an additive white
12198	12460	code which implicitly does time averaging, even when no diversity is present. Fig. 7 shows the block diagram of the transmission scheme with fading and cochannel interference. The assumptions are , , and  as follows. 1) PSK modulation. 2) independent diversity branches whose signal-to-noise ratio is inversely proportional to (this assumption is made in order to disregard the SNR
12198	13132	and coding, are of primary theoretical and practical interest. Extensive study of this issue has been undertaken, as evidenced in the small sample of references , , , , , , , , , , , , , , , and references therein. Characterization of the properties of these schemes, which still maintain optimality in terms of throughput
12198	13133	are of primary theoretical and practical interest. Extensive study of this issue has been undertaken, as evidenced in the small sample of references , , , , , , , , , , , , , , , and references therein. Characterization of the properties of these schemes, which still maintain optimality in terms of throughput on the
12198	13133	and  to be referred to in the following. Some practical implication of successive interference cancellation as the effect of imperfect cancellation and residual noise is addressed in , , , , , and . The effect of successive cancellation on other information-theoretic measures, such as the cutoff rate is discussed in , where this measure does not seem to be a
12198	13133	implies that the interference they inflict on other adjacent cells is small. The associated increase in capacity is remarkable, and this different powercontrol procedure undermines the arguments in  and  claiming limited incentive of employing optimal multiuser processing in case where intercell interference is present. Similar results can be deduced from  by reinterpreting the
12198	13133	more closely for classical constraints such as the inherent lack of synchronization, presence of memory, and the associated information-theoretic implications (see, for example, , , , ) in the fading regime are yet to be studied.s2662 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 Arbitrarily varying channels and compound multiuser channels are
12198	12484	implications of communications over channel models which more accurately approximate current and future communication systems. (See, for example, work related to chaotic dynamical systems , .) These models cover a whole spectrum of classical media (as HF channels and meteor burst channels, used for many decades), and more recent channels and applications (as microwave wideband
12198	12522	signal sets have high and , and small High-diversity integral lattices from algebraic number fields: The algebraic approach , , , , , , , , , , , , , ,  allows one to build a generator matrix exhibiting a guaranteed diversity. As a special case, high-diversity constellations can be generated by rotations , ,
12198	12542	Transmitter-Antenna Diversity: Multiple transmit antennas can also be used to provide diversity, and hence to improve the performance of a communication system in a fading environment; see, e.g., , , , , . Transmitter diversity has been receiving in the recent past a fresh look. As observed in , “it is generally viewed as more difficult to exploit than receiver
12198	12567	criterion is to design coded schemes such that their minimum Euclidean distance is maximized. This is correct on the Gaussian channel with high SNR (although not when the SNR is very low: see ), and is often accepted, faute de mieux, on channels that deviate little from the Gaussian model (e.g., channels with a moderate amount of intersymbol interference). However, the Euclideandistance
12198	12460	diversity branches, the probability that the signal will be simultaneously faded on all branches can be made small. Another approach, which was investigated by the authors in , , and , is philosophically different, as it is based upon the observation that, under fairly general conditions, a channel affected by fading can be turned into an additive white Gaussian noise (AWGN)
12198	12460	does time averaging, even when no diversity is present. Fig. 7 shows the block diagram of the transmission scheme with fading and cochannel interference. The assumptions are , , and  as follows. 1) PSK modulation. 2) independent diversity branches whose signal-to-noise ratio is inversely proportional to (this assumption is made in order to disregard the SNR increase that
12198	12460	Coherent detection with perfect channel-state information. 5) Synchronous diversity branches. 6) Independent cochannel interference, and a single interferer. The codes examined in , , and  are the following: J4: Four-state, rate- TCM scheme based on 8-PSK and optimized for Rayleigh fading channels . U4: Four-state rate- TCM scheme based on 8-PSK and optimized for the Gaussian
12198	12460	so that a lower interleaving depth (and, consequently, a lower overall delay) can be compensated by an increase of When differential or pilot-tone, rather than coherent, detection is used , a BER floor occurs which can be reduced by introducing diversity. As for the effect of cochannel interference, even its BER floor is reduced as is increased (although for its elimination multiuser
12198	12602	methods for MAI cancellation and detection are akin to iterative methods for decoding turbo codes. Therefore, it is not surprising that these approaches have merged in some recent publications , , , . G. Spatio-Temporal Equalization in Multiuser Communications Multiple antennas provide additional degrees of freedom for suppressing ISI, CCI, and ICI. In general, the spatial
12198	12602	by recent work which mostly resorts to iterative algorithms, and in which the coding part is inherent within the equalization process itself (which may cope also with multiuser interference). See , , , as well as , ,  for some selected (and not necessarily representative) references to this area, which recently happened to be at focus of advanced research, and
12198	12606	data is detected by passing the received signal through one or the other of these detectors. Other methods for performing blind multiuser detection have been developed by Honig et al. , Madhow , Talwar et al. , van de Veen et al. , Miyajima et al. , Juntti  and Paulraj et al. . I. Concluding Remarks In this section we have provided an overview of equalization
12198	12609	delay in SIC or ISDIC may be alleviated to some extent by devising methods that perform parallel interference cancellation (PIC), as described by Patel and Holtzman  and others , . The use of iterative methods for MAI cancellation and detection are akin to iterative methods for decoding turbo codes. Therefore, it is not surprising that these approaches have merged in some
12198	12611	detector in which hard decisions are used to detect the symbols in the intermediate stages. Instead of hard decisions, one may employ soft decisions as proposed by Kechriotis and Manolakos . Recently, Müller and Huber  have proposed an improvement in which an adaptive detector is employed that adapts to the decreasing interference power during the iterations. Such cancelers are
12198	12614	system. In this paper, the performance of the multichannel DFE was also evaluated when used for multiuser detection in an asynchronous CDMA system with Rayleigh fading. The paper by Lindskog et al.  also treats the use of a multichannel DFE to equalize signals in an antenna array for a TDMA system. Ratnavel et al.  investigate space–time equalization for GSM digital cellular systems based
12198	13294	These types of algorithms are called fast RLS algorithms. The linear and decision-feedback equalizers based on the RLS criterion may also be implemented in the form of a lattice structure , . The lattice structure and the RLS equations for updating the equalizer coefficients have been described in several references, for example, see –. The convergence rate is identical to
12198	12627	published on the use multiple antennas for wireless communications. We cite a few representative papers below. For additional references, the reader may refer to the paper by Paulraj and Lindskog , which provides a taxonomy for space–time processing in wireless communication system. Tidestav et al. , analyzed the performance of a multichannel DFE that performs combined temporal and
12198	12638	MAI cancellation and detection are akin to iterative methods for decoding turbo codes. Therefore, it is not surprising that these approaches have merged in some recent publications , , , . G. Spatio-Temporal Equalization in Multiuser Communications Multiple antennas provide additional degrees of freedom for suppressing ISI, CCI, and ICI. In general, the spatial dimension
12198	12638	which mostly resorts to iterative algorithms, and in which the coding part is inherent within the equalization process itself (which may cope also with multiuser interference). See , , , as well as , ,  for some selected (and not necessarily representative) references to this area, which recently happened to be at focus of advanced research, and produced so far
12198	12638	techniques. A relevant example of this is the recent emergence of practical successive interference cancellation (, , and references therein) as well as equalization and decoding ,  via iterative methods . These methods demonstrate remarkable performance in the multiple-access channel, and a deeper information-theoretic approach accounting for the basic ingredients of this
12198	12652	For additional references, the reader may refer to the paper by Paulraj and Lindskog , which provides a taxonomy for space–time processing in wireless communication system. Tidestav et al. , analyzed the performance of a multichannel DFE that performs combined temporal and spatial equalization, where the multiple antenna elements may be either at the base-station receiver or at the
12198	12653	and the phase of the channel response from the received signal. This cyclostationarity property of the received signal forms the basis for channel estimation algorithms devised by Tong et al. . It is also possible to estimate the channel response from the received signal by using higher order statistical methods. In particular, the impulse response of a linear discrete timeinvariant
12198	12661	through one or the other of these detectors. Other methods for performing blind multiuser detection have been developed by Honig et al. , Madhow , Talwar et al. , van de Veen et al. , Miyajima et al. , Juntti  and Paulraj et al. . I. Concluding Remarks In this section we have provided an overview of equalization techniques applied to fading dispersive channels.
12198	12665	6, OCTOBER 1998 Blind-equalization algorithms have also been developed for CDMA systems in which intersymbol interference (ISI) is present in the received signal in addition to MAI. Wang and Poor ??? have developed a subspace-based blind method for joint suppression of ISI and MAI for timedispersive CDMA channels. The time-dispersive CDMA channel is first formulated as a multiple-input,
12198	12680	optimality. For the case where the processing gain is exactly or larger, orthogonal spreading sequences maintain optimality. For general results with asymmetric power constraints, see . The question now is, what happens in a fading regime? In  it is shown, for example, that when fading is present orthogonal DS-CDMA is not always (for any i.i.d. fading distribution)
12686	12687	implementing and using complex security policies that are distributed across multiple hosts. The methods for distributing and assembling pieces of the security policy can be described using logics , and distributed authentication systems have been built by first designing an appropriate logic and the implementing the system around it . The most general of the logics – that is, the
12686	12688	security policy can be described using logics , and distributed authentication systems have been built by first designing an appropriate logic and the implementing the system around it . The most general of the logics – that is, the one that allows for expressing the widest range of security policies – was recently introduced by Appel and Felten (AF logic) . The AF logic is a
12686	12689	security policy can be described using logics , and distributed authentication systems have been built by first designing an appropriate logic and the implementing the system around it . The most general of the logics – that is, the one that allows for expressing the widest range of security policies – was recently introduced by Appel and Felten (AF logic) . The AF logic is a
12686	12690	around it . The most general of the logics – that is, the one that allows for expressing the widest range of security policies – was recently introduced by Appel and Felten (AF logic) . The AF logic is a higher-order logic that differs from standard ones only by the inclusion of a very few rules that are used for defining operators and lemmas suitable for a security logic. A
12686	12691	security policy can be described using logics , and distributed authentication systems have been built by first designing an appropriate logic and the implementing the system around it . The most general of the logics – that is, the one that allows for expressing the widest range of security policies – was recently introduced by Appel and Felten (AF logic) . The AF logic is a
12686	12692	implementing and using complex security policies that are distributed across multiple hosts. The methods for distributing and assembling pieces of the security policy can be described using logics , and distributed authentication systems have been built by first designing an appropriate logic and the implementing the system around it . The most general of the logics – that is, the
12694	12707	To keep these efforts in check and at the same time meet the design time requirements, a design flow and methodology for DSP-ASIC macro-cells that favor reuse and early error detection is essential . 1.1 Related work The productivity offered by the expressive power of RTL languages is way below critical. This level is clearly too low for complex design. Researchers have worked on developing
12694	12708	some functional constraints can be distributed on each GFIP (e.g. noise budgeting, I/O frequencies management). This specification results in a hybrid description (Hybrid Functional Model ) of the algorithm since only some of the parameters are fixed, but the others remain generic. The next step is to determine the architectural parameters values which satisfy a trade-off between
12711	12716	(((’1’,’shoe’,’style’, ),),(1,0.2)))) (1,( 9s(((’1’,’shoe’,’color’, ),),(1,0.5)), ((((shoe’,’color’, ),),(1,0.5))))) (1,( (((’3’,’delivery’,’delay’,)),(1,0.2)))) (0.04,( (((’2’,’payment’,’price’,),),(”300.0-min(att.value)”,50.0)))) ) An interesting aspect of this representation is that we can also express uncertainty in our own
12711	12716	(((’1’,’shoe’,’style’, ),),(1,0.2)))) (1,0.8( (((’1’,’shoe’,’color’, ),),(1,0.5)), ((((shoe’,’color’, ),),(1,0.5))))) (1,0.3,( (((’delivery’,’delay’,)),(1,0.2)))) (0.04,1.0( (((’payment’,’price’,),),(”300.0-min(att.value)”,50.0)))) ) In this example, we estimate that there’s a 30% chance that you will only buy black wingtips or
12723	12720	for extending the Informix DBMS kernel with user-defined access methods represents a great advance, but it is still only a first step. Following the ideas of Hellerstein et al.  and Aoki , a generic, extendible tree-based access method with “industrial strength” concurrency control and recovery protocols could be integrated into the kernel of the DBMS. Such a generic access method
12723	12722	This facilitates dissemination of research results and the transition from research results to products. The paper describes a prototype implementation of a new access method, termed the GR-tree , as an Informix DataBlade. Based on the R -tree  (an improved version of the R-tree originally proposed by Guttman ), this tree indexes now-relative bitemporal data, which is data with
12723	12722	regions can be indexed using adapted spatial indices. An essential challenge in indexing bitemporal data is to properly handle now-relative intervals. The next section briefly presents the GR-tree  which contends well with this requirement and outperforms other indices for now-relative bitemporal data. A description of the implementation of the GR-tree as an Informix DataBlade follows next.
12723	12722	query region actually overlaps with the retrieved spatial objects. Because the R -tree cannot handle the growing bitemporal regions presented in Section 2, it was modified, leading to the GR-tree . Variables UC and NOW were introduced in node entries at all tree levels, making it possible to record the exact geometry and the temporal behavior of the bitemporal regions in leaf-node entries.
12723	12722	resolve variables UC and NOW according to the current time. New insertion algorithms that take into account the varying shapes of the bitemporal regions were designed for the GR-tree. Reference  contains in-depth descriptions of the algorithms and performance tests, showing that the GR-tree outperforms the other proposed indices for general bitemporal data. 4. The Steps Needed to Implement
12723	12723	operator class. Section 5.3 provides insights into the possible index storage options. Deletions and the handling of the database variables UC and NOW are discussed in the full version of the paper . 5.1. Physical Representation of a Time Extent The GR-tree indexes the time extents associated with the database records. We have so far assumed that each of the four timestamps is in a separate
12723	12723	were necessary developing the GR-tree DataBlade. More details about these subjects, as well as some hints about the coding guidelines and testing options are given in the full version of the paper . 6.1. Tools Used Informix provides a set of tools, called the DataBlade Developer’s Kit, that support DataBlade development. The core tool is BladeSmith, a GUI tool that allows a developer to
12723	12723	writing of the purpose functions included considerations on how to re-use search information during the index scan (am getnext() returns one qualifying row at a time) and how to implement deletions . Being quite useful for addition of new data types, BladeSmith provides virtually no support for developing access method DataBlades. The purpose functions and operatorclass functions are defined
12723	10210	a prototype implementation of a new access method, termed the GR-tree , as an Informix DataBlade. Based on the R -tree  (an improved version of the R-tree originally proposed by Guttman ), this tree indexes now-relative bitemporal data, which is data with associated valid-time and transaction-time values . Many real-world databases contain a significant portion of this type of
12723	12724	existing framework for extending the Informix DBMS kernel with user-defined access methods represents a great advance, but it is still only a first step. Following the ideas of Hellerstein et al.  and Aoki , a generic, extendible tree-based access method with “industrial strength” concurrency control and recovery protocols could be integrated into the kernel of the DBMS. Such a generic
12723	12726	that needs to be modified. The upper levels of the subtree are locked so that only readers can access them . To achieve much higher levels of concurrency, B-link trees  and Rlink trees  were proposed, and Kornacker et al.  generalized the ideas of R-link trees to apply to a broader class of tree-based access methods. Using the link-based concurrency control protocols, a lock
12723	12727	levels of the subtree are locked so that only readers can access them . To achieve much higher levels of concurrency, B-link trees  and Rlink trees  were proposed, and Kornacker et al.  generalized the ideas of R-link trees to apply to a broader class of tree-based access methods. Using the link-based concurrency control protocols, a lock on a parent node can be released before
12723	12729	entire tree or the subtree that needs to be modified. The upper levels of the subtree are locked so that only readers can access them . To achieve much higher levels of concurrency, B-link trees  and Rlink trees  were proposed, and Kornacker et al.  generalized the ideas of R-link trees to apply to a broader class of tree-based access methods. Using the link-based concurrency
12723	5239	is not fixed, but instead tracks the current time and continuously extends as time passes. Table 1 exemplifies now-relative bitemporal data, which is represented using TQuel’s four-timestamp format . With this format, each tuple has four time attributes. Nowrelative tuples are represented using UC (denoting ”until changed”) and NOW variables  for transaction- and validtime end attributes,
988511	12776	of a restricted permutation. For example, in  it is shown the surprising fact that the joint distribution of these two statistics is the same in 132- and in 321-avoiding permutations. In , involutions avoiding any single pattern of length 3 are studied with respect to the number of fixed points. Another paper  deals with the enumeration of permutations with a given number of
988511	12776	132, 213, 321}, |In(?)| = ? ? n , and that for ? ? ?n/2? {231, 312}, |In(?)| =2n?1 . From Lemma 6.1 it follows that for all k ? 0, ik n(132) = ik n(213) and ik n (231) = ikn (312). It is shown in  (see also  for a bijective proof) that in fact ik n(132) = ik n(321). So, for single restrictions there are three cases to consider. Theorem 6.3 () Let n ? 1, k ? 0. We have (1) i0 n(123)
988511	12778	statistic, the number of descents. Recall that i ? n ? 1isadescent of ? ?Sn if ?i >?i+1. Denote by des(?) the number of descents of ?. Theorem 3.5 can be generalized as follows. Theorem 3.6 () The generating function for 321-avoiding permutations with respect to fixed points, excedances, and descents is ? ? Similarly, 1+ ? n?0 ??Sn(321) ? n?1 ??Sn(132) x fp(?) q exc(?) p des(?) z n 2 =
988511	12779	in this topic is currently growing. Two of the most studied permutation statistics have been the number of fixed points and the number of excedances of a restricted permutation. For example, in  it is shown the surprising fact that the joint distribution of these two statistics is the same in 132- and in 321-avoiding permutations. In , involutions avoiding any single pattern of length 3
988511	12779	The correspondences between the first two columns have been discussed above. Now we show how ? maps the statistics on the second column to those on the third one. We repeat the reasoning in . For this purpose, instead of using D = ?(?), it will be convenient to consider the path U from the lower-left corner to the upper-right corner of the array of ?. We will talk about tunnels of U to
988511	12779	and that 231 ? 312. These are the only equivalences that follow from the trivial bijections. Recently it was shown that, surprisingly, 321 ? 132. The first proof of this result appears in  (see  for a bijective proof). The weaker version 321 ? 132 was proved earlier in . So, we have the following equivalence classes of patterns of length 3 with respect to fixed points and excedances:
988511	12779	|In(?)| = ? ? n , and that for ? ? ?n/2? {231, 312}, |In(?)| =2n?1 . From Lemma 6.1 it follows that for all k ? 0, ik n(132) = ik n(213) and ik n (231) = ikn (312). It is shown in  (see also  for a bijective proof) that in fact ik n(132) = ik n(321). So, for single restrictions there are three cases to consider. Theorem 6.3 () Let n ? 1, k ? 0. We have (1) i0 n(123) = i2 ? ? ?
988511	12781	? ?Sn(312). We have (1) fp(?) =ct(?(?)), (2) exc(?) = rt(?(?)), (3) fp(?) =td0(?(¯?)), (4) exc(?) =td<0(?(¯?)). 2.5 Combinatorial classes and generating functions. Here we direct the reader to  for a detailed account on combinatorial classes and the symbolic method. Let A be a class of unlabelled combinatorial objects and let |?| be the size of an object ? ?A.IfAndenotes the objects in A
988511	12782	000000000000000000000 111111111111111111111 Figure 1: The bijection ?. The bijection ? is essentially the same bijection between Sn(132) and Dn given by Krattenthaler  (see also ), up to reflection of the path from a vertical line. Next we define the inverse map ??1 : Dn ?? Sn(132). Given a Dyck path D ?Dn, the first step needed to reverse the above procedure is to
988511	12782	use a bijection between Sn(123) and Dn, which we denote by ?. This bijection appears in an equivalent form in , and it is closely related to the bijections between Sn(321) and Dn given in . We can define ? as follows. Let ? ?Sn(123) be represented as an n × n array with a cross on each square (i, ?i). Consider the path with down and left steps that goes from the upper-right corner to
988511	12783	111111111111111111111 000000000000000000000 111111111111111111111 Figure 1: The bijection ?. The bijection ? is essentially the same bijection between Sn(132) and Dn given by Krattenthaler  (see also ), up to reflection of the path from a vertical line. Next we define the inverse map ??1 : Dn ?? Sn(132). Given a Dyck path D ?Dn, the first step needed to reverse the above procedure
988511	12783	? can have at most one big fixed point and one small fixed point. In this subsection we use a bijection between Sn(123) and Dn, which we denote by ?. This bijection appears in an equivalent form in , and it is closely related to the bijections between Sn(321) and Dn given in . We can define ? as follows. Let ? ?Sn(123) be represented as an n × n array with a cross on each square (i,
988511	12783	(1 ? z) 3 . = 1+xz +(x2 ? 4q)z2 +(?3xq + q + q2 )z3 +(xq + xq2 ? 3x2q +3q2 )z4 (1 ? qz2 )(1 ? 4qz2 . ) Proof. Consider the bijection ? : Sn(132) ?? D n described in Subsection 2.2. It is shown in  that the height of the Dyck path ?(?) is the length of the longest increasing subsequence of ?. In particular, ? ?Sn(12 ···(k +1), 132) if and only if ?(?) hasheight at most k. Thus, by Lemma 2.3,
988511	12784	statistics is the same in 132- and in 321-avoiding permutations. In , involutions avoiding any single pattern of length 3 are studied with respect to the number of fixed points. Another paper  deals with the enumeration of permutations with a given number of fixed points avoiding simultaneously two or more patterns of length 3. Finally,  considers additional restrictions on
988511	12785	of ? as follows. For each cross, shade the cell containing it and the squares that are due south and due east of it. The diagram is defined as the region that is left unshaded. It is shown in  that this gives a bijection between Sn(132) and Young diagrams that fit in the shape (n ? 1,n? 2,...,1). Consider now the path determined by the border of the diagram of ?, thatis,thepath with up
988511	12786	in this topic is currently growing. Two of the most studied permutation statistics have been the number of fixed points and the number of excedances of a restricted permutation. For example, in  it is shown the surprising fact that the joint distribution of these two statistics is the same in 132- and in 321-avoiding permutations. In , involutions avoiding any single pattern of length 3
988511	12786	bijections. Recently it was shown that, surprisingly, 321 ? 132. The first proof of this result appears in  (see  for a bijective proof). The weaker version 321 ? 132 was proved earlier in . So, we have the following equivalence classes of patterns of length 3 with respect to fixed points and excedances: 3.1 a) 123 a) 123 b) 132 ? 213 ? 321 c) 231 ? c’) 312 For this case we have not
988511	12786	? can have at most one big fixed point and one small fixed point. In this subsection we use a bijection between Sn(123) and Dn, which we denote by ?. This bijection appears in an equivalent form in , and it is closely related to the bijections between Sn(321) and Dn given in . We can define ? as follows. Let ? ?Sn(123) be represented as an n × n array with a cross on each square (i,
988511	12786	For n ? 12 the result can be checked by exhaustive enumeration of all derangements. Let us assume that n ? 13. From part (2) of Corollary 3.2, we have that s 0 n (123) ? Cn ? 2Cn?1. It is known  that s0 n (132) = Fn, then-th Fine number. Therefore, the theorem will be proved if we show that Fn < Cn ? 2Cn?1 (1) for n ? 13. Using the identity Fn = 1 ?n?2 ?1 2 i=0 ( 2 )iCn?i, we get the
988511	12789	is not hard to see that in fact it holds for all n ? 13. ? 3.2 b) 132 ? 213 ? 321 This case has already been studied in . The corresponding generating function, which appeared independently in  for the case of the pattern 321, is the following. Theorem 3.5 () F132(x, q, z) =F213(x, q, z) =F321(x, q, z) = 2 = 1+(1+q ? 2x)z + ? 1 ? 2(1 + q)z +(1?q) 2 . z2 the electronic journal of
8920392	12793	the mechanism of Notch-dependent boundary formation in the Drosophila large intestine by experimentation and computational modeling and simulation by Genomic Object Net (GON) . In the last study , we demonstrated that several simulation patterns were obtained only by changing a few parameter values and initial conditions of a common model. In this study, we improved the parameters in order
8920392	12794	of Notch ?E only by manipulating the parameters. 2 Simulation Results and Experimental Observations The Delta-Notch signaling pathway was modeled by Hybrid Functional Petri net (HFPN) , which includes the intracellular regulatory circuit as well as cell-to-cell interactions (Figure 1). Figure 2 shows the simulation model consisting of 60 cells. In addition, we introduced enzyme
10040545	12804	are vector fields with polynomial coefficients which are homogeneous of degree 1, then the flow generated by E is of the form above and, hence, is explicitly integrable in closed form. See §2 and  for more details. Section 2 provides some background on vector fields, flows and Lie algebras. Section 3 gives an algorithm for producing flows associated with free, nilpotent Lie algebras. Section
10040545	12804	E2 on R N which generate a Lie algebra which is isomorphic to the free, nilpotent Lie algebra g2,r on two generators of rank r. Here N is the dimension of the Lie algebra. The proof can be found in . In the next section we consider the analogous problem when the Lie algebra satisfies relations. Recall that the flows of these vector fields can all be integrated explicitly in closed form. Fix
10040545	12804	using a lemma which is actually stronger than the theorem and which illuminates the structure of the vector fields Ej. We must introduce the minimum order of a polynomial which was defined in . 9sDefinition 4.2 Let A be any non-constant monomial in the variables x1, . . . , xn. Define the minimum order of A by m(A) = min{j : xj|A}. If A is a polynomial with no constant term, then define
10040545	12806	approximating a general flow by a flow which is explicitly integrable in closed form. In this paper we describe an infinite dimensional family of explicitly integrable flows E; in a companion paper  we give an algorithm which, given a general flow F , will, in some ? The author is a National Science Foundation Postdoctoral Research Fellow. † The author is a National Science Foundation
8920397	13536	Sequences 1. INTRODUCTION Orthogonal spreading sequences are used in direct sequence code division multiple access (DS CDMA) systems for channel separation and to provide a spreading gain, e.g. . The most popular class of such spreading sequences are the sets of Walsh-Hadamard sequences , which are easy to generate. However, the cross-correlation between two Walsh-Hadamard sequences can
8920397	13538	the case for an up-link (mobile to base station) transmission, due to differences in the corresponding propagation delays. As a result, significant multi-access interference (MAI) 1s2 Chapter 9  occurs which needs to be combated either by complicated multi-user detection algorithms , or reduction in bandwidth utilization. Moreover, due to their very regular structure, Walsh-Hadamard
8920397	12820	. In addition to improving synchronization properties, scrambling also helps in reducing MAI. Another important drawback of using Walsh-Hadamard sequences or modified Walsh-Hadamard sequences  is the fact that sequence length must be equal to the integer power of 2. This is not the case if orthogonal sequences based on complementary Golay sequences are used instead of Walsh-Hadamard
8920397	12820	for application in DS CDMA systems, , , . In this paper, we want to extend those considerations to the sequences based on Golay-Hadamard matrices modified using the method introduced in . As a result of modification, the new sets of orthogonal sequences are characterised with much lower peaks in aperiodic cross-correlation functions, and exhibit good autocorrelation properties,
8920397	12820	3. SEQUENCE MODIFICATION METHOD Further improvement to the values of correlation parameters of the sequence sets based on Golay-Hadamard matrices, can be obtained using the method introduced in . That method is based on the fact that for a matrix H to be orthogonal, it must fulfil the condition . T HH = NI (4) T where H is the transposed Hadamard matrix of order N, and I is the N × N unity
8920397	13542	sequences. Complementary pairs and sets of orthogonal spreading sequences defined on the base of Golay complementary sequences have been long studied for application in DS CDMA systems, , , . In this paper, we want to extend those considerations to the sequences based on Golay-Hadamard matrices modified using the method introduced in . As a result of modification, the new sets
8920397	13543	sequences. Complementary pairs and sets of orthogonal spreading sequences defined on the base of Golay complementary sequences have been long studied for application in DS CDMA systems, , , . In this paper, we want to extend those considerations to the sequences based on Golay-Hadamard matrices modified using the method introduced in . As a result of modification, the new sets of
8920397	12823	Golay Sequences for Asynchronous DS CDMA Applications c S ( ? ) + cS ( ? ) = 0 ? ? 0 (1) 1 2 where ( ) 1 ? c S and ( ) 2 ? cS denote the aperiodic autocorrelation functions . It can be proven ,  that if matrices A and B are the circulant matrices created from a pair of Golay complementary sequences S1, and S2 then the matrix ? A B ? G = ? T T ? (2) ?B ? A ? is a Hadamard matrix. The
8920397	12827	sequences can be used to create quadri-phase Hadamard matrices and the resulting quadri-phase spreading sequences can be easily used in DS CDMA systems. Holzmann and Kharaghani have published in  the results of a computer search for quadri-phase Golay sequences of lengths up to 13, and in  a methodology for designing of quadri-phase Golay sequences is given. To show a usefulness of
8920397	12827	systems, let us consider here the correlation properties of spreading sequence sets defined by Hadamard matrices derived from the complementary Golay sequences of length 13 and 16. EXAMPLE 1 In , two quadri-phase complementary Golay sequences of length 13 are presented. These are: S1-13 =  S2-13 =  To construct the set of
8920397	12827	mean square aperiodic autocorrelation value for the sequence set. EXAMPLE 2 Another tested set of is obtained from two 16-chip bipolar Golay complementary sequences, S1-16 and S2-16, also given in :s9. On a Use of Golay Sequences for Asynchronous DS CDMA Applications S1-16 =  S2-16 =  The resulting set of 32 Golay-Hadamard sequences of length 32 has the
8920398	12831	variables). On the brighter side is the fact that they can determine whether the global minimum has been reached with a specified tolerance. We consider a recently developed method of cutting angle . This method constructs a saw-tooth cover of the objective function f(x) – the max-min type auxiliary function that always underestimates f(x). The maxima of the auxiliary function are taken at
8920398	12831	angle method has been formulated for IPH functions, every Lipschitz function can be transformed to a restriction of a certain IPH function to the unit simplex with the help of an additive constant . Let g : S ? R be a Lipschitz function. Then f ( x) = g( x) + c is an IPH function on the unit simplex with c ? 2L ? min g( x) where L is the least Lipschitz constant of g in the L1 -norm. Since
8920398	12831	step of the Algorithm 1 is Step 1, minimization of the auxiliary function. This problem is essentially of combinatorial nature. Some properties of the auxiliary function (1) are studied in . Among them we note the following. Theorem 1  Let x>0 be a local minimizer of (x) k k k Then there exists a subset L { 1 , 2 ,..., n } d d d 1) x = , ,..., with d k l 1 1 k l 2 2 k l n n li
8920398	12831	function at Step 1 of the algorithm, we need to examine all its local minima, and hence all valid combinations of the support vectors. This process can be significantly accelerated (as reported in ) by noticing, that h Ksh K K K ?1 i xi i= 1,..., n ( x) = max h ( x), min l Then, if we have already computed all the local minima of the auxiliary function 1( ) x hK ? at the previous iteration,
8920398	12831	of the last support vector K l . This means that we need to examine only those combinations of support vectors that include vector K l (i.e., one of ki K l = l ). The cutting angle algorithm of  works based on the above theorem, by examining all possible combinations of n support vectors (out of K). Recently  it was established that valid combinations of support vectors are related to
8920398	12844	interior of the unit simplex. i N + 1 i= 1 i On the other hand, from differentiability of ? (t) with respect to t (and hence p) follows that ? (t) is Lipschitz (see also a more general result in ). Hence we can apply the cutting angle method. 3. Cutting angle method The idea of the cutting angle method is to construct a saw-tooth cover of the objective function – an auxiliary max-min type
8920398	12850	the derivatives of the spline . Dierckx  employs Gauss-Newton method with a penalty function to handle the restrictions. Alternatives to nonlinear knots optimisation involve knot reduction  and using knots optimized for piecewise polynomials (or deficient splines) . The latter technique could be used to find a good initial approximation for Gauss-Newton method. In this paper we
8920398	12851	constraints. Penalty functions and logarithmic transformation of coordinates are used to transform it to the unconstrained problem, and local gradient-based techniques are employed for minimization .sThe great number of local minima presents well-known difficulties in this problem of nonconvex optimization (cf. “lethargy” property ). The local optimization algorithms descend to the nearest
8920398	12851	choice. Loach and Wathen  investigate linear splines with free knots and use coordinate descent as a less expensive alternative to Gauss-Newton method, employed in . Schutze and Schwetlick  use damped Gauss-Newton method and Kaufman approximation to the Jacobian, thus avoiding the need for logarithmic coordinate transformation, although at each step of the Gauss-Newton method they
8920398	12851	nonlinear part takes the form min? ( t) , s.t. t , i = 0,..., N ti < i+ 1 where ? ( t) = min? ( a; t) . Local optimisation techniques have been previously employed to solve this nonlinear problem . The derivatives with respect to t can be found explicitly, and fast Newton-type methods allow one to descend into the nearest local minimum. However, the problem is that ? (t) is nonconvex, and it
8920398	12851	not exist (the discrete gradient method uses a generalization of the gradient, so called subgradient, to treat nonsmooth functions). Example 3. Approximation of f ( x) = arctan( 10x) on  . Following  we used 41 data points equidistant in  with added pseudorandom noise uniformly distributed in . Unconstrained splines, even when the data contains no noise,
8920398	12852	constraints. Penalty functions and logarithmic transformation of coordinates are used to transform it to the unconstrained problem, and local gradient-based techniques are employed for minimization .sThe great number of local minima presents well-known difficulties in this problem of nonconvex optimization (cf. “lethargy” property ). The local optimization algorithms descend to the nearest
8920398	12852	choice. Loach and Wathen  investigate linear splines with free knots and use coordinate descent as a less expensive alternative to Gauss-Newton method, employed in . Schutze and Schwetlick  use damped Gauss-Newton method and Kaufman approximation to the Jacobian, thus avoiding the need for logarithmic coordinate transformation, although at each step of the Gauss-Newton method they
8920398	12852	nonlinear part takes the form min? ( t) , s.t. t , i = 0,..., N ti < i+ 1 where ? ( t) = min? ( a; t) . Local optimisation techniques have been previously employed to solve this nonlinear problem . The derivatives with respect to t can be found explicitly, and fast Newton-type methods allow one to descend into the nearest local minimum. However, the problem is that ? (t) is nonconvex, and it
8920398	12852	methods give different results when started from different initial points (see ). Notably, when the initial point is the uniform partition of the interval, none of the methods described in  converges to the global minimum. The discrete gradient method alone, despite its ability to avoid shallow local minima, does not converge to the global minimum either. From other starting points it
8920398	614	local methods. As its name implies, global optimization methods aim at finding the global minimum of the objective function. There are two broad categories: stochastic and deterministic approaches . Stochastic techniques are more widely known: examples are random search, simulated annealing and genetic algorithms. We consider the other category: deterministic methods. Deterministic global
8920398	614	problems of minimization of the saw-tooth cover. The cutting angle method is based on results in abstract convexity . The cutting angle method arises, as do the Piyavskii and Mladineo methods , as a special case of the generalized cutting plane method described in . One-dimensional Lipschitz global optimization algorithms and their multidimensional extensions, such as Piyavskii
8920401	12856	can be generated from original graph through three steps: triangulating graph, identifying clusters and building optimal join tree. The procedure of build optimal join tree is widely studied. See (Huang & Darwiche 1996) for detailed discussion. Triangulating Original Graph An undirected graph is triangulated if and only if every cycle of length four or greater contains an edge that connects two nonadjacent nodes
8920401	12857	X and Y contain X?Y. However, we do not require the FG(V ) of node V in original graph G is included in at least one cluster. Detailed discussion of building optimal join tree can be found in (Jensen & Jensen 1994). Figure 1 is an illustration of the process of transforming Road Game into optimal join tree. Road game has 2k players, each player own a slot along the road. Player has to decide which types of
8920401	12859	is a tree. Two variances of the abstract tree algorithm are developed in the same paper to compute exact and ?-approximated Nash equilibrium respectively. Later this approach is extended by (Ortiz & Kearns 2003) to solve graphical games with loops. (Vickrey & Koller 2002) relaxes the undirect graph restriction into directed graph and solving the problem by using variable elimination algorithm in CSP by
8920401	12859	also proves that exact tree algorithm computes a Nash equilibrium for the tree game (G, M) in exponential time in the number of vertices of G. Variable Elimination in Loopy Graphical Games In (Ortiz & Kearns 2003), a generalized Nash propagation algorithm of abstract tree algorithm is proposed. Unlike the abstract tree algorithm, the proposed NashProp algorithm can operate on the graph with loops directly.
5540748	12861	is a very important aspect of computer graphics, especially when comparing the performance of graphics rendering algorithms. In the past, two kinds of metrics have been used. Physical metrics , compare images in terms of the numerical differences between their pixel values. A new trend in computer graphics is to use perceptual metrics that are based on computational models of human
5540748	12862	James A. Ferwerda Program of Computer Graphics Cornell University Fabio Pellacini Pixar Animation Studios Emeryville, CA a) Raytraced reflection b) Environment mapped reflection c) VDP result  Fig 1. Images rendered with different reflection algorithms and VDP map showing areas of visible difference. Abstract - In this paper we introduce Functional Difference Predictors (FDPs), a new
5540748	12862	have been widely used by graphics researchers to compare rendering algorithms and to determine when images produced by different algorithms will be visually indistinguishable from one another (see  for recent reviews). However when we look at images, we do not see pixels. Rather, we see objects with distinct shapes, sizes, locations, motions, and materials. We use the visual information
5540748	12865	a manageable domain. We chose to study two tasks that have widespread utility in both computer graphics and real world applications: material and spatial layout estimation. Since previous studies  have shown that material and layout perception are affected by the characteristics of surface reflections, we manipulated this visual cue to determine how errors in rendering reflections affect
5540748	12866	have been widely used by graphics researchers to compare rendering algorithms and to determine when images produced by different algorithms will be visually indistinguishable from one another (see  for recent reviews). However when we look at images, we do not see pixels. Rather, we see objects with distinct shapes, sizes, locations, motions, and materials. We use the visual information
5540748	12867	a manageable domain. We chose to study two tasks that have widespread utility in both computer graphics and real world applications: material and spatial layout estimation. Since previous studies  have shown that material and layout perception are affected by the characteristics of surface reflections, we manipulated this visual cue to determine how errors in rendering reflections affect
5540748	12867	printed side-by-side at the top of a page and each of the four questions were printed below. The images were tonemapped and color corrected for the printing process using the procedure described in . The subjects replied to the questions by marking checkboxes on each sheet. Each subject was shown each image pair only once. The pairs were presented in random order, and the horizontal positions
5540748	12869	beginning. Watson et al.  and Rushmeier et al.  have studied the correlation between VDP measures and subjects’ ratings of shape in the context of geometric compression. Rademacher et al.  conducted an experiment to measure the perception of visual realism and its correlation with various visual cues. Finally, Wanger et al.  and Rodger and Browse  have explored how different
5540748	12870	have been widely used by graphics researchers to compare rendering algorithms and to determine when images produced by different algorithms will be visually indistinguishable from one another (see  for recent reviews). However when we look at images, we do not see pixels. Rather, we see objects with distinct shapes, sizes, locations, motions, and materials. We use the visual information
5540748	12872	developed do provide an essential foundation for this work. In the computer graphics literature itself, research in this area is just beginning. Watson et al.  and Rushmeier et al.  have studied the correlation between VDP measures and subjects’ ratings of shape in the context of geometric compression. Rademacher et al.  conducted an experiment to measure the perception of
5540748	8920403	FDPs are formulated with respect to tasks, since they assess the fidelity of the visual information required for the task. Therefore there are potentially as many FDPs as there are classes of tasks . With this in mind, it is interesting to note that VDPs are in fact a specific instance of FDPs where the task is detecting contrast differences between images. This means that the FDP framework we
5540748	8920403	first glance this might seem unachievable since there are potentially an infinite variety of tasks and errors. However we believe that the problem is tractable, because recent perception research  has shown that visual tasks can be organized into classes in terms of the visual information that is essential for the task and the information that is marginal or irrelevant. A single formulation
5540748	12875	the experimental methodologies developed do provide an essential foundation for this work. In the computer graphics literature itself, research in this area is just beginning. Watson et al.  and Rushmeier et al.  have studied the correlation between VDP measures and subjects’ ratings of shape in the context of geometric compression. Rademacher et al.  conducted an experiment to
12877	12883	both the use of the structure for knowledge sharing and the visualisation design, a prototype has been developed to allow a group of students to use the visualisation design for sharing knowledge . The prototype intends to test both the functionality and effectiveness of the use of the structure for knowledge sharing and a visualisation design to support user learning. The learning results
12893	12895	QoS issues (like packet delay, jitter and error probability) connection-level issues (related to connection establishment and management) need to be addressed in mobile wireless networks . A Call Admission Control (CAC) policy is required to address this problem. CAC in single service cellular systems has been thoroughly studied, see for instance the seminal work by Hong and
12893	12900	Hong and Rappaport  or more recent papers like  and references therein. While most of these papers provide intuitive reservation schemes for the CAC a more insightful approach is adopted in  and , where the CAC is regarded as an optimization problem. Admission control in the presence of mobility and multiple services is not that well studied although some works in this direction can
12893	12900	is used the optimization is carried without considering the second components of the system state, i.e. the number of MT labeled as H, and the optimal policy results to be of the guard channel type . In our numerical results we used the following settings, unless otherwise indicated: C = 10, Cp = 60, Nh = µr/µc = 2, µ ?1 p /µ ?1 r = 0.5, ? = 20, x = U/2, SH = 0.4, ?n = 1. The value of ? is
12893	12910	multiservice scenarios. Our goal is to obtain the optimal policy for a given amount of information provided by the MP scheme. We consider that such approach has not been sufficiently explored. In  the authors determine a near-optimal policy by means of a genetic algorithm that takes into account not only the cell state but also the state of the neighboring cells, in a single service
12893	12912	specifically we used a policy improvement method. This approach is applied to a single service scenario. The second is an automatic learning approach based on the theory of reinforcement learning , more specifically we used the average reward reinforcement learning algorithm proposed in . This approach is applied to a multiservice scenario. DP gives an exact solution and allows to
12893	12913	scenario. The second is an automatic learning approach based on the theory of reinforcement learning , more specifically we used the average reward reinforcement learning algorithm proposed in . This approach is applied to a multiservice scenario. DP gives an exact solution and allows to evaluate the theoretical limits of incorporating movement prediction in the CAC problem, whereas RL
12893	12913	(bias) and ?(x, a) is the average sojourn time in state x under action a. The greedy policy ? ? defined by selecting actions that minimize the right-hand side of the above equation is gain-optimal . If the parameters of the model can be derived, then the solution to the Bellman equations can be obtained through dynamic or linear programming techniques. In multiservice scenarios, where the
12893	12913	and make the problem intractable (course of dimensionality). We propose an alternative approach based on a reinforcement learning algorithm named Semi-Markov Average Reward Technique (SMART) . The Bellman equations can be rewritten as ? w(x, a) ? g ? ?(x, a) + ? h ? (x, a) = min a?Ax x?S x?S Pxy(a) min a ? h ?Ay ? (y, a ? ? ) where h ? (x, a) is the average expected relative value of
12893	12914	as an infinite-horizon finite-state semi-Markov decision process (SMDP) under the average cost criterion, which is more appropriate for the problem under study than other discounted cost approaches . It is evident that we search for policies that minimize L. The decision epochs correspond to the time instants in which an arrival occurs. Given that no actions are taken at call departures, then
8920406	12926	the original image. To overcome this problem, we have used a morphological filter. Among all morphological operators (see  for more details), the morphological grayscale dual reconstruction (see ) has the advantage of preserving peaks and modifying valleys (Fig 1). This is a crucial point if we want to preserve without spreading the noisy and complex (usually creased) background and
12928	12929	makes use of the saturation technique. In the table, we have compared the performance of transient solutions using two representations: (1) Kronecker representation generated by the APNN toolbox , and (2) matrix diagram representation generated by the algorithm given in this paper. CTMCs represented by both representations have been solved using a state-level AFI-compliant transient solver.
12928	12930	with MDDs. Concurrently with this work, other researchers have devised approaches to combine lumping due to structural symmetries with compact representation using a Kronecker representation  and symbolic data structures . We also designed an algorithm to obtain an MDD representation of the lumped state space from the MDD generated by the state-space generation algorithm. The
12928	499	to lump equivalent states. In the following, we give a brief description of the two data structures. Multi-valued Decision Diagrams Review. MDDs  generalize binary decision diagrams (BDDs) . They are useful for encoding a set of vectors S ? × m c=1 Sc since they can represent functions of the form f : × m c=1 Sc ?{0, 1} for finite sets Sc = {0,...,|Sc|?1}, c ?{1,...,m}. Hence,
12928	12931	algebra formalisms and stochastic automata, so approaches that make use of a compositional structure in stochastic process algebras can also be used to generate lumped overall state spaces (e.g., ). Nevertheless, even a lumped state space can be extremely large, and further work on “largeness tolerance” techniques is needed to practically support such lumped state spaces. For example, binary
12928	12932	so-called Kronecker representations are built upon a specific matrix algebra whose operators (Kronecker product and sum) serve as composition operators to build Q from component matrices (e.g., ). In those approaches, the composition of a system from subsystems is built upon synchronization of actions, under certain assumptions about the structure of a model. Alternatively, component
12928	12932	in the MD as triples (s, s ? ,?), where ? results from the product of values found on a path from the root node to a leaf node in the MD. This procedure is inspired by the Act-RwCl algorithm of , formulated for matrix diagrams in . DFS gains its efficiency by following paths through all entries of a matrix Ri a,c before returning to level i ? 1. In this way it amortizes the cost of
12928	12932	see, our implementation of iteration on matrix diagrams is at most 1.5 times slower than APNN’s iteration on the Kronecker representation in which an efficient variant of the Act-RwCl algorithm of  is used. Notice that APNN toolbox is not able to exploit structural symmetries in the model in order to lump the underlying CTMC. Therefore, a direct comparison between APNN toolbox and our
12928	12934	referred to as symbolic state-space exploration and representation techniques (e.g., ), and in some cases, they allow one to verify logical properties of systems with “10 20 states and beyond” . For the numerical analysis of CTMCs, it is also necessary to represent the generator matrix Q in a space-efficient manner. Different approaches exist, and one possibility is to follow a
12928	12935	on it. For some modeling formalisms, the equivalence that is used for lumping is established by the modeling formalism itself; for instance, this is the case for stochastic well-formed nets (SWNs)  and stochastic activity network-based composed models (SANs) . It can also be shown that lumping has the property of a congruence that is preserved by parallel composition in a number of
12928	12936	vanishing states, and 2) atomic components that share SVs cannot stop one another from proceeding locally. The latter property gives us the ability to use an approach similar to saturation  (in firing local actions) and generate a subset of the state space of an atomic component independently from other atomic components as long as the fired actions are independent from the shared SVs
12928	12936	to be applied on the MDD, firing a global action in state-sharing composed models is inherently more difficult than firing a synchronizing action in an action-synchronization model, as discussed in . The reason is that in the latter case, the sets of SVs of atomic submodels are disjoint, and due to the product-form behavior , the changes that need to be applied on a node v (in the level
12928	12936	schemes for that canonical MD. Related work We have described a method to compute an MDD and MD for a lumped CTMC of composed models that share state variables. Existing results of Ciardo and Miner  for MDD and MD generation of composed models that share actions are related and have been used here; the concept of a local transitive closure has been discussed in different contexts .
12928	12937	S. DERISAVI, P. KEMPER, AND W. H. SANDERS functions for global actions. The dynamic generation of state spaces for atomic components has been developed independently of the recent result in , which is nevertheless conceptually closely related. Note that the encoding of matrix entries we selected in the MD implies that the weight of a state transition results from a product of values
12928	12938	under certain assumptions about the structure of a model. Alternatively, component matrices can be combined using a suitable variant of a decision diagram, namely a matrix diagram (MD) (e.g., ). MDs have been proposed for systems that again are built in a compositional manner upon synchronization of actions. Another approach is to employ multi-terminal binary decision diagrams (MTBDDs)
12928	12938	of the MD, the reduction operator for matrix diagrams is applied to minimize space requirements of the overall structure. The final step for the MD construction is to use the approach of , which is to project the rows and columns of the MD to Slumped and S, respectively. The resulting MD provides only rates of state transitions from s ?Slumped to s ? ?S. However, note that s ? may
12928	12938	must be mapped to the corresponding index value in {0,...,|S ?1|} to support a matrix-vector multiplication. Other MD approaches perform that mapping by an offset function ? encoded in an MDD . In our case, we can look up ?(s) fromtheMDDofSlumped only for s ?Slumped by the help of the offset computation known for MDDs. If s ? ??Slumped, a straightforward option is to sort entries of s ?
12928	12938	schemes for that canonical MD. Related work We have described a method to compute an MDD and MD for a lumped CTMC of composed models that share state variables. Existing results of Ciardo and Miner  for MDD and MD generation of composed models that share actions are related and have been used here; the concept of a local transitive closure has been discussed in different contexts .
12928	12939	under certain assumptions about the structure of a model. Alternatively, component matrices can be combined using a suitable variant of a decision diagram, namely a matrix diagram (MD) (e.g., ). MDs have been proposed for systems that again are built in a compositional manner upon synchronization of actions. Another approach is to employ multi-terminal binary decision diagrams (MTBDDs)
12928	12939	equal if they are terminal nodes of the same value or if they have equal tuples (xc,fxc=0,...,f xc=|Sc|?1). A non-terminal node is redundant if all of its pointers point to the same node. We follow  and consider ordered MDDs, where equal nodes have been merged and redundant nodes are retained only to ensure that a pointer of a level-c node can lead only to a level-(c + 1) node or to the
12928	12939	i.e., real values are multiplied along a path and summed over all paths. This definition allows us to use MDs to encode a matrix like R. Algorithms for manipulating MDs are described in detail in . In order to make the MDs that we generate compatible with the MDD of the state space, we encode the SVs in Vc in level c of the MD, as we did for the MDD. 3. Symbolic Generation of Lumped State
12928	12939	we compute the union of all the resulting U ? sets and add it to U. Given S, we could obtain an MD of the unlumped CTMC, and apply known approaches for the MD-based numerical analysis of CTMCs . Part of our goal, however, is to reduce the number of states in the resulting CTMC; therefore, as discussed in the next subsection, we compute Slumped. 3.2. MDD Conversion to Lumped State Space In
12928	12939	and then use the MDD to obtain a projection to the lumped state space. Conceptually, MD generation with the help of a Kronecker representation and MDD projection follows the line of arguments in . However, it differs in important aspects. In particular, the Kronecker representation we derive contains functional transitions  that are subsequently resolved to constant values in the MD.
12928	12940	schemes for that canonical MD. Related work We have described a method to compute an MDD and MD for a lumped CTMC of composed models that share state variables. Existing results of Ciardo and Miner  for MDD and MD generation of composed models that share actions are related and have been used here; the concept of a local transitive closure has been discussed in different contexts .
12928	12941	states as paths in a directed acyclic graph. Techniques that generate state spaces using decision diagrams are referred to as symbolic state-space exploration and representation techniques (e.g., ), and in some cases, they allow one to verify logical properties of systems with “10 20 states and beyond” . For the numerical analysis of CTMCs, it is also necessary to represent the generator
12928	12942	would be possible using lumping or symbolic representation techniques alone. Implementation in Möbius. In order to test the efficiency of the developed algorithms, we implemented them within Möbius . We have completed the implementation of the MDDbased state space (SS) generation, the lumping algorithms, and the MD-based generation of the lumped CTMC for composed models that consist of an
12928	12942	to support numerical analysis using the Möbius state-level AFI (Abstract Functional Interface) . The SSG implementation interacts with the component models using the Möbius model-level AFI , thus supporting any atomic model type that Möbius supports, including stochastic activity networks, PEPA (Performance Evaluation Process Algebra), and Buckets and Balls, and accepts composed
12928	12944	to obtain an order by submatrices, since the top-level node of the MD imposes a block structure on the resulting matrix, which is naturally followed by the DFS enumeration procedure. Following , the enumeration of matrix entries suffices to implement matrixvector multiplication x·Rlumped, which in turn is essentially what is needed to perform iterative solution methods like the Power
12928	12944	that consist of an arbitrary number of replicate and join operators. We also implemented iterators to support numerical analysis using the Möbius state-level AFI (Abstract Functional Interface) . The SSG implementation interacts with the component models using the Möbius model-level AFI , thus supporting any atomic model type that Möbius supports, including stochastic activity
12928	12945	Another approach is to employ multi-terminal binary decision diagrams (MTBDDs) that store Q(s, s ? )attheendofa path through a BDD, where the path itself encodes the transition (s, s ? ) (e.g., ). MTBDDs do not rely on a given structure of a model, but they only perform well if there are not too many different entries in Q(s, s ? ). Preliminary work on combining state-sharing and action
12928	12949	under certain assumptions about the structure of a model. Alternatively, component matrices can be combined using a suitable variant of a decision diagram, namely a matrix diagram (MD) (e.g., ). MDs have been proposed for systems that again are built in a compositional manner upon synchronization of actions. Another approach is to employ multi-terminal binary decision diagrams (MTBDDs)
12928	12949	of the MD, the reduction operator for matrix diagrams is applied to minimize space requirements of the overall structure. The final step for the MD construction is to use the approach of , which is to project the rows and columns of the MD to Slumped and S, respectively. The resulting MD provides only rates of state transitions from s ?Slumped to s ? ?S. However, note that s ? may
12928	12949	must be mapped to the corresponding index value in {0,...,|S ?1|} to support a matrix-vector multiplication. Other MD approaches perform that mapping by an offset function ? encoded in an MDD . In our case, we can look up ?(s) fromtheMDDofSlumped only for s ?Slumped by the help of the offset computation known for MDDs. If s ? ??Slumped, a straightforward option is to sort entries of s ?
12928	12949	slower running times than accessing the elements of the MD in DFS order. As a side remark, we note that we can also use the current enumeration of entries to create an additional, canonical MD  and use existing MD multiplication schemes for that canonical MD. Related work We have described a method to compute an MDD and MD for a lumped CTMC of composed models that share state variables.
12928	12952	on that structure for numerical analysis. The proposed approach has been implemented and used for the numerical state-space analysis of a highly redundant faulttolerant parallel computer system . We also consider a well-known performance model of a communication protocol . Results for these models are presented in Section 5. We conclude in Section 6. 2. Background 2.1. Hierarchical
12928	12952	and join operators, and hence provides a more complete test of our algorithms and implementation. Space does not permit us to describe the model here, but a full description can be found in . cardinality=3 memory_module Rep2 Join1 cardinality= N Rep1 cpu_module errorhandlers io_port_module Figure 4. The composed model of the parallel computer system We built a composed model for the
12928	12953	for lumping is established by the modeling formalism itself; for instance, this is the case for stochastic well-formed nets (SWNs)  and stochastic activity network-based composed models (SANs) . It can also be shown that lumping has the property of a congruence that is preserved by parallel composition in a number of process algebra formalisms and stochastic automata, so approaches that
12928	12953	use MDDs and MDs. In particular, our efforts have resulted in a new algorithm that symbolically generates the state space of a hierarchical model (which is built using join and replicate operators ) in the form of an MDD. The replicate operator imposes symmetries that create regular structures in the state space, and therefore make symbolic exploration of the state space efficient with MDDs.
12928	12953	of the CTMC of a hierarchical composed model that is built on shared state variables (SVs) among submodels. This composition operation is the same as the one used in SAN-based reward models , but is different from actionsynchronization composition, which has been used in superposed generalized stochastic Petri nets, (stochastic) process algebras, and stochastic automata networks. In
12928	12953	build models of complete systems from smaller and simpler models, we define two composition operators, “join” and “replicate,” which are based on sharing SVs of the models on which they are defined . The join operator combines a number of (possiblys4 S. DERISAVI, P. KEMPER, AND W. H. SANDERS non-identical) models by sharing a subset of their SVs, while the replicate operator combines a number
12928	12953	representation of a state as a vector of natural numbers. An important property of the replicate operator is that it generates a behavior that enables lumping on the associated CTMC of a model . We can define the lumped state space of a model with full state space S through the help of equivalence relations. In particular, for Rnc(VJ,M) with index c and cardinality nc, letlc be the number
12956	12957	is just one venue profiting this robustness. 4.5. OTHER DNA APPROACHES TO EVOLUTION OF STRATEGIES One particularly attractive alternative DNA computing approach would use continuous evolution (Ackermann et al., 1999; Schmitt and Lehman, 1999; Breaker et al., 1994). The virtue of this approach is that no external intervention is required. After all ingredients are combined, the system seeks an equilibrium.
12956	12960	52 public ; 2 Simplified Hold’em poker 52 private + 1 ?, 0, 1, - 1 adaptive behavioral adaptive behavioral maximizing ; 5 (Barone and While, 1998) 52 public plus 8 tight/loose strategy behavioral (Barone and While, 1999) while passive/aggressive ; 5 Five-card draw poker 52 with ; 2 52 3 0, 0, 5 levels 4 tight/loose while adaptive behavioral maximizing (Kendall and Willdig, 2001) replacement 5 levels while
12956	12978	match similar situations to a greater or lesser extent, we can use training by examples (Wood et al., 2001a). This is a standard approach to programming fuzzy decisions in conventional computers (Herrera et al., 1994; Pena-Reyes and Sipper, 2001). Programming fuzzy decisions by training is an important advantage. To design near-complementary sequences that would probabilistically bind according to arbitrary
12956	12981	poker. For these games, all Nash equilibrium strategies are interchangeable and have the same expected payoff 3 . For zero-sum games, there are methods for finding one Nash equilibrium efficiently (Koller and Pfeffer, 1997). This can be done in time polynomial in the number of nodes in the game tree, even in the worst case 4 . 2.3. SEEKING ADAPTIVE STRATEGIES Nash equilibria yield valuable insights into the possible
12956	12981	Computation of one Nash equilibrium for a simplified poker with about 10 5 nodes has been demonstrated. It is a sobering fact that five-card draw poker has about 10 25 nodes in its game tree (Koller and Pfeffer, 1997). 5 There is a famous analysis of a two-person simplified poker game (von Neumann and Morgenstern, 1944, Chapter 19). (This game is is cited in the top line of Table7.) The analysis demonstrates
12956	12996	strategies are equivalent and interconvertible, if we assume total recall of all prior choices made playing a game (Kuhn, 1953). 2.2. NASH EQUILIBRIA OF GAMES It is the celebrated result of Nash (VanDamme, 1991) that for any game with only a finite number of pure strategies, there exist mixed strategies in equilibrium. When strategies are in equilibrium, no individual player is able to improve his or her
3891167	13006	Naturally, there exists amaximum number of such QoS slots that the system can service without overloading, as having been addressed in previous works in admission control (Chang and Zakhor 1996; Chen and Chen 1996; Chen and Hsi 1998; Vin et al. 1995). Clients with higher QoS requirements must each occupy two or more such slots, e.g., for avideo server, this may correspond to a bigger frame size with color
3891167	13006	qh and ql defined above. Table 1summarizes the set of model parameters to be used in this simulation study. Following our earlier work on admission control policies without QoS negotiation control (Chen and Chen 1996; Chen and Hsi 1998), we consider astrategy in which we divide the Nslots into three parts: nh, nl and nm, with nh specifically being allocated to high-priority clients, nl being allocated to
3891167	13007	exists amaximum number of such QoS slots that the system can service without overloading, as having been addressed in previous works in admission control (Chang and Zakhor 1996; Chen and Chen 1996; Chen and Hsi 1998; Vin et al. 1995). Clients with higher QoS requirements must each occupy two or more such slots, e.g., for avideo server, this may correspond to a bigger frame size with color video display. For
3891167	13007	Table 1summarizes the set of model parameters to be used in this simulation study. Following our earlier work on admission control policies without QoS negotiation control (Chen and Chen 1996; Chen and Hsi 1998), we consider astrategy in which we divide the Nslots into three parts: nh, nl and nm, with nh specifically being allocated to high-priority clients, nl being allocated to low-priority clients and
3891167	13009	1993; Oyang et al. 1995; Vina et al. 1994). To date, there are two approaches by which QoS control can be implemented. One approach is based on adaptive, distributed control (Davies et al. 1994; Noble et al. 1995) wherein each client monitors the QoS received and automatically increases or decreases its resource requirement according to actual QoS level delivered to it and also by the amount of resources
8920412	13017	is certainly predictable, since crashing relies only on a simple external mechanism (the power switch), but it may be unsafe or disruptive or both, unless the application is known to be crash-only . An alternative would be machine-level crashes in a system 1 By framework we refer to a componentized middleware such as J2EE or CORBA.sdesigned specifically so that a combination of
8920412	13018	points without any extra work for application programmers. For example, we modified the source code of the JBoss open-source application server to collect and report code-path observations . It is more difficult to add application-generic control points that are predictable, safe and non-disruptive. Crashing and rebooting a machine is certainly predictable, since crashing relies only
8920412	13018	a nonzero false positive rate, we must make sure that any action we take is safe, predictable and non-disruptive. In this case, we respond by selectively “microrebooting” the suspected-faulty EJB’s  without causing unavailability of the entire application. Although this work is still in progress, we have demonstrated that EJB microreboots are predictable and non-disruptive, and there is reason
8920412	13019	we are already prototyping) for distributed network applications that exploit SLT-based monitoring at multiple levels. 6 Related Work Anomaly detection has been used to infer errors in systems code , debug Windows Registry problems , detect possible violation of runtime variable assignment invariants , and discover source code bugs by distributed assertion sampling . The latter is
8920412	13021	levels. 6 Related Work Anomaly detection has been used to infer errors in systems code , debug Windows Registry problems , detect possible violation of runtime variable assignment invariants , and discover source code bugs by distributed assertion sampling . The latter is particularly illustrative of SLT’s ability to mine large quantities of observations for interesting patterns
8920412	13023	session state management subsystem . Each replica reports the values of several resource-usage and forwardprogress metrics once per second, and these time series are fed to the Tarzan algorithm , which discretizes the samples to obtain binary strings and counts the relative frequencies of all substrings within these strings. Normally, these relative frequencies are about the same across
8920412	13024	in systems code , debug Windows Registry problems , detect possible violation of runtime variable assignment invariants , and discover source code bugs by distributed assertion sampling . The latter is particularly illustrative of SLT’s ability to mine large quantities of observations for interesting patterns that can be directly related to dependability. System parameter tuning
8920412	13025	behaviors of the system, and to detect potential problems such as the example above. 2 Approach and Assumptions We assume typical request-reply based Internet services, with separate session state  used to synthesize more complex interactions from a sequence of otherwise stateless request-reply pairs. Past approaches to statistical monitoring of such services have primarily relied on a priori
8920412	13025	events at various timescales should be comparable across all the replicas. We successfully used this method to detect anomalies in replicas of SSM, our session state management subsystem . Each replica reports the values of several resource-usage and forwardprogress metrics once per second, and these time series are fed to the Tarzan algorithm , which discretizes the samples to
8920412	13027	that can be directly related to dependability. System parameter tuning and automatic resource provisioning have also been tackled using PCFG-based approaches  and closed-loop control theory , although such approaches generally cannot detect functional or structural deviations in system behavior unless they manifest as performance anomalies. The Recovery-Oriented Computing project
8920412	5865	level, we note that the needed infrastructure is largely in place for applying it at all levels of functionality all the way down to the hardware. A legacy of the Active Networking research agenda  is a new generation of user-programmable network devices for storage virtualization, server load bal4 Client requests Responses Recovery actions to other datacenters Observations from other
8920412	13028	network applications that exploit SLT-based monitoring at multiple levels. 6 Related Work Anomaly detection has been used to infer errors in systems code , debug Windows Registry problems , detect possible violation of runtime variable assignment invariants , and discover source code bugs by distributed assertion sampling . The latter is particularly illustrative of SLT’s
13029	13031	to improve the convergence rate and the convergence characteristics over well known training algorithms. In particular, a scaled version of the conjugate gradient method, suggested by Perry , , which employ the spectral steplength of Barzilai and Borwein , , was presented. The learning rate was automatically adapted at each epoch according to Shanno’s technique which exploits
13029	13031	positive multiplier, and d0 = ?g0. Conjugate gradient methods differ in their choice for the multiplier ?k used to construct the search direction. In this method, as the previous one introduced in  and , the multiplier ?k is given by the following formula ?k = (?kyk ? sk) T gk+1 s T k yk where yk = gk+1 ? gk. For ?k = 1 the above formula is reduced to the formula introduced by Perry in
13029	13035	Thus, in order to evaluate the performance of the training algorithms better, the simulations conducted using the same initial weight vectors that have been chosen by the Nguyen - Widrow method . This technique of weight initialization results in distributing the initial weights at the hidden layer in such a way that it is more likely that each input pattern will cause a hidden neuron to
13029	13036	?gk?2 = 0 (16)sA.E. Kostopoulos, D.G. Sotiropoulos, and T.N. Grapsa This implies that every limit point of a sequence generated by the algorithm is stationary. More details can be found in , . At this point we will summarize the new training algorithm. Algorithm 2.1 1. Initialization: 1.1 Number of epochs k = 0. 1.2 Error goal:= eg. 1.3 Parameters M ? 1 and 0 < c1 ? c2 < 1. 1.4 Weight
13029	13042	Sotiropoulos, and T.N. Grapsa known nonlinear conjugate gradient algorithms. Conjugate gradient methods produce generally faster convergence than the gradient based algorithms. In a recent article , we have introduced an efficient training algorithm based on the technique of nonmonotone spectral conjugate gradient, and we have achieved to improve the convergence rate and the convergence
13029	13042	multiplier, and d0 = ?g0. Conjugate gradient methods differ in their choice for the multiplier ?k used to construct the search direction. In this method, as the previous one introduced in  and , the multiplier ?k is given by the following formula ?k = (?kyk ? sk) T gk+1 s T k yk where yk = gk+1 ? gk. For ?k = 1 the above formula is reduced to the formula introduced by Perry in . In our
13029	13042	that condition (9) is satisfied. Moreover, in order to avoid unnecessary reduction of the learning rate parameter, we enforce that the learning rate satisfies condition (10). In our previous work , we utilized Shanno’s choice for the learning rate adaption given by the formula ? 1 , if k = 0; ?g0?2 ?k = ?k?1?dk?1?2 (12) , otherwise. ?dk?2 where dk is the conjugate gradient direction, dk?1 is
13044	13046	device behavior for a wide range of workload and device pairs and interploates among tables entries in predicting. Anderson’s models are used in an automated storage provision tool, Ergastulum , which formulates automatic storage infrastructure provisioning as an optimization problem and uses device models to guide the search algorithm in locating the solution. Our approach improves on
13044	13049	be exploited. Researchers have long utilized performance models for such prediction to compare alternative storage device designs. Given sufficient effort and expertise, accurate simulations (e.g., ) or analytic models (e.g., ) can be generated to explore design questions for a particular device. Unfortunately, in practice, such time and expertise is not available for deployed
13044	13049	Work Performance modeling has a long and successful history. Almost always, however, thorough knowledge of the system being modeled is assumed. Disk simulators, such as Pantheon  and DiskSim , use software to simulate storage device behavior and produce accurate per-request response times. Developing such simulators is challenging, especially when disk parameters are not publicly
13044	13049	KB 99.9% - 7.40 ms Table 2: Trace summary. We model an Atlas 10K 9GB and a RAID 5 disk array consisting of 8 Atlas 10K disks. The response time is collected by replaying the traces on DiskSim3.0 . Traces. We use three sets of real-world traces in this study. Table 2 lists the summary statistics of the edited traces. The first two, cello92 and cello99 capture typical computer system research
13044	6356	rate of 0.05. Half of the training set is used in building the model and the other half for validation. Such a model takes a long time to converge. ?©?©?????????£¦£???????????£¨?????¢?¤?¦ ? The  maps the input data into a high dimensional space and performs a linear regression there. Our model uses the radial basis function K ¡ xisx¢sexp ¡ ???? x ? xi ??? 2¢sas the kernel function, and ?
13044	13050	times. Developing such simulators is challenging, especially when disk parameters are not publicly available. Predicting performance 1susing simulators is also resource intensive. Analytical models  are more computationally efficient because these models describe device behavior with a set of formulae. Finding the formula set requires deep understanding of the interaction between storage
13044	13053	space, and there is no notion of sequential scans. In contrast, the performance variability can be several orders of magnitude between random and sequential accesses for I/O workloads. Ganger  pointed out the complexity of I/O workloads, and even the detection of sequential scans is a hard problem . Gomez et al.  identified self-similarity in I/O traffic and adopted structural
13044	13054	and discussion in the early stage of this project.sKeywords: Performance prediction, storage device modelings1 Introduction The costs and complexity of system administration in storage systems  and database systems  make automation of administration tasks a critical research challenge. One important aspect of administering self-managed storage systems, particularly large
13044	2673	and storage. Workload characterization is an important part of device modeling because it provides a suitable representation of workloads. Despite abundant published work in modeling web traffic , I/O traffic modeling receives less attention. Direct application of web traffic analysis methods to I/O workloads is not adequate because of the different locality models. Network traffic has a
13044	13063	times. Developing such simulators is challenging, especially when disk parameters are not publicly available. Predicting performance 1susing simulators is also resource intensive. Analytical models  are more computationally efficient because these models describe device behavior with a set of formulae. Finding the formula set requires deep understanding of the interaction between storage
13044	10652	and storage. Workload characterization is an important part of device modeling because it provides a suitable representation of workloads. Despite abundant published work in modeling web traffic , I/O traffic modeling receives less attention. Direct application of web traffic analysis methods to I/O workloads is not adequate because of the different locality models. Network traffic has a
13044	13064	2 lists the summary statistics of the edited traces. The first two, cello92 and cello99 capture typical computer system research I/O workloads, collected at HP Labs in 1992 and 1999 respectively . We preprocess cello92 to concatenate the LBNs of the three most active devices from the trace to fill the modeled device. For cello99, we pick the three most active devices, among the 23 devices,
13044	13065	be exploited. Researchers have long utilized performance models for such prediction to compare alternative storage device designs. Given sufficient effort and expertise, accurate simulations (e.g., ) or analytic models (e.g., ) can be generated to explore design questions for a particular device. Unfortunately, in practice, such time and expertise is not available for deployed
13044	13065	Figure 5: Prediction accuracy of the request-level model. The actual and predicted average response times are 137.96 ms and 133.01 ms respectively. The corresponding demerit, defined in  as the root mean square of horizontal distance between the actual and predicted curves in (b), is 46.06 milliseconds (33.4% of the actual average response time). effective in capturing
13044	13067	utilized performance models for such prediction to compare alternative storage device designs. Given sufficient effort and expertise, accurate simulations (e.g., ) or analytic models (e.g., ) can be generated to explore design questions for a particular device. Unfortunately, in practice, such time and expertise is not available for deployed infrastructures, which are often comprised
13044	13067	times. Developing such simulators is challenging, especially when disk parameters are not publicly available. Predicting performance 1susing simulators is also resource intensive. Analytical models  are more computationally efficient because these models describe device behavior with a set of formulae. Finding the formula set requires deep understanding of the interaction between storage
13044	13068	utilized performance models for such prediction to compare alternative storage device designs. Given sufficient effort and expertise, accurate simulations (e.g., ) or analytic models (e.g., ) can be generated to explore design questions for a particular device. Unfortunately, in practice, such time and expertise is not available for deployed infrastructures, which are often comprised
13044	13068	times. Developing such simulators is challenging, especially when disk parameters are not publicly available. Predicting performance 1susing simulators is also resource intensive. Analytical models  are more computationally efficient because these models describe device behavior with a set of formulae. Finding the formula set requires deep understanding of the interaction between storage
13044	13070	paper. 2 Related Work Performance modeling has a long and successful history. Almost always, however, thorough knowledge of the system being modeled is assumed. Disk simulators, such as Pantheon  and DiskSim , use software to simulate storage device behavior and produce accurate per-request response times. Developing such simulators is challenging, especially when disk parameters are not
13044	13071	self-similarity in I/O traffic and adopted structural models to generate I/O workloads. Kurmas et al.  employed an iterative approach to detect important workload characteristics. Rome  provided a general framework of workload specifications. All the approaches, in one way or another, use empirical distributions derived from given workloads as the parameter values. Our previous
13073	891	on the size of the arena. 2 Related Work Early work on leader-follower control strategies for formations was reported in .  demonstrated distributed control for maintaining robot formations.  demonstrated behavior-based control on physical robots that could negotiate obstacles. They were the first to adopt a set of metrics for formation evaluation as well as defining the various
13073	13074	members of the formation. While there is a robot which performs the role of the formation’s leader, as a formation dynamically organizes, the robot that performs this role can (and does) change. In  the authors describe a behavior-based approach to formations based on potential fields. Separate motor schemas compute a vector for moving to the proper formation position, avoiding static
13073	13076	similar to  in that formations are grown from simpler arrangements. However our approach does not used fixed attachment sites for each robot or predetermine a robot’s position in the formation.  describes a framework for vision-based control of formations using a single omni-directional camera. Their approach emphasizes the ability to switch between simple decentralized controllers and
13073	13076	to vary but within rigid limits. In our approach, the control graph for our formations is implicit and distributed across all the robots involved in the formation. One advantage to the approach in  is the ability to prove formation stability over a range of external inputs. In  the authors describe a virtual structure approach to formations. Their approach incorporates formation feedback
13073	13077	were the first to adopt a set of metrics for formation evaluation as well as defining the various formation position strategies (i.e. unit-center,sleader or neighbor-referenced). More recently,  developed a behavior-based approach for formations where each robot is designated a unique friend robot to follow by visual servoing. A conductor robot that is viewed by all followers maintains the
13073	13077	with a forward-looking mode of sensing include those that are connected, are constructed of line segments, and do not require any backward-looking sensing (often referred to as frontally concave ). Examples of these shapes are diamonds, triangles, hexagons and wedges. Many of these restrictions are artifacts of the mode of sensing and not our approach. Using multiple laser range-finders or
13073	8113	will be sent to the appropriate followers. 4 Experiments Experiments were performed in simulation using the Stage simulator  as well as on Active Media’s Pioneer 2-DX robots using Player . Each robot (simulated or physical) was outfitted with a laser range-finder and a unique laser beacon. 4.1 Experimental Methods The simulations were performed using five, ten and twenty robots in a
13073	13080	formation of spacecraft as key technological milestones for the twenty first century. Applications of space based autonomous vehicles range from ground surveillance to interferometry experiments . Our goal is to have multiple robots organize and maintain simple geometric formations without centralized coordination and using only local sensing. We have designed a bottom-up approach to
13073	13081	have the ability to form formations such as flocks of birds or schools of fish. Animals that can combine their sensing abilities have shown to better avoid predators and efficiently forage for food . Both the Air Force and NASA have identified autonomous formation of spacecraft as key technological milestones for the twenty first century. Applications of space based autonomous vehicles range
13073	13084	is implicit and distributed across all the robots involved in the formation. One advantage to the approach in  is the ability to prove formation stability over a range of external inputs. In  the authors describe a virtual structure approach to formations. Their approach incorporates formation feedback where by the formation leader receives feedback from its followers. Their approach
8920414	13088	systems, there are pre-defined points in the code of the agent where a migration may occur. These points are method call usually called move, jump, fork go or migrate. Wasp , D’Agents  and Ara  are implementing the program counter migration. Initialization migration. This migration technique is a weak alternative to the program counter migration. It means that the execution of
8920414	13088	(variable stack) and operands of computations being on the stack (operand stack) up to the point of interruption. Stack migration depends on the program counter migration. The systems , D’Agents  and Ara  are implementing this feature. Resource migration. External resources that an agent may hold in the moment of migration are references to external objects such as remote objects (CORBA,
8920414	13088	state at source code level and another solution at both, the byte code and interpreter level. There are already implementations integrating migration in the source code  and the interpreter  and , but as far as we know there is none for byte code. 4.1 Solutions Provided by Java Core API The Java core API already provides solutions for code and member migration. Code migration is
8920414	13089	there are pre-defined points in the code of the agent where a migration may occur. These points are method call usually called move, jump, fork go or migrate. Wasp , D’Agents  and Ara  are implementing the program counter migration. Initialization migration. This migration technique is a weak alternative to the program counter migration. It means that the execution of an mobile
8920414	13089	and operands of computations being on the stack (operand stack) up to the point of interruption. Stack migration depends on the program counter migration. The systems , D’Agents  and Ara  are implementing this feature. Resource migration. External resources that an agent may hold in the moment of migration are references to external objects such as remote objects (CORBA, EJB, RMI),
8920414	13089	at source code level and another solution at both, the byte code and interpreter level. There are already implementations integrating migration in the source code  and the interpreter  and , but as far as we know there is none for byte code. 4.1 Solutions Provided by Java Core API The Java core API already provides solutions for code and member migration. Code migration is achieved by
8920414	13818	the Virtual Machine and Instrumenting the Java Byte Code This solution does not change the agent’s code but slightly modifies the compiled Java byte code and extends the virtual machine (VM) . We propose to extend the virtual machine with a JNIbased shared library. Based on the Java Virtual Machine Debugger Interface (JVMDI)  this plugin has access to intern execution data such as
13091	13092	substitution principle. 2 The rule simply states that if we have the 2 A theory of hypothetical hypotheticals would be able to express this in a less awkward—but perhaps no less alarming—way. Abel  for instance gives such a third-order encoding of the ?µ?I ?;? ? N : A@? ?;? ? M : A ? B @? ?;? ? MN : B @? ? E ?;? ? M : ?A@? ?;? ? unbox M : A@? ?E ?;? ? M : A@? ?;? ? here M : ?A@? ?I ?;?,u:A??
13091	13092	? All of the proofs in this section have been formalized in the Twelf system  and verified by its metatheorem checker . 7 The most natural LF encoding of falsehood is 3 rd -order ; we use a 2 nd -order encoding (proving the falsehood substitution theorem by hand) because third-order proof checking is not yet in the distribution. 4 Examples In this section we give proof terms
13091	13095	well known. Essentially, if we interpret the type ¬A as a continuation expecting a value of type A, then the types of these operators are classical tautologies. Griffin first proposed this in 1990  with later refinements by (for example) Murthy . Parigot’s ?µ-calculus  takes this idea and develops it into a full-fledged natural deduction system for classical logic. 1 It began to become
13091	13096	motion as in our mobility rules. In particular, due to the lack of symmetry it is not possible to return to world after a remote procedure call from it, except by returning a value. Jia and Walker  give a judgmental account of an S5-like system based on intuitionistic hybrid logic. Hybrid logics internalize worlds inside propositions by including a proposition that a value of type A resides
13091	13097	Services are similarly restricted to depth-one arrow types. By using ? for mobile code and ? for stationary resources, we believe our resulting calculus is both simpler and more general. Moody  gives a system based on the constructive modal logic S4 due to Pfenning and Davies . This language is based on judgments Atrue (here), Aposs (somewhere), and Avalid (everywhere) rather than the
13091	13098	This paper is an exploration of distributed control flow using a propositions-as-types interpretation of classical modal logic. We build on our previous intuitionistic calculus, Lambda 5 , which is a simple programming language (and associated logic) for distributed computing. Lambda 5 focuses particularly on the spatial distribution of programs, and allows the programmer to express
13091	13098	bias. The logic features a novel decomposition into locally-acting introduction and elimination rules, i.e. ?A@w A@? as well as motion rules for moving between worlds, i.e. ?A@? ?A@? ??? We argue  that this results in a more appropriate operational interpretation. Our classical system also features this decomposition, and like Lambda 5, we are able to retain a crisp connection to the
13091	7106	code and ? for stationary resources, we believe our resulting calculus is both simpler and more general. Moody  gives a system based on the constructive modal logic S4 due to Pfenning and Davies . This language is based on judgments Atrue (here), Aposs (somewhere), and Avalid (everywhere) rather than the “explicit worlds” formulation of Lambda 5 and C5. The operational semantics of his
13091	13101	point we use letcc to create a real A?? ? assumption to discharge u. The remaining cases are similar or straightforward. ? All of the proofs in this section have been formalized in the Twelf system  and verified by its metatheorem checker . 7 The most natural LF encoding of falsehood is 3 rd -order ; we use a 2 nd -order encoding (proving the falsehood substitution theorem by hand)
13091	13103	to discharge u. The remaining cases are similar or straightforward. ? All of the proofs in this section have been formalized in the Twelf system  and verified by its metatheorem checker . 7 The most natural LF encoding of falsehood is 3 rd -order ; we use a 2 nd -order encoding (proving the falsehood substitution theorem by hand) because third-order proof checking is not yet in
13091	13104	the strength of the modal connectives; different assumptions about accessibility give rise to different modal logics. For modelling a network where the worlds are nodes, we choose Intuitionistic S5 , whose relation is reflexive, symmetric, and transitive—every world is related to every other world. Therefore, except when comparing it to other systems, we essentially dispense with the
13091	13104	at some world. Though the logic distinguishes between ?A@? and ?A@? ? , both have precisely the same immediate consequences. The typical rule for eliminating ?, for instance as given by Simpson  is ?A@? A@? ? With this rule, it never really matters where ?A exists, since we can eliminate it instantly to any world. However, we really do care operationally where mobile code resides, and so
13106	13109	browsed on small devices, the user experience is unacceptable. Current approaches for adapting web pages can be divided into two categories: the first one is to transform existing web pages such as , while the other attempts to introduce new formats and mechanisms. As we notice, few of current approaches considered the priorities of different parts in a page. What’s more, none of them let
13106	13110	has been adapted to three typical screen sizes: 640x240 (Handheld PC), 240x320(PDA) and 128x160(Smartphone). For images, an attention model based adaptation and browsing scheme is developed in . Besides that the notion of attention object is just equivalent to information object, other differences are: • The image attention model adds a ROI property to each information object. It is
3812068	13845	computing proposes systems that are permanently present and active, virtually invisible, intelligent, personal assistants. As described by Weiser , Mann , Pentland  and Starner ,  such a system should be an active extension of the user, enhancing his intelligence, augmenting his ability to communicate and interact with the environment and assisting him in a variety of
3812068	13115	integrated into some sort of belt or backpack harness. For some purposes, in particular in well defined industrial applications, such designs are justified and have proven to be successful tools ???. In , a systematic design process has been proposed for such systems. When it comes to realizing the vision of a wearable computer as a context aware, proactive and intelligent personal
3812068	13119	by many in the community, few attempts have been made so far to model, evaluate and implement such systems. In particular, except for the evaluation of the power consumption of individual devices , there are no quantitative results documenting under what circumstances the distributed, heterogeneous approach actually outperforms classical centralized architectures. There are methods available
3812068	13121	centralized architectures. There are methods available to explore the design space of computer architectures. In particular, if the optimization process has do deal with conflicting criteria , . Many known approaches to the design of architectures deal with heterogeneous systems consisting of different sorts of components, e. g. –. However, there are no results available that take
3812068	13121	is specified in the form of integer linear equations . However, in case of non-linear fitness functions and multi-objective criteria it is advantageous to use evolutionary search techniques , . As already mentioned, we are faced with a number of conflicting objectives trading the system wearability factor against power consumption. In addition, there are also conflicts arising from
3812068	13121	The architectures associated with Pareto-optimal fitness vectors represent the tradeoffs in the wearable system design. We have used the well known evolutionary multi-objective optimizer SPEA2 , ,  to select promising architectures for the next iteration. It may be noted that due to the heuristic nature of the search procedure, no statements about the optimality of the final set
3812068	13123	specified in the form of integer linear equations . However, in case of non-linear fitness functions and multi-objective criteria it is advantageous to use evolutionary search techniques , . As already mentioned, we are faced with a number of conflicting objectives trading the system wearability factor against power consumption. In addition, there are also conflicts arising from the
3812068	13126	has do deal with conflicting criteria , . Many known approaches to the design of architectures deal with heterogeneous systems consisting of different sorts of components, e. g. ???. However, there are no results available that take into account peculiarities of distributed wearable systems capable of dealing with dynamically varying, context dependent usage scenarios. C.
3812068	13127	the ’task-device-binding’ and ’performance estimation’ modules. In the current implementation we rely on an evolutionary optimization algorithm that has been successfully used for similar problems . The execution modeling is based on a simple statistical model that abstracts from more complex scheduling and data dependence analysis issues. The wearability quantification used for the examples
3812068	13127	not valid for LD > 1 . The effective execution time Tef is then approximated by the following equation: Te Tef = (5) 1 ? (LD ? Te · R) It is assumed that on average a task is completed within Tef . The effective execution time is used to verify the application’s timing constraints. 3) Device Set: The specification of the device set is based on an abstract classification of devices into five
3812068	13861	to their complexity as described in Section III-A.1. The average execution time is calculated from the data sheet for each class. Non-implemented instructions are emulated by existing instructions , . In our implementation, we follow  in estimating an average number of cycles per instruction (CPI) for superscalar devices; describing the speedup resulting from instruction parallelism.
3812068	13862	complexity as described in Section III-A.1. The average execution time is calculated from the data sheet for each class. Non-implemented instructions are emulated by existing instructions , . In our implementation, we follow  in estimating an average number of cycles per instruction (CPI) for superscalar devices; describing the speedup resulting from instruction parallelism. We
3812068	13128	III-A.1. The average execution time is calculated from the data sheet for each class. Non-implemented instructions are emulated by existing instructions , . In our implementation, we follow  in estimating an average number of cycles per instruction (CPI) for superscalar devices; describing the speedup resulting from instruction parallelism. We assume a CPI of 0.83 for a two-issue and
3812068	13129	modes. They include idle power Pi, sleep power Ps and energy consumption per cycle. For the time being, we use idle power and sleep power from the data sheet. And we follow the approximation from , the power consumption of the Hitachi SH-4 and the SA-1100 devices differs less than 10% during program execution. Consequently, we assumed as a first order model that the consumed energy only
3812068	769	The energy consumption E at frequency f is interpolated using the equation ? f E = Edyn + Estat = ?1 · fmin where the parameters ?1 and ?2 are calculated from the specified Emin and Emax values . ??? 2 + ?2 · fmin f , (1) April 29, 2003 DRAFTsIEEE TRANSACTIONS ON COMPUTERS 17 2) Performance Calculation: To evaluate the performance of a set of tasks the following calculations for delay and
3812068	13130	associated with Pareto-optimal fitness vectors represent the tradeoffs in the wearable system design. We have used the well known evolutionary multi-objective optimizer SPEA2 , ,  to select promising architectures for the next iteration. It may be noted that due to the heuristic nature of the search procedure, no statements about the optimality of the final set of solutions
3812068	13132	as well. C. External Module Extension For computationally intensive tasks wearable computers often use external compute servers to which data is sent for processing over a wireless connection. , . In our methodology, such an external server can be modeled just as any module that requires a wireless connection. Since wearability and power consumption of an external server have no
3812068	13133	well. C. External Module Extension For computationally intensive tasks wearable computers often use external compute servers to which data is sent for processing over a wireless connection. , . In our methodology, such an external server can be modeled just as any module that requires a wireless connection. Since wearability and power consumption of an external server have no impact on
3642013	13138	this information is readily accessible to the designer. Another application is image-based rendering, where a user is allowed to move about in a scene by warping captured or rendered images . If these images have limited dynamic range, it is next to impossible to adapt the exposure based on the current view, and quality is compromised. Using HDR pixels, a natural view can be provided
3642013	13141	and green areas have inaccurate color. Fig. 5b shows the same color negative scanned into our 32-bit/pixel high dynamic range TIFF format and tone mapped using a histogram compression technique . Fig. 6c shows the same HDR TIFF remapped using the perceptual model of Pattanaik et al . Figs. 6a and 6b show details of light and dark areas of the HDR image whose exposure has been adjusted
3642013	13141	original negative. Without an HDR encoding, this information is either lost or unusable. Figure 5. The left image(a) shows the YCC encoding after remapping with a high dynamic range tone operator . Unfortunately, since YCC has so little dynamic range, most of the bright areas are lost. The right image (b) shows the same operator applied to a 32-bit HDR TIFF encoding, showing the full dynamic
3642013	13142	our 32-bit/pixel high dynamic range TIFF format and tone mapped using a histogram compression technique . Fig. 6c shows the same HDR TIFF remapped using the perceptual model of Pattanaik et al . Figs. 6a and 6b show details of light and dark areas of the HDR image whose exposure has been adjusted to show the detail captured in the original negative. Without an HDR encoding, this
3642013	13142	the image detail recorded on the negative. The lower-left image (b) shows house details boosted by 3 f-stops. The right image (c) shows our HDR TIFF mapped with the Pattanaik-Ferwerda tone operator . Discussion It is clear from looking at these images that current methods for tone-mapping HDR imagery, although better than a simple S-curve, are less than perfect. It would therefore be a mistake
13146	13154	at capturing complex non-linear characteristics of FTS , . Support vector machines represent another powerful regression technique that immediately found applications in financial forecasting , . While there is already extensive knowledge available in pattern recognition domain, it has been rarely used for FTS prediction. The major problem lies in the fact that classification model
13146	13155	A temporal classification models would have to provide a specific definition of classes or obtain it from the series by discretisation. Although some work has already been done in this field , , ,  there is still lack of pattern recognition based models that would offer immediate investment applications surpassing in functionality and performance the traditional regression based
1045129	13158	in the presence of network delay, motivated by two lines of recent research. First, extensive experimental results have been conducted to compare the performance of Vegas and Reno, e.g., , , . Its dynamic and fairness properties have also been studied in , , , but these papers consider only a single bottleneck link and network delay is not accounted for in the study of
1045129	13159	will be cleared in equilibrium. Since queues are now empty, queueing delay can no longer serve as a feedback signal. ECN marking must be used to explicitly feedback the prices, e.g., using REM , , . Imagine now a network with both types of links, one does not use ECN nor perform AQM to clear their queues and one does. The first type maintains a queue but does not mark, while the
1045129	13160	, . In particular, a TCP/AQM algorithm is designed in  that maintains linear stability for arbitrary network delays and capacities. It is in the class of “dual” algorithms of ,  that use static source algorithms, and it employs a sophisticated scaling with respect to network delays and capacities to achieve high utilization and fast response without compromising stability.
1045129	13161	research. First, extensive experimental results have been conducted to compare the performance of Vegas and Reno, e.g., , , . Its dynamic and fairness properties have also been studied in , , , but these papers consider only a single bottleneck link and network delay is not accounted for in the study of its dynamics. Optimization based models are used in ,  to analyze
1045129	13165	in the presence of network delay, motivated by two lines of recent research. First, extensive experimental results have been conducted to compare the performance of Vegas and Reno, e.g., , , . Its dynamic and fairness properties have also been studied in , , , but these papers consider only a single bottleneck link and network delay is not accounted for in the study of its
1045129	13166	version. In contrast to Reno and its variants, Vegas seems particularly well-suited for high speed networks. Reno and its variants, with RED, become unstable as network capacity increases , . It also must maintain an exceedingly small loss probability in equilibrium that is difficult to reliably use for control. Vegas, on the other hand, scales correctly with capacity. Moreover,
1045129	8600	out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility is (often implicitly) defined by its TCP algorithm; see also , , , , . These models mostly focus on the equilibrium structure, and do not adequately deal with network delay. To complement this series of work, we use here a multi-link multi-source model, Stabilized
1045129	13169	version. In contrast to Reno and its variants, Vegas seems particularly well-suited for high speed networks. Reno and its variants, with RED, become unstable as network capacity increases , . It also must maintain an exceedingly small loss probability in equilibrium that is difficult to reliably use for control. Vegas, on the other hand, scales correctly with capacity. Moreover, while
1045129	2358	link and network delay is not accounted for in the study of its dynamics. Optimization based models are used in ,  to analyze a general network of Vegas. In particular, it is shown in ,  that any TCP/AQM (active queue management) protocol can be interpreted as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility is
1045129	8602	as carrying out a distributed primal-dual algorithm over the Internet to maximize aggregate utility, and a user’s utility is (often implicitly) defined by its TCP algorithm; see also , , , , . These models mostly focus on the equilibrium structure, and do not adequately deal with network delay. To complement this series of work, we use here a multi-link multi-source
1045129	8602	, , . In particular, a TCP/AQM algorithm is designed in  that maintains linear stability for arbitrary network delays and capacities. It is in the class of “dual” algorithms of ,  that use static source algorithms, and it employs a sophisticated scaling with respect to network delays and capacities to achieve high utilization and fast response without compromising
1045129	8603	as an alternative to TCP Reno. Unlike Reno (or its variants such as NewReno and SACK), that uses packet loss as a measure of congestion, Vegas uses queueing delay as a measure of congestion . Vegas introduces a new congestion avoidance mechanism that corrects the oscillatory behavior of AIMD. While the AIMD algorithm induces loss to learn the available network capacity, a Vegas source
1045129	8603	in the routers along the path. Provided there is enough buffering, a network of Vegas sources will stabilize around a proportionally fair equilibrium and packet loss will be eliminated; see  for details. In this paper, we study the stability of this equilibrium in the presence of network delay, motivated by two lines of recent research. First, extensive experimental results have been
1045129	8603	studied in , , , but these papers consider only a single bottleneck link and network delay is not accounted for in the study of its dynamics. Optimization based models are used in ,  to analyze a general network of Vegas. In particular, it is shown in ,  that any TCP/AQM (active queue management) protocol can be interpreted as carrying out a distributed primal-dual
1045129	8603	to keep the gain over the feedback loop under control. It turns out that the (implicit) link algorithm of Vegas has exactly the right scaling with respect to capacity as used in , ; see . This built-in scaling with capacity makes Vegas potentially scalable to high bandwidth, in stark contrast to Reno and its variants. The source algorithm of Vegas, however, has a different scaling
1045129	8603	the equilibrium queueing delay can be excessive at low capacity, it is reduced as capacity increases. Other problems, such as error in propagation delay estimation due to queues and rerouting , , may be less severe at high capacity, as buffers clear more frequently. II. NETWORK MODEL A network is modeled as a set of L links (scarce resources) with finite capacities c = (cl, l ? L). They
1045129	13170	First, extensive experimental results have been conducted to compare the performance of Vegas and Reno, e.g., , , . Its dynamic and fairness properties have also been studied in , , , but these papers consider only a single bottleneck link and network delay is not accounted for in the study of its dynamics. Optimization based models are used in ,  to analyze a
1045129	13170	while the equilibrium queueing delay can be excessive at low capacity, it is reduced as capacity increases. Other problems, such as error in propagation delay estimation due to queues and rerouting , , may be less severe at high capacity, as buffers clear more frequently. II. NETWORK MODEL A network is modeled as a set of L links (scarce resources) with finite capacities c = (cl, l ? L).
1045129	13170	low bandwidth regime in order for Vegas to reach equilibrium is much less severe in the high bandwidth regime. Moreover, problem with error in propagation delay estimation and persistent congestion ,  is also eased with high capacity, as buffers empty more frequently. Though there are other issues with using delay for congestion control, it seems that unless ECN is widely deployed, these
1045129	2360	been studied in , , , but these papers consider only a single bottleneck link and network delay is not accounted for in the study of its dynamics. Optimization based models are used in ,  to analyze a general network of Vegas. In particular, it is shown in ,  that any TCP/AQM (active queue management) protocol can be interpreted as carrying out a distributed
1045129	13171	the effect of delay on stability of Vegas, and how to stabilize it. Second, this paper is motivated by the stability theory for linear distributed and delayed system recently developed in , , , , . In particular, a TCP/AQM algorithm is designed in  that maintains linear stability for arbitrary network delays and capacities. It is in the class of “dual” algorithms
1045129	13171	scalable stability, however, dictates a specific source utility function and hence fairness in rate allocation. By introducing a slower timescale dynamics into the source algorithm, the TCP/AQM of  is extended in  to track any utility function, or fairness, on a slow timescale, provided there is a known bound on network delays. The main insight from this series of work is to scale down
1045129	13171	in order to keep the gain over the feedback loop under control. It turns out that the (implicit) link algorithm of Vegas has exactly the right scaling with respect to capacity as used in , ; see . This built-in scaling with capacity makes Vegas potentially scalable to high bandwidth, in stark contrast to Reno and its variants. The source algorithm of Vegas, however, has a
1045129	13171	r Ur(xr) subject to link capacity constraints, with the utility functions (see  for details) Ur(xr) = ?rdr log xr Hence Vegas achieves weighted proportional fairness . The link algorithm of ,  is the same as (3) of Vegas, except that, there, c is a virtual capacity that is strictly less than real link capacity in order to clear the queue in equilibrium. There, pl(t) can be
1045129	13171	the rest of the paper that all sources have the same target queue length, ?rdr = ? for all r (otherwise, take ? to be the minimum ?rdr in the stability results that follow). B. Stability Following , we can express the error equations of (1)– (2) in matrix form, in Laplace, as where ?y(s) = R(s)?x(s) (9) ?q(s) = diag{e ?sTr }R T (?s)?p(s) (10) Rlr(s) = ? ? ?s? e ls if Rlr = 1 0 otherwise The
11844529	8447	of other design parameters on adaptive routing performance, including . More extensive performance studies on routing strategy have been done for deterministic schemes, in papers such as . Several of these studies have presented analytic models of deterministic routing schemes as well as simulation experiments. The work presented here is a performance evaluation study of one of the
11844529	2077	of other design parameters on adaptive routing performance, including . More extensive performance studies on routing strategy have been done for deterministic schemes, in papers such as . Several of these studies have presented analytic models of deterministic routing schemes as well as simulation experiments. The work presented here is a performance evaluation study of one of the
11844529	13207	emphasis is on algorithm design and performance analysis of the algorithms is secondary. Some studies have addressed the e ects of other design parameters on adaptive routing performance, including . More extensive performance studies on routing strategy have been done for deterministic schemes, in papers such as . Several of these studies have presented analytic models
11844529	13208	This work is supported in part by NSF Grant MIP-9113268 Various strategies for preventing deadlock have been proposed, these include the Turn model , variations on the use of virtual channels , and others involving restrictions on the choice of paths or involving the use of additional hardware .In most of these studies, however, the emphasis is on algorithm design and
11844529	13209	include the Turn model , variations on the use of virtual channels , and others involving restrictions on the choice of paths or involving the use of additional hardware .In most of these studies, however, the emphasis is on algorithm design and performance analysis of the algorithms is secondary. Some studies have addressed the e ects of other design parameters on
11844529	2081	of other design parameters on adaptive routing performance, including . More extensive performance studies on routing strategy have been done for deterministic schemes, in papers such as . Several of these studies have presented analytic models of deterministic routing schemes as well as simulation experiments. The work presented here is a performance evaluation study of one of the
11844529	13210	of other design parameters on adaptive routing performance, including . More extensive performance studies on routing strategy have been done for deterministic schemes, in papers such as . Several of these studies have presented analytic models of deterministic routing schemes as well as simulation experiments. The work presented here is a performance evaluation study of one of the
11844529	356	This work is supported in part by NSF Grant MIP-9113268 Various strategies for preventing deadlock have been proposed, these include the Turn model , variations on the use of virtual channels , and others involving restrictions on the choice of paths or involving the use of additional hardware .In most of these studies, however, the emphasis is on algorithm design and
11844529	360	in order to limit system cost and complexity. This work is supported in part by NSF Grant MIP-9113268 Various strategies for preventing deadlock have been proposed, these include the Turn model , variations on the use of virtual channels , and others involving restrictions on the choice of paths or involving the use of additional hardware .In most of these
11844529	361	and to that of dimension ordered deterministic routing. 2 Network and Message Model The interconnection network model used in this study is that of a k-ary n-cube. Virtual cut-through switching  is used: message advancement is similar to wormhole routing, except that the body of a message can continue to progress even while the message head is blocked, and the entire message can be bu
11844529	13215	emphasis is on algorithm design and performance analysis of the algorithms is secondary. Some studies have addressed the e ects of other design parameters on adaptive routing performance, including . More extensive performance studies on routing strategy have been done for deterministic schemes, in papers such as . Several of these studies have presented analytic models
11844529	2096	This work is supported in part by NSF Grant MIP-9113268 Various strategies for preventing deadlock have been proposed, these include the Turn model , variations on the use of virtual channels , and others involving restrictions on the choice of paths or involving the use of additional hardware .In most of these studies, however, the emphasis is on algorithm design and
13248	13249	independent subtree requires a full pass over the string. Several O(n 2 ) and O(n log n) algorithms for constructing suffix trees are described in . A top-down approach has been suggested in . In , the authors explore the benefits of using a lazy implementation of suffix trees. In this approach, the authors argue that one can avoid paying the full construction cost by constructing
13248	13250	a result, the average LCP is higher than that for uniformly distributed data. Figure 6 shows a histogram for the longest common prefixes generated while constructing suffix trees on the SwissProt  and a 50 MB Human DNA sequence . Notice that both sequences have a high probability that the LCP will be greater than 1. Even among biological datasets, the differences can be quite dramatic.
13248	13250	it was not designed as an in-memory technique. For this experiment, we used five different data sources : chromosome 2 of Drosophila Melanogaster from GenBank , a slice of the SwissProt dataset  having 20 million symbols, and the text from the 1995 collection from project Gutenberg . We also chose two strings that contain uniformly distributed symbols from an alphabet of size four and
13248	13250	Structure SwissProt Human DNA (size in pages) (size in pages) String 6,250 6,250 Suffixes 1,250 6,250 Temp 1,250 6,250 Tree 4,100 16,200 Table 2: Array Sizes trees on the SwissProt database  and a 50 Mbps slice of the Human Chromosome-1 database . A prefixlen of 1 was used for partitioning in the first phase. The size of each of the arrays for these datasets is summarized in Table
13248	13250	experimental evaluations show that TDD scales gracefully as the dataset size increases. The TDD approach lets us build suffix trees on large frequently used sequence datasets such as UniProt/TrEMBL  in a few minutes. Algorithms to construct suffix trees on this scale (to our knowledge) have not been mentioned in literature before. The TDD approach outperforms a popular disk-based suffix tree
13248	13251	use an O(n 2 ) algorithm with a better locality of reference. In one pass over the string, they index all suffixes with the same prefix by inserting them into an on-disk subtree managed by PJama , a Java based object store. Construction of each independent subtree requires a full pass over the string. Several O(n 2 ) and O(n log n) algorithms for constructing suffix trees are described in
13248	13251	algorithm is from the OASIS search tool , which is part of the Periscope project . The OASIS implementation uses a shared buffer cache instead of the persistent Java object store, PJama , described in the original proposal . The buffer manager employs the CLOCK replacement policy. The OASIS implementation performed better than the implementation described in . This is not
13248	13252	(and rapidly growing) size of many string datasets underscores the need for fast disk-based suffix tree construction algorithms. A few recent research efforts have also considered this problem , though neither of these approaches scales well for large datasets (such as a large chromosome, or an entire eukaryotic genome). In this paper, we present a new approach to efficientlysconstruct
13248	13252	Recently, Bedathur et al. developed a buffering strategy, called TOP-Q, which improves the performance of the Ukkonen’s algorithm (which uses suffix links) when constructing on-disk suffix trees . A different approach was suggested by Hunt et al.  where the authors drop the use of suffix links and use an O(n 2 ) algorithm with a better locality of reference. In one pass over the string,
13248	13252	sizes of string data. We also describe how the design choices we have made in TDD overcome the performance bottlenecks present in other proposed techniques. 4.1 I/O Benefits Unlike the approach of  where the authors use the best in-memory O(n) algorithm (Ukkonen) as the basis for their disk-based algorithm, we use the theoretically less efficient O(n2 ) wotdeager algorithm . A major
13248	13252	Size) (a) SwissProt Figure 10: Temp Buffer Figure 11: Tree Buffer Comparison of TDD with TOP-Q Very recently, Bedathur and Haritsa have proposed the TOP-Q technique for constructing suffix trees . TOP-Q is a new low overhead buffer management method which can be used with Ukkonen’s construction algorithm. The goal of these researchers is to invent a buffer management technique that does not
13248	13252	To compare TDD with TOP-Q, we obtained a copy of the TOP-Q code from the authors. This version of the code only supports building suffix tree indices on DNA sequences. As per the recommendation in , we used a buffer pool of 880M for the internal nodes and 800M for the leaf nodes (this was the maximum memory allocation possible with the TOP-Q code). On 50Mbp of Human Chromosome-1, TOP-Q took
13248	13254	data structures. The focus of this paper is only on suffix trees. Our solution uses a simple partitioning strategy. However, a more sophisticated partitioning method has been proposed recently , which can complement our existing partitioning method. 3 The TDD Technique Most suffix tree construction algorithms do not scale due to the prohibitive disk I/O requirements. The high percharacter
13248	13255	may be able to identify opportunities for prefetching the cache lines . In addition, recently proposed techniques for overlapping execution with main-memory latency, such as software pipelining , can easily be incorporated in TDD. 4.3 Effect of Alphabet Size and Data Skew There are two properties of the input string that can affect the execution time of suffix tree construction techniques:
13248	13256	computational models such as RAM, PRAM, and various other external memory models . Suffix arrays have also been used as an alternative to suffix trees for specific string matching tasks . However, in general, suffix trees are more versatile data structures. The focus of this paper is only on suffix trees. Our solution uses a simple partitioning strategy. However, a more
13248	13257	computational models such as RAM, PRAM, and various other external memory models . Suffix arrays have also been used as an alternative to suffix trees for specific string matching tasks . However, in general, suffix trees are more versatile data structures. The focus of this paper is only on suffix trees. Our solution uses a simple partitioning strategy. However, a more
13248	13258	query, once the suffix tree is built on the database string. Suffix trees can also be used to solve approximate string matching problems efficiently. Some bioinformatics applications such as MUMmer , REPuter , and OASIS  exploit suffix trees to efficiently evaluate queries on biological sequence datasets. However, suffix trees are not widely used because of their high cost of
13248	13261	independent subtree requires a full pass over the string. Several O(n 2 ) and O(n log n) algorithms for constructing suffix trees are described in . A top-down approach has been suggested in . In , the authors explore the benefits of using a lazy implementation of suffix trees. In this approach, the authors argue that one can avoid paying the full construction cost by constructing
13248	13262	requires a full pass over the string. Several O(n 2 ) and O(n log n) algorithms for constructing suffix trees are described in . A top-down approach has been suggested in . In , the authors explore the benefits of using a lazy implementation of suffix trees. In this approach, the authors argue that one can avoid paying the full construction cost by constructing the
13248	13262	component of the TDD technique is our suffix tree construction algorithm, called PWOTD (Partition and Write Only Top Down). This algorithm is based on the wotdeager algorithm suggested by Kurtz . We improve onsString: ATTAGTACA$ 0 1 2 3 4 5 6 7 8 9 CA$ GTACA$ $ TTAGTACA$ A TAGTACA$ T A CA$ GTACA$ $ GTACA$ 0 1 2 3 4 5 6 7 A T 00000000000 11111111111 00000000000 11111111111 7 4 9 3
13248	13262	to the first child. Note that the leaf nodes do not have a second entry. The leaf node requires only the starting index of the label; the end of the label is the string’s terminating character. See  for a more detailed explanation. The PWOTD algorithm consists of two phases. In phase one, we partition the suffixes of the input string into |A| prefixlen partitions, where |A| is the alphabet
13248	13262	the approach of  where the authors use the best in-memory O(n) algorithm (Ukkonen) as the basis for their disk-based algorithm, we use the theoretically less efficient O(n2 ) wotdeager algorithm . A major difference between the two algorithms is that the Ukkonen algorithm sequentially accesses the string data and then updates the suffix tree through random traversals, while our TDD approach
13248	13264	this paper, building a suffix tree on moderately sized datasets, such as a single chromosome of the human genome, takes over 1.5 hours with the best known existing disk-based construction technique . In contrast, the techniques that we develop in this paper reduce the construction time by a factor of 5 on inputs of the same size. Even though suffix trees are currently not in widespread use,
13248	13264	(and rapidly growing) size of many string datasets underscores the need for fast disk-based suffix tree construction algorithms. A few recent research efforts have also considered this problem , though neither of these approaches scales well for large datasets (such as a large chromosome, or an entire eukaryotic genome). In this paper, we present a new approach to efficientlysconstruct
13248	13264	called TOP-Q, which improves the performance of the Ukkonen’s algorithm (which uses suffix links) when constructing on-disk suffix trees . A different approach was suggested by Hunt et al.  where the authors drop the use of suffix links and use an O(n 2 ) algorithm with a better locality of reference. In one pass over the string, they index all suffixes with the same prefix by
13248	13264	experimental evaluation of the different suffix tree construction techniques. In addition to TDD, we compare Ukkonen’s algorithm  for in-memory construction performance, and Hunt’s algorithm  for disk-based construction performance. Ukkonen’s and Hunt’s algorithms are considered the best known suffix tree construction algorithms for the in-memory case and the disk based case
13248	13264	, which is part of the Periscope project . The OASIS implementation uses a shared buffer cache instead of the persistent Java object store, PJama , described in the original proposal . The buffer manager employs the CLOCK replacement policy. The OASIS implementation performed better than the implementation described in . This is not surprising since PJama incurs the overhead
13248	13267	space. Because this span of memory is too large to fit in the processor cache, each access has a very high probability of incurring the full main-memory latency. Using an array based representation , where the pointers to the children are stored in an array with an element for each symbol in the alphabet, can reduce the number of cache misses. However, this representation uses up a lot more
13248	13267	will have to be examined while searching for the right place to insert. This leads to a longer running time for Ukkonen. There are hash-based and array based approaches that alleviate this problem , but at the cost of consuming much more space for the tree. A larger representation naturally implies that we are limited to building trees on smaller strings. We experimentally demonstrate these
13248	13269	tree is built on the database string. Suffix trees can also be used to solve approximate string matching problems efficiently. Some bioinformatics applications such as MUMmer , REPuter , and OASIS  exploit suffix trees to efficiently evaluate queries on biological sequence datasets. However, suffix trees are not widely used because of their high cost of construction. As we
13248	13271	the database string. Suffix trees can also be used to solve approximate string matching problems efficiently. Some bioinformatics applications such as MUMmer , REPuter , and OASIS  exploit suffix trees to efficiently evaluate queries on biological sequence datasets. However, suffix trees are not widely used because of their high cost of construction. As we show in this paper,
13248	13271	in C. The algorithm operates entirely in main memory, and there is no persistence. The representation uses 32 bytes per node. Our implementation of Hunt’s algorithm is from the OASIS search tool , which is part of the Periscope project . The OASIS implementation uses a shared buffer cache instead of the persistent Java object store, PJama , described in the original proposal .
13248	13272	computational models such as RAM, PRAM, and various other external memory models . Suffix arrays have also been used as an alternative to suffix trees for specific string matching tasks . However, in general, suffix trees are more versatile data structures. The focus of this paper is only on suffix trees. Our solution uses a simple partitioning strategy. However, a more
13248	13273	memory, and there is no persistence. The representation uses 32 bytes per node. Our implementation of Hunt’s algorithm is from the OASIS search tool , which is part of the Periscope project . The OASIS implementation uses a shared buffer cache instead of the persistent Java object store, PJama , described in the original proposal . The buffer manager employs the CLOCK
13248	14015	Endowment. Proceedings of the 30th VLDB Conference, Toronto, Canada, 2004 University of Michigan 1301 Beal Avenue; Ann Arbor, MI 48109-2122; USA {tatas,hankinsr,jignesh}@eecs.umich.edu teen months . Consequently, methods for efficiently querying large string datasets are critical to the success of these emerging database applications. Suffix trees are versatile data structures that can help
13248	13277	currently not in widespread use, there is a rich history of algorithms for constructing suffix trees. A large focus of previous research has been on linear-time suffix tree construction algorithms . These algorithms are well suited for small input strings where the tree can be constructed entirely in main memory. The growing size of input datasets, however, requires that we construct suffix
13248	13277	Section 4 . Section 5, presents the experimental results, and Section 6 presents our conclusions. 2 Related Work Linear time algorithms for constructing suffix trees have been described by Weiner , McCreight , and Ukkonen . Ukkonen’s is a popular algorithm because it is easier to implement than the other algorithms. It is an O(n), in-memory construction algorithm based on the clever
58405	13280	Giuseppe Riccardi and Dilek Hakkani-TürsAT&T Labs-Research, 180 Park Avenue, Florham Park, NJ, ¡ USA dtur¢ dsp3, @research.att.com the most informative ones with respect to a given cost function. The goal of the active learning algorithm is to select the examples for labeling which will have the largest performance improvement. Unsupervised learning aims at utilizing unlabeled examples, to
58405	13281	order. were not selected for transcription using their ASR output and word confidence scores. Since no human-intervention is employed to use this data, it is called unsupervised learning . In Section 2, we describe our algorithms for active and unsupervised learning and how we combine them. In Section 3, we describe our experiments and results. 2. APPROACH In this section, we
58405	13282	order. were not selected for transcription using their ASR output and word confidence scores. Since no human-intervention is employed to use this data, it is called unsupervised learning . In Section 2, we describe our algorithms for active and unsupervised learning and how we combine them. In Section 3, we describe our experiments and results. 2. APPROACH In this section, we
58405	13282	speech utterances are computed in the following way: ?? ? ? ???¦????? ?? ????????????? ?? ? (3) ???¦???¦?¦????? The acoustic models can be trained by using all the data in a similar fashion . ??? ?s3. EXPERIMENTS AND RESULTS We performed a series of experiments to verify that word confidence scores can be used to identify correctly recognized words, utterance confidence scores can be
58405	13284	order. were not selected for transcription using their ASR output and word confidence scores. Since no human-intervention is employed to use this data, it is called unsupervised learning . In Section 2, we describe our algorithms for active and unsupervised learning and how we combine them. In Section 3, we describe our experiments and results. 2. APPROACH In this section, we
58405	13285	order. were not selected for transcription using their ASR output and word confidence scores. Since no human-intervention is employed to use this data, it is called unsupervised learning . In Section 2, we describe our algorithms for active and unsupervised learning and how we combine them. In Section 3, we describe our experiments and results. 2. APPROACH In this section, we
58405	13285	models, using a small set of transcribed data,¤¦¥ . Using these models, we compute the speech utterances confidence scores and predict which candidate utterances are recognized (in)correctly . The utterances are ranked according to their estimated correctness ( the higher the score the higher the rank order) We then add the transcribed utterances to¤ ¥ and exclude them from¤¨§ . This
58405	13287	then how we compute word and utterance confidence scores from lattice output of ASR. 2.1. Active Learning Inspired by the certainty-based active learning methods to reduce the transcription effort , we select the examples that we predict that the speech recognizer has misrecognized and give them to human labelers for transcription. We use these transcribed utterances for training acoustic and
58405	13290	transcribe, and automatic speech recognition accuracy can be improved by exploiting untranscribed data. For all these experiments, we used utterances from the How May I Help You???? speech database . The language models used in all our experiments are trigram models based on Variable Ngram Stochastic Automata . 3.1. Training and Test Data In the How May I Help You???? speech database there
4802028	13293	the minimal number of rules needed to represent the function in a particular language. XCS-NGA is related to a number of other machine learning algorithms. For example, CMAC function approximators  adapt the weight of each region in each of multiple partitions of the input space. Partitions may be regular or generated at random, and XCS-NGA di ers essentially only in the details of how
4802028	13294	test in the LCS literature . It has also been used with other machine learning systems including neural networks , decision trees , and the GPAC algorithm . See  for a review of some of the earlier work using the multiplexer. A number of studies have looked at larger multiplexer functions, e.g.
4802028	13300	to evaluate alternative systems, mechanisms, and parameter settings. Of these data sets, the venerable 6 multiplexer (a 6-bit Boolean function) is the most widely used test in the LCS literature . It has also been used with other machine learning systems including neural networks , decision trees , and the GPAC algorithm . See  for a review of some of
4802028	13313	more complex than 6-bit constant functions. Elsewhere, wehave demonstrated a strong correlation between the size of the minimal representation of these functions and their di culty for XCS . One consequence is that even successful solution by XCS of a large multiplexer, such as the 70-bit multiplexer , does not mean that XCS can solve all 70-bit functions with comparable e ort;
4802028	13314	and representation. For example, with the language wehave used the 6-bit parity functions are much more complex than 3 In simple terms, the shortest possible representation in a given formalism .sthe6multiplexer, which in turn is considerably more complex than 6-bit constant functions. Elsewhere, wehave demonstrated a strong correlation between the size of the minimal representation of
4802028	13316	10, 39, 16, 18, 19, 40, 9, 20, 3, 8, 21]. It has also been used with other machine learning systems including neural networks , decision trees , and the GPAC algorithm . See  for a review of some of the earlier work using the multiplexer. A number of studies have looked at larger multiplexer functions, e.g. , up to the 70-bit multiplexer .
4802028	13326	test in the LCS literature . It has also been used with other machine learning systems including neural networks , decision trees , and the GPAC algorithm . See  for a review of some of the earlier work using the multiplexer. A number of studies have looked at larger multiplexer functions, e.g.
34689	13331	online sources do not even provide the coordinates of the corner points of the maps. In previous work, we developed an approach to automatically conflating road vector data with satellite imagery . In this paper we describe how we address the even more challenging problem of automatically conflating maps with satellite imagery. Since we build on our previous work, we first review our
34689	13331	and inaccurate, we find the road intersections in the imagery by first aligning road vector data with imagery and then locating the road network intersection points from the vector data. In , we described several techniques for automatic conflation of road vector data with satellite imagery. The most effective technique we found exploits a combination of the knowledge of the road
34689	13331	imagery or maps . Our work significantly differs from the previous work in terms of our approach to conflate vector data with satellite imagery. These differences are described in detail in . 5 Discussion Given the huge amount of geospatial data now available, our ultimate goal is to be able to automatically integrate this information using the limited information available about each
34689	13332	for the maps are known in advance, although we do assume that we know the general region. There has been a considerable amount of work on conflating vector data with satellite imagery or maps . Our work significantly differs from the previous work in terms of our approach to conflate vector data with satellite imagery. These differences are described in detail in . 5 Discussion Given
34689	13335	for the maps are known in advance, although we do assume that we know the general region. There has been a considerable amount of work on conflating vector data with satellite imagery or maps . Our work significantly differs from the previous work in terms of our approach to conflate vector data with satellite imagery. These differences are described in detail in . 5 Discussion Given
34689	13337	previously unknown. 4 Related Work While the conflation technique was described in  in 1993, there has been relatively little work on automatically conflating maps with satellite imagery. In , the authors describe how an edge detection process can be used to determine a set of features that can be used to conflate two image data sets. However, their work requires that the coordinates of
8920449	2253	we focus on finding serial episodes from data streams. To the best of our knowledge the problem of mining serial episodes from data streams has been studied in depth only for length-1 episodes . 2 Concepts Definition 1 (Sequence and stream) Let I be a set of items. A sequence s over I is an ordered list ss . . . s = s of elements. The length of the sequence s, i.e.,
8920449	13340	click streams, market basket data, web logs, and other time series. One of the most popular patterns mined from sequential data are the episodes, i.e., directed acyclic graphs with labeled nodes . An important subclass of episodes are the serial episodes, which are essentially sequences. Serial episodes are useful in many applications, including network monitoring and molecular biology.
8920449	13340	the order they are in the stream. In the context of this paper, this time information can be ignored. There are many ways how the number of occurrences of a sequence t in a sequence s are counted . The notion of an occurrence of t in s, however, is virtually always based on t being a subsequence of s. Definition 2 (Subsequence) A sequence t is contained in a sequence s, i.e., t is a
8920451	13364	units of the block length 10bp (base pairs). Wavelengths (or periodicities) close to the model predicted values have been reported in weather and climate variability , prime number distribution , Riemann zeta zeros (non-trivial) distribution , stock market economics . (d) The conventional power spectrum plotted as the variance versus the frequency in log-log scale will now
4823376	13385	Storage 155 – random access to the matrix, – effective selection of any submatrix – persistence of the matrix (usage of secondary memory). 3 Sparse matrices and finite automata Culik and Valenta  introduced finite automata for compression of bi-level and simple color images. A digitized image of the finite resolution m × n consists of m × n pixels each of which takes a Boolean value (1 for
4823376	13388	space in the multi-dimensional approach, we use multi-dimensional data structures for their indexing, e.g., paged and balanced multi-dimensional data structures like UB-tree , BUB-tree , R-tree , and R ? -tree . (B)UB-tree data structure applies Z-addresses (Z-ordering)  for mapping a multi-dimensional space into single-dimensional. Intervals on Z-curve (which is defined
4823376	10210	in the multi-dimensional approach, we use multi-dimensional data structures for their indexing, e.g., paged and balanced multi-dimensional data structures like UB-tree , BUB-tree , R-tree , and R ? -tree . (B)UB-tree data structure applies Z-addresses (Z-ordering)  for mapping a multi-dimensional space into single-dimensional. Intervals on Z-curve (which is defined by this
8920461	13412	of occurrence (Hooge et. al., 1994). Time series analyses of global market economy also exhibits power-law behaviour (Bak et al., 1992; Mantegna and Stanley, 1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible
8920461	8444779	of global market economy also exhibits power-law behaviour (Bak et al., 1992; Mantegna and Stanley, 1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible multifractal structure (Farmer, 1999) and has suggested an analogy to fluid turbulence
8920461	13418	of global market economy also exhibits power-law behaviour (Bak et al., 1992; Mantegna and Stanley, 1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible multifractal structure (Farmer, 1999) and has suggested an analogy to fluid turbulence
8920461	13419	1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible multifractal structure (Farmer, 1999) and has suggested an analogy to fluid turbulence (Ghashghaie et al., 1996; Arneodo et al., 1998). Sornette et al. (1995) conclude that the
8920461	13420	1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible multifractal structure (Farmer, 1999) and has suggested an analogy to fluid turbulence (Ghashghaie et al., 1996; Arneodo et al., 1998). Sornette et al. (1995) conclude that the
8920461	13427	exhibits power-law behaviour (Bak et al., 1992; Mantegna and Stanley, 1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible multifractal structure (Farmer, 1999) and has suggested an analogy to fluid turbulence (Ghashghaie et al., 1996;
8920461	13348	repeats (Holste et al., 2001). A summary of recent results relating to long-range correlation (LRC) in DNA sequences is given in the following. Based on spectral analyses, Li et al. found ( Li, 1992; Li and Kaneko, 1992; Li, Marr and Kaneko, 1994) that the frequency spectrum of a DNA sequence containing mostly introns shows 1/f ? behavior, which evidences the presence of long-range
8920461	13352	Mantegna and Stanley, 1995; Sornette et al., 1995; Chen, 1996a,b; Stanley, Amaral, Buldyrev, Havlin et al., 1996; Feigenbaum and Freund, 1997a,b; Gopikrishnan et al., 1999; Plerou et al., 1999; Stanley et al., 2000; Feigenbaum, 2001a, b) with possible multifractal structure (Farmer, 1999) and has suggested an analogy to fluid turbulence (Ghashghaie et al., 1996; Arneodo et al., 1998). Sornette et al. (1995)
8899881	13476	of a supervised learning algorithm by incorporating large amounts of unlabeled data into the training data set. 2.1 Co-training Starting with a set of labeled data, co-training algorithms (Blum and Mitchell, 1998) attempt to increase the amount of annotated data using some (large) amounts of unlabeled data. Shortly, co-training algorithms work by generating several classifiers trained on the input labeled
8899881	13476	the case when no additional unlabeled data are used.sOne important aspect of co-training consists in the relation between the views used in learning. In the original definition of co-training, (Blum and Mitchell, 1998) state conditional independence of the views as a required criterion for co-training to work. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is
8899881	13477	may continue for several iterations. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.sOne important aspect of co-training consists in the relation between the views
8899881	13477	assumption. He is proposing a greedy algorithm to maximize agreement on unlabelled data, which produces good results in a co-training experiment for named entity classification. Moreover, (Clark et al., 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. In this work, we
8899881	13477	2003) define self-training as a “single-view weakly supervised algorithm”, build by training a committee of classifiers using bagging, combined with majority voting for final label selection. (Clark et al., 2003) provide a different definition: self-training is performed using “a tagger that is retrained on its own labeled cache on each round”. We adopt this second definition, which also agrees with the
8899881	13477	weaken the independence condition, and may sometime make the behavior of co-training similar to a self-training process. However, as theoretically shownsin (Abney, 2002), and then empirically in (Clark et al., 2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. Despite the fact that parameters observed for optimal settings
8899881	13478	have a large number of senses not belonging to well-defined topical domains show little or no benefit from a bootstrapping procedure. Using the domains attached to word senses, as introduced in (Magnini et al., 2002), we observed that words that have a large subset of their senses not belonging to a specific domain (e.g. restraint, facility) achieve little or no improvement through co-training, which is
13482	13483	purpose, both knowledge-based and residue hydrophobicity-based. Although the performances of almost all of these methods are rather high, short loops and long helices are predicted less accurately . One of the problems of estimating accuracy of different prediction methods is the absence of experimentally reliable trans-membrane annotations to compare with. Thus, one is forced to compare
13482	13485	entire MPTopo database (Fig. 2d). In general the OM-assigned transmembrane segments are shorter, and indeed, as shown in Fig. 2d, they compare better with predicted length distributions, either KD  hydrophobicity-based or by a typical knowledge-based method, TMHMM . Figure 2: In conclusion, by employing structure/hydrophobicity-based alternate annotation for actual transmembrane segments,
13482	13486	segments are shorter, and indeed, as shown in Fig. 2d, they compare better with predicted length distributions, either KD  hydrophobicity-based or by a typical knowledge-based method, TMHMM . Figure 2: In conclusion, by employing structure/hydrophobicity-based alternate annotation for actual transmembrane segments, we have shown that transmembrane helix prediction methods are not poor
8920491	13495	prioritized addition), and consolidation (elimination of any and all inconsistencies). Whether belief change operations are performed on theories (Alchourrón, Gärdenfors, & Makinson 1985) or bases (Nebel 1989; Hansson 1991; 1993b), with ideal agents or those that are resource-bounded (Wassermann 1999; Williams 1997), there is no doubt that the order of operations typically affects the makeup of the
8920491	13496	belief change operations are performed on theories (Alchourrón, Gärdenfors, & Makinson 1985) or bases (Nebel 1989; Hansson 1991; 1993b), with ideal agents or those that are resource-bounded (Wassermann 1999; Williams 1997), there is no doubt that the order of operations typically affects the makeup of the resulting belief base. If a KRR system gains new information that, in hindsight, might have
8920499	13531	from the research and results. 1.1 Parallel IP Packet Forwarding We have investigated a novel architecture, called the High-Performance QoS-capable IP Router (HPQR) architecture, shown in Figure 1. In our research of HPQR we concentrate on the control plane and QoS issues, as there are many excellent results in the data transmission path, and several innovative router architectures exist
8020	13536	ViewerSoftwareServer and offers services to theMobileDeviceManager. Although the need of requires-interfaces is obvious for interoperability and substitutability check (and well-known in literature ), current component models like Sun’s EJB or Microsoft’s .NET only contain provides interfaces. (One notable exception is CORBA 3.0). 3. Contractual Use of Components in Software Architectures Much
8020	13541	components within component architectures and considering critical properties of the deployment environment, one can now compute the overall architectural properties. Our methods are described in  in more detail. They necessitate parameterised contracts and are currently limited to noncyclic architectures (i.e. layers of abstract machines). In our example, we can compute the timing of the
8020	13542	checks within software architectures and component adaptation in case of incompatibility, predicting properties of the overall architecture from known component properties has gained attraction . From a conceptual point of view, one can consider software architectures as structuring principles and methods for Heinz W. Schmidt Center for Distributed Systems and Software Engineering Monash
8020	13543	There is a range of formalisms used for specifying preand postconditions, defining a range of interface models for components (see for extensive discussions and various models e.g., ). This leads naturally to different kinds of contracts for components . Another degree of freedom in the abstract principle of design-by-contract and our extension to architecture-bycontract is
13691	13692	rc, r, 2r, and 2r 2 , respectively. We then get x T M T RMx = r 2 c (4xy + z 2 - det(R) w 2 ). (2) 8 If i, j, k are equal to 2, 3, 4 respectively, M is equal to M = # # # # R -c R R - r R M -2 r 0 -r R M 0 0 r 2 M 0 0 0 r c # # # # , M = r (R (R R -R 2 ) +R(r R -R R) +R(R R -
13691	13692	teapot, the pencil box and the chess set (Figure 8). They were modelled with the SGDL modelling kernel . The chess set was rendered with a radiosity algorithm using the virtual mesh paradigm . All computations were made with the strongly simplified variant of our implementation. The teapot (Figure 8.a) is made of 18 distinct quadrics (one hyperboloid of one sheet, one cone, one circular
13691	13692	two tangent conics >> parameterization of conic:  >> cut parameter: (u, v) =  >> size of input: 3.3222, height of output: 1.4631 >> parameterization of conic:
13691	13692	>> cut parameter: (u, v) =  >> size of input: 4.0592, height of output: 0.71248 >> parameterization of line:  >> cut parameter: (u, v) =  >> cut parameter: (u, v) =  >> size of input: 4.0592, height of output: 0.79955 >> parameterization of line:  >> cut parameter: (u, v) =  >>
13691	13694	of a method for parameterizing the intersection of two implicit quadrics with integer coefficients of arbitrary size. It is based on the nearoptimal algorithm recently introduced by Dupont et al. . Unlike existing implementations, it correctly identifies and parameterizes all the connected components of the intersection in all cases, returning parameterizations with rational functions
13691	13694	each particular case. Such geometric approaches are however essentially limited to the class of so-called natural quadrics, i.e., the planes, right cones, circular cylinders and spheres. Apart from , perhaps the most interesting of the known algorithms for computing an explicit representation of the intersection of two arbitrary quadrics is the method of Wang, Joe and Goldman . This
13691	13694	(Note that quadrics with rational or finite floating-point coefficients can be trivially converted to integer form.) This implementation is based on the parameterization method described in . Precisely, our implementation has the following features: . it computes an exact parameterization of the intersection of two quadrics with integer coefficients of arbitrary size; . it places no
13691	13694	can also be queried via a web interface. The paper is organized as follows. After some preliminaries, we recall in Section 3 the main ideas of the parameterization algorithm we introduced in  and describe its implementation in Section 4. In Section 5, we prove theoretical bounds on the size of the output coefficients when the intersection is generic and compare those bounds to observed
13691	13694	are empty, quadrics of inertia (3, 1) are ellipsoids, hyperboloids of two sheets or elliptic paraboloids, and quadrics of inertia (2, 2) are hyperboloids of one sheet or hyperbolic paraboloids (see  for a complete characterization of affine quadrics). Also, quadrics of inertia (2, 1) are cones or cylinders. All the quadric surfaces except those of inertia (3, 1) are ruled surfaces, i.e.
13691	13708	Manor  use a classification of quadric intersections by the Segre characteristic (see ) to drive the parameterization of the intersection by the pencil method. Recently, Wang, Goldman and Tu  further improved the method making it capable of computing structural information on the intersection and its various connected components and able to produce a parameterization by rational
13691	13709	Apart from , perhaps the most interesting of the known algorithms for computing an explicit representation of the intersection of two arbitrary quadrics is the method of Wang, Joe and Goldman . This algebraic method is based on a birational mapping between the intersection curve and a plane cubic curve. The cubic curve is obtained by projection from a point lying on the intersection.
13691	13709	output exact parameterizations in all cases. However, for the sake of illustration, our first two examples are taken from the paper describing the plane cubic curve method of Wang, Joe and Goldman . 8.1 Example 1: smooth quartic Our first example is Example 4 from . The two quadrics are a quadric of inertia (2, 1) (an elliptic cylinder) and a quadric of inertia (2, 2) (a hyperboloid of
13691	13709	a square as determinant. So the parameterization obtained is optimal in the extension of Z on which its coefficients are defined. 8.2 Example 2: smooth quartic Our second example is Example 5 from . It is the intersection of a sphere and an ellipsoid that are very similar (see Figure 9.b): # 19 x 2 + 22 y 2 + 21 z 2 - 20 w 2 = 0, x 2 + y 2 + z 2 - w 2 = 0. In , the authors compute the
13711	13712	p, a situation s, and a finite horizon h ? 0. The predicate DoG then determines a strategy ? for both agents a and o, the reward to agent a under this strategy ?, and the success probability pr ?  of ?. Note that due to the finite horizon, if the program p fails to terminate before the horizon h is reached, then it is stopped, and the best partial strategy is returned. Intuitively, our aim
13711	13712	two such options cannot be “freely” mixed for different team members. It is thus necessary that there is some form of coordination to agree on one common optimal strategy inside a team (see e.g.  for coordination in multi-agent systems). We assume that the coordination is done by centrally controlling a team. Alternatives are either allowing for local communication between team members, or
13711	13713	is given a planning ability to achieve a goal or to maximize a reward function. Recently, an integration of the programming and the planning approach has been suggested through the language DTGolog , which integrates explicit agent programming in Golog  with decision-theoretic planning in Markov decision processes (MDPs) . DTGolog allows for partially specifying a control program in a
13711	13713	as follows: • We define the language GTGolog, which integrates explicit agent programming in Golog with gametheoretic multi-agent planning in Markov games. GTGolog is a generalization of DTGolog  to a multi-agent setting, which allows for modeling two competing agents as well as two competing teams of cooperative agents. • The language GTGolog allows for specifying a control program for a
13711	13713	actions as primitives to simplify the presentation, more complex parallel actions can be easily treated as well, deploying the concurrent version of the situation calculus . Analogously to , we represent stochastic actions by means of a finite set of deterministic actions. When a stochastic action is executed, then “nature” chooses and executes with a certain probability exactly one
13711	13713	to specify the probability distribution over the deterministic components, we define prob(moveS(a, x, y), moveT o(a, x, y), s) = 0.9 and prob(moveS(a, x, y), moveT o(a, x, y + 1), s) = 0.1. Like , we assume that the domain is fully observable. To this end, we introduce observability axioms, which disambiguate the state of the world after executing a stochastic action. For example, after
13711	13713	0 ? pr = 0 ? ?? ? , v ? : P oss(a, s) ? DoG(p, do(a, s), h?1, ? ? , v ? , pr) ? ? = a; ? ? ? v = v ? +reward(s, a) Informally, if a is not executable, then p stops with success probability 0. As in , Stop is a fictitious action of zero-cost, which stops the program execution. If a is executable, then the optimal execution of a; p in s depends on that one of p in do(a, s). 3. Stochastic first
13711	13715	necessarily pure) policy pairs, and it may have exponentially many Nash pairs. Nash pairs for G can be computed by finite value iteration from local Nash pairs of two-player matrix games as follows . We assume an arbitrary Nash selection function f for two-player matrix games with the action sets A and O. For every state s ? S and number of steps to go h ? {0, . . . , H}, the twoplayer matrix
13711	13715	?n??m then ?n,m ? v = ?n ?m i=1 j=1 vi,j · ?a(ai) · ?o(oj) ? pr = ?n ?m i=1 j=1 pr i,j · ?a(ai) · ?o(oj) Intuitively, we compute a Nash strategy by finite horizon value iteration for Markov games . For each possible pair of action choices, the optimal strategy is calculated. Then, a Nash strategy is locally extracted from a matrix game by using the function selectNash. Here, ?a and ?o are
13711	13715	{o1, . . . , om}. It is then easy to verify that pr = 1 for every success probability pr computed in DoG for such p. By induction on H ? 0, it follows that DoG encodes the finite value iteration in . ? We next show that DoG produces optimal results. Given a finite horizon H ? 0, a strategy ? for a GTGolog program p is obtained from the H-horizon part of p by replacing agent and opponent
13711	13716	that we control simply as a part of “nature”. Game theory  deals with optimal decision making in the multi-agent framework with competing and cooperating agents. In particular, Markov games , also called stochastic games , generalize matrix games from game theory, and are multi-agent generalizations of MDPs with competing agents. In this paper, we present a combination of explicit
13711	13716	o ? O, (ii) ? a?A ?(a) = 1, and (iii) ?(a) ? 0 for all a ? A. Moreover, agent a’s expected reward under a Nash pair is the optimal value of the above linear program. 2.3 Markov Games Markov games , or also called stochastic games , generalize both matrix games and MDPs. Roughly, a Markov game consists of a set of states S, a matrix game for every state s ? S, and a transition function
13711	13716	reward is the same under any Nash pair, and Nash pairs can be freely “mixed” to form new Nash pairs . ? 4 Soccer Example We consider a slightly modified version of the soccer example by Littman  (see Fig. 1): The soccer field is a 4 × 5 grid. There are two players, A and B, each occupying a square, and each able to do one of the following actions on each turn: N, S, E, W, and stand (move
13711	8279	2 Preliminaries In this section, we recall the basic concepts of the situation calculus and Golog, of matrix games, and of Markov games. 2.1 Situation Calculus and Golog The situation calculus  is a first-order language for representing dynamic domains. Its main ingredients are actions, situations, and fluents. A situation is a first-order term encoding a sequence of actions. It is either
13711	13718	in realistic applications. We also report on a first prototype implementation of a simple GTGolog interpreter.s2 INFSYS RR 1843-04-02 The work closest in spirit to this paper is perhaps Poole’s one , which shows that the independent choice logic can be used as a formalism for logically encoding games in extensive and normal form. Our view in this paper, however, is much different, as we aim at
8920531	13726	polling mechanism, too. Implement an eventDispatch message. The eventDispatch message deals with how to dispatch eventHandlers for processing the events. Reactor pattern  and Proactor pattern  describe two corresponding solutions and detailed implementation steps for building eventDispatch. Implement the eventHandler messages. According to the characteristics of the external object,
8920531	13726	can be used to implement the update messages in Computing, Execution and Expression for flexible assignment of concrete schemes related to the application domains. Proactor design pattern  and Reactor design pattern  can be selected to build eventDispatch of the Acquisition component in ACEE. Acknowledgements Many thanks to Christa Schwanninger who was the shepherd of this pattern
8920531	13727	interface must include polling mechanism, too. Implement an eventDispatch message. The eventDispatch message deals with how to dispatch eventHandlers for processing the events. Reactor pattern  and Proactor pattern  describe two corresponding solutions and detailed implementation steps for building eventDispatch. Implement the eventHandler messages. According to the characteristics of
8920531	13727	the update messages in Computing, Execution and Expression for flexible assignment of concrete schemes related to the application domains. Proactor design pattern  and Reactor design pattern  can be selected to build eventDispatch of the Acquisition component in ACEE. Acknowledgements Many thanks to Christa Schwanninger who was the shepherd of this pattern and gave me lots of concrete
13758	6716	need for root level access,etc. Having a buyer’s requirements along these dimensions match with what a provider has on offer is rare. In contrast, Virtuoso’s abstraction, a virtual machine  (VM), is very low level and simple. The buyer receives a remote machine, configured with the CPU, memory, and disk resources he desires, that is indistinguishable from an actual physical machine.
13758	13759	A detailed argument for the virtual machine / virtual network model of distributed computing is available elsewhere , as well as detailed information on VNET and other elements of the system . This report thoroughly describes the implementation of the Virtuoso component of the system. The goal of Virtuoso is to create a marketplace for virtual machines, making it straightforward for a
13758	13760	performance of a distributed or parallel application running in a buyer’s VMs. A detailed argument for the virtual machine / virtual network model of distributed computing is available elsewhere , as well as detailed information on VNET and other elements of the system . This report thoroughly describes the implementation of the Virtuoso component of the system. The goal of
13758	13762	A detailed argument for the virtual machine / virtual network model of distributed computing is available elsewhere , as well as detailed information on VNET and other elements of the system . This report thoroughly describes the implementation of the Virtuoso component of the system. The goal of Virtuoso is to create a marketplace for virtual machines, making it straightforward for a
13758	13763	A detailed argument for the virtual machine / virtual network model of distributed computing is available elsewhere , as well as detailed information on VNET and other elements of the system . This report thoroughly describes the implementation of the Virtuoso component of the system. The goal of Virtuoso is to create a marketplace for virtual machines, making it straightforward for a
13758	13764	often taking advantage of the fact that not all of the data needs to be sent to migrate a machine. Specifically, data can be fetched only when it is needed—for example on-demand paging (Rosenblum ). However, this increases how much the system relies on the operation of all of its nodes and decreases overall reliability. If possible, a migration system should attempt to keep all of the
13758	13765	efforts at migration have used mechanisms which were specific to a virtual machine for example, the use of a VMWare C-shim library to capture disk writes for the creation of redo logs (Rosenblum ). This is a breach of independence between the migration system and the virtual machine monitor. 7.1.4 Machine encapsulation Previous work has been done in building migration systems, often taking
13758	13765	none. The machine was then run and suspended—the time spent is mainly the time it took to send VMWare’s state file (.vmss), which is mainly memory contents. The “ballooning” technique described in  would potentially help to reduce these costs. The next two tests used the disk 28 slightly—the second test being an automated one. 28 VMWare’s undoable disk option actually writes to a redolog file
13758	13768	A detailed argument for the virtual machine / virtual network model of distributed computing is available elsewhere , as well as detailed information on VNET and other elements of the system . This report thoroughly describes the implementation of the Virtuoso component of the system. The goal of Virtuoso is to create a marketplace for virtual machines, making it straightforward for a
13758	13768	Connect/Disconnect VNET configuration Providers (provider install) VMs VMM VNET daemon Base OS Figure 1: A high-level view of Virtuoso and related components. and VNET (which is described elsewhere ). For this reason and for simplicity, we have included VNET in the figure and in the following discussion. The buyer, front-end, and provider software works together to provide the buyer of a VM
13758	6726	need for root level access,etc. Having a buyer’s requirements along these dimensions match with what a provider has on offer is rare. In contrast, Virtuoso’s abstraction, a virtual machine  (VM), is very low level and simple. The buyer receives a remote machine, configured with the CPU, memory, and disk resources he desires, that is indistinguishable from an actual physical machine.
13779	14531	location, private and secure. Over the past year, we have designed, implemented, and deployed a public-area wireless network called CHOICE that addresses these challenges in one integrated system . CHOICE is a service platform on which any number of network providers can offer network services, enabling users to choose the kind of service that best fits their needs. It supports two kinds of
13779	13782	the surrounding environment intelligently to user devices. 2.3.1 Issues and Differences Context and location-aware applications have been well researched but generally in enterprise settings  . Typically, these systems have relied on technologies such as badges and emitters (based on IR technology) that are attached to users, equipment, and building walls that enable the system to create
13779	9683	empirically determined models . As an example, Figure 1 shows the variation of signal strength as a function of the separation between transmitter and receiver in an Aironet 802.11 wireless LAN . Error distance (meters) Mean Median 90th %tile 20 18 16 14 12 10 8 6 4 2 0 0 1 2 3 4 5 Number of Access Points Figure 2: The effect of the number of access points on the error in the location
13779	13788	first enters the PAWN, the client module uses the information contained in the broadcast beacon to establish the initial connection to the Web server and prompts the user to begin authentication . After the authentication succeeds, the client module receives and stores the (key,token) pair, enables packet tagging, and sets the default gateway to the advertised TCG. When the user returns to
13779	13789	list service. When a user first connects to the PAWN via CHOICE, his pre-configured buddy list is extracted and Figure 6: location-based buddy list architecture sent to a backend eventing server . The eventing server already knows who the user is (via CHOICE authentication) so it stores this information in a local database. Additionally the WISH client running on the user’s machine
13790	13793	to the rate at which the network converges to the fixed point; and throughput efficiency refers to the objective that the network operates at close to the bottleneck-link capacity. It is shown in  that these properties can be deduced from the network matrix A. We briefly summarise here the relevant results in these papers. Theorem 2.1  Let A be defined as in Equation (6). Then A is a
13790	13793	delay, 40 packet queue). (ii) Network responsiveness: The magnitude of the second largest eigenvalue ?n?1 of the matrix A bounds the convergence properties of the entire network. It is shown in  that all the eigenvalues of A are real and positive and lie in the interval , where the ?i are ordered as 0 < ?1 ? ?2 ? .... ? ?n?1 ? ?n < 1. In particular, the second largest eigenvalue is
13790	13796	fairness and TCP-fairness are ensured by adjusting ?i according to ?i = 2(1 ? ?i). Comment 1: Networks of synchronised sources and drop-tail queues have already been the subject of several studies . The novelty of our approach is that we use facts from the theory of positive matrices to analyse not only the network steady-state behaviour but also the network dynamics, directly relating the
13790	13797	fairness and TCP-fairness are ensured by adjusting ?i according to ?i = 2(1 ? ?i). Comment 1: Networks of synchronised sources and drop-tail queues have already been the subject of several studies . The novelty of our approach is that we use facts from the theory of positive matrices to analyse not only the network steady-state behaviour but also the network dynamics, directly relating the
13790	13798	fairness and TCP-fairness are ensured by adjusting ?i according to ?i = 2(1 ? ?i). Comment 1: Networks of synchronised sources and drop-tail queues have already been the subject of several studies . The novelty of our approach is that we use facts from the theory of positive matrices to analyse not only the network steady-state behaviour but also the network dynamics, directly relating the
13790	2355	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	5142	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13802	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13803	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13804	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13805	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13807	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13166	main mathematical results presented in this paper. Comment 2: Before proceeding we note that networks of unsynchronised sources have also been the subject of wide study in the TCP community: see  and the accompanying references for further details. While most of this work has concentrated on developing and analysing 11 ? ? .sTCP models that are based upon fluid analogies, several authors
13790	13809	?iwi. Network parameters: B=100Mb, qmax=80 packets, ¯ T =20ms, T0=102ms; T1=42ms; no background web traffic. flows with drop-tail queues can exhibit a rich variety of deterministic dropbehaviours . However, most real networks carry at least a small amount web traffic. In Figure 8 we plot NS simulation results where the mean congestion window as the level of background web traffic is varied
13790	13809	the effect of this traffic is enough to disrupt the coherent structure associated with phase effects and other complex phenomena previously observed in simulations of unsynchronised networks . From the packet-based simulation results we can determine the proportion of congestion events corresponding to both flows simultaneously seeing a packet drop, flow 1 seeing a drop only, and flow 2
13790	8700	Figure 8 we plot NS simulation results where the mean congestion window as the level of background web traffic is varied (background information on the web traffic generator in NS is described in ). To illustrate the impact of small amounts of web traffic, these results are given for a network condition where phase effects are particularly pronounced: the congestion window time histories
13790	13811	is defined by the requirement that Sx ?= x ? ?Sx? < ?x? , ? x ? R n?1 , S ? S . (29) This is true for our set S, as the matrices S ? S are symmetric and of spectral radius at most 1. It is know , that finite sets of matrices that are paracontracting have left convergent products, i.e., for any sequence {S(k)}k?N in S the following limit exists lim S(k)S(k ? 1) . . . S(0) . (30) k?? For
13790	13812	convergent products, i.e., for any sequence {S(k)}k?N in S the following limit exists lim S(k)S(k ? 1) . . . S(0) . (30) k?? For related literature on paracontracting sets of matrices we refer to  and references therein. In the following we prove results on the convergence of products of the matrices in A to the set of column-stochastic matrices of rank 1. To this end it will be convenient
828109	13815	Context captures broader system state that can be used to provide new attributes for files, and to propagate attributes among related files; context is also how humans often remember previous items , and so should fit the primary role of semantic file systems well. Based on our study of ten systems over four months, the addition of context-based mechanisms, on average, reduces the number of
828109	13815	the way that humans remember things, such as the location of a previously stored file; a recent study of user search behavior found exactly this tendency: users usually use context to locate data . This paper makes the case for context-based attribute assignment, introducing approaches from two categories: application assistance and user access patterns. Application assistance uses
828109	13815	their primary shortcoming: the unavailability of quality attributes. Context attributes capture the state of users while they accessing their files, a key way that users recall and search for data . Combining context and content attributes with attribute propagation increases both the number of classifiable files, and the number of attributes assigned to each file. The result is a more
828109	13816	examination of the generated relationships. 4.1 Algorithm design A number of existing projects have extracted file inter-relationships from access patterns to provide file prefetching and hoarding . These algorithms convert the file system into a graph, with files at the nodes and each relationship specified as a weighted link between nodes. The weight of a link is the number of times that
828109	13816	next access given a particular file, but rather find several relationships that may be of use. As such, models that use very strict cutoff rules, such as last-successor, firstsuccessor, and Noah , cannot be captured by our algorithms, although many of the underlying techniques are the same. 4.2 Experimental setup As a starting point for evaluation, we assume that there exists a perfect
828109	743	could also provide context hints to the indexing system. Networked attributes: Researchers have explored the problem of providing an attribute-based namespace across a network of computers. Harvest  and the Scatter/Gather system  provide a way to gather and merge attributes from a number of different sites. The Semantic Web  proposes a framework for annotating web documents with XML
828109	6538	upon user input (user submitted web pages) and content analysis (word counts, word proximity, etc.). Although valuable, the success of these systems has been eclipsed by the success of Google . To provide better search results, Google utilized the text associated with a link to decide on attributes for the linked site. This text provides the context of both the creator of the linking
828109	7799	the indexing system. Networked attributes: Researchers have explored the problem of providing an attribute-based namespace across a network of computers. Harvest  and the Scatter/Gather system  provide a way to gather and merge attributes from a number of different sites. The Semantic Web  proposes a framework for annotating web documents with XML tags, providing applications with
828109	13818	formats (e.g., Word or PowerPoint) to reduce file contents into raw text, allowing text analysis of the contents. Researchers continue to work on content analysis of music, images, and video files , but this remains a complex problem that is far from solved. File system attributes: Most filesystems store a small set of context attributes within each file’s metadata, such as the owner of the
828109	13820	attributes in existing search tools. Going further, the Lifestreams system uses this context information as a first-class entity, providing a stream-like view of all documents based on access time . The utility of these few context attributes suggests that additional context information could be quite helpful to the user. 3.1.2 Context-based schemes Application assistance: Although personal
828109	13823	hierarchical and attribute-based naming schemes. Sechrest and McClennen  detail a set of rules for constructing various mergings of hierarchical and flat namespaces using Venn diagrams. Gopal  defines five goals for merging hierarchical name spaces with attribute-based naming and evaluates a system that meets those goals. By assigning multiple attributes to a single file, the information
828109	13823	from ). The system traces application requests to watch for file updates, and potentially to overlay its indexing system on the existing file system interface by extending the namespace . When a file is updated, the tracer notifies its transducer, which reads the file and updates its attributes in the index. Applications query the index using either a separate interface or a
828109	13824	algorithms Using these five components, we can capture the behavior of various existing cache hoarding and prefetching tools. For example, the model examined by Griffioen and Appleton  uses a directed graph style, the open filter, a fixed window size of 2, the simple stream extractor, and a 0% cutoff. The model examined by Lei and Duchamp  uses a directed graph style, the
828109	13825	Mobile cache hoarding: As distributed and decentralized systems have become more prevalent, many groups have examined the idea of automatically hoarding networked files locally on mobile nodes . The Aura project has examined schemes for allowing users to specify hoarded files by specifying tasks , a set of applications and files that make up a particular context. Assuming that users
828109	13826	Mobile cache hoarding: As distributed and decentralized systems have become more prevalent, many groups have examined the idea of automatically hoarding networked files locally on mobile nodes . The Aura project has examined schemes for allowing users to specify hoarded files by specifying tasks , a set of applications and files that make up a particular context. Assuming that users
828109	13826	examination of the generated relationships. 4.1 Algorithm design A number of existing projects have extracted file inter-relationships from access patterns to provide file prefetching and hoarding . These algorithms convert the file system into a graph, with files at the nodes and each relationship specified as a weighted link between nodes. The weight of a link is the number of times that
828109	13827	their accesses records a set of temporal relationships between files. These relationships have previously been used to guide a variety of performance enhancements (e.g., prefetching, cache hoarding ). Another possible use of this information is to help propagate information between contextually related files. For example, accessing “SemanticFS.ps” and “Gopal98.ps” followed by updating
828109	13828	examination of the generated relationships. 4.1 Algorithm design A number of existing projects have extracted file inter-relationships from access patterns to provide file prefetching and hoarding . These algorithms convert the file system into a graph, with files at the nodes and each relationship specified as a weighted link between nodes. The weight of a link is the number of times that
828109	13828	extractors. The Simple extractor treats the trace as a single stream of requests. The UID extractor creates a separate stream of requests for each user ID in the trace. The Access Tree extractor  creates a stream of requests based on the file operations performed by a parent process and all of its sub-processes (as illustrated in Figure 2). The root of an access tree is a process spawned
828109	13828	examined by Griffioen and Appleton  uses a directed graph style, the open filter, a fixed window size of 2, the simple stream extractor, and a 0% cutoff. The model examined by Lei and Duchamp  uses a directed graph style, the open filter, a fixed window size of 1, the access tree extractor, and a 0% cutoff. The goal of this work is not to capture the exact next access given a particular
828109	13829	and search mechanisms. 2.2 Indexing and search tools Today, many users rely upon indexing and search tools to help find files lost in their directory hierarchy. Tools such as find/grep and Glimpse  use text analysis and file pathnames to provide searchable attributes for files. Others, such as Microsoft Windows’ search utility, use filters to gather attributes from well-known file formats
828109	13831	their accesses records a set of temporal relationships between files. These relationships have previously been used to guide a variety of performance enhancements (e.g., prefetching, cache hoarding ). Another possible use of this information is to help propagate information between contextually related files. For example, accessing “SemanticFS.ps” and “Gopal98.ps” followed by updating
828109	13832	do not exist between files. As such, it is likely that their initial tools will instead leverage their extensive knowledge of text analysis and indexing. Improved interfaces: The Haystack project  aims to improve the organization of user data by providing improved interfaces that manipulate data using open, well-annotated file formats. By easing the methods for users to input information
828109	13834	the idea of automatically hoarding networked files locally on mobile nodes . The Aura project has examined schemes for allowing users to specify hoarded files by specifying tasks , a set of applications and files that make up a particular context. Assuming that users are willing to provide this information, it could also provide context hints to the indexing system.
828109	13836	Mobile cache hoarding: As distributed and decentralized systems have become more prevalent, many groups have examined the idea of automatically hoarding networked files locally on mobile nodes . The Aura project has examined schemes for allowing users to specify hoarded files by specifying tasks , a set of applications and files that make up a particular context. Assuming that users
13837	13838	F (X) can be generated as a pseudo-random sequence y1, . . . , yl of elements yi ? X ±1 such that yi ?= y ?1 i+1 , where the length l is also chosen pseudo-randomly. However, it has been shown in  that randomly taken cyclically reduced words in F are already minimal with asymptotic probability 1. Therefore, a set of randomly generated cyclic words in F would be highly biased toward the class
13837	13838	a weight lij which is the number of times the subwords xix ?1 j and xjx ?1 i occur in v. Whitehead Graph is one of the main tools in exploring automorphic properties of elements in a free group . Now, let w ? F (X) be a cyclically reduced word. We define features of element w as follows. Let l(w) be a vector of edge weights in the Whitehead Graph W G(w) with respect to a fixed order. We
13837	13840	is not minimal then there exists an automorphism t ? ?(X) such that |t(w)| < |w|. Unfortunately, its complexity depends on cardinality of ?(X) which is exponential in the rank of F (X). We refer to  for a detailed discussion on complexity of Whitehead’s algorithms. In this paper we focus on the Recognition Problem for minimal elements in F . It follows immediately from the Whitehead’s result
13837	13840	if and only if there exists a sequence of Whitehead automorphisms t1 . . . tm ? ?(X) such that tm . . . t1(x) = w for some x ? X ±1 . Elements in SP are generated by the procedure described in , which, roughly speaking, amounts to a random choice of x ? X ±1 and a random choice of a sequence of automorphisms t1 . . . tm ? ?(X). – A test set S10 which is generated in a way similar to the
13837	13840	randomly chosen automorphisms from ?(X). The number of such automorphisms is chosen uniformly randomly from the set {1, . . . , 10}, hence the name. For more details on the generating procedure see . To show that performance of Support Vector Machines is acceptable for free groups, including groups of large ranks, we run experiments with groups of ranks 3,5,10,15,20. For each group we
13837	2249	groups of small ranks (see Section 3.3). In this paper we describe a probabilistic classification system to recognize Whitehead-minimal elements which is based on so-called Support Vector Machines . Experimental results described in the last section show that the system performs very well on different types of test data, including data generated in groups of large ranks. The paper is
13837	2249	Machine (SVM) is a statistical classifier that attempts to construct a decision hyperplane (w, b) in such a way that the margin of separation ? between positive and negative examples is maximized .sWe wish to find a hyper-plane which will separate the two classes such that all points on one side of the hyper-plane will be labelled +1, all points on the other side will be labelled -1. Define
13837	2249	groups of small ranks (see Section 3.3). In this paper we describe a probabilistic classification system to recognize Whitehead-minimal elements which is based on so-called Support Vector Machines . Experimental results described in the last section show that the system performs very well on different types of test data, including data generated in groups of large ranks. The paper is
13837	2249	Machine (SVM) is a statistical classifier that attempts to construct a decision hyperplane (w, b) in such a way that the margin of separation ? between positive and negative examples is maximized .sWe wish to find a hyper-plane which will separate the two classes such that all points on one side of the hyper-plane will be labelled +1, all points on the other side will be labelled -1. Define
8920544	13847	large values (it can be shown that the probability of eventual ruin is the same as the stationary tail waiting probability for an M/G/1 queue, where the service times are Pareto random variables (Juneja et al. 1999)). Martin J. Fischer Denise M. B. Masi Mitretek Systems 3150 Fairview Park Drive South Falls Church, VA 22042, U.S.A. 2 HEAVY-TAILED DISTRIBUTIONS AND THE PARETO A cumulative distribution function,
8920547	13858	because a workflow process may enforce a security violation by specifying two tasks, which are individually allowed, but together violate a separation of duty policy. Previous work on this subject  used specialized workflow infrastructures to maintain both the workflow specification and the security policy, and developed specialized analyzers to verify the consistency of the jointly
8920547	13861	composed of a set of rules comprising the knowledge on constraint resolution, and an engine that knows how to handle those rules. Rules are expressed in the CHR (Constraint Handling Rules) language , for which several engines already exist. The engine works by repeatedly applying the CHR rules on the constraints produced by the language translators until a inconsistency is detected or no more
8920547	13862	inconsistencies with a workflow specification written in an off-the-shelf workflow process definition language, namely WPDL (Workflow Process Definition language) . 2 Security language (SPL) SPL  is a security language designed to express policies that aim to decide about the acceptability of events. The acceptability of each event depends on the properties of the event (e.g. author, target
8920547	13863	Most of them use specialized workflow languages with specific flow constructions , some are based in colored annotated Petri nets , and a few others define processes as sets of constraints . Although most of these models are not equivalent due to properties present in some of them and absent in others, it is often possible to create an equivalent model based on a different paradigm.
3758209	13866	that can be supported by a network with all users active concurrently. A better understanding of the structure of the feasibility region is essential for developing optimal MAC strategies , , . The structure of this set sheds light on when smart scheduling strategies can improve system performance. In particular, if the feasibility region is a convex set, then concurrent
3758209	13866	is a log-convex function on G. To this end, let ˆq =(ˆq1,...,ˆqK) ? GK and ?q =(?q1,...?qK) ? GK are two arbitrary vectors of QoS requirements. Given ˆq, ?q ? GK , define q(?) =(1? ?)ˆq + ??q, ? ?  . In other words, q(?) describes a straight line connecting ˆq and ?q. Now we are ready to prove the following theorem. Theorem 1: Suppose that V is irreducible and fixed. If ? : G ? R+ defined by
3758209	13866	In other words, if log ?((1 ? ?)ˆx + ??x) ? (1 ? ?) log ?(ˆx)+? log ?(?x) , (10) for all ˆx, ?x ? G and ? ? , then log ?(q) ? (1 ? ?) log ?(ˆq)+? log ?(?q) (11) for all ˆq, ?q ? G K and ? ? . Proof: See Appendix VII-A An immediate consequence of the theorem is that the feasibility region is a convex set if ? is log-convex. To see this, it is convenient to rewrite (11) as ?(q) ? ?(ˆq)
3758209	13866	inequality above is convex, we have max{?(ˆq),?(?q)} ??(q(?)),?? . In fact, any positive log-convex function is a convex one . Consequently, if ˆq and ?q are feasible, then q(?) with ? ?  is feasible as well since 1 > max{?(ˆq),?(?q)} ??(q(?)) . We summarize this observation in a corollary. Corollary 1: Suppose that V is irreducible. If ? : G ? R+ is log-convex, then F? is a convex
3758209	13866	SIR. Obviously, since ?(x) =1/x, x > 0, is log-convex on G = R+, we conclude that ? is log-convex on RK + and the feasible ESG region is a convex set. Figure 1 depicts ?(q(?)) as a function of ? ?  for these three examples of the function ? : G ? R+ and some given ˆq, ?q ? GK . We assumed that Vk,l = 1/N for each 1 ? k, l ? K, k ?= l. The simulation confirms the results of the theoretical
3758209	13867	signal-to-interference ratio (SIR) at the output of the linear receiver. For instance, under certain conditions, the data rate achieved by a user is a strictly increasing function of its SIR , . We say that QoS requirements are feasible if there exists a power allocation for which each user meets its QoS requirement with all users being active concurrently. A set of feasible QoS
3758209	13868	that can be supported by a network with all users active concurrently. A better understanding of the structure of the feasibility region is essential for developing optimal MAC strategies , , . The structure of this set sheds light on when smart scheduling strategies can improve system performance. In particular, if the feasibility region is a convex set, then concurrent transmissions
3758209	13869	there exists a power allocation for which each user meets its QoS requirement with all users transmitting concurrently. The problem of power control has received much attention in recent years (see  and  for references). Power control is a central mechanism for resource allocation and interference management in wireless systems. Dynamic allocation of power is a popular strategy to combat
3758209	13871	is a central mechanism for resource allocation and interference management in wireless systems. Dynamic allocation of power is a popular strategy to combat detrimental effects of multipath fading . This paper generalizes some of the results of  and  and also extends them to channels whose inputs are subject to power constraints. In communication networks like the uplink channel, there
3758209	14627	we point out that the structure of the feasibility region for MIMO (multiple-input-multipleoutput) systems with scheduling and successive interference cancellation was investigated in  and . Reference  investigated the structure of the feasible SIR region under the linear minimum mean square error receiver. The paper is organized as follows: Section II introduces the channel model
3758209	13876	REGION We consider a K?user power-controlled CDMA system. Assuming no inter-symbol interference and a linear receiver structure, SIR at the output of the k-th receiver is of the form ,  SIRk(p) := = ? Kl=1 Vk,kpk plVk,l + Ck? 2 l?=k pk ?Kl=1 pl l?=k Vk,l , 1 ? k ? K. Ck + ?2 Vk,k Vk,k Here and hereafter, pk is the power at which the user k transmits, the product plVk,l > 0
3758209	13876	which is equivalent to ?k ? SIRk(p), 1 ? k ? K, (3) SIRk(p) SIRk(p) 1 ? min = min . (4) 1?k?K ?k 1?k?K ?(qk) One of the central problems in wireless network design is the problem of admissibility . The problem is whether or not there exists a power allocation p ? RK + for which (3) holds, where the power allocation is often limited by a total power constraint K? ?p?1 = pk ? Ptot < +? . (5)
3758209	13876	large values of SIR, 1 2 log(SIRk(?k) is a reasonable approximation to the Shannon capacity (using independent decoding). 2. Another basic performance measure is the effective bandwidth of a user , . Accordingly, users with given SIR requirements are admissible in a CDMA system if and only if the sum of their effective bandwidths is smaller than the processing gain of the system. The
3758209	13876	sequences assigned to the users form a Welch-Bound-Equality sequence set, the feasible log-EB region coincides with a set of feasible effective bandwidths under a scaled matched-filter receiver . 3. Another example is qk = B , 1 ? k ? K, where Rk Rk?k denotes a given data rate and B is the bandwidth, which is the same for all users. In this case, qk is called the effective spreading gain
3758209	13880	values of SIR, 1 2 log(SIRk(?k) is a reasonable approximation to the Shannon capacity (using independent decoding). 2. Another basic performance measure is the effective bandwidth of a user , . Accordingly, users with given SIR requirements are admissible in a CDMA system if and only if the sum of their effective bandwidths is smaller than the processing gain of the system. The effective
3758209	13880	of its SIR requirement and depends on the multiuser receiver employed. Assuming the linear MMSE receiver, the effective bandwidth of each ?k user, say user k, becomes minimal and is equal to 1+?k . Now let qk = log ??k , 1 ? k ? K, and call the 1+?k corresponding feasibility region the feasible log-EB region. In this case, we have and hence ? ? ? ?? ?(x) = log ?(x) = exp(x) ,x<0 , 1 ? exp(x)
3758209	13882	be as above. Then, we have ûk = yk · xk, 1 ? k ? K, where y and x are the left and the right eigenvector of V , respectively, so that y T x =1and Vx= ?(V ) x V T y = ?(V ) y. Proof: It follows from  that Â is ( ??)k,l =âk,l = Vk,lxl . ?(V ) xk By definition, we have ÂT u = u. Combining this with the equation above yields uk = K? l=1 or, equivalently, âl,kuk = 1 ?(V ) ?(V ) · uk xk = K? K? l=1
3758209	13882	paper shows how fast the minimum total power tends to infinity when QoS requirements are sufficiently close to the boundary. VII. APPENDIX A. Proof of Theorem 1 Recall that ?k = ?(qk), 1 ? l ? K. By,wehave ? ?K K? log ?(q) =sup ukak,l log Vk,l + ukak,l log ?l A?S ? K? k,l=1 k?=l k,l=1 k?=l ukak,l log ak,l ? k,l=1 k?=l (28) where S := S(V ) is a set of all doubly stochastic matrices so that
8920551	13888	processing community has become aware of the potential for massive parallelism and high computational density in FPGAs. For example, FPGAs have been used for real-time point tracking , stereo  and color-based object detection . Unfortunately, the difficulty of implementing complex algorithms in circuit design languages has discouraged many computer vision researchers from exploiting
8920551	13888	and easy for the compiler to analyze. Most interesting is the window generator, which extracts sub-arrays from a source array. Here is a median filter written in SA-C: uint8 R = for window W in A { uint8 med = array_median (W); } return (array (med)); The for loop is driven by the extraction of 3x3 sub-arrays from array A. All possible 3x3 arrays are taken, one per loop iteration. The
8920551	13888	and debugging.sAs an example, consider the following SA-C fragment, which convolves an image with a 3×3 (vertically-oriented) Prewitt edge detection mask: int16 main(uint8 Image) int16 H = { {-1, 0, 1}, {-1, 0, 1}, {-1, 0, 1}} ; int16 R = for window W in Image { int16 iph = for h in H dot w in W return( sum(h*(int16)w) ); } return( array(iph) ); } return R; The dataflow
8920551	13888	unrolling produces the effect of multidimensional partial loop unrolling. For example, a stripmine pragma can be added to the median filter: uint8 R = // PRAGMA (stripmine (6,4)) for window W in A { uint8 med = array_median (W); } return (array (med)); This wraps the existing loop inside a new loop with a 6x4 window generator. Loop unrolling then replaces the inner loop with eight
8920551	13888	median filter is followed by an edge detector:sFigure 1. Dataflow graph for applying the vertical mask of the Prewitt edge detector, as compiled from the SA-C code above.suint8 R = for window W in A { uint8 med = array_median (W); } return (array (med)); uint8 S = for window W in R { uint8 pix = prewitt (W); } return (array (pix)); where prewitt is defined by the user as a
8920551	13890	how reconfigurable systems are programmed from a hardwareoriented circuit design paradigm to a softwareoriented algorithmic one. To this end, we have developed a high-level language (called SA-C ) for expressing image processing algorithms, and an optimizing SA-C compiler that targets FPGAs. Together, these tools allow programmers to quickly write algorithms in a high-level language,
8920551	13890	of SA-C, focusing on the language features that 1) support computer vision andsimage processing, and 2) enable efficient compilation to FPGAs. (A more complete description of SA-C can be found in , while a programmer’s reference is available on the web 1 ). It then describes the compilation process, in which SA-C algorithms are translated first into non-recursive data flow graphs and then
13899	13893	their transmission rates gradually, resulting in graceful degradation of the QoS in the presence of network congestion. The elasticity of these services can be modeled by concave utility functions . The other corresponds to realtime services, such as streaming video and audio services. These services are less elastic than data services. In response to network congestion, they can decrease
13899	13893	shroff}@ecn.purdue.edu below a certain bit rate, the quality of audio communication falls dramatically). The elasticity of these services can be modeled by using sigmoidal-like utility functions . We call an increasing function f(x) a sigmoidal-like function, if it has one inflection point xo, and d2f(x) dx2 > 0, forx < xo and d 2 f(x) dx2 < 0, forx>xo, as shown in Fig. 1. There have been a
13899	2354	forx < xo and d 2 f(x) dx2 < 0, forx>xo, as shown in Fig. 1. There have been a number of papers that have studied utility based rate control problems by exploiting the elasticity of services, e.g., , , , , , , , . Most of these works use a utility and pricing framework that attempts to obtain the optimal rate allocation that maximizes the total system utility using the
13899	2354	its transmission rates in an attempt to maximize its net utility, which is defined by U(x) ? ?x, where U is a utility function, ? is price for unit rate, and x is the amount of rate allocation. In , the author shows that the problem can be decomposed into the user problem and the network problem. Based on the decomposed problem, the authors in  propose a distributed algorithm with a
13899	13896	dx2 < 0, forx>xo, as shown in Fig. 1. There have been a number of papers that have studied utility based rate control problems by exploiting the elasticity of services, e.g., , , , , , , , . Most of these works use a utility and pricing framework that attempts to obtain the optimal rate allocation that maximizes the total system utility using the price as a control
13899	13896	projection algorithm for the dual and show that the algorithm converges to the optimal solution. Their algorithm is implemented using Random Exponential Marking (REM) in a following paper . In , the authors develop a Nash bargaining solution that is proportionally fair and Paretooptimal. The authors solve the dual problem using a gradient projection algorithm and implement the algorithm
13899	13896	as we will show later, Q(?) may not be everywhere differentiable. Hence, even though Q(?) is a convex function, we cannot use a simple gradient based algorithm to find a minimizer as in , , since Q(?) does not have a gradient at the point where it is not differentiable. To solve problem (B), we will first study the properties of Q(?) by using the theory of the subdifferentials. We
13899	13896	) is a subgradient of Q(?) at ? = ? (n) . To make (10) converge to ? o , the optimal solution of problem (B), we must have an appropriate sequence of ? (n) . In gradient based algorithms in , , there exists a constant step size, ? (n) = ?, such that ? (n) converges to ? o . However, in the subgradient based algorithm, we cannot guarantee convergence of ? (n) using a constant step size,
13899	13896	when Ui is a sigmoidallike function. Thus, if there exist users having sigmoidal-like utility functions, the rate allocation resulting from solving the dual problem, such as the algorithms in ,  (that converges to an efficient rate allocation with concave utility functions), may cause congestion without convergence. To resolve this situation, we will impose a “self-regulating” property on
13899	13897	forx>xo, as shown in Fig. 1. There have been a number of papers that have studied utility based rate control problems by exploiting the elasticity of services, e.g., , , , , , , , . Most of these works use a utility and pricing framework that attempts to obtain the optimal rate allocation that maximizes the total system utility using the price as a control signal. In
13899	13897	the authors use a similar approach as in . However, they consider the problem with random loss in the network and implement the algorithm using Explicit Congestion Notification (ECN) marking. In , a window based algorithm is proposed by generalizing the works in  and . In , the authors propose a subgradient based algorithm using the number of congested links on the user’s path as
13899	13898	as shown in Fig. 1. There have been a number of papers that have studied utility based rate control problems by exploiting the elasticity of services, e.g., , , , , , , , . Most of these works use a utility and pricing framework that attempts to obtain the optimal rate allocation that maximizes the total system utility using the price as a control signal. In this
13899	13898	loss in the network and implement the algorithm using Explicit Congestion Notification (ECN) marking. In , a window based algorithm is proposed by generalizing the works in  and . In , the authors propose a subgradient based algorithm using the number of congested links on the user’s path as an indicator of network congestion. The common feature in the afore-mentioned works is
13899	2360	with random loss in the network and implement the algorithm using Explicit Congestion Notification (ECN) marking. In , a window based algorithm is proposed by generalizing the works in  and . In , the authors propose a subgradient based algorithm using the number of congested links on the user’s path as an indicator of network congestion. The common feature in the afore-mentioned
13899	13899	being considered in this paper. We propose and study the rate control algorithm in Section III. For the sake of brevity, proofs are omitted. Interested readers are referred to our technical report . We provide numerical results for the proposed algorithm in Section IV and conclude in Section V. II. SYSTEM DESCRIPTION AND BASIC PROBLEM We consider a system that has a single bottleneck link
13899	13900	i=1 Mi > C. Note that since we allow non-concave utility functions, problem (A) is a non-convex programming problem, which, usually, is more difficult to solve than a convex programming problem. In , a similar problem to (A) was studied in the context of the power allocation in wireless environment. However, the algorithm in  requires a central controller, such as a base-station in
13899	13900	max {Ui(x) ? ?x}, ? i. (3) 0?x?Mi 1 Since, in this paper, we focus on a single bottleneck link, L =1 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sThe properties of xi(?) were studied in . First, we define for user i as: ? max i ? max i = min{? ? 0 | max {Ui(x) ? ?x} =0}. (4) 0?x?Mi We can calculate it by the following equation: ? max i ? ?? = ?? dUi(x) dx |x=0, if x o i =0, dUi(x)
13904	13911	the test so it produces acceptable probabilities of detection. Existing algorithms for watermarking still images usually work either in the spatial domain – or in a transformed domain , –. In this paper, we will deal with the analysis of watermarking methods in the discrete cosine transform (DCT) domain, whichs56 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 9, NO. 1,
13904	13912	small value by adequately designing the test so it produces acceptable probabilities of detection. Existing algorithms for watermarking still images usually work either in the spatial domain ??? or in a transformed domain , –. In this paper, we will deal with the analysis of watermarking methods in the discrete cosine transform (DCT) domain, whichs56 IEEE TRANSACTIONS ON IMAGE
13904	13913	the test so it produces acceptable probabilities of detection. Existing algorithms for watermarking still images usually work either in the spatial domain – or in a transformed domain , ???. In this paper, we will deal with the analysis of watermarking methods in the discrete cosine transform (DCT) domain, whichs56 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 9, NO. 1, JANUARY
13904	13919	test so it produces acceptable probabilities of detection. Existing algorithms for watermarking still images usually work either in the spatial domain – or in a transformed domain , –. In this paper, we will deal with the analysis of watermarking methods in the discrete cosine transform (DCT) domain, whichs56 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 9, NO. 1, JANUARY 2000 is
13904	13920	their performance are known. In this regard, the theoretical approach presented here is crucial if we want to compare different methods and know what are their fundamental limits of performance . This paper is organized as follows. In Sections II and III, the watermark generation and the watermark verification processes, respectively, are presented and formulated in mathematical notation.
13904	13920	of common images. As we will see in the following sections, the use of these models when designing the watermark detector and decoder will lead to considerable improvements in performance  with respect to the correlating detector structure, usually proposed in previous literature, which would be optimum only if the original image (i.e., the channel additive noise) could be modeled as
13904	13921	to the direct-sequence spread spectrum modulation schemes used in communications (Fig. 1). In this paper, we will assume that the DCT is applied in blocks of 8 × 8 pixels, as in the JPEG algorithm . We will allow the watermark to carry a hidden message with information that could be used, for instance, to identify the intended recipient of the protected image. This message is mapped by an
13904	13923	coefficient . To obtain the mask , it is necessary to use a psychovisual model in the DCT domain. For the work presented in this paper, we have followed the model proposed by Ahumada et al. , , which has been applied by Watson to the adaptive computation of quantization matrices in the JPEG algorithm . This model has been here simplified by disregarding the so-called contrast-masking
13904	13929	Estimates of can be obtained by matching the sample mean absolute value and the sample variance of the DCT coefficients to those of the generalized Gaussian distribution, as proposed in  and , by solving where (25) (26) We propose an alternative technique, consisting in computing the ML estimator for the parameter. This ML estimate can be obtained by maximizing the log-likelihood
13904	13932	original image during watermark extraction and detection: sensibility to geometrical transformations such as scaling and rotation. An iterative 2-D synchronization algorithm as that discussed in  is a possible solution to this problem, although its suitability for implementation in a real system still has to be studied by means of a theoretical analysis. Channel coding schemes for error
13904	13933	a real system still has to be studied by means of a theoretical analysis. Channel coding schemes for error protection purposes can help in further improving the performance of the watermark decoder . The analysis of the impact of channel codes on the performance and the derivation of the most appropriate coding schemes is left as a future line of research. APPENDIX A SUFFICIENT STATISTICS FOR
13934	13936	Cooperative relaying has so far been considered primarily at the physical layer. Based on the ideas of user cooperation This work was supported by Vodafone Group Research and Development. diversity , Laneman et al.  propose various cooperative protocols for the three-terminal case. It is shown that diversity gains can be achieved; yet, these come at the cost of repetition coding and an
13934	13937	has so far been considered primarily at the physical layer. Based on the ideas of user cooperation This work was supported by Vodafone Group Research and Development. diversity , Laneman et al.  propose various cooperative protocols for the three-terminal case. It is shown that diversity gains can be achieved; yet, these come at the cost of repetition coding and an increase of the link
13934	13938	the inability of the relay(s) to receive and transmit simultaneously at the same frequency. It is worth noting that these drawbacks govern conventional and cooperative relaying in the same way. In , we proposed and analysed a simple cooperative decode-and-forward protocol that simultaneously exploits pathloss reduction, diversity gains, and the broadcast nature of the wireless medium.
13934	13944	(iii) the total number of resources should be kept at a minimum as available bandwidth and allowed delay are limited. This problem of resource assignment corresponds to a graph colouring problem ; for the problem at hand we have chosen to solve it using the so-called smallest-last ordering algorithm . The algorithm’s outcomes for a cellular two-hop scenario is depicted in Figs. 2. We
13934	13946	that assigning orthogonal resources also calls for an increased spectral efficiency of the individual transmissions. Note that routing and resource assignment form a joint optimization problem , which eventually might lead to different solutions for conventional and cooperative relaying. For simplicity, and in order to demonstrate the commonality of conventional and cooperative relaying,
13934	13948	transmission powers, but also demonstrates the feasibility of power control mechanisms in cooperative networks. Towards this end, we have used the distributed power control algorithm presented in . D. Pole Capacity, Processing Gain, and Data Rate We proceed by establishing the baseline case for comparison for cellular networks. For direct transmission from all nodes to the central base
13951	13958	based on low-level alerts from individual stages . To correlate IDS alerts for detection of an intrusion scenario, recent studies have employed two different approaches: a probabilistic approach (, , ) and an expert system approach (, , , , ). The probabilistic approach represents known intrusion scenarios as Bayesian networks. The nodes of Bayesian networks are IDS
13951	13958	Cayzer, J Kim, J McLeod current probabilistic approach fails to detect intrusions that do not show strong similarities between alert features but have causal relationships leading to final attacks (, , ). 3. A DT-Inspired Approach to Intrusion Detection We propose AIS based on DT ideas that can handle the above IDS alert correlation problems. As outline previously, the DT explains the
13951	13973	interested in its merits for scaling up AIS applications by overcoming self-nonself discrimination problems. 1. Introduction The key to the next generation Intrusion Detection System (IDS) (, , ) that we are planning to build is the combination of recent Artificial Immune System (AIS) / Danger Theory (DT) models (, , , ) with our growing understanding of cellular
13951	13973	to detect unauthorised use, misuse and abuse of computer systems by both system insiders and external intruders. Most current IDS define suspicious signatures based on known intrusions and probes . The obvious limit of this type of IDS is its failure of detecting previously unknown intrusions. In contrast, the HIS adaptively generates new immune cells so that it is able to detect previously
13951	13985	metaphor for IDS than the traditional self-nonself viewpoint, regardless whether the theory holds for the HIS, something that is currently hotly debated amongst immunologists (, , ). In particular, the DT provides a way of grounding the response, i.e. linking it directly to the attacker and it removes the necessity to map self or nonself . In our model, self-nonself
13986	13993	according to auditory feedback from the system. In practice, we envision using this system in one of three possible ways: with a desktop camera; with a camera on a stand such as a microphone stand ; with a handheld camera. Presently, the area, height, width, and aspect ratio, are ouput as MIDI control changes. Other methods of interfacing with synthesis modules are being investigated. 3.
13986	13994	by the avian syrinx. Keywords Mouth Controller, Face Tracking, Bioacoustics 1. INTRODUCTION Several studies have explored the use of the mouth or vocal tract for controlling audio synthesis . The motivations for this line of research relate to the role of the mouth in speech, singing, and facial expression, as previously discussed in some detail . The current study builds on
13986	13994	the image boundary, but this does not affect mouth cavity segmentation. Colour and intensity thresholding are used to segment the shadow area inside the open mouth, as with the headworn mouthesizer . The mouth appears as a dark, relatively red region in the image. Pixels with red component above a certain threshold and intensity less than another threshold are selected as beloning to the
8920554	13996	Internet, highly-dynamic distributed systems are becoming increasingly important. Examples of this growing importance can be found in recent researches in largescale peer-to-peer protocols , as well as in ad hoc network technologies . Intuitively, a highly-dynamic distributed system can be defined as a system whose configuration changes very often. What may vary in a configuration
8920554	13996	lead to the best global route between two peers. Moreover, when it is used in tree topologies, the restriction imposed on the paths makes the delay between nodes increase exponentially along routes , aggravating the harm of an early bad decision. The cost involved in keeping track of the closest peers in largescale highly-dynamic systems also prohibits the usage of perfect PNS in real systems
8920554	13997	idea is that whenever a configuration change occurs, the second layer will eventually converge to it, and as soon as this happens, the first layer will exhibit optimal behavior. 4. Some Examples In , we apply this approach to probabilistic reliable broadcast. Our optimal algorithm relies on the assumption that each node knows the topology and the reliability of nodes and links, and uses this
8920554	13997	a Maximum Reliability Tree (MRT) of the system. The MRT is a spanning tree containing the most reliable paths connecting all nodes. We calculate the MRT using a modified version of Prim’s algorithm . In our adaptive protocol, in addition to determining the MRT, nodes constantly try to approximate the topology and the reliability of nodes and links. If these remain stable “long enough”, our
8920554	13997	approach, an adaptive protocol has to maintain the topology information in each node and make it converge toward the actual system configuration. The strategy could be similar to the one used in , with periodic heartbeat messages exchanged between nodes to propagate topology information. Putting optimal and adaptive algorithms together, we get a complete adaptive protocol for building the
8920554	7016	Internet, highly-dynamic distributed systems are becoming increasingly important. Examples of this growing importance can be found in recent researches in largescale peer-to-peer protocols , as well as in ad hoc network technologies . Intuitively, a highly-dynamic distributed system can be defined as a system whose configuration changes very often. What may vary in a configuration
8920554	7016	to routing in peer-to-peer overlay networks. The well-known proximity neighbor selection (PNS) strategy selects the closest neighbors of a node in the underlying topology to build its routing table . However, this approach does not necessarily lead to the best global route between two peers. Moreover, when it is used in tree topologies, the restriction imposed on the paths makes the delay
8920554	7016	track of the closest peers in largescale highly-dynamic systems also prohibits the usage of perfect PNS in real systems and, therefore, some simpler heuristics must be used to approximate it . Our approach can be used to address this problem in two ways. First, it can be used to build the topology efficiently. Second, during routing a node can forward a message to the neighbor that will
62683	14002	be known in advance for identifying the positions of detected objects. In addition, many communication protocols of sensor networks are built on the knowledge of the geographic positions of sensors , , . However, in most cases, sensors are deployed without their position information known in advance, and there is no supporting infrastructure available to locate them after deployment.
62683	14002	installed with antenna array or ultrasound receivers. The second class of positioning methods relies on a large amount of sensor nodes with positions known densely distributed in a sensor network , , . These nodes with positions known, which are also named as beacons or anchor nodes, are arranged in grid across the network to estimate other nodes’ positions nearby them. The third class
62683	14003	with antenna array or ultrasound receivers. The second class of positioning methods relies on a large amount of sensor nodes with positions known densely distributed in a sensor network , , . These nodes with positions known, which are also named as beacons or anchor nodes, are arranged in grid across the network to estimate other nodes’ positions nearby them. The third class of
62683	14005	research community as a fundamentally new tool for a wide range of monitoring and data-gathering applications. Many applications with sensor networks are proposed, such as habitat monitoring , , , , health caring , , battle-field surveillance and enemy tracking , , and environment observation and forecasting , , . A general setup of a wireless sensor
62683	14007	in advance for identifying the positions of detected objects. In addition, many communication protocols of sensor networks are built on the knowledge of the geographic positions of sensors , , . However, in most cases, sensors are deployed without their position information known in advance, and there is no supporting infrastructure available to locate them after deployment. It is
62683	14007	topology of the sensor networks and complex terrain where the sensor networks are deployed. Moreover, cumulative measurement error is a constant problem of some existing sensor positioning methods , , . In order to accurately position sensors in anisotropic network and complex terrain and avoid the problem of cumulative errors, we propose a distributed method based on the
62683	14007	1111111111111 0000000000000 1111111111111 0000000000000 1111111111111 Grass A B r2 Clear ground Fig. 4. Anisotropic terrain condition leading to different radio ranges The last class of methods , ,  locally calculate maps of adjacent nodes with trilateration or multilateration and piece them together to estimate nodes’ physical or relative positions. The performance of these
62683	14007	the area can have different radio ranges, and a uniform radio range calculation will lead to serious errors (in , , ) and the errors may propagate throughout the sensors in the network , . IV. CALCULATING RELATIVE POSITIONS WITH MULTIDIMENSIONAL SCALING The multidimensional scaling (MDS), a technique widely used for the analysis of dissimilarity of data on a set of objects,
62683	14008	research community as a fundamentally new tool for a wide range of monitoring and data-gathering applications. Many applications with sensor networks are proposed, such as habitat monitoring , , , , health caring , , battle-field surveillance and enemy tracking , , and environment observation and forecasting , , . A general setup of a wireless sensor
62683	5746	randomly and densely deployed in a certain area. Each compact sensor usually is capable of sensing, processing data at a small scale, and communicating through omni-directional radio signal , . Because omni-direction radio signal attenuates with distance, only sensors within certain range can communicate with each other. This range is called radio range r. Wireless sensor networks
62683	14015	community as a fundamentally new tool for a wide range of monitoring and data-gathering applications. Many applications with sensor networks are proposed, such as habitat monitoring , , , , health caring , , battle-field surveillance and enemy tracking , , and environment observation and forecasting , , . A general setup of a wireless sensor network
62683	14020	proposed, such as habitat monitoring , , , , health caring , , battle-field surveillance and enemy tracking , , and environment observation and forecasting , , . A general setup of a wireless sensor network consists of a large number of sensors randomly and densely deployed in a certain area. Each compact sensor usually is capable of sensing, processing
62683	14021	the sensor networks and complex terrain where the sensor networks are deployed. Moreover, cumulative measurement error is a constant problem of some existing sensor positioning methods , , . In order to accurately position sensors in anisotropic network and complex terrain and avoid the problem of cumulative errors, we propose a distributed method based on the multidimensional scaling
62683	14021	0000000000000 1111111111111 0000000000000 1111111111111 Grass A B r2 Clear ground Fig. 4. Anisotropic terrain condition leading to different radio ranges The last class of methods , ,  locally calculate maps of adjacent nodes with trilateration or multilateration and piece them together to estimate nodes’ physical or relative positions. The performance of these algorithms relies
62683	14021	can have different radio ranges, and a uniform radio range calculation will lead to serious errors (in , , ) and the errors may propagate throughout the sensors in the network , . IV. CALCULATING RELATIVE POSITIONS WITH MULTIDIMENSIONAL SCALING The multidimensional scaling (MDS), a technique widely used for the analysis of dissimilarity of data on a set of objects, can
62683	14022	is achieved in this way. However, because of its simplicity, it is widely used in previous research. Later, Time of Arrival (ToA) and Time Difference of Arrival (TDoA) are used by Savvides et al.  and Priyantha et al.  to reduce the errors of range estimation, but these methods require each sensor node being equipped CPU with powerful computation capability. Recently, Niculescu et al.
62683	14022	exchange to find the distances from the non-anchor nodes to the anchor nodes. Based these distances, each node can estimate its position by performing a trilateration or multilateration , . The performance of the algorithms is deteriorated by range estimation errors and inaccurate distance measures, which are caused by complex terrain and anisotropic topology of the sensor network.
62683	14022	and terrain condition (Figure 4). Thus, sensors at different locations of the area can have different radio ranges, and a uniform radio range calculation will lead to serious errors (in , , ) and the errors may propagate throughout the sensors in the network , . IV. CALCULATING RELATIVE POSITIONS WITH MULTIDIMENSIONAL SCALING The multidimensional scaling (MDS), a
62683	14023	new tool for a wide range of monitoring and data-gathering applications. Many applications with sensor networks are proposed, such as habitat monitoring , , , , health caring , , battle-field surveillance and enemy tracking , , and environment observation and forecasting , , . A general setup of a wireless sensor network consists of a large number of
62683	14024	of the algorithms is deteriorated by range estimation errors and inaccurate distance measures, which are caused by complex terrain and anisotropic topology of the sensor network. Savarese  try to improve the above approach by iteratively computing. However, this method adds a large number of communication costs into the algorithm and still cannot generate good position estimation in
62683	14024	and terrain condition (Figure 4). Thus, sensors at different locations of the area can have different radio ranges, and a uniform radio range calculation will lead to serious errors (in , , ) and the errors may propagate throughout the sensors in the network , . IV. CALCULATING RELATIVE POSITIONS WITH MULTIDIMENSIONAL SCALING The multidimensional scaling (MDS), a technique
62683	5015	advance for identifying the positions of detected objects. In addition, many communication protocols of sensor networks are built on the knowledge of the geographic positions of sensors , , . However, in most cases, sensors are deployed without their position information known in advance, and there is no supporting infrastructure available to locate them after deployment. It is
62683	14026	as a fundamentally new tool for a wide range of monitoring and data-gathering applications. Many applications with sensor networks are proposed, such as habitat monitoring , , , , health caring , , battle-field surveillance and enemy tracking , , and environment observation and forecasting , , . A general setup of a wireless sensor network consists
62683	14028	all sensors in a sensor network. The work presented in the paper is the extension of our previous study on sensor positioning methods and dimensionality reduction techniques , , , , . In the paper, we first analyze some challenges of sensor positioning problem in real applications. The conditions that most existing sensor positioning methods fail to perform well are the
62683	14029	all sensors in a sensor network. The work presented in the paper is the extension of our previous study on sensor positioning methods and dimensionality reduction techniques , , , , . In the paper, we first analyze some challenges of sensor positioning problem in real applications. The conditions that most existing sensor positioning methods fail to perform well are the
14030	7973	the reliability and stability of IBGP operation also depend on the quality of underlying TCP and IGP routing. BGP sessions are sensitive to transport layer stability and routing layer reliability , especially the IBGP sessions which may cross multiple network hops. However, the impact on the reliability of IBGP sessions and IBGP networks, which is from the IGP routing and the physical
14030	14032	reliability and stability of IBGP operation also depend on the quality of underlying TCP and IGP routing. BGP sessions are sensitive to transport layer stability and routing layer reliability , especially the IBGP sessions which may cross multiple network hops. However, the impact on the reliability of IBGP sessions and IBGP networks, which is from the IGP routing and the physical
14030	14036	which makes IGP routing recovery time unexpectedly prolonged. For example, minute level (even greater than 10 minutes) routing outage is reported from a real measurement in an IP backbone  1 .This shows that long outage of IGP routing exists in practice, and we need to study its impact on time critical network protocols, such as BGP. TCP provides reliable communication support to BGP
14030	14037	among different IBGP sessions. For example, in Fig. 1, IBGP session between H and E is not statistically independent of the session between G and E, because they share one physical link. Cui et al.  give an 1 The long delay is caused by the router oscillation between the ‘faulty’ state and the normal state without being detected by peer routers, and the failure to set ‘Infinity Hippty Cost’
62281	14048	exemplar-based tracking 1. Introduction There is, of course, a substantial literature on tracking, driven either by image features (Amini et al., 1988; Kass et al., 1987) or by raw intensity (Bascle and Deriche, 1995; Black and Jepson, 1996; Hager and Toyama, 1996), or both (Cootes et al., 1998). Tracking can be formulated in a probabilistic framework in both the feature-driven (Terzopoulos and Szeliski, 1992)
62281	14049	1. Introduction There is, of course, a substantial literature on tracking, driven either by image features (Amini et al., 1988; Kass et al., 1987) or by raw intensity (Bascle and Deriche, 1995; Black and Jepson, 1996; Hager and Toyama, 1996), or both (Cootes et al., 1998). Tracking can be formulated in a probabilistic framework in both the feature-driven (Terzopoulos and Szeliski, 1992) and intensity-driven
62281	14051	setting while avoiding the use of explicit models to describe target objects. The use of exemplars, for example, the contour exemplars in Fig. 1, offers an alternative that can tackle this problem (Brand, 1999; Efros and Leung, 1999; Freeman and Pasztor, 1999; Frey and Jojic, 2000; Gavrila and Philomin, 1999). Exemplar-based models can be constructed very directly from training sets, without the need to
62281	1102	on tracking, driven either by image features (Amini et al., 1988; Kass et al., 1987) or by raw intensity (Bascle and Deriche, 1995; Black and Jepson, 1996; Hager and Toyama, 1996), or both (Cootes et al., 1998). Tracking can be formulated in a probabilistic framework in both the feature-driven (Terzopoulos and Szeliski, 1992) and intensity-driven (Storvik, 1994) settings. The probabilistic formulation
62281	1102	only the mouth, and no background, is visible. While distraction is not a problem, the complex articulations of the mouth make tracking difficult (even state-ofthe-art face-tracking algorithms (Cootes et al., 1998; Neven, 2000) have difficulty tracking lip and tongue articulation). 5.1. Tracking Human Motion For the person tracking experiments, training and test sequences show various people walking from
62281	14052	avoiding the use of explicit models to describe target objects. The use of exemplars, for example, the contour exemplars in Fig. 1, offers an alternative that can tackle this problem (Brand, 1999; Efros and Leung, 1999; Freeman and Pasztor, 1999; Frey and Jojic, 2000; Gavrila and Philomin, 1999). Exemplar-based models can be constructed very directly from training sets, without the need to set up complex
62281	14053	maps that proves so powerful with non-probabilistic exemplars (Gavrila and Philomin, 1999); • finally, image noise is treated as white despite known, strong statistical correlations between pixels (Field, 1987). The problem, therefore, is to combine exemplars in a metric space (Gavrila and Philomin, 1999) with a probabilistic treatment (Frey and Jojic, 2000), retaining the best features of each approach.
62281	14054	models to describe target objects. The use of exemplars, for example, the contour exemplars in Fig. 1, offers an alternative that can tackle this problem (Brand, 1999; Efros and Leung, 1999; Freeman and Pasztor, 1999; Frey and Jojic, 2000; Gavrila and Philomin, 1999). Exemplar-based models can be constructed very directly from training sets, without the need to set up complex intermediate representations, such
62281	14056	of exemplars, for example, the contour exemplars in Fig. 1, offers an alternative that can tackle this problem (Brand, 1999; Efros and Leung, 1999; Freeman and Pasztor, 1999; Frey and Jojic, 2000; Gavrila and Philomin, 1999). Exemplar-based models can be constructed very directly from training sets, without the need to set up complex intermediate representations, such as parameterized contour models or 3D articulated
62281	14056	ruling out nonlinear transformations that can help with invariance to scene conditions, including the conversion of images to edge maps that proves so powerful with non-probabilistic exemplars (Gavrila and Philomin, 1999); • finally, image noise is treated as white despite known, strong statistical correlations between pixels (Field, 1987). The problem, therefore, is to combine exemplars in a metric space (Gavrila
62281	14056	to be amenable to straightforward analytical description; instead X is defined in terms of a set {˜xk, k = 1,...,K} of exemplars, together with a distance function ?, inthe spirit of Gavrila (Gavrila and Philomin, 1999). For example, the face of a particular individual, might be represented by a set of exemplars ˜xk consisting of normalized (registered), frontal views of that face, wearing a variety of
62281	14056	where r1(s),...,rd(s) are now independent curve basis functions such as parametric B-splines (Bartels et al., 1987). In this case, the distance measure ? used is a (non-symmetric) chamfer distance (Gavrila and Philomin, 1999). The chamfer distance is defined to be ?(˜z, z) = min s ? ? (s) ds g(|rz(s ? ) ? r˜z(s)|), (4) where g(·) is the profile of the chamfer. A particularly interesting case is the quadratic chamfer,
62281	14056	and they justify the need for the ability to handle metrics that are not embedded in a vector space. 6. Conclusion The Metric Mixture approach combines the advantages of exemplar-based models (Gavrila and Philomin, 1999) with a probabilistic framework (Frey and Jojic, 2000) into a single probabilistic exemplar-based paradigm. The power of the M 2 technique comes from its generality: both object models and noise
62281	14061	quadratic chamfer distance, appears to behave consistently: it converges to d = 10 as data set size increases. so the integral is performed using a form of particle filter (Gordon et al., 1993; Isard and Blake, 1996). To display results, we calculate ˆXt = arg max pt(Xt). Figure 1 shows cropped, sample images of tracking on a sequence that was not in the training sequence. Tracking in this case is
62281	14062	distance. Keywords: probabilistic tracking, exemplar-based tracking 1. Introduction There is, of course, a substantial literature on tracking, driven either by image features (Amini et al., 1988; Kass et al., 1987) or by raw intensity (Bascle and Deriche, 1995; Black and Jepson, 1996; Hager and Toyama, 1996), or both (Cootes et al., 1998). Tracking can be formulated in a probabilistic framework in both the
62281	14064	features (e.g., ridges) enhanced, or nonlinearly filtered to produce a sparse binary image with feature pixels marked. A given image z is to be approximated, in a standard pattern theoretic manner (Mumford, 1996), as an ideal image or object x ? X that has been subjected to a geometrical transformation T? from a continuoussset ? ? A, i.e.: z ? T?x. (1) 2.1. Transformations and Exemplars The partition of
62281	1069	intrinsic local dimensionality. 5.1.2. Practical Tracking. We can now compute observation likelihoods as in Eq. (7) and track using the following Bayesian framework. A classical forward algorithm (Rabiner, 1989) would give pt(Xt) ? p(Xt | z1,...,zt) as: pt(Xt) = ? ? p(yt | Xt)p(Xt | Xt?1)pt?1(Xt?1), Figure 4. Convergence of d-estimate with cluster size, for synthesized polygons. (a) Estimated dimension
62281	14066	Hager and Toyama, 1996), or both (Cootes et al., 1998). Tracking can be formulated in a probabilistic framework in both the feature-driven (Terzopoulos and Szeliski, 1992) and intensity-driven (Storvik, 1994) settings. The probabilistic formulation has the attraction that uncertainty is handled in a systematic fashion, allowing principled handling of sensor fusion and temporal fusion. Many such
8920563	14070	the system is in. With no visual feedback, for example, the user may not know whether a silence from the system means that the system is working or that the system didn't receive the last command , . With the loss of visual display comes a loss of common vocabulary and of a common ground for referring to actions and objects of interest. When using a graphical user interface (GUI), a user
8920563	14070	• Warn the user about the length of upcoming aural presentations. Arons, for example, used a short, highpitched tone to signal a brief presentation and a longer, low-pitched tone for a long one . • Confirmation and error dialogues may not translate well into speech presentations. Users are likely to fail to respond to questions such as “Is that OK?” If such dialogues are needed, prompt for
8920563	14072	speech, yes/no questions are rarely answered with a simple yes or no. In the case of confirmation messages such as these, people tend to respond with the next relevant action they wish to take . The very need for a hands-free interface suggests that the user is likely to be interrupted during presentation of the documentation. With a visual display, it is relatively easy to attend to the
8920563	14073	the systems for accessing airline flight information by telephone, the vocabulary is limited instead. The system may use a series of carefully-worded prompts to request information from the user , e.g., “If you know your flight number, please say it now.” In this way, the system attempts to limit the user’s likely input to a small set of words at each point in the dialogue. If these words
8920563	14077	is established by the surrounding text. If the user must refer to them verbally, however, the reference may be ambiguous. In an early implementation of a speech-controlled web browser, House  allowed for users cycling through the available links on a page using commands such as “next.” This works, but clearly is awkward and slow compared to pointing. Yankelovich found that users tend
8920563	14077	of contents and index, the visual display provides both a fast means of output and a source of target vocabulary items for a speech recognizer. This would build on the technique proposed by House  for spoken-language access to Web pages. Browsing or scanning, too, is made easier with visual output. The actions of leafing through a manual could be approximated through voice commands such as
8920563	14082	is the most likely—and usable—way for the user to communicate with the documentation system. Other possibilities are slim. It is true that a foot-operated mouse has been proposed for desk-workers  and a limited form of eye-tracking has been made to work for users unable to manipulate a traditional mouse, but these are awkward at best and would be problematic in an environment where the user
8920563	14082	occurs, users often attempt to speak more distinctly on the assumption that doing so will improve the speech recognizer’s performance. Unfortunately, this hyperarticulation makes matters worse . Speech recognizers are trained on the relaxed pronunciations normally used in fluent speech, e.g., “fordy” for the number forty. When a user presents hyperarticulate speech, the speech recognizer
8920563	14082	the input (“Did you say ‘fourteen’?”). The unsuspecting user responds by trying to pronounce the words even more carefully, and the interaction rapidly degenerates into a spiral of errors . 3.1 Background Current speech-input systems achieve their success by sharply limiting some aspect of either speaker variability or phonetic ambiguity. Dictation systems, such as IBM’s ViaVoice
8920563	14086	and many of the signal characteristics that speech recognizers use are altered for nearly every phoneme . Worse, the changes vary both by speaker  and by the amount and type of noise , making it particularly difficult for speech recognizers to compensate for the effect. When a recognition error occurs, users often attempt to speak more distinctly on the assumption that doing so
8920563	14088	to the meaning of a sentence. For example, even if the command “Switch to weather” were misrecognized as “Please weather,” the correct action will still be taken (example from Yankelovich et al. ). Noise poses additional problems for speech recognition. Background noise may confuse or drown out the speech signal. Although humans are remarkably proficient at isolating and following a single
8920563	14088	on the fly if need be. This approach has several shortcomings, however. The fact that the user must manipulate and navigate the documentation display by voice imposes some additional considerations . In a GUI, for example, one discovers what commands are available by moving the mouse to a menu and pressing a button. How should that action be expressed verbally? It is likely that the user will
8920563	14088	using commands such as “next.” This works, but clearly is awkward and slow compared to pointing. Yankelovich found that users tend use relational and positional terms to specify which is wanted , suggesting that references such as “the first one” or “the second ‘here’ button” may be expected. Finally, one of the advantages of speech over direct manipulation is the ability to refer to
8920563	14088	Monday&quot; over the use of absolute terms such as &quot;Monday, the 10th.&quot; Even when users are very familiar with a graphical interface for a similar task, they may not remember or use the GUI terminology . Yankelovich also found that other GUI conventions do not translate well into speech-only interfaces. For example, users often ignored confirmation requests, e.g., &quot;Your message is being sent to
8920563	14088	distracted by other events. Include stop-continue-repeat commands to allow the user to quickly interrupt and then resume presentation of the documentation. • Keep presentations as short as possible . A literal reading of written documentation is likely to be too wordy for effective verbal presentation. Elide words where the previous presentation has established contexts. Use hierarchical
14089	14854	savings are achieved by optimizing the number of multiplications and additions necessary while preserving the acceptable quality of the DCT coefficients produced as defined by the MPEG-4 standard . Hardwaresmultipliers are inherently more complex and power consumptive compared to adders so greatest effort is focused on minimizing these. where F = Af (1) F(u) = = ? 2 N C(u) N?1 ? f(x)cos N?1
14089	14854	Performance As mentioned in the previous sub-section, 13 weights are required for each coefficient computation (N in total) for strict compliance with the MPEG-4 standard performance requirements . The DCT coefficients are said to be compliant if, when reconstructed to pixels using an inverse transform (IDCT), they are within two grey levels of the same data that has undergone a double
14089	14095	technique for implementing a dot product of a variable data vector with a vector of constants is Distributed Arithmetic (DA), especially if energy efficiency is paramount to the implementation . In short DA is an efficient architecture for computing a dot product by transforming it to a series of additions, look-ups and shift operations. DA distributes the bits of the input variable
14122	14124	such that the sum of the weights of the arcs on the path is minimised. Most algorithms or applications in the graph framework use a labeling approach, in particular the one due to Dijkstra , . Buckley and Yang developed a regularised shortest path extraction algorithm for rectangular images . The algorithm uses dynamic programming (DP) techniques for shortest path extraction. They
14122	14124	make the top and bottom touch, so that a toroid shape is formed. A. Shortest Path Extraction Algorithm A huge number of scientific papers are devoted to the shortest path problem. We refer to ,  for a general view of the problem and of the most efficient algorithms proposed. Let G = (N, A) be a directed graph, where N is the set of nodes of cardinality n, and A is the set of arcs of
14122	14127	on the starting and ending positions of the path. A number of authors used dynamic programming technique to obtain a shortest path in a rectangular matrix for stereo disparity measurement , , , . All these applications impose no constraints on the starting and ending positions of the shortest path. Research partially supported by grants: MURST and INDAM-GNAMPA of Italy.
14122	14130	path should be extracted. That is the starting and the ending positions of the path should be at neighbouring points. In some image analysis applications, object boundaries need to be extracted . In these applications, it is necessary to make sure that the boundary extracted are closed contours. Panoramic stereo images are becoming available for 3D applications. In 360 ? panoramic stereo
14122	14130	does not join up together at the starting and the ending positions as shown in Figure 7(b). B. Boundary Detection du Buf et al described their first results on diatom contour extraction in . In a preprocessing step initial contours are extracted using a conventional edge-following algorithm like Canny’s. The object contours are extracted by using the best-fitting ellipse and a
5859	14132	segmentation from multiple views has only been studied in the case of affine cameras, because in this case the motion of each one of the rigidly moving objects lives in a four-dimensional subspace . That is, if xfp is the image of point Xp ? P3 in frame f and Af ? R2×4 is the affine camera matrix in frame f, then xfp = Af Xp. Therefore if we stack all the image measurements into a 2F × P
5859	14132	= ? ? A1 . AF ? ? ? X1···XP 2F ×4 ? 4×P . (1) Therefore rank(W) ? 4 and the motion of each object lives in a four-dimensional subspace of R2F . This subspace constraint was used by Boult and Brown  to propose the first multiframe motion segmentation algorithm based on thresholding the leading singular vector of W. Costeira and Kanade (CK)  extended this approach by thresholding the entries
5859	14133	subspace constraint was used by Boult and Brown  to propose the first multiframe motion segmentation algorithm based on thresholding the leading singular vector of W. Costeira and Kanade (CK)  extended this approach by thresholding the entries of the so-called shape interaction matrix Q = VV? , (2) which is built from the SVD of the measurement matrix W = USV ? and has the property that
5859	14134	point-tracks). Essentially what this algorithm is doing is simply alternating between computing Ak and Bk using least-squares. Similar (not identical) alternation algorithms have been proposed in . As mentioned  gives theoretical justification for this algorithm. 2.2. Fitting motion subspaces using GPCA We have reduced the motion segmentation problem to finding a set of linear subspaces
5859	5855	? and has the property that ? =0 if i and j correspond to the same motion Qij . (3) ?= 0 otherwise Unfortunately, thresholding the entries of the shape interaction matrix is very sensitive to noise . Wu et al.  reduce the effect of noise by building a similarity matrix from the distances among the subspaces obtained by the CK algorithm. Kanatani modifies the entries of the interaction
5859	5855	and also incorporates the fact that the subspaces are affine . Another disadvantage of the CK algorithm is that equation (3) holds if and only if the motion subspaces are linearly independent . That is, the algorithm is not provably correct for most practical motion sequences which usually exhibit partially dependent motions, such as when two objects have the same rotational but
5859	3145	obtained by the CK algorithm. Kanatani modifies the entries of the interaction matrix using the geometric information criterion  and also incorporates the fact that the subspaces are affine . Another disadvantage of the CK algorithm is that equation (3) holds if and only if the motion subspaces are linearly independent . That is, the algorithm is not provably correct for most
5859	14137	displacement of trajectories corresponding to the same motion is smaller than the one of trajectories corresponding to different motions. This assumption is, however, not provably correct. Kanatani  has also studied the case of partially dependent (degenerate) motions under the assumption that the type of degeneracy is known, e.g. 2-D similarity motion or pure translation. In these particular
5859	5857	the estimates of our algorithm by using it to initialize any iterative technique that alternates between motion estimation and data clustering, such as K-subspace  or EM for mixtures of PCA’s . Figure 2 shows the shape interaction (similarity) matrix used by the CK algorithm for the Can-Book sequence. We built the interaction matrix by using different values for the rank of the W matrix.
5859	14139	segmentation from multiple views has only been studied in the case of affine cameras, because in this case the motion of each one of the rigidly moving objects lives in a four-dimensional subspace . That is, if xfp is the image of point Xp ? P3 in frame f and Af ? R2×4 is the affine camera matrix in frame f, then xfp = Af Xp. Therefore if we stack all the image measurements into a 2F × P
5859	14139	feature points, e.g. . Once the features have been clustered, one can estimate the motion and structure of each moving object using the standard factorization approach for affine cameras, e.g. . We therefore have the following algorithm for motion estimation and segmentation from multiple affine views. Algorithm 1 (Multiframe motion segmentation algorithm) Given a matrix W ? R2F ×P
5859	14140	process that alternates between feature clustering and motion estimation, similarly to the Expectation Maximization (EM) algorithm. Other EM-like approaches to motion segmentation can be found in . 1.1. Contributions of this paper In this paper, rather than insisting on heuristics that modify a motion segmentation algorithm that is provably correct only when the motion subspaces are fully
5859	5858	factorization, and can be extended to most two-view motion models in computer vision, such as affine, translational and planar homographies, by fitting and differentiating complex polynomials . The case of multiple moving objects seen by three perspective views has also been recently solved by exploiting the algebraic and geometric properties of the multibody trifocal tensor . 1 To
5859	3153	contain the data points (or come close to them). The points in question are the columns of the projected data matrix ˆW. We solve this problem by fitting and differentiating polynomials using GPCA . 2.2.1 Representing motion subspaces with polynomials It serves our purposes to fit a slightly more general model to the points, fitting them by an algebraic variety. In essence, notice that the n
5859	3153	p to fit all subspaces, we effectively choose a single one of these hyperplanes containing the low-dimensional subspace. 3 As long as this hyperplane does not correspond 3 It was shown in  that even though for degenerate motions the polynomial p may not be factorizable, its derivative at a point in one of theswith the hyperplane defining one of the other motions, there will be no
5859	3153	from a non-degenerate one, unless the low-dimensional subspace lies inside the hyperplane corresponding to the non-degenerate motion. Modelling low-dimension subspaces We refer the reader to  for a possible extension of the above techniques to model low-dimension subspaces explicitly. To do this, instead of fitting the data to a single polynomial, we need to compute a set of independent
5859	3154	two lines in R 3 passing through the origin, then one can first project the two lines onto a plane in general position 2 and then cluster the data inside that plane. More generally the principle is : Theorem 2.1. If a set of vectors {wi} all lie in a linear subspace of dimension k in R N , and if ? represents a linear projection into a subspace S of dimension K, then the points {?(wi)} lie in
5859	3154	contain the data points (or come close to them). The points in question are the columns of the projected data matrix ˆW. We solve this problem by fitting and differentiating polynomials using GPCA . 2.2.1 Representing motion subspaces with polynomials It serves our purposes to fit a slightly more general model to the points, fitting them by an algebraic variety. In essence, notice that the n
5859	14141	for the understanding of dynamic scenes, in which both the camera and multiple objects move. The case of multiple moving objects seen by two perspective views was recently studied by Vidal et al. , who proposed a generalization of the 8-point algorithm based on the so-called multibody epipolar constraint and its associated multibody fundamental matrix. The method simultaneously recovers
5859	14142	for the understanding of dynamic scenes, in which both the camera and multiple objects move. The case of multiple moving objects seen by two perspective views was recently studied by Vidal et al. , who proposed a generalization of the 8-point algorithm based on the so-called multibody epipolar constraint and its associated multibody fundamental matrix. The method simultaneously recovers
5859	5860	whosuse the singular vectors of the normalized shape interaction matrix to build a similarity matrix from which the clustering of the features is obtained using spectral clustering techniques (see  and references therein). This solution is based on the expectation that on the average the angular displacement of trajectories corresponding to the same motion is smaller than the one of
5859	5860	< 1 if they belong to different motions. Given the so-defined similarity matrix S ? R P ×P , one can apply any spectral clustering technique to obtain the segmentation of the feature points, e.g. . Once the features have been clustered, one can estimate the motion and structure of each moving object using the standard factorization approach for affine cameras, e.g. . We therefore have
5859	14143	motions, such as when two objects have the same rotational but different translational motion relative to the camera, or vice versa. This has motivated the recent work of Zelnik-Manor and Irani  whosuse the singular vectors of the normalized shape interaction matrix to build a similarity matrix from which the clustering of the features is obtained using spectral clustering techniques (see
14161	14171	circuit. The idea is extended in FIRES  for sequential circuits, which uses illegal state information as an extra criterion for untestable faults identification. Then, the authors of  proposed a generalized FIRES algorithm, which identifies c-cycle redundancies without simplifying assumption and state transition information. And on top of FIRES algorithm,  proposed a
14161	14177	target fault and propagate the fault effect. When the broad-side approach is used, this need to assign a large number of inputs (primary or state inputs) to binary values in time frame 1. If PODEM  algorithm is used to generate test patterns, this will be achieved by a series of backtrace operations in time frame 1. Whether setting ??¡ to a 1 in time frame 2 leads to a solution to generate a
14161	14177	Otherwise, we continue to backtrace to excite X1 s-a-1. If not successful, we declare X slow to rise untestable. 6.5 Experimental Results We implemented a constrained broadside ATPG based on PODEM  in C++, as well as the implication-based untestable transition fault identification, also in C++. We further analyzed the effectiveness of our ATPG algorithm by comparing it with a conventional
14161	14181	? ¦ and assign =1 and continue the ATPG process. During the ATPG backtrace, an implication stack is dynamically updated to record the implication list of earlier backtrace choices similar to . We maintain two dynamic implication lists: ¡ are necessary for setting X1=0, and ¡ ???¡s? setting X2=1. If there is conflict between ¡ ???¡s?£¢ ¡ for storing the implications that ¢ ? for storing
14161	14182	increase the size of the final solution. Our experiments suggest that the chain lengths of 3 or 4 give the best results. Secondly, expanding on the essential test definition for stuck-at fault from , we define an essential vector for transition faults to be any test vector that excites (or detects) at least one transition fault that is not excited (or detected) by any other test vector in the
14161	14183	Several delay fault models have been developed, such as transition delay fault , gate delay fault , path delay fault , and assumes a large segment delay fault models . A transition fault at node ¡ delay at ¡ such that the transition at ¡ will not reach the latch or primary output within the clock period. The path delay fault model assumes a small delay at each
14161	14183	at one gate in the CUT. However, unlike the transition delay model which does not take into account fault sizes, the gate delay model 2stakes into account fault sizes. The segment delay fault model  is a trade-off between the path delay fault and transition delay fault models. Detection of a delay fault normally requires the application of a pair of test vectors; the first vector, called
14161	14183	Testing Several delay fault models have been developed for delay defects: transition delay fault , gate delay fault , path delay fault , and segment delay fault models . A transition fault at node ¡ assumes a large delay at ¡ such that the transition at ¡ will not reach the latch or primary output within the clock period. The path delay fault model assumes a small
14161	14183	is lumped at one gate in the CUT. However, unlike transition delay model which does not take into account fault sizes, gate delay model takes into account fault sizes. Segment delay fault model  is a trade-off between path delay fault and transition delay fault models. 2.1 Functional Delay Testing In the past, testing circuit’s performance was typically accomplished with functional test
14161	14183	geometries, longer wires, increasing metal density, etc. The three most prevalent fault models for delay testings are: transition fault , path delay fault , and segment delay fault . Among them, the transition fault model are most widely used in the industry due to its simplicity and similarity to the stuck-at fault model. In this chapter we only target at the transition fault
14161	14184	implication-based approach is proposed for identifying non-robust untestable path delay faults in combinational circuits. Additional general implication-based algorithms are given in  on identifying untestable segment delay faults and robustly untestable, non-robustly untestable and functionally unsensitizable path delay faults. Most recently, approaches proposed in
14161	14185	when ¡ s@-1 is detectable if and only if there does not exist any s@ test inswhich sets ¡ both cases an initialization vector does not exist. Table 3.2: Results for Exhaustive Patterns. Circuit  S@ Trans Exhaust Pairwise Vec FC(%) FC(%) Vec TFC(%) c880 128 100 95.51 16256 100 c1355 198 99.77 94.23 39006 99.77 c1908 143 99.67 93.03 20306 99.67 c3540 202 96.29 88.68 40602 96.27 c5315 157
14161	14185	S38584 893 95.34 91.63 796556 95.02 to 1. In To determine the maximal transition coverage obtainable by exhaustively pairing all s@ vectors was computed. Results are shown in Table 3.2. STRATEGATE  stuck-at test sets were used. Columns 3, 6 of Table 3.2 imply that by including all possible combination of vector pairs, the transition fault coverage reach the s@ coverage in most cases. In a few
14161	14186	in our graph-formulated ATPG algorithm described in section 4.2.1. In general, identifying functionally untestable fault in sequential circuits is of the same complexity as sequential ATPG. In , a method for identifying untestable stuck-at faults in sequential circuits by maximizing local conflicting value assignments has been proposed. The technique first computes a large number of logic
14161	14186	in terms of the size of the circuit. In this section, we describe a novel untestable transition fault identification method by combining a transition fault implication engine and Broadside ATPG. In , a method for identifying untestable stuck-at faults in sequential circuits by maximizing local conflicting value assignments has been proposed. The technique first quickly computes a large number
14161	14186	faults, in addition to searching for the impossibilities locally around each gate, we also check the excitability of the initial value in the previous time frame. Thus, the implication engine in  can be extended to quickly identify a large set of untestable transition faults in the circuit. Although the transition fault implication engine helps us in identifying a large number of untestable
14161	14186	Transition Faults. tal number of functionally untestable transition faults in the circuit, and region S1 contains the redundant stuck-at faults identified by stuck-at fault implication-based method ; region S2 contains the untestable transition faults by our new transition fault implication engine and S3 is the set of untestable transition faults identified by the Broadside ATPG. Note that the
14161	14186	the authors of  proposed a generalized FIRES algorithm, which identifies c-cycle redundancies without simplifying assumption and state transition information. And on top of FIRES algorithm,  proposed a multiple-node implication approach to maximize local conflicting value assignment for the purpose of untestable faults identification. For delay faults, several approaches [BI94, LMB97,
14161	14193	untestable faults which do not impair normal operation of the circuit may become testable in scan testing . This scenario 102sis known as overtesting and may result in yield loss . Unfortunately, identifying the functionally untestable transition faults in large-scale sequential circuit can be prohibitively expensive, because it is of the same complexity as sequential ATPG,
14161	14195	Skewed-load transition tests also lead to larger test data volume. Compared to stuck-at tests, the increase in the number of vectors required for enhanced scan to get complete coverage is about 4X  For skewed-load transition test, it has been observed that the data volume for an ASIC has an increase of 5.9X . For most circuits, test sets generated by the skewed-load approach achieve
14161	14197	and applying similar patterns to the different partitions  have been proposed. The techniques proposed here compliments the work on compressing individual vectors. In our previous work , we presented two algorithms for computing transition test patterns from generated s@ test vectors, which can reduce the test set size by 20%. Nevertheless, there has not been much work on
14235	14236	we may have some subtle attacks depending on how messages are exchanged over the network. Several methodologies for the analysis of security protocols have been developed in recent years, e.g. . For our analysis we consider honest participants S of a protocol plus an intruder X , as a compound system SjX , i.e. a system whose components run in parallel and that can interact. Basically,
14235	7156	we may have some subtle attacks depending on how messages are exchanged over the network. Several methodologies for the analysis of security protocols have been developed in recent years, e.g. . For our analysis we consider honest participants S of a protocol plus an intruder X , as a compound system SjX , i.e. a system whose components run in parallel and that can interact. Basically,
14235	14241	An operational language is adopted for the description of the protocols. It is a CCS process algebra style language, , with slight variations for the treatment of cryptographic functions, . We do not need a priori any intruder specification and moreover we make no assumption on its behavior, X can be any term of the algebra. As the other protocol participants, X has a set of message
14235	14241	the standard notation with which security protocols are usually specified in literature - for information about the operational language used to describe the protocol specifications we refer to  - in this paper we prefer to follow a more intuitive notation. We then consider, with a simple example, the uncertainty about protocol specifications correctness. We will show how the security
14235	14242	we may have some subtle attacks depending on how messages are exchanged over the network. Several methodologies for the analysis of security protocols have been developed in recent years, e.g. . For our analysis we consider honest participants S of a protocol plus an intruder X , as a compound system SjX , i.e. a system whose components run in parallel and that can interact. Basically,
14235	10320	origin should be guaranteed by the fact that only the sender knows the private key and thus only he could generate the encryption. These concepts are the basis for digital signature schemes, e.g. . Basically, a digital certificate is an electronic document that links an identity (i.e. a person or a machine) and a public key. It is issued by a Certification Authority (CA) that can vouch for
29776585	14245	is used to issue several requests using the HTTP/1.1 “keep-alive” feature. 6.2 Experimental setup We downloaded 3.5 million pages in April 2004 from over 50,000 Web sites using the WIRE crawler . Based on previous studies of the Chilean Web , we estimate that this sample represents accurately a large fraction of the Chilean Web. We restricted the crawler to download only the first
29776585	14248	policies using a real Web crawler. The last section presents our conclusions. The results presented here were obtained in a joint work with Mauricio Marin, Andrea Rodriguez and Ricardo Baeza-Yates . 6.1 The problem of crawler scheduling We consider a Web crawler which has to download a set of pages, with each page p having size S p measured in bytes, using a network connection of capacity B,
29776585	14249	size of the batch. Another approach to short-term scheduling is to dynamically adjust the number of threads by predicting the bandwidth at which each Web site will transfer pages. Diligenti et al.  maintain several observed values for predicting connection speed, and group the observations by time of the day to account for the periodicity in Web server response time. 6.5 Downloading the real
29776585	14252	documents Figure 6.3: Distribution of site sizes. There is another serious practical constraint: the HTTP request has latency, and the latency time can be over 25% of the total time of the request . This latency is mainly the time it takes to establish the TCP connection and it can be partially overcome if the same connection is used to issue several requests using the HTTP/1.1 “keep-alive”
29776585	14254	even if we consider only the pages whose Pagerank monotonically increase, the average relative error for estimating the Pagerank four months 5sahead is about 78%. Also, a study by Ntoulas et. al  reports that “the link structure of the Web is significantly more dynamic than the contents on the Web. Every week, about 25% new links are created”. Depth Under this strategy, the crawler visits
14256	14257	consistent with econometric studies showing that the income elasticity for the demand for nonstaple foods is typically considerably higher than that for staples (see Bouis and Novenario-Reese 1997, Alderman and Lindert 1998, and Hoddinott and Skoufias 2000 for recent examples). The mixed evidence on the associations between dietary diversity and caloric acquisition requires a little more detailed explanation. A good
14293	14298	Relevant in this context also is the recent general trend towards greater concern in the AI community with emotional aspects of intelligence, sparked mainly by the work of Breazeal and Brooks (Brooks et al. 1998; Breazeal 1998). Goals and Hypotheses We study response hedging and expressions of affect in human and machine tutoring sessions. This study incorporates both exploratory and hypothesis testing
14293	14301	it is possible that a difference in communication modality can affect student hedging behavior. As well, decoding students’ affect may be easier from speech, due to tonal and prosody cues (Forbes-Riley & Litman 2004). One specific result of importance to ITS is that hedging is not a clear indication of student uncertainty or misunderstanding, as had been believed. Indeed, examination of the types of hedges
14293	14310	better may aid in the development of more effective tutoring systems. In this paper, we study the differences between student reactions to our Intelligent Tutoring System (ITS), CIRCSIMTutor (Michael et al., 2003), and the human tutors on which it was modeled. Our goal is to characterize student hedges and expressions of affect and try to determine how our ITS could understand them and respond effectively.
14293	14310	dialogue, from hour-long sessions (numbered K52-K76 in our corpus). Human/Computer Tutoring Sessions In November 2002, most of the first year class at Rush Medical College used CIRCSIM-Tutor (Michael et al., 2003) for one hour in a regularly scheduled laboratory session. Some students worked in pairs, some alone, so we wound up with only 66 transcripts (the H/C sessions), which we used as the basis for our
14293	14315	1984), since hedging can be a politeness or face-saving strategy, and not necessarily an expression of uncertainty. Of particular relevance are recent results on hedging in tutoring systems (Shah et al., 2002), which found that women hedge significantly more often than men when making initiatives in tutoring dialogues. If such differences are consistent, it should influence how tutoring systems interact
14293	14318	conclusion. Differences between how people respond to human beings and how they respond to computers have been informally documented since the first experiments with natural language interfaces (Thompson, 1980). A better elucidation of the issues may improve intelligent systems design. Specifically, understanding these issues better may aid in the development of more effective tutoring systems. In this
8884949	4160	that events execute in a causally consistent way. A simulation is causally consistent if each simulation object is accessed by events in nondecreasing time–stamp order. The time warp algorithm (Jefferson 1985) is an example of an optimistic algorithm for PDES. It is optimistic in the sense that each processor executes every event it knows about in time–stamp order under the optimistic assumption that
8884949	14323	world view. Maisie is a parallel simulation language that supports the active-server world view. Maisie improves upon GTW by making constructs for parallel execution more transparent to the user (Bagrodia 1991). The SIMKIT language is another system that supports an active-server world view (Gomes et al. 1995). To the best of our knowledge, ParaSol and APOSTLE are the only two PDES systems which support
8884949	14324	Maisie improves upon GTW by making constructs for parallel execution more transparent to the user (Bagrodia 1991). The SIMKIT language is another system that supports an active-server world view (Gomes et al. 1995). To the best of our knowledge, ParaSol and APOSTLE are the only two PDES systems which support the active-process world view. The APOSTLE system manages simulation process state as continuations
8884949	14325	with compile time transformation of simulation code. APOSTLE employs the semi-conservative breathing time-buckets algorithm to enforce the causality constraint on shared memory architectures (Booth and Bruce 1997). ParaSol implements simulation processes as user level threads. ParaSol uses the optimistic time warp synchronization algorithm on distributed memory architectures. 3 PARASOL BACKGROUND ParaSol is
8884949	14329	SAM must adjust the thread’s recorded stack pointer by an offset corresponding to the difference between the stack’s address at the source processor and the address at the destination processor (Mascarenhas and Rego 1996). Furthermore, ha may not 455 Proxy Object Migration Thread Processor 1 Processor 2 Figure 4: Thread Migration in ParaSol use pointers which refer to objects on its stack since SAM can not update
8920592	14333	will lead to time compact schemes. They are both one-step and one-stage schemes. 20 CHAPTER 3. TIME DISCRETIZATION METHODS This approach has been used by Numerov in  and  and by Tuomela in . A similar approach has been used by Lax and Wendroff in . A similar concept is the modified equations approach, see e.g. ,  and . The performance of the time compact scheme based on
3799907	9385	support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent proposal that addresses approximate query answering . Further, simply using two existing indices, one of each type, does
3799907	9385	may be considered as candidates for extension to also index past positions. Tayeb et al.  use PMRQuadtrees, Kollios et al.  employ the so-called dual data transformation, Agarwal et al.  use the ideas of so-called kinetic data structures , and Chon et al.  use a space-time grid. While each of these has its strong points, each one also exhibits limitations in relation to our
3799907	9385	involve two separate indices: one for all past times, and one for the present and future times. As described in Section 3.2, several proposals for the latter type of indexing exist already (e.g., ) and can be reused. For the former type of indexing, we may consider using some type of partially persistent data structure (PP-structure) or using some variant of the R-tree. The use of a
3799907	14349	support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent proposal that addresses approximate query answering . Further, simply using two existing indices, one of each type, does
3799907	14349	involve two separate indices: one for all past times, and one for the present and future times. As described in Section 3.2, several proposals for the latter type of indexing exist already (e.g., ) and can be reused. For the former type of indexing, we may consider using some type of partially persistent data structure (PP-structure) or using some variant of the R-tree. The use of a
3799907	10422	past positions. Tayeb et al.  use PMRQuadtrees, Kollios et al.  employ the so-called dual data transformation, Agarwal et al.  use the ideas of so-called kinetic data structures , and Chon et al.  use a space-time grid. While each of these has its strong points, each one also exhibits limitations in relation to our objective of obtaining a practical index that works for
3799907	14354	Tayeb et al.  use PMRQuadtrees, Kollios et al.  employ the so-called dual data transformation, Agarwal et al.  use the ideas of so-called kinetic data structures , and Chon et al.  use a space-time grid. While each of these has its strong points, each one also exhibits limitations in relation to our objective of obtaining a practical index that works for objects moving in
3799907	14355	representing positions as linear functions of time reduces the numbers of updates needed to maintain a reasonable precision by as much as a factor of three in comparison to using constant functions . Linear functions are thus much better than constant functions. The use of more complex approximations seems less appropriate for indexing purposes. The information needed to derive linear
3799907	14355	representing positions as linear functions of time reduces the numbers of updates needed to maintain a reasonable precision by as much as a factor of three in comparison to using constant functions . Linear functions are thus much better than constant functions. The use of more complex approximations seems less appropriate for indexing purposes. The information needed to derive linear
3799907	10211	position since the last sample is given by a linear function. Previous proposals for indexing moving objects either support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent
3799907	10211	-tree builds on this work in that it applies partial persistence techniques to an R-tree extension in order to solve a problem that is more challenging to partial persistence. Two other works  assume a static database of object evolutions and consider the partitioning of these evolutions into smaller time intervals, the goal being to reduce dead space when the data is indexed with the
3799907	14356	support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent proposal that addresses approximate query answering . Further, simply using two existing indices, one of each type, does
3799907	14356	each covering a part of the recent past. Updates apply to the most recent quadtree. As time passes, the old quadtree becomes empty, and a new one is created. The proposal by Jensen et al.  also partitions the recent past, but allows a number of so-called phases that are a multiple of two. Each phase has an associated B-tree. Object positions are represented as one-dimensional points
3799907	14356	involve two separate indices: one for all past times, and one for the present and future times. As described in Section 3.2, several proposals for the latter type of indexing exist already (e.g., ) and can be reused. For the former type of indexing, we may consider using some type of partially persistent data structure (PP-structure) or using some variant of the R-tree. The use of a
3799907	14356	presented in this paper could be applied to other indexing techniques that use linear functions to capture continuous change. For 25sexample, although the recently proposed STRIPES  and Bs-tree  techniques are very different from the TPR-tree based techniques, it may be possible to apply techniques similar to the ones presented in this paper to these two, in order to extend them to
3799907	10213	the current and predicted future positions of moving points exist that may be considered as candidates for extension to also index past positions. Tayeb et al.  use PMRQuadtrees, Kollios et al.  employ the so-called dual data transformation, Agarwal et al.  use the ideas of so-called kinetic data structures , and Chon et al.  use a space-time grid. While each of these has its
3799907	14357	position since the last sample is given by a linear function. Previous proposals for indexing moving objects either support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent
3799907	14357	i.e., a sequence of connected line segments. The R-tree is easily capable of indexing line segments, but there also seems to be consensus that the R-tree is not well suited for this problem (e.g., ). Pfoser et al.  suggest two variations of the R-tree for polyline indexing: TB-tree and STR-tree. Both attempt, to 4svarying degrees, to group together segments from the same polyline, the
3799907	14357	-tree builds on this work in that it applies partial persistence techniques to an R-tree extension in order to solve a problem that is more challenging to partial persistence. Two other works  assume a static database of object evolutions and consider the partitioning of these evolutions into smaller time intervals, the goal being to reduce dead space when the data is indexed with the
3799907	14361	support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent proposal that addresses approximate query answering . Further, simply using two existing indices, one of each type, does
3799907	14361	these. Both proposals assume that each linear function is updated within a specified, global maximum duration of time. They also assume a global maximum velocity for all objects. Patel et al.  index the points that result from the dual data transformation by means of two quadtrees, each covering a part of the recent past. Updates apply to the most recent quadtree. As time passes, the old
3799907	14361	involve two separate indices: one for all past times, and one for the present and future times. As described in Section 3.2, several proposals for the latter type of indexing exist already (e.g., ) and can be reused. For the former type of indexing, we may consider using some type of partially persistent data structure (PP-structure) or using some variant of the R-tree. The use of a
3799907	14361	framework presented in this paper could be applied to other indexing techniques that use linear functions to capture continuous change. For 25sexample, although the recently proposed STRIPES  and Bs-tree  techniques are very different from the TPR-tree based techniques, it may be possible to apply techniques similar to the ones presented in this paper to these two, in order to
3799907	14362	position since the last sample is given by a linear function. Previous proposals for indexing moving objects either support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent
3799907	14362	the goal being to answer queries that retrieve object evolutions consisting of multiple segments. They index positions for an object only up to the time of the most recent sample. Porkaew et al.  suggest to allow one or more line segments of a polyline to extend into the future. These segments then represent predictions. They also suggest to use a standard R-tree for indexing of the
3799907	14363	and MV3R-tree can index only the positions of an object up until the time of the most recent sample, so the proposals do not solve the more general problem considered here. Song and Roussopoulos  address the problem of tracking and recording positions of moving objects using hashing. The space is subdivided into zones and their approach works at the granularity of zones. Future queries are
3799907	14365	time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent proposal that addresses approximate query answering . Further, simply using two existing indices, one of each type, does not solve the general indexing problem: for any object, its position for times in-between the time of the most recent sample and
3799907	14365	has its strong points, each one also exhibits limitations in relation to our objective of obtaining a practical index that works for objects moving in one, two, and three dimensions. Sun et al.  propose a method for approximate query answering based on multidimensional histograms. In contrast to our proposal, no individual positions of objects are indexed. Instead, a histogram representing
3799907	14366	support only the past positions, up until the most recent position sample (e.g., ), or they support only the positions from the current time and into the future (e.g., ). No index combines these capabilities, with the exception of one recent proposal that addresses approximate query answering . Further, simply using two existing indices, one of each type, does
3799907	14366	Jensen  propose the Rs¢¡s-tree, which extends the TPR-tree to accommodate data with so-called expiration times associated that indicate when the data is no longer considered valid. Tao et al.  adopt assumptions about the query workload that differ slightly from those underlying the TPRtree. This leads to the use of a new measure when grouping objects into index tree nodes. Due to a
3799907	14366	involve two separate indices: one for all past times, and one for the present and future times. As described in Section 3.2, several proposals for the latter type of indexing exist already (e.g., ) and can be reused. For the former type of indexing, we may consider using some type of partially persistent data structure (PP-structure) or using some variant of the R-tree. The use of a
7917711	14370	of the new facilities of a temporal extended query language. We use the temporal query language ATSQL  in this paper. This language divides queries on temporal tables into three categories . Temporal upward compatible (TUC) ATSQL queries are “regular” SQL-92 2s queries on temporal tables; they simply consider only the current tuples and thus return a conventional snapshot table.
7917711	14372	the accurate result cannot be returned. Name Dept T-Start T-Stop Joe Shoe 1990-01-17 commit time of T Figure 1: In Transaction T , What is the Transaction-time Period of the Tuple? Previous works  on timestamping temporal data have either made the implicit assumption that transactions have no duration, or they have not considered transactions that modify and subsequently query the modified
7917711	14372	timestamp attributes, the commit times are retrieved from theTime table in the never approach; in the lazy approach, the timestamps are then applied to the timestamp attributes. Finger and McBrien  studied the use of NOW in the valid-time dimension, which corresponds to until changed for transaction time. They take into consideration that the actual execution of a transaction has a duration
7917711	5309	modifications in the transaction-time dimension. For this reason, we consider here TUC-modifications on the more general bitemporal database case, which supports both valid and transaction time . We only consider TUC-modifications, because we assume this type of modification is the most prevalent in a temporal database system. The modification routines are implemented as stored procedures
7917711	5245	the accurate result cannot be returned. Name Dept T-Start T-Stop Joe Shoe 1990-01-17 commit time of T Figure 1: In Transaction T , What is the Transaction-time Period of the Tuple? Previous works  on timestamping temporal data have either made the implicit assumption that transactions have no duration, or they have not considered transactions that modify and subsequently query the modified
7917711	5245	this paper. She did not discuss transactions containing modifications followed by queries which refer to timestamps. With respect to revisiting tuples for applying the correct timestamps, Postgres  uses the lazy or the never approach in its integrated architecture. The commit times of transactions are stored in a specialTime table. When a query uses the timestamp attributes, the commit times
7917711	5244	if timestamps are often referenced in queries and modifications. However, the approach is not cost-efficient if timestamps are rarely referenced. The “low-system-usage” approach is used in Postgres . However, the approach is not well-suited in a stratum because it requires scheduling of an asynchronous process based on the system load. It is hard to get this fine-level degree of control of the
7917711	5244	this paper. She did not discuss transactions containing modifications followed by queries which refer to timestamps. With respect to revisiting tuples for applying the correct timestamps, Postgres  uses the lazy or the never approach in its integrated architecture. The commit times of transactions are stored in a specialTime table. When a query uses the timestamp attributes, the commit times
431430	14385	whereas we derive refactorings from our algebraic laws, which, when compared with most refactorings, are simpler, separate concerns, and involve localised changes to the code. Bergstein  presents a small set of primitive transformations which forms a basis for object-preserving class reorganizations, meaning that programs accept the same inputs and produce the same outputs. The set
431430	14386	do not rely on copy semantics, except the simulation law for change of representation. To be valid with a reference semantics, such a law would have to consider pointer confinement issues as in , for example. A common criticism to the algebraic style is that merely postulating algebraic laws can give rise to complex and unexpected interactions between programming constructions. This can be
431430	14387	a compilation process; its sole purpose is to prove a completeness result and, therefore, suggest that our set of laws is expressive. A first version of our completeness proof is presented in ; here we present a generalisation in the handling of recursive methods. More importantly, we consider the soundness of our laws; this was not addressed in . Besides clarifying aspects of the
431430	14388	which should be provided by tools that allow the user to compose existing refactorings to define new ones . Similarly, executable languages for specifying new refactorings from scratch  should be able to express the laws presented here. A stronger completeness result, considering a normal form withouth objectoriented constructs at all, could even suggest a minimal set of
14413	14414	data. KEYWORDS: neuro-fuzzy system, membership function, core region, rule confidence, robustness, medical data INTRODUCTION An up to date neuro-fuzzy system is the so-called Fuzzy-RecBF-DDA , , that we recently used in an improved implementation for rule generation and classification in the medical domain  (analysis of septic shock patient data). The system is based on the
14413	14415	An up to date neuro-fuzzy system is the so-called Fuzzy-RecBF-DDA , , that we recently used in an improved implementation for rule generation and classification in the medical domain  (analysis of septic shock patient data). The system is based on the dynamic, geometric adaptation of trapezoids. Another wellknown system is NEFCLASS  that is based on a fuzzy backpropagation
14413	14415	the effect of membership unrobustness we calculated - in a similar way as in Example 2 - the rule confidence for asymmetric trapezoid membership functions obtained by training the system . As benchmark data we used datasets from the Proben archive . Here, we present the results using the dataset Diabetes1 with overlapping class regions. Approx. 25 % of the data could not be
14413	14415	creatinin and sodium are not relevant. Technical note: not all the values of the variables for different membership degrees need to differ (e.g. heart frequency). This is due to algorithm  where the trapezoid shape may degenerate to a rectangular shape for some dimensions in the positive and/or negative direction. Rule 1 ( ? = 0. 3 ): Rule 1* ( ? = 1. 0 ): if var heart frequency ?
8920606	14427	domain modeler. 10sSTARS-PA29-AC01/001/01 2.2.2 SEP Background Under funding by the DARPA STARS Program, ODM has been extensively documented in a guidebook  as well as in shorter papers  . The guidebook provides explanations of key concepts, a formal process model (documented in IDEF 0 process modeling notation   ), work product descriptions and templates, and detailed,
8917798	14473	work is still required to fill the gap existing between the conceptualization of the cybertherapy experience and the actual implementations. As conceptualized and well presented by Mantovani  and Riva ,sinterpersonal communication in virtual environments, though problematic and often erroneous, constitutes an efficient basis for interpersonal relationship. In Riva's  words,
8917798	14475	by Mantovani  and Riva ,sinterpersonal communication in virtual environments, though problematic and often erroneous, constitutes an efficient basis for interpersonal relationship. In Riva's  words, “Communication is as the outcome of a complex coordinated activity, an event that generates conversational space within the weave of personal and social relationships. Thus, communication is
8920629	14503	to offer a high-level interface similar to Matlab that has real-time performance (though with reduced functionality). Several researchers have developed libraries specifically for vision-based HCI. These libraries, however, tend to be application-specific or designed to investigate a particular software design. As such, these libraries are not suitable for distribution as a CV tool to a
8920629	14503	based on TLIBs8 Sébastien Grange, Terrence Fong, and Charles Baur 4.2 Medical Interfaces We recently developed a comp uter vision system to replace standard mouse functions with hand gestures . This application is relevant to the recent introduction of computerized tools in the operating room, since surgeons must have easy control over computers without compromising the sterility of the
8920629	14507	by our work in medical UI. A primary focus is to add tools and techniques for dynamic gesture recognition. Thus, we are now integrating a HMM-based method for gesture detection and classification . Another possible addition to TLIB is an implementation of the face detection methods proposed in . This functionality will complement the techniques based on color and disparity that are
8920629	14514	Thus, we are now integrating a HMM-based method for gesture detection and classification . Another possible addition to TLIB is an implementation of the face detection methods proposed in . This functionality will complement the techniques based on color and disparity that are already contained in TLIB. In addition, we plan to develop a variety of techniques to identify and
8920643	15313	of a large number of concurrent users accessing the different services. 2. MDS2 The Monitoring and Discovery Service (MDS2) is the Grid information service used in the Globus Toolkit. MDS2, built on top of the Lightweight Directory Access Protocol (LDAP) (v3), is used primarily to address the resource selection problem, namely, how a user identifies the host or set of
8920643	14533	at all the critical points in the code, then links the application with the NetLogger library. NetLogger is a lightweight tool and adds little overhead to existing program when used appropriately . By adding NetLogger calls we broke the end-to-end path of a MDS2 request into seven phases: (1) ClientConnect, (2) Client-Bind, (3) Server-InitSearch, (4) Server-SearchIndex, (5) Server-Invoking,
8920643	14535	to support the discovery and monitoring of the distributed resources for various tasks. Indepth studies are needed to understand any performance limitations in common settings. In our previous work, we investigated the behaviors of the Globus Toolkit Monitoring and Discovery Service (MDS2), the most common monitoring system currently used for production Grids, with the focus on
8920643	14535	CPU-load. Throughput is defined as the average number of requests (or queries) processed by a MDS2 service component per second. ORT, equivalent to the metric response time used in our previous work, denotes the average amount of time (in seconds) from the point a user sends out a request till the user gets the response back. It is calculated at the client side. RPT is defined as the average
14545	14546	¢ ©¤s? ¢ ¦ ¢s¤s? ? ©s¢ ¦ ¢ ¤ ¨ ¢ ¨ ?¨¦ © ? ??? ? ? ? ? Here, we derive upper bounds for the time and communication complexity of EPIC, and contrast it with the performance of BGP and Ghost Flushing. ? The abstract AS level (? ??? graph ) The set of AS nodes. ? The set of AS-AS edges. ? ? Size of the graph i.e. ????? ? Diameter of ? i.e. ????????? Length of a longest simple path in ? Hold
14545	14546	this section, we discuss the results of simulation results carried out with the SSFNet simulation package. In particular, we contrast the performance of our solution against BGP and GhostFlushing . We used different topology families for our simulations: Cliques, Waxman ¡ Random Graphs §¢¡ ¡ ¤s§??¨ ), and Barabasi-Albert (BA) Random Graphs. The latter two topology families (s? were generated
14545	14546	The drawback is that these rules are applicable only in specific settings, for example, when the paths from from different neighbors have a mutual dependency. Ghost Flushing, described in , is a simple idea to reduce convergence following a failure. The underlying idea is to aggressively send withdrawals, forcing the invalid routes to be flushed from the network. While this idea is
14545	15328	is the set of candidate routes (for that destination). Subsequently, the BGP router invokes a route selection process— guided by local policies— to select a single “best” route from this set . After this, the selected best route is subjected to some export policies and then announced to all the router’s neighbors. Importantly, prior to being announced to an external neighbor (in a
14545	14549	increases the load on routers, which are forced to process updates for transient routes. In severe cases, this additional load can cause routers to “tip over”, leading to cascaded network failures . Third, normal path exploration may be incorrectly identified as instability (i.e., flapping routes), triggering damping mechanisms at routers . Lastly, it complicates the task of
14545	7178	prolongs the protocol convergence after a network failure or repair event. Labovitz et.al. showed that in the worst case, as many as ¡£¢¥¤§¦©¨ alternate paths may be explored after a failure . However, in practice, such a worst-case scenario is rare in todays Internet: common routing policies, which reduce the number of available routes, and protocol timers, that limit how fast updates
14545	7178	at node ? , with length ??? ???? ? ? ? ? ? ??????? ???? Table 3: Notation used in the analysis. In order to make the analysis tractable, we use the discrete-time synchronized model described in : in each time step, a node processes all of the messages received in the last stage and then selects a single “best path”. Then, if the best path has changed, it is exported to all its neighbors.
14545	2711490	However, BGP may still take relatively long time to converge following a network failure. Experimental studies show that in practice, BGP can take up to fifteen minutes to converge after a failure . The root cause of this slow convergence is the dependency among paths announced through the network, which leads to path exploration: when a previously announced path is withdrawn, other paths
14545	14552	over”, leading to cascaded network failures . Third, normal path exploration may be incorrectly identified as instability (i.e., flapping routes), triggering damping mechanisms at routers . Lastly, it complicates the task of identifying the root-causes of routing updates, essential in understanding inter-domain routing dynamics . In this paper, we propose a simple and novel
14560	2281	On the University of Washington campus network in June 2002, KaZaA consumed approximately 37% of all TCP traffic, which was more than twice the Web traffic on the same campus at the same time . But file sharing is not only having an important impact on Internet usage and traffic; it is also profoundly impacting sales in the music and video recording industries. For example, in a recent
14560	2281	analyzes P2P traffic by measuring flowlevel information collected at multiple border routers across a large ISP-network. By measuring KaZaA trafficsin the University of Washington campus,  studies filesharing workloads and develops models for multimedia workload. A crawling system was previously developed for the Gnutella P2P network . Developing a crawling system for KaZaA is
14560	15345	due to illicit sharing of copyrighted songs in P2P file sharing systems . Each week there are more than one billion downloads of music files, and over 60 million Americans have downloaded music   . Because of the potential of huge financial losses, the music industry has attempted to throttle P2P file sharing activity on three distinct fronts. First, it has Yongjian Xi Department of
14560	15346	be popular, the record company might pay a pollution company to spread bogus copies of the song through one or more P2P networks. The approach is described in Overpeer’s US patent applications . A similar approach is described in a recent patent application from the University of Tulsa . The patentsdescribes cooperative scanning, manufacturing, sharing and supervisory control software
14560	15347	through one or more P2P networks. The approach is described in Overpeer’s US patent applications . A similar approach is described in a recent patent application from the University of Tulsa . The patentsdescribes cooperative scanning, manufacturing, sharing and supervisory control software to share decoy (that is, polluted) media at a volume that renders media search ineffectual .
14560	15347	users use applications called “rippers” to extract audio media from compact discs and store extracted audio content on hard drives, where they can be transformed by an “encoder” into the MP3 format . Such “encoder” and “ripper” processes have recently been bundled into “1-Step” software, making duplication and distribution of mpeg audio files even easier . Typically, the software
14560	2285	Furthermore, it is scalable in that the crawling time is inversely proportional to the number of Linux boxes in the platform. A crawling system was previously developed for the Gnutella P2P network . Developing a crawling system for KaZaA is significantly more challenging for two reasons. First, KaZaA is 10-100 times larger than Gnutella, both in terms of the number of peers and traffic.
14560	2285	the University of Washington campus,  studies filesharing workloads and develops models for multimedia workload. A crawling system was previously developed for the Gnutella P2P network . Developing a crawling system for KaZaA is significantly more challenging for two reasons. First, KaZaA is 10-100 times larger than Gnutella, both in terms of the number of peers and traffic.
14560	15349	downloaded with the fingerprints in the database. If a match is not found, the client peer concludes that the file is polluted and deletes all file portions it has downloaded. The Sig2dat project  makes available a tool for obtaining the KaZaA ContentHash of any file. This tool is increasingly being used by KaZaA users, who post file names and corresponding ContentHash values on Web sites
14560	14564	to download from the friend of friend and notifies his direct friend of the problem. The idea is similar to the trust mechanism used in PGP . • Reputation systems: Reputation systems such as ,  allow peers to rank each other. These reputation systems can potentially be used to reduce pollution. The reputation system would identify malicious peers that have been responsible for
14560	14565	download from the friend of friend and notifies his direct friend of the problem. The idea is similar to the trust mechanism used in PGP . • Reputation systems: Reputation systems such as ,  allow peers to rank each other. These reputation systems can potentially be used to reduce pollution. The reputation system would identify malicious peers that have been responsible for injecting
14560	14567	campus network) and then processed off-line.  talks about P2P application specific signatures; these signature techniques could be deployed by an ISP to identify and filter illicit P2P traffic.  analyzes P2P traffic by measuring flowlevel information collected at multiple border routers across a large ISP-network. By measuring KaZaA trafficsin the University of Washington campus,
14560	188	importantly, the Gnutella protocol is in the public domain, whereas the KaZaA protocol is proprietary with little information available to the research community about how it operates. See also  for some additional work on crawling Gnutella and Napster. There has been some recent measurement work on spread of spyware in networked systems. In  the authors develop signatures for popular
14560	14568	community about how it operates. See also  for some additional work on crawling Gnutella and Napster. There has been some recent measurement work on spread of spyware in networked systems. In  the authors develop signatures for popular spyware and obtain traces of network activity within the University of Washington campus to quantify the spreading of these programs. VIII. CONCLUSION We
8920647	15383	demands heard. Furthermore, sites closer to market are likely to be more commercially oriented, and therefore have higher payoffs to effective irrigation. However, Fujita, Hayami, and Kikuchi (1999) point out that in rural communities with little exposure to urban market activities, members expect to continue their interaction indefinitely, and hence have incentives to cooperate. Access to
8920647	15386	organization. Thus, not having an organization does not seem to constrain farmers from coming together to make their demands heard as the need arises (Chambers 1988). For IGNP, Ramanathan and Ghose (1994) explain that without local decision-making forums, people move immediately from &quot;atomized relationships&quot; of individual farmers with the bureaucracy, into mass political movements. Because these
8920647	15387	organization. Thus, not having an organization does not seem to constrain farmers from coming together to make their demands heard as the need arises (Chambers 1988). For IGNP, Ramanathan and Ghose (1994) explain that without local decision-making forums, people move immediately from &quot;atomized relationships&quot; of individual farmers with the bureaucracy, into mass political movements. Because these
8920647	15391	without any organization, forming registered water users' associations may strengthen the effectiveness of lobbying efforts, especially for demanding a greater voice in decision11 Blomqvist (1998) refers to activities such as collective lobbying for more water as external solutions, in contrast to mobilizing resources for maintenance, which is an internal solution. See also Chambers
8920647	15392	without any organization, forming registered water users' associations may strengthen the effectiveness of lobbying efforts, especially for demanding a greater voice in decision11 Blomqvist (1998) refers to activities such as collective lobbying for more water as external solutions, in contrast to mobilizing resources for maintenance, which is an internal solution. See also Chambers
8920647	15393	demands heard. Furthermore, sites closer to market are likely to be more commercially oriented, and therefore have higher payoffs to effective irrigation. However, Fujita, Hayami, and Kikuchi (1999) point out that in rural communities with little exposure to urban market activities, members expect to continue their interaction indefinitely, and hence have incentives to cooperate. Access to
8920647	15394	demands heard. Furthermore, sites closer to market are likely to be more commercially oriented, and therefore have higher payoffs to effective irrigation. However, Fujita, Hayami, and Kikuchi (1999) point out that in rural communities with little exposure to urban market activities, members expect to continue their interaction indefinitely, and hence have incentives to cooperate. Access to
8920647	15395	demands heard. Furthermore, sites closer to market are likely to be more commercially oriented, and therefore have higher payoffs to effective irrigation. However, Fujita, Hayami, and Kikuchi (1999) point out that in rural communities with little exposure to urban market activities, members expect to continue their interaction indefinitely, and hence have incentives to cooperate. Access to
8920647	15396	demands heard. Furthermore, sites closer to market are likely to be more commercially oriented, and therefore have higher payoffs to effective irrigation. However, Fujita, Hayami, and Kikuchi (1999) point out that in rural communities with little exposure to urban market activities, members expect to continue their interaction indefinitely, and hence have incentives to cooperate. Access to
8920664	14597	of Joensuu P. O. Box 111 FIN-80101 Joensuu Finland franti@cs.joensuu.fi conventional K-Means algorithm converges to a locally optimal solution. The extended versions of K-Means such as K-Median , adaptive K-Means  and kernel KMeans  were recently developed to overcome this local optimality problematic. The K-Median algorithm searches each cluster centroid from data samples such that
8920664	14601	section, the proposed approach is compared to the other two K-Means clustering algorithms: the PCAbased suboptimal K-Means algorithm  and the kd-tree based K-Means clustering algorithm . Finally, conclusions are drawn in section 6. Function SubOptimalKMeans(X, k, m) input: Dataset X Number of clusters k Number of iterations m output: Class labels P OPT C ? Randomly choose cluster
14605	14607	under study. So far a lot of retrieval methods have been proposed . Their indexing schemes range from feature-level indexing for layout structure  to word-level indexing for OCR’d text . In this paper, we focus our discussion on the word-level indexing. With this indexing scheme, the central issue has been how to deal with OCR errors. This is because, once the problem of OCR
14605	14608	under study. So far a lot of retrieval methods have been proposed . Their indexing schemes range from feature-level indexing for layout structure  to word-level indexing for OCR’d text . In this paper, we focus our discussion on the word-level indexing. With this indexing scheme, the central issue has been how to deal with OCR errors. This is because, once the problem of OCR
8920668	14639	computing platforms has prompted processor designers to add multimedia instructions to microprocessor instruction set architectures (ISAs). These include MAX-2 for the PA-RISC architecture , MMX, SSE and SSE-2 for the Intel IA-32 architecture , and a superset of these to the Itanium IA-64 architecture . Although these multimedia instructions may be very effective, they still
8920668	14640	, and a superset of these to the Itanium IA-64 architecture . Although these multimedia instructions may be very effective, they still incur the overhead of their base microprocessor ISA. PLX  is a new ISA designed from scratch for fast and efficient multimedia processing. Prior work has demonstrated its effectiveness for integer media applications . This paper describes the new
8920668	14640	PLX FP ISA PLX FP introduces 32 new FP registers F0-F31, with register F0 hardwired to the value 0.0. Like the PLX integer ISA, the PLX FP ISA is also datapath scalable and fully subword-parallel . Datapath scalability means that the PLX floating-point registers and functional units cansbe 32, 64 or 128 bits in a given PLX implementation. The recommended datapath width is 128 bits,
8920668	14644	is very low because the processors are not optimized for 3D graphics processing. The addition of 3D graphics ISA extensions, such as SSE and SSE-2 , IA64 , AMD 3DNow! , AltiVec in PowerPC , and recently, ARM VFP , partially alleviates the problem. They boost the speed of certain 3D graphics operations significantly, but the performance still falls short of that demanded for
8920671	14655	of differential detection and convolutional decoding. The TDD algorithm, that performs this joint optimization, is similar to the iterative decoding algorithm for serial concatenated codes , where the place of the inner decoder is taken by SOML-DD. For DAPSK modulation this SOML-DD issPSfrag replacements Received Symbols Incoherent Demod. Incoherent Demod. ?(??k,??k) ?(?2?k,?2?k) ? ?
8920673	14668	modeling and the injection of realistic faults . This approach, in combination with fault simulation approaches for analog and mixedsignal electronic parts  and for digital parts using VHDL , allows for the evaluation of test strategies via fault simulation, the calculation of fault coverage figures and the determination of suitable test patterns. Sensor qualification can then be fully
14670	9700	the returned list may be higher if the safe parameter is not true because, for instance, malicious nodes have caused the local node to build a routing table that is biased towards malicious nodes . nodehandle  neighborSet (int ?num) This operation produces an unordered list of nodehandles that are neighbors of the local node in the ID space. Up to num node handles are returned. nodehandle
14670	9700	among the members of Pastry’s leafset. local lookup translates into a simple lookup of Pastry’s routing table; if safe is true, the lookup is performed in Pastry’s constrained routing table . The update operation is triggered by a change in Pastry’s leafset, and the neighbor set (returned by neighborSet) consists of the leafset. Tapestry The route operation is identical to the Tapestry
14670	8977	Internet Indirection Infrastructure , and other mechanisms and protocols such as caching and replication.sDHT. A distributed hash table (DHT) provides two operations: (1) put(key, value), and (2) value = get(key). A simple implementation of put routes a PUT message containing value and the local node’s nodehandle, S, using route(key, , NULL). The key’s root, upon receiving the
14670	8978	O(lg N) messages where N is the number of nodes in the overlay. Structured overlays can be used to construct services such as distributed hash tables , scalable group multicast/anycast , and decentralized object location . These services in turn promise to support novel classes of highly scalable, resilient, distributed applications, including cooperative archival storage,
14670	14671	allows them to locate objects by exchanging O(lg N) messages where N is the number of nodes in the overlay. Structured overlays can be used to construct services such as distributed hash tables , scalable group multicast/anycast , and decentralized object location . These services in turn promise to support novel classes of highly scalable, resilient, distributed applications,
14670	14671	is always stored at the live overlay node(s) to which the key is mapped by the KBR layer. Values can be objects of any type. For example, the DHT implemented as part of the DHash interface in CFS  stores and retrieves single disk blocks by their content-hashed keys. The DOLR abstraction provides a decentralized directory service. Each object replica (or endpoint) has an objectID and may be
14670	14672	of nodes in the overlay. Structured overlays can be used to construct services such as distributed hash tables , scalable group multicast/anycast , and decentralized object location . These services in turn promise to support novel classes of highly scalable, resilient, distributed applications, including cooperative archival storage, cooperative content distribution and
14670	14672	nodes are assigned uniform random nodeIds from a large identifier space. Application-specific objects are assigned unique identifiers called keys, selected from the same id space. Tapestry , Pastry  and Chord  use a circular identifier space of n-bit integers modulo 2 n (n = 160 for Chord and Tapestry, n = 128 for Pastry). CAN  uses a d-dimensional cartesian identifier
14670	14672	be surrogate routed to. The update operation is trigged when a node receives an acknowledged multicast for a new inserting node, or when it receives an object movement request during node deletion . 6 Discussion and future work Settling on a particular key-based routing API were complicated by the tight coupling between applications and the lookup systems on which they were developed. Current
14670	939	selected from the same id space. Tapestry , Pastry  and Chord  use a circular identifier space of n-bit integers modulo 2 n (n = 160 for Chord and Tapestry, n = 128 for Pastry). CAN  uses a d-dimensional cartesian identifier space, with 128-bit nodeIds that define a point in the space. Each key is dynamically mapped by the overlay to a unique live node, called the key’s root.
14670	939	The call returns nodes with a rank up to and including max rank. If max rank exceeds the implementation’s maximum replica set size, then its maximum replica set is returned. Some protocols (, ) only support a max rank value of one. With protocols that support a rank value greater than one, the returned nodes may be used for replicating data since they are precisely the nodes which become
14670	940	assigned uniform random nodeIds from a large identifier space. Application-specific objects are assigned unique identifiers called keys, selected from the same id space. Tapestry , Pastry  and Chord  use a circular identifier space of n-bit integers modulo 2 n (n = 160 for Chord and Tapestry, n = 128 for Pastry). CAN  uses a d-dimensional cartesian identifier space, with
14670	8995	we have higher level abstractions provided by some of the existing systems. Most applications and higher-level (tier 2) services use one or more of these abstractions. Some tier 2 systems, like i3 , use the KBR directly. The KBR API at tier 0 will be defined in detail in the following section. Here, we briefly explain the tier 1 abstractions and their semantic differences. The key operations
14670	8995	we briefly sketch how tier 1 abstractions (DHT, DOLR, CAST) can be implemented on top of the routing API. We also show how to implement a tier 2 application, Internet Indirection Infrastructure , and other mechanisms and protocols such as caching and replication.sDHT. A distributed hash table (DHT) provides two operations: (1) put(key, value), and (2) value = get(key). A simple
14670	8995	msg], NULL). Internet Indirection Infrastructure (i3). i3 is a communication infrastructure that provides indirection, that is, it decouples the act of sending a packet from the act of receiving it . This allows i3 to provide support for mobility, multicast, anycast and service composition. There are two basic operations in i3: sources send packets to a logical identifier and receivers express
14670	943	random nodeIds from a large identifier space. Application-specific objects are assigned unique identifiers called keys, selected from the same id space. Tapestry , Pastry  and Chord  use a circular identifier space of n-bit integers modulo 2 n (n = 160 for Chord and Tapestry, n = 128 for Pastry). CAN  uses a d-dimensional cartesian identifier space, with 128-bit nodeIds that
14670	5903	nodes are assigned uniform random nodeIds from a large identifier space. Application-specific objects are assigned unique identifiers called keys, selected from the same id space. Tapestry , Pastry  and Chord  use a circular identifier space of n-bit integers modulo 2 n (n = 160 for Chord and Tapestry, n = 128 for Pastry). CAN  uses a d-dimensional cartesian identifier
14670	5903	stored. The call returns nodes with a rank up to and including max rank. If max rank exceeds the implementation’s maximum replica set size, then its maximum replica set is returned. Some protocols (, ) only support a max rank value of one. With protocols that support a rank value greater than one, the returned nodes may be used for replicating data since they are precisely the nodes which
14670	9002	O(lg N) messages where N is the number of nodes in the overlay. Structured overlays can be used to construct services such as distributed hash tables , scalable group multicast/anycast , and decentralized object location . These services in turn promise to support novel classes of highly scalable, resilient, distributed applications, including cooperative archival storage,
14674	4053	causes the processed images to be blurred. If this motion could be measured, the blurring could be removed by postprocessing the image. Such a positioning system has been designed and simulated  and is currently under construction. It will operate in conjunction with the KiwiSAS IV towfish (also currently under construction) to measure its sway. The technical details of its design and
14674	14676	removes the largest deviations and can be expanded to remove the remaining errors in the future. The KiwiSAS III has an Inertial Navigation System (INS) installed to measure angular towfish motion . This provides data which is especially useful for bathymetric work but the main cause of SAS image blur, sway motion , cannot be measured accurately using this system. Even very small sway
14689	14690	from the sources given the maximally contained rewritings. Certainly we may obtain tuples by simply executing the rewritings over the sources. However the notion of certain answers, introduced in , singles out a class of tuples that are of particular interest. Informally a tuple is a certain answer if it is an answer in all possible database instances (under the global schema) consistent
14689	14690	video stores in Ume?a that are not providing inventory information. In this case A is the certain answer set and B is termed the uncertain answer set. The certain answer set, in correspondence with , is the set of tuples that are answers to the query no matter what additional data sources are added to the system. The uncertain answer set is the set of tuples that could be rendered in or out of
14689	14690	queries including general negation over sources described with conjunctive views. The problem of identifying certain answers over data integration systems has some rather negative complexity results. Unfortunately the complexity measures are in data complexity (e.g. number of provided tuples), rather than query complexity. The only class that gives polynomial complexity is that of conjunctive
14689	14694	quite complex. Still since the complexity is worst case and is in query length, we anticipate that the approach will scale in real world environments. It should be noted that the description logics community has investigated ‘query’ containment under the name of concept subsumption. Description logics use unary and binary predicates which formalize traditional semantic network role/filler
14689	14695	place predicates. This has limited their impact in database environments. The re– 19 – cent introduction of DLR based description logics which are based on a unary concept and n-ary relationships may hold some promise. Still, as an example, it is difficult to envision current description logics representing concepts such as “the movies whose director plays an acting role.” Most work that
14689	14696	(the select-projectjoin queries of the relational algebra) as a special case where query equivalence is decidable. Subsequent work specified a first order query hierarchy over query languages. Roughly speaking, L is contained within the class of conjunctive queries closed over complementation and L pos is contained within the class of conjunctive queries closed over composition. Clearly
14689	14699	language in which user queries and data source descriptions may be expressed. The type of queries (and views) we consider are the schema tuple queries 2 . Such queries are tuple relational queries that return only whole tuples from relations of the schema, not arbitrary combinations of projected attributes. Thus, to obtain movies of the year 1999, we would writesMovie¢ m£¥¤ m¦ year § 1999¨
14689	14699	from  and its sister collection  which chronicles interesting undecidable classes of first order formula.s– 18 – As expected, the general problem of equivalence between relational algebra expressions was shown to be undecidable. Other work singled out conjunctive queries (the select-projectjoin queries of the relational algebra) as a special case where query equivalence is
14689	14701	complex. Still since the complexity is worst case and is in query length, we anticipate that the approach will scale in real world environments. It should be noted that the description logics community has investigated ‘query’ containment under the name of concept subsumption. Description logics use unary and binary predicates which formalize traditional semantic network role/filler
14689	14701	? B¢ x£ as . Thus the focus here is on the ‘decidable’ description logics which have the (¢ conjunction ) and complement (? operator ). These are the description logics of the type ALC and beyond which have sound and complete subsumtion procedures. A serious limitation of description logics however, is their restriction to one and two place predicates. This has limited their impact in
14689	14702	case that the the number of conditions within a query is bounded, rewriting algorithms are still able to scale up to large numbers of sources. The bucket algorithm, the inverse-rules approach and recently the mini-con algorithm are all efforts toward scalable rewriting algorithms for conjunctive queries over conjunctive views. The work here considers user queries including general
14689	14703	considerations, however, are beyond the scope of this current work and shall be the subject of future work. 5 Discussion 5.1 Related Work This work extends  which assumed a Universal Relation for the global schema and an extended form of relational algebra for the query language. The main improvement here is to lift these results to arbitrary relational schemas and to better specify the
14689	14704	when restricted to the schema tuple queries specified using L and Q . Of particular interest are problems in updates over views, semantic query optimization and cooperative query answering. 7 Acknowledgments I thank Steve Hegner for his valuable comments on an earlier draft of this report. I thank also the Department of Computing Science, at the University of Ume?a for providing a
14689	14705	database relations (known as global-as-view). Under local-as-view, data sources may dynamically join the data integration system without necessitating a reorganization of the global schema. See  for further discussion. The global schemas we shall consider are relational and are constrained under sets of functional dependencies. We also assume that the data sources provide correct and
14689	14705	just the ‘views’ defining the sources. This relates to the general problem of answering queries using views. In the context of data integration systems, the following steps are normally carried out:s– 2 ??? 1.) Obtain a set of maximally contained rewritings of the user’s query over the ‘views’ defining the sources. 2.) Execute query rewritings to obtain a maximal set of answers from the
14689	14705	particular interest. Informally a tuple is a certain answer if it is an answer in all possible database instances (under the global schema) consistent with the contents of the given sources. See  for the formal definition of certain answers. As we shall see, the approach within this report eases the task of identifying certain answers considerably. 1.2 Broadening the System Response Answers
14689	7747	However since it is often the case that the the number of conditions within a query is bounded, rewriting algorithms are still able to scale up to large numbers of sources. The bucket algorithm, the inverse-rules approach and recently the mini-con algorithm are all efforts toward scalable rewriting algorithms for conjunctive queries over conjunctive views. The work here considers
14689	7750	within a query is bounded, rewriting algorithms are still able to scale up to large numbers of sources. The bucket algorithm, the inverse-rules approach and recently the mini-con algorithm are all efforts toward scalable rewriting algorithms for conjunctive queries over conjunctive views. The work here considers user queries including general negation over sources described with
14689	14714	database relations (known as global-as-view). Under local-as-view, data sources may dynamically join the data integration system without necessitating a reorganization of the global schema. See  for further discussion. The global schemas we shall consider are relational and are constrained under sets of functional dependencies. We also assume that the data sources provide correct and
8920677	23612	been identified as: limited trader investment, thin markets, weak transport and other infrastructure, lack of public market information, high transaction costs, and high inland transport costs (Fafchamps and Minten, 1999, Gabre-Madhin, 1998; Staatz et al, 1989; Coulter and Golob, 1992).s52 6. DEMOGRAPHIC TRANSITION The latter half of this century has been marked by historically unprecedented rates of population
14743	15576	had been urged by everyone from parents to practitioners to policy-makers. In response, a compelling agenda for change had been articulated by the National Council of Teachers of Mathematics (1989, 1991). For the team at RDI, these new `Standardsa, which called upon teachers to encourage mathematical `inquirya among their students, seemed to represent a radical departure from the pedagogies most
1017411	14767	by multiple applications. Maintaining healthy configurations of a computer platform with a large installed base and numerous third-party software packages has been recognized as a daunting task . The considerable number of possible configurations and the difficulty in specifying the “golden state” , the perfect configuration, have made the problem appear to be intractable. In this
1017411	15583	Automatic Troubleshooting, Golden State, Statistics, Bayesian Estimates, PeerPressure 1. INTRODUCTION Technical support contributes 17% of the total cost of ownership of today’s desktop PCs . An important element of technical support is troubleshooting misconfigured applications. Misconfiguration troubleshooting is particularly challenging, because configuration information can be
1017411	14768	base and numerous third-party software packages has been recognized as a daunting task . The considerable number of possible configurations and the difficulty in specifying the “golden state” , the perfect configuration, have made the problem appear to be intractable. In this paper, we address the problem of misconfiguration troubleshooting. There are two essential goals in designing
1017411	14771	minimize the number of manual steps and the number of users involved. To diagnose misconfigurations of an application on a sick machine, it is natural to find a healthy machine to compare against . Then, the configurations that differ between the healthy and the sick are misconfiguration suspects. However, it is difficult to identify a Copyright is held by the author/owner.
8920698	15647	in the field of policy-based networking (PBN) is being reflected in the form of new working groups, conferences and commercial products supporting PBN. The IETF resource allocation protocol (RAP)  working group is active in the field of QoS policy. It has defined, among other standards, the PAC framework  and the common open policy service (COPS) protocol , a simple clientserver
8920700	1177	Jennings & Wooldridge, 1998). We illustrate the traditional bottom-up process of integrating such heterogeneous information systems, by means of schema integration in federated database systems (Sheth & Larson, 1990). A federated database system is an integration of autonomous database systems, where both local applications and global applications accessing multiple database systems are supported. For
14848	14852	dialogues. 1 Introduction Spoken dialogue systems are emerging as an intuitive interface for providing conversational access to online information sources (Eckert et al., 1997; Gorin et al., 1997; Dahlback et al., 1999; Zue et al., 2000; Walker et al., 2001; Glass and Seneff, 2003; Pieraccini et al., 1997; Quast et al., 2003; J. Gustafson, 1999; Polifroni and Chung, 2002; Denecke, 2002; Seneff, 2002; Zue and
14848	14853	pragmatic issues to consider in obtaining spelled data from a user whether via keypad or speech. The problem of disambiguating keypad sequences has been addressed using both dictionary-based (Davis, 1991) as well as probabilistic (MacKenzie et al., 2001) approaches. In both input modes, the user may use abbreviations such as “S T P E T E R S B U R G” for “Saint Petersburg”. Spoken spelling is
14848	14854	but it remains to be tested in real user dialogues. 1 Introduction Spoken dialogue systems are emerging as an intuitive interface for providing conversational access to online information sources (Eckert et al., 1997; Gorin et al., 1997; Dahlback et al., 1999; Zue et al., 2000; Walker et al., 2001; Glass and Seneff, 2003; Pieraccini et al., 1997; Quast et al., 2003; J. Gustafson, 1999; Polifroni and Chung,
14848	14856	which treats the spoken word as an unknown word and proposes an N-best list of hypothesized spellings for the synthesized letter sequence. For speech recognition, we use the SUMMIT framework (Glass et al., 1996), and the unknown word is modeled according to techniques described in (Bazzi and Glass, 2002). Following the first stage recognition, a two-stage matching process first consults a list of “cities
14848	14859	spelled data from a user whether via keypad or speech. The problem of disambiguating keypad sequences has been addressed using both dictionary-based (Davis, 1991) as well as probabilistic (MacKenzie et al., 2001) approaches. In both input modes, the user may use abbreviations such as “S T P E T E R S B U R G” for “Saint Petersburg”. Spoken spelling is especially difficult, because the recognition accuracy
14848	14860	for spoken letters can be quite low. For instance, the members of the “E-set” (B, C, D, E, G, P, T, V, Z) are well-known for being confusable to a recognizer, as discussed in previous studies (Marx and Schmandt, 1994). This problem is compounded by the fact that humans spell words in creative ways. Some may spell in military style (e.g., “Alpha Bravo Charlie” for “A B C”) or in simile (e.g., “B as in ‘Boy”’).
14848	14861	for providing conversational access to online information sources (Eckert et al., 1997; Gorin et al., 1997; Dahlback et al., 1999; Zue et al., 2000; Walker et al., 2001; Glass and Seneff, 2003; Pieraccini et al., 1997; Quast et al., 2003; J. Gustafson, 1999; Polifroni and Chung, 2002; Denecke, 2002; Seneff, 2002; Zue and Glass, 2000). While the effectiveness of such systems ? This research was supported by an
14848	14862	(Eckert et al., 1997; Gorin et al., 1997; Dahlback et al., 1999; Zue et al., 2000; Walker et al., 2001; Glass and Seneff, 2003; Pieraccini et al., 1997; Quast et al., 2003; J. Gustafson, 1999; Polifroni and Chung, 2002; Denecke, 2002; Seneff, 2002; Zue and Glass, 2000). While the effectiveness of such systems ? This research was supported by an industrial consortium supporting the MIT Oxygen Alliance. has
14848	14864	about the word, which can facilitate his understanding of the word. Several dialogue systems have employed such a strategy in dealing with a confusing or unknown word (Bauer and Junkawitsch, 1999; Schramm et al., 2000). We aim to employ such a speak-and-spell strategy in our system. In this work, we focus on the class of place names, specifically cities, states, and airports. Such names are prevalent and
14848	14864	city, there is hope that the system will find the user’s intended city name. Such an approach of access to a much larger vocabulary in spell mode has been successfully applied, for example, in (Schramm et al., 2000). In the most extreme case, the large external database would be the World Wide Web. One could imagine the system posing a search query on the Web to determine that the closest major city is Green
14848	14864	Some may simply enter a letter sequence containing several meaningful chunks, as in “N E W Y O R K J F K N Y C” for Kennedy Airport in New York City. Many of these issues have been addressed in (Schramm et al., 2000).s3 MERCURY Error Recovery Strategy The MERCURY system, accessible via a toll free number 2 , provides information about flights available for over 500 cities worldwide. We have invested
14848	14866	A letter-to-sound model is used to map from a graph of letter hypotheses proposed by the first stage recognizer to their corresponding plausible pronunciations, using techniques described in (Seneff et al., 1996). The final set of hypotheses is obtained by merging hypotheses produced from both halves of the user utterance. Once again, both the short list and the large lexicon are searched for a match. The
14848	14867	to utilize a pronounced version of a word to greatly enhance the accuracy of a letter recognition task, and have successfully integrated this technology into a personal name enrollment task (Seneff et al., 2003; Chung et al., 2003). Our interest here was in evaluating whether a similar technique would be useful for the error recovery problem. It is difficult, however, to develop and perfect an algorithm
14848	14869	et al., 1999; Zue et al., 2000; Walker et al., 2001; Glass and Seneff, 2003; Pieraccini et al., 1997; Quast et al., 2003; J. Gustafson, 1999; Polifroni and Chung, 2002; Denecke, 2002; Seneff, 2002; Zue and Glass, 2000). While the effectiveness of such systems ? This research was supported by an industrial consortium supporting the MIT Oxygen Alliance. has improved significantly over the past several years, a
14870	14872	and voluntary speech productions have been investigated. In this very preliminary report duration variations for two speakers are discussed. 1 Introduction The aim of the VeriVox project (Karlsson et al. 2000) was to improve the reliability of automatic speaker verification (ASV) by developing novel, phonetically informed methods for coping with the variation in a speaker’s voice. To get a speech
14870	14872	(Karlsson et al 1998, Karlsson 1999) that the elicited involuntary speech variations actually cover some of these normal variations. It was shown in the ASV experiments performed within VeriVox (Karlsson et al 2000) that including the voluntary speech variations in the enrolment set enhanced the performance on the involuntary speech variations. One question that remains to be answered is if all voluntary
14870	14872	in a pilot study. Material from only two of these 50 speakers will be discussed in this paper. Further discussions of the speech database can be found in earlier publications (Karlsson et al. 1998, Karlsson et al. 2000). 3 Measurements The segment durations are measured automatically using in-house alignment software (Sjölander 2001) and the segment boundaries are then corrected by hand. Data for all 50 speakers
14874	2249	Only Approved for External Publication Published in Eurospeech 2003, 1-4 September 2003, Geneva, Switzerland © Copyright IEEEs1 Introduction During the last years Support Vector Machines (SVM’s)  have become extremely successful discriminative approaches to pattern classification and regression problems. Excellent results have been reported in applying SVM’s in multiple classification and
14874	2249	the paper and suggestions for future work in section 5. 2 Kernels for SVM’s Support Vector Machines were first introduced by Vapnik and evolved from the theory of Structural Risk Minimization . SVM’s learn the boundary regions between samples belonging to two classes by mapping the input samples into a high dimensional space and seeking a separating hyperplane in this space. The
14874	2249	. , l are the training data. Each point of xi belongs to one of the two classes identified by the label yi ? {?1, 1}. The coefficients ?i and b are the solutions of a quadratic programming problem . ?i are non-zero for support vectors (SV) and are zero otherwise. K is the kernel function. Classification of a test data point x is performed by computing the right-hand side of Eq. (1). Much of
14874	14875	SVM’s in multiple classification and regression benchmarks. In the general area of speech and speaker recognition SVM’s have also been studied over the last years. For example, among others  compares the use of traditional based kernel SVM’s with Gaussian classifiers,  examines the use of SVM’s for phonetic classification, and  studies the use of SVM’s to classify telephone
14874	14876	area of speech and speaker recognition SVM’s have also been studied over the last years. For example, among others  compares the use of traditional based kernel SVM’s with Gaussian classifiers,  examines the use of SVM’s for phonetic classification, and  studies the use of SVM’s to classify telephone handsets based on speech signals. SVM’s are model free methods that do not make any
14874	14876	each utterance X of variable length into a single vector UX. For more details the reader is refered to . The Fisher kernel approach has been successfully applied to speech signals before, see . 3s3 Probabilistic Distance Kernels Our new algorithm starts with a statistical model p(x|?i) of the data, i.e., for each utterance Xi = {x1, x2, . . . , xl} we estimate the parameters ?i of a
14874	14878	Gaussian, or polynomial don’t take full advantage of the nuances of speech signals. An example of previous attempts in speaker identification and verification using these kernels is described in . On the other hand statistical models such as Gaussian Mixture Models (GMM) or Hidden Markov Models make strong assumptions about the data, are simple to learn and estimate, and are well understood
14874	14879	decisions must be based on. Secondly these kernels are quite generic and do not take advantage of the statistics of the individual speech signals we are targeting. The Fisher kernel approach  is a first attempt at solving these two issues. It assumes the existence of generative model that explains well all possible data. For example, in the case of speech signals the generative model
14874	14879	probability of mixture k given the observed feature vector xt. Effectively we transform each utterance X of variable length into a single vector UX. For more details the reader is refered to . The Fisher kernel approach has been successfully applied to speech signals before, see . 3s3 Probabilistic Distance Kernels Our new algorithm starts with a statistical model p(x|?i) of the
14874	14880	each utterance X of variable length into a single vector UX. For more details the reader is refered to . The Fisher kernel approach has been successfully applied to speech signals before, see . 3s3 Probabilistic Distance Kernels Our new algorithm starts with a statistical model p(x|?i) of the data, i.e., for each utterance Xi = {x1, x2, . . . , xl} we estimate the parameters ?i of a
14874	14881	of the original feature data x. Notice that this similar to the Arithmetic harmonic sphericity (AHS) distance quite popular in the speaker identification and verification research community . Our approach, while independently derived, shows remarkable similarity to the Information Diffusion kernel proposed in . There are however some differences. Among others, our approach is
14874	14881	AHS, SVM using Fisher kernel, SVM using GMM/KL Divergence based kernels, and SVM using Full-Covariance/AHS distance based kernels. We compared the performance of all these classifiers.  and  describe in detail the first and second classification approaches. For the Fisher kernel experiments we used as ? parameters the prior probabilities of each mixture Gaussian as described in section
14874	14882	popular in the speaker identification and verification research community . Our approach, while independently derived, shows remarkable similarity to the Information Diffusion kernel proposed in . There are however some differences. Among others, our approach is conceptually much simpler and is applied to continuous data sets as opposed to discrete data sets such as text corpora. 4
8718103	14897	applications and is specifically designed for simulation optimization and hence accounts explicitly for the inherent randomness (Ólafsson, 1999; Ólafsson and Kim, 2001; Ólafsson and Shi, 2000; Shi and Ólafsson, 1997; 1998; 2000). The NP method uses iterative partitioning of the feasible region to narrow the focus of the optimization search and concentrate the computational effort where good solutions are
14906	14909	kernel . The need for clear delineation between some notion of ‘kernel’ code and ‘application’ code, even in a Java virtual machine, to provide isolation and resource control was argued by . Kernel boundaries serve as safe ‘termination boundaries’, so that termination is deferred if code is running in the kernel. In the OKE we recognise the importance of safe termination, as discussed
14906	14917	an important distinction between the OKE and the other systems is that we look at a very commonly used OS, namely Linux. Trust management and code loading in active networks were first discussed in . A high-level type-safe language (PLAN) was used, to provide restrictions of a packet’s service environment based on its level of privilege. Although the OKE was one of the first implementations
14906	14919	Other interpreted solutions also suffer from bad performance, as is witnessed by the well-known BSD Packet Filters, which are implemented in an interpreted stack language and execute in the kernel . The need for clear delineation between some notion of ‘kernel’ code and ‘application’ code, even in a Java virtual machine, to provide isolation and resource control was argued by . Kernel
14906	14920	a small part of the full spectrum of security issues, and the OKE therefore allows for a much wider range of restrictions. PCC achieves safety through the means of proof generation and checking . In PCC, the kernel publishes a safety policy, specifying formally what code is acceptable. The code loading party should provide a formal proof that the code adheres to this policy. PCC achieves
14906	14921	work in software fault isolation (SFI), proof carrying code (PCC), language restriction, extensible compilers, and operating systems such as SPIN, Nemesis, and the ExoKernel. SFI, advocated by , can check memory accesses with a runtime overhead that can be reasonably small, depending on the application and the form of SFI used. However, memory access is just a small part of the full
7748358	15792	emotional support (Volet & Ang, 1998), has not been given much attention in the literature. The value of interdependent forms of learning among CHC students is quite enduring as re#ected in Volet's (1998) &quot;ndings that after years in Australia, Australian students of Chinese ethnic Singaporean and Malay backgrounds had maintained in the host country a strong predisposition for interdependent forms of
14984	14985	gain in expanding the leaf against not expanding. The second condition, it must exist statistical support in favor of the best splitting test which is asserted using the Hoeffding bound as in VFDT . Functional Leaves To classify an unlabeled example, the example traverses the tree from the root to a leaf. It follows the path established, at each decision node, by the splitting test at the
14984	14985	When evaluating the splitting-criteria the merit of the best attributes could be closed enough that the difference in gains does not satisfy the Hoeffding bound. This aspect has been pointed out in . In VFDT, the authors propose the use of a user defined constant, ?, that can decide towards a split (given that ? < ?), even when the Hoeffding bound is not satisfied. In UFFT when there is a tie
14984	14986	the classes into two super-classes. Obviously, the cluster method can not be applied in the context of learning from data streams. We propose another methodology based on round-robin classification . The round-robin classification technique decomposes a multi-class problem into k binary problems, that is, each pair of classes defines a two-classes problem. In  the author shows the
14984	14987	of the classes, but also the conditional probabilities of the attributevalues given the class. In this way, there is a much better exploitation of the information available at each leaf . Forest of Trees The splitting criteria only applies to two class problems. Most of real-world problems are multi-class. In the original paper  and for a batch-learning scenario, this problem
1838925	15015	(Duffie and Singleton , Duffie, Schroder and Skiadas , Duffie and Huang  and Duffie ), Lando  , Flesaker et.al. , Artzner and Delbaen   and Jarrow and Turnbull . Most of these approaches first specify a mechanism that triggers the default (e.g. the firm's value process that hits a barrier or the first jump of a Cox process) and then go on to derive a
1838925	15018	is assumed to follow a diffusion process and default is modeled as the first time the firm's value hits a pre-specified boundary. The models of Merton , Black and Cox  Longstaff and Schwartz , Das  and Geske  are representatives of this approach. The second, more recent approach, leaves the direct reference to the firm's value and models the time of default directly as the time
1838925	15020	option pricing theory. Here the firm's value is assumed to follow a diffusion process and default is modeled as the first time the firm's value hits a pre-specified boundary. The models of Merton , Black and Cox  Longstaff and Schwartz , Das  and Geske  are representatives of this approach. The second, more recent approach, leaves the direct reference to the firm's value and
1838925	15021	and Runggaldier   for the application to interest rate theory 10 . 6.1 Change of Measure First, Girsanov's theorem (theorem 1) becomes 11 10 For other financial applications see also Merton  and Jarrow and Madan . 11 See Jacod and Shiryaev  and Bjork, Kabanov and Runggaldier . 24 Theorem 4 Letsbe a n-dimensional predictable processess1 (t); : : : ;sn (t) and \Phi(t; q) a
1838925	15023	for h(t; T ) that is known to generate positive short rates, e.g. the square root model of Cox, Ingersoll and Ross  or the model with lognormal interest rates by Sandmann and Sondermann . 4.2 Negative Forward Spreads There is one additional caveat when using the nonnegative rate model for the forward rate spreads: Even though we require that the `short' spread h(t; t) ? 0 is
1838925	15023	spreads. 13 Models of the short rate are by (among others): Vasicek , Cox, Ingersoll and Ross , Ho and Lee , Black, Derman, Toy , Hull and White  and Sandmann and Sondermann . 29 8 Jumps in the Defaultable Rates In the presence of multiple defaults (with ensuing restructuration) it is more realistic to allow the risky rates to change discontinuously at times of default,
8920737	15028	the restriction on delay distributions, but cannot model asynchrony. We therefore introduce the generalized semi-Markov decision process (GSMDP), based on the GSMP model of discrete event systems (Glynn 1989), as a model for asynchronous stochastic decision processes. A GSMDP, unlike an SMDP, can remember if an event enabled in the current state has been continuously enabled in previous states without
8920737	15028	The generalized semi-Markov process (GSMP), first introduced by Matthes (1962), is an established formalism in queuing theory for modeling continuous-time stochastic discrete event systems (Glynn 1989). We add a decision dimension to the formalism by distinguishing a subset of the events as controllable and adding rewards, thereby obtaining the generalized semi-Markov decision process (GSMDP). A
15029	15031	reused, is outside Parlay’s scope. This is where the MDA approach promises to be of benefit. 3 Overview of MDA MDA (Model-Driven Architecture) is OMG’s vision of enterprise application integration . MDA is based on model transformation principles, some known from earlier research and development within the CASE (ComputerAided Software Engineering) community. MDA is a promising approach mainly
15029	15857	tools are entering the market, and an increasing number of companies are deploying solutions based these technologies. In order to evaluate MDA we are participating in a Eurescom project (see ) together with a number of telecommunication operators and tool vendors. Our goal in this project has been mainly to evaluate MDA as a possible enabler for improving application development
15029	15858	language) for the telecommunication domain. This metamodel will be MOF-compliant and will focus on the specific needs of the telecommunication industry. Some of these needs are identified to be : ¯ Telephony networks: UML needs to be extended with concepts that constitute the domain of telephony networks and related services. Examples of such concepts are call, conference, voice message,
15029	15033	on UML. UML is particularly suited for thesMDA approach because of its extension mechanisms . Business and platform metamodels can be defined as UML profiles (for an example of UML profiles see ). In addition, having a formal meta metamodel ensures consistency and interoperability. 4 Using MDA and Parlay to develop applications Using the MDA approach in combination with Parlay is one of
15038	15040	2 Related Work Form surveying the literature we identify two main approaches to sketch-based modeling. The first is based on the calligraphic interfaces that use gestures and pen-input as commands . These gestural modeling systems rely on gestures as commands for generating solids from 2D sections. One good example is Sketch , where the geometric model is entered by a sequence of gestures
15038	6318	2 Related Work Form surveying the literature we identify two main approaches to sketch-based modeling. The first is based on the calligraphic interfaces that use gestures and pen-input as commands . These gestural modeling systems rely on gestures as commands for generating solids from 2D sections. One good example is Sketch , where the geometric model is entered by a sequence of gestures
15038	6318	allows retrieving additional information such as the pressure applied on the stylus tip at each point of the stroke. Raw strokes are then processed by an enhanced analyzer based on the CALI library . This library recognizes elemental geometric forms and gestural commands in real time, using fuzzy logic. Recognized gestures are inserted into a list, ordered according to a computed degree of
15038	15041	interfaces that use gestures and pen-input as commands . These gestural modeling systems rely on gestures as commands for generating solids from 2D sections. One good example is Sketch , where the geometric model is entered by a sequence of gestures according to a set of conventions regarding the order in which points and lines are entered as well as their spatial relations.
15038	15043	oriented to mechanical designs that consists of a 2D drawing environment based on constraints. From these it is possible to generate 3D models through modeling gestures. Another system, Teddy  is oriented to free-form surface modeling using a very simple interface of sketched curves, pockets and extrusions. Users draw silhouettes through a series of pen strokes and the system
15038	15043	provides immediate feedback, since it operates online as a user draws the sketch. This concept of beautification bears some relation to the drawing beautification proposed by Igarashi , . Also it exploits the auto-constraining capabilities of the parametric engine in a way transparent to the user. Both parametric and beautified models are continuously updated by the system,
15038	15050	However, it is well known by psychologists that humans can identify 3D models from 2D images by using a simple set of perceptual heuristics . Authors such as Marill , Leclerc and Fischler , and Lipson and Shpitalni  provide interesting references in the development of this field. The Digital Clay system , which supports basic polyhedral objects in combination with a
15038	15051	psychologists that humans can identify 3D models from 2D images by using a simple set of perceptual heuristics . Authors such as Marill , Leclerc and Fischler , and Lipson and Shpitalni  provide interesting references in the development of this field. The Digital Clay system , which supports basic polyhedral objects in combination with a calligraphic interface for data input,
15038	15053	Clay system , which supports basic polyhedral objects in combination with a calligraphic interface for data input, uses Huffman-Clowes algorithms to derive three-dimensional geometry. Stilton  uses a calligraphic interface directly implemented on a VRML environment. Its reconstruction process uses an optimization technique based on genetic algorithms. Finally, CIGRO  which supports
15038	15054	provides immediate feedback, since it operates online as a user draws the sketch. This concept of beautification bears some relation to the drawing beautification proposed by Igarashi , . Also it exploits the auto-constraining capabilities of the parametric engine in a way transparent to the user. Both parametric and beautified models are continuously updated by the system,
15038	15055	Stilton  uses a calligraphic interface directly implemented on a VRML environment. Its reconstruction process uses an optimization technique based on genetic algorithms. Finally, CIGRO  which supports drawing constrained polyhedral objects (normalon and quasi-normalon ones), and, in combination with a calligraphic interface for data input, uses an axonometric inflation engine to
15038	15055	contrast to surveyed work, the GEGROSS application integrates both gestural and reconstruction based approaches. This extends the capabilities of our previous reconstruction modeling system CIGRO , in order to support gestural modeling. At its current level of development, system behavior is explicitly controlled by users, who can alternate between gestural or reconstruction operation modes.
8832677	2483	the true price of the American option. This suggests that a tight upper bound can be obtained by using an accurate approximation, ?Vt, to define ?t. One possibility (see Haugh and Kogan 2001, and Andersen and Broadie 2001 for further comments related to the choice of ?t) istodefine?t as a martingale: ?0 = ?V0 ?t+1 = ?t + ?Vt+1 ? Bt+1 ?Vt Bt ? Et ? ?Vt+1 Bt+1 (5) ? ?Vt ? . (6) Bt Let V 0 denote the upper bound we get
8832677	15068	for example, Bertsekas and Tsitsiklis 1996) have had considerable success in tackling large-scale complex problems and have recently been applied successfully to problems in financial engineering (Brandt et al. 2001; Longstaff and Schwartz 2001; and Tsitsiklis and Van Roy 2001). One difficulty with ADP, however, is in establishing how far the sub-optimal ADP solution to a given problem is from optimality. In
8832677	2497	have been used to construct and evaluate solutions to these problems. While itshas long been recognized that simulation is an indispensable tool for financial engineering (see the surveys of Boyle, Broadie and Glasserman 1997, and Staum 2002), it is only recently that simulation has begun to play an important role in solving control problems in financial engineering. These control problems include portfolio optimization
8832677	2514	of (3) equals the true price of the American option. This suggests that a tight upper bound can be obtained by using an accurate approximation, ?Vt, to define ?t. One possibility (see Haugh and Kogan 2001, and Andersen and Broadie 2001 for further comments related to the choice of ?t) istodefine?t as a martingale: ?0 = ?V0 ?t+1 = ?t + ?Vt+1 ? Bt+1 ?Vt Bt ? Et ? ?Vt+1 Bt+1 (5) ? ?Vt ? . (6) Bt Let V
15106	15094	to 7, we have verified that it is still optimal under their M (8) model when the number of 0’s is limited to 11.sPatternHunter II 173 3 The Complexity of Finding Optimal Spaced Seeds Many authors  have proposed heuristic or exponential time algorithms for the general seed selection problem: find one or many optimal spaced seeds so that a maximum number of target regions are each hit by at
15106	15095	and Waterman in  as multiple filtration techniques, used in FLASH system by Califano and Rigoutsos , and to cover a region with near certainty by multiple randomized hash functions by Buhler . Two problems had postponed us from implementing multiple optimal spaced seeds in the original PatternHunter: large memory requirements for multiple hashtables and the difficulty of finding the
15106	15098	memory requirements for multiple hashtables and the difficulty of finding the optimal seed combination. Since , many researchers have recently further studied various aspects of spaced seeds  and given exponential or heuristic algorithms for computing the optimal seed(s). However, the complexity of finding optimal spaced seeds remains open. Even the complexity of computing the hit
15106	15098	to 7, we have verified that it is still optimal under their M (8) model when the number of 0’s is limited to 11.sPatternHunter II 173 3 The Complexity of Finding Optimal Spaced Seeds Many authors  have proposed heuristic or exponential time algorithms for the general seed selection problem: find one or many optimal spaced seeds so that a maximum number of target regions are each hit by at
15106	13258	methods based on dynamic programming, like SSearch , are often too slow to be practical, even with supercomputers. Specialized software, such as MegaBlast , BLAT , and MUMmer , were developed to speed up Blast for highly similar sequences. Many commercial and academic parallel “BlastMachines” were also built to cope with the huge computational load. PatternHunter  is
15106	15106	better sensitivity than reducing the weight of the single seed by one. We have observed similar phenomena for weight-12 general purpose seeds. The figure for weight-12 seeds can be found in . According to a lemma in , reducing the weight by one increases the expected number of hits by a factor of four (the alphabet size) in DNA homology search. Doubling the number of the seeds,
15106	15106	is noticeably worse than the other two seeds, while the performances of PH II’s seed and their manually designed ?sp seed are almost identical. A figure illustrating this comparison can be found in . Brejova, Brown, and Vinar  proposed an HMM model M (8) for spaced seeds and demonstrated its clear advantage on coding regions. Using the same EST sequence data, Figure 4 provides a comparison
15106	15106	showed that the multiple seeds computed with M (3) (0.8, 0.8, 0.5) and the seeds computed with M (8) model perform approximately the same. A figure that illustrates the performances can be found in . The above comparisons reveal that although the more sophisticated model of the coding regions provided remarkable improvement on the seed performance in  and , this improvement does not
15106	15106	the problem cannot be approximated within + ? for any ? > 0, Section 3.2. ratio 1 ? 1 e Because of the space limit, we omit all the proofs in this section. However, the proofs can be found in . 3.1 Computing the Hit Probability of Multiple Seeds Is NP-hard Theorem 2 Computing the hit probability of many seeds on a uniformly distributed random region is NP-hard. 3.2 The Hardness of
15106	15108	Solutions Inc., 2B-145 Columbia W., Waterloo, ON, Canada N2L 3L2. 4 CWI, P.O. Box 94079, 1090 GB Amsterdam, The Netherlands. Abstract Extending the single optimized spaced seed of PatternHunter  to multiple ones, PatternHunter II simultaneously remedies the lack of sensitivity of Blastn and the lack of speed of SmithWaterman, for homology search. At Blastn speed, PatternHunter II
15106	15108	, were developed to speed up Blast for highly similar sequences. Many commercial and academic parallel “BlastMachines” were also built to cope with the huge computational load. PatternHunter  is a new generation general purpose homology search tool designed to meet this tremendous need. At Blastn default sensitivity, PatternHunter runs at MegaBlast speed . PatternHunter was used
15106	15108	sensitivity problem: PatternHunter II aims to achieve a sensitivity approaching that of Smith-Waterman with a speed similar to the default Blastn.sPatternHunter II 165 One new idea in PatternHunter  was the introduction of an “optimized spaced seed”. In Blast, exact matches of k continuous letters is used as a “seed” to find long matches around it, whereas in PatternHunter, a seed is k
15106	15108	us from implementing multiple optimal spaced seeds in the original PatternHunter: large memory requirements for multiple hashtables and the difficulty of finding the optimal seed combination. Since , many researchers have recently further studied various aspects of spaced seeds  and given exponential or heuristic algorithms for computing the optimal seed(s). However, the
15106	15108	optimal seeds is NP-hard. It cannot be approximated within ratio e?1 e . • The problem of finding even one optimal seed is NP-hard. 2 Optimized Multiple Spaced Seeds and PatternHunter II Following , we denote a spaced seed as a binary string. Let a be a seed. The length of a is denoted by |a|, while ?a? denotes the weight of a, i.e. the number of 1’s in a. Intuitively, a 1 in the seed
15106	15113	homology search is very time consuming and often needs supercomputers to conduct. Yet, as GenBank doubles in size every 18 months  and the list of completed genomes (now including human , mouse , and rice amongst many other species) expands quickly, the current computational cost is only the tip of the iceberg. While having made tremendous contributions to science in the past
15106	11690	the exhaustive Smith-Waterman methods based on dynamic programming, like SSearch , are often too slow to be practical, even with supercomputers. Specialized software, such as MegaBlast , BLAT , and MUMmer , were developed to speed up Blast for highly similar sequences. Many commercial and academic parallel “BlastMachines” were also built to cope with the huge computational
8920752	15115	sources. The treatment of semi-structured data sources is the subject of substantial research attention, largely because much information available on the Web is semi-structured. A recent survey  notes that there is no theory or precise definition of semi-structured data. A document may contain its own metadata, but the common case is for the logical structure to be implicitly defined by a
8920752	10915	two major perspectives in recent research efforts: from a natural language processing perspective (e.g. , , , ) and through extended database query languages (e.g. , , ). Our approach, which shares elements with both these perspectives, is distinguished by the explicit use of an abstract document model to structure and restrict the amount and
8920752	8177	perspectives in recent research efforts: from a natural language processing perspective (e.g. , , , ) and through extended database query languages (e.g. , , ). Our approach, which shares elements with both these perspectives, is distinguished by the explicit use of an abstract document model to structure and restrict the amount and complexity
8920752	15117	This is closely related to work on document classification, which characterises a document as relevant or not according to the terms it contains compared with a collection of documents (e.g. , , ). Information extraction projects typically assume that there are many documents that will meet a user's requirements and that they are better met by returning a small number of
8920752	1190	and an outline of future work. 2. Related work Information mediation research issues are being addressed in several projects, such as COIN , TSIMMIS , Information Manifold . The focus of the Dyade Médiation work is on the development of components to permit the rapid development of applications which combine and extract information from heterogeneous sources and
8920752	15119	section headings). The extraction of useful content has been tackled from two major perspectives in recent research efforts: from a natural language processing perspective (e.g. , , , ) and through extended database query languages (e.g. , , ). Our approach, which shares elements with both these perspectives, is distinguished by the explicit use
8920752	15119	projects typically assume that there are many documents that will meet a user's requirements and that they are better met by returning a small number of relevant documents (high precision) , ,  at the expense of better recall. In the application domains we are concerned with, good recall is essential, even at the expense of lower precision. A critical issue for all
8920752	15119	required to manually construct a dictionary of patterns and rules for the linguistic analysis of a domain is substantial. Attempts to reduce the effort required rely on a tagged training corpus ,  or a pre-classified document collection . These approaches, and several other systems used in the MUC evaluations, rely on the linguistic analysis of parts of the text identified
8920752	403	The extraction of useful content has been tackled from two major perspectives in recent research efforts: from a natural language processing perspective (e.g. , , , ) and through extended database query languages (e.g. , , ). Our approach, which shares elements with both these perspectives, is distinguished by the explicit use of an
8920752	403	to manually construct a dictionary of patterns and rules for the linguistic analysis of a domain is substantial. Attempts to reduce the effort required rely on a tagged training corpus ,  or a pre-classified document collection . These approaches, and several other systems used in the MUC evaluations, rely on the linguistic analysis of parts of the text identified by trigger
15124	15961	and that students need to see the connection between math, science, technology, social studies, and English; therefore, teachers should use interdisciplinary instruction. Other educators, DeLuca (1992) and James (1991), plead the case for problemcentered instruction as an authentic way to focus on the development of students’ higher-level cognitive skills. Measuring Technological Literacy
15141	15142	Rotenberg’s, as he used a trace processor model  while we based ours on a more conventional superscalar design. Recently, Austin proposed a novel fault detection architecture called DIVA . DIVA uses a very simple in-order processor as a checker for a larger out-of-order, speculative processor. The DIVA architecture has both advantages and disadvantages compared to SRT processors.
15141	1939	could apply to any SMT processor, we will use the CPU model shown in Figure 2 for illustration and evaluation. Our base processor design is inherited from the SimpleScalar “sim-outorder” code , from which our simulator is derived. In our SMT version of this machine, the fetch stage feeds instructions from multiple threads (one thread per cycle) to a simple fetch/decode queue. The decode
15141	1939	METHODOLOGY In this section, we describe our simulator, baseline SMT architecture, and benchmarks. Our simulator is a modified version of the “sim-outorder” simulator from the Simplescalar tool set . We extended the original simulator by replicating the necessary machine context for SMT, adding support for multiple address spaces, and increasing the coverage and types of collected statistics.
15141	15149	for overhead due to SMT, complexity of an out-of-order machine, and wire delays in future high-frequency microprocessors. To approximate the performance of a processor employing a trace cache , we fetch up to three basic blocks (up to a maximum of eight instructions) per cycle regardless of location. We evaluated our ideas using 11 SPEC95 benchmarks  shown in Table 2. We compiled all
15141	15150	units. For example, the Alpha 21464  implements a four-threaded SMT machine that can issue up to eight instructions per cycle from one or more threads. As noted previously by Rotenberg , an SMT processor is attractive for fault detection because it can provide redundancy by running two copies of the same program simultaneously. Such a simultaneous and redundantly threaded (SRT)
15141	15150	methodology and Section 5 discusses our results. Section 6 discusses further techniques to improve fault coverage in an SRT processor. Section 7 discusses related work, including the AR-SMT design , an independently developed example of an SRT processor. Finally, Section 8 presents our conclusions. 2. BACKGROUND This section provides background on the two areas we combine in this paper:
15141	15150	corresponding redundant instructions experienced the same fault, it would very likely affect different bits of the result, leading to a detectable fault. 7. RELATED WORK Rotenberg’s AR-SMT design  was the first to use simultaneous multithreading to detect transient hardware faults. AR-SMT incorporates two redundant threads: the “active”, or Athread, and the “redundant”, or R-thread.
15141	15151	in the same thread to provide a consistent view of the register file. We can avoid complex forwarding logic by using the separate per-thread register files of the SMT architecture as “future files” . That is, as each instruction retires from the RUU, it updates the appropriate perthread register file, as in a standard SMT processor. This register file then reflects the up-to-date but
15141	15154	a software recovery sequence. In this paper, we investigate the use of simultaneous multithreading as a hardware mechanism to detect transient hardware faults. Simultaneous multithreading (SMT)  is a novel technique to improve the performance of a superscalar microprocessor. An SMT machine allows multiple independent threads to execute simultaneously—that is, in the same cycle—in
15141	15154	cost, however, neither system replicates large components such as main memory; instead, both use ECC to cover memory faults. 2.2 Simultaneous Multithreading (SMT) Simultaneous Multithreading (SMT)  is a technique that allows fine-grained resource sharing among multiple threads in a dynamically scheduled superscalar processor. An SMT processor extends a standard superscalar pipeline to
8920766	15157	artificial intelligence on explaining decision-theoretic advice (Klein and Shortliffe, 1994), and on previous work in computational linguistics on generating natural language evaluative arguments (Elhadad, 1995) . On the one hand, the study on explaining decision-theoretic advice produced a rich quantitative model that can serve as a basis for strategies to select and organize the content of
15159	15160	in the main memory. A SAX Parser is invoked to parse the published XML messages then the SAX events are streamed through the finite state automata to match the XPath expressions in subscriptions.  is the first paper that studies the problem of matching an XML document against many XPath expressions.  proposes using an in-memory FSA algorithm as the solution and later  introduces state
15159	15160	makes it difficult to add or to delete a subscription, especially when the system is running. To the best of our knowledge, this paper is the first research that attacks the problems of  and  using a relational database. By using a database, our solution is not limited by the amount of available physical memory; therefore can handle orders of magnitude more subscriptions. The
15159	15162	conclude in Section 6. 2. RELATED WORK The earliest topic (or subject) based publish/subscribe systems have been studied extensively and there are mature and scalable implementations, for example, . More recent publish/subscribe systems use a more flexible, and more powerful, content-based paradigm. Some content-based pub/sub systems treat subscriptions as data and the matching of
15159	15163	the problem of matching an XML document against many XPath expressions.  proposes using an in-memory FSA algorithm as the solution and later  introduces state sharing in the FSA construction.  builds an index on sub-strings of path expressions that only contain parent-child relationship, and introduces sharing of computations between the common substrings.  use FSA to evaluate
15159	15164	to optimize a group of similar queries that share common computations. TriggerMan  uses signatures to group similar queries together in order to provide a scalable trigger mechanism. NiagaraCQ  also uses signatures to group similar continuous queries. NiagaraCQ uses a table of constants extracted from the queries along with a join to evaluate a group of queries simultaneously. Recently,
15159	15164	rules are expressed in SQL. Some of the systems can handle XML messages, but the system either uses a wrapper to extract tuple data from XML messages  or uses a simple language for the rules . Other systems treat the subscriptions as filters on the messages. For example,  constructs an in-memory decision tree for the subscriptions. For each message, the system walks down the decision
15159	15165	can be shared between predicates. If a predicate is very common, it may be desirable to pull this predicate to front to maximize sharing even though the predicate is not very selective. We refer to  for studies of the cost models and global optimization strategies in similar situations. The simple sharing optimization strategy used is especially attractive in the first stage (equal stage)
15159	15166	in subscriptions.  is the first paper that studies the problem of matching an XML document against many XPath expressions.  proposes using an in-memory FSA algorithm as the solution and later  introduces state sharing in the FSA construction.  builds an index on sub-strings of path expressions that only contain parent-child relationship, and introduces sharing of computations between
15159	15166	of experiments use the NASA dataset. The NASA data set has a recursive DTD and the nesting structure is much more complex than the synthetic Stock dataset. We used the XPath query generator from  to randomly generate branching XPath subscriptions. Following the practice in , the atomic value in each linear path predicate is set to a value that appears in some XML document of the dataset.
15159	15167	in the FSA construction.  builds an index on sub-strings of path expressions that only contain parent-child relationship, and introduces sharing of computations between the common substrings.  use FSA to evaluate XPath expressions over a stream of XML data.  proposes building state “lazily” as the solution to tame the exponential explosion of the number of states. In , the
15159	15168	that only contain parent-child relationship, and introduces sharing of computations between the common substrings.  use FSA to evaluate XPath expressions over a stream of XML data.  proposes building state “lazily” as the solution to tame the exponential explosion of the number of states. In , the states of the FSA are treated as a “cache” for matching XPath expressions and
15159	15168	makes it difficult to add or to delete a subscription, especially when the system is running. To the best of our knowledge, this paper is the first research that attacks the problems of  and  using a relational database. By using a database, our solution is not limited by the amount of available physical memory; therefore can handle orders of magnitude more subscriptions. The focus of
15159	15168	the nesting structure is much more complex than the synthetic Stock dataset. We used the XPath query generator from  to randomly generate branching XPath subscriptions. Following the practice in , the atomic value in each linear path predicate is set to a value that appears in some XML document of the dataset. There are two to ten linear path predicates in each subscription with an average
15159	15168	(kernel version 2.4.20). Our experiments varied the number of subscriptions. Table 5.3 shows the number of subscriptions and linear path predicates for each experiment. Compared to the work in , which solves the matching problem with in memory finite state automata, our experiments contain two order of magnitude as many as linear path predicates using only half the amount of physical
15159	15171	in the FSA construction.  builds an index on sub-strings of path expressions that only contain parent-child relationship, and introduces sharing of computations between the common substrings.  use FSA to evaluate XPath expressions over a stream of XML data.  proposes building state “lazily” as the solution to tame the exponential explosion of the number of states. In , the states
15159	15176	along with a join to evaluate a group of queries simultaneously. Recently, commercial database vendors started to use their relational database engine to implement publish/subscribe system . The scalability of databasebased solutions is not limited by the amount of physical memory. However, most of these systems only handle tuple-like messages and the rules are expressed in SQL.
15159	16009	should have many publishers and the published XML messages can have very flexible document structures. Subscription rules should be expressed by a powerful language based on XPath  or XQuery  and support joins of the messages with existing reference data. Subscribers may also want to specify acceptable delay of notifications as a quality of service (QoS) requirement. A pub/sub system
15178	15224	possible misconceptions, and then design an appropriate educational setting. In any case, teachers need to be able to respond to situations in their classroom they might not have anticipated (Kennedy, 1998). * Consequently, the number of topics in the curriculum will probably have to decrease. For teachers this implies the acceptance of the idea that ``less is better,'' and resisting ``the temptation
2158416	8843	So far, the algorithmic problems modeled with PlaNet mainly re ect our theoretical research interests. • Algorithms to compute a random triangulation and a Delaunay triangulation, respectively (, cf. ). These algorithms are mainly intended to serve as a basis for random generation of instances. • Various smaller algorithms such as removing a random couple of edges, random paths, and
2158416	15283	• The vertex-disjoint Menger problem, that is, nd the maximum number of internally vertex-disjoint paths such that all paths connect the same pair {s; t} of vertices. The linear-time algorithm from  has been integrated. • An algorithm for computing a minimum (s; t)-separator given a maximum number of vertex-disjoint (s; t)-paths. • The edge-disjoint version of the former problem. The
2158416	15283	et al. / Discrete Applied Mathematics 92 (1999) 91–110 Fig. 7. An instance of type “vertex-disjoint Menger problem”, with source 30 and target 17, and the solution constructed by the algorithm from . In black-and-white mode, the vertex-disjoint paths from the source to the target are dashed. Heuristic 5.21. Min Parenthesis Interval II and All Paths Shortest Paths. This method calls as a
2158416	15283	• The vertex-disjoint Menger problem, that is, nd the maximum number of internally vertex-disjoint paths such that all paths connect the same pair {s; t} of vertices. The linear-time algorithm from  has been integrated. • An algorithm for computing a minimum (s; t)-separator given a maximum number of vertex-disjoint (s; t)-paths. • The edge-disjoint version of the former problem. The
2158416	15283	et al. / Discrete Applied Mathematics 92 (1999) 91–110 Fig. 7. An instance of type “vertex-disjoint Menger problem”, with source 30 and target 17, and the solution constructed by the algorithm from . In black-and-white mode, the vertex-disjoint paths from the source to the target are dashed. Heuristic 5.21. Min Parenthesis Interval II and All Paths Shortest Paths. This method calls as a
2158416	15287	class from this polymorphic class. 1 If one or more algorithms shall be extracted and run without any graphics, a dummy class must be derived, whose methods are void. In a preceding project, CRoP , which solves combinatorial VLSI routing problems, graphics and algorithms were separated from each other as follows. An algorithm produces not only a nal result, but also records all actions that
2158416	15287	class from this polymorphic class. 1 If one or more algorithms shall be extracted and run without any graphics, a dummy class must be derived, whose methods are void. In a preceding project, CRoP , which solves combinatorial VLSI routing problems, graphics and algorithms were separated from each other as follows. An algorithm produces not only a nal result, but also records all actions that
232087	15295	unfortunately, the approach cannot be easily extended to multiple output subgraphs and the property of maximal size does not represent optimality under constraints on the number of inputs. In , the identification problem is addressed in a manner M0 0 & 2 4 & -32768 & delta & & M2 M1 8 257 similar to ours in the context of hardware/software partitioning. A simple clustering algorithm is
232087	15295	deterministic functionality (see Section 4). Our algorithm is more expensive but considers the complete design space. Section 7 shows the superiority of the algorithm presented here with respect to  and to . 3. MOTIVATION AND CONTRIBUTIONS Figure 1 shows the dataflow graph of the basic block most frequently executed in a typical embedded processor benchmark. We use this simple but realistic
232087	15295	More specifically, this work will improve the state-of-theart in three respects: Firstly, prior work was mostly limited to instructions with a single output (with the exceptions of two outputs in  and several outputs only in very specific cases in ). Our technique identifies custom instructions with any number of outputs up to a user-specified constraint. Note that current VLIW
232087	15295	show the potentials of our algorithms with respect to the state of the art, we have implemented two identification algorithms which are denoted by Clubbing and MaxMISO,andarepublishedrespectivelyinandin. The first is a greedy linear-complexity algorithm that can detect n-input m-output graphs, where n and m are user parameters. The second is a linear complexity algorithm that identifies
232087	15297	problem that our algorithm solves in Section 4, but use this generic formulation to discuss related work. A recent example of synthesis of application-specific instructions can be found in : the goal is to add special single- and multiple-cycle instructions to a small set of primitive instructions. The authors essentially concentrate on a selection problem which targets a maximal
232087	15297	of node types successions—e.g., multiplications followed by additions—rather than of frequency of execution of specific nodes. The emphasis on recurrent patterns somehow relates this work to : the authors observe that the number of operations per cluster is typically small and conclude that simple pairs of operations appear the best candidates. Their work does not account for
232087	15298	the area and energy cost of top-notch superscalar or multithreaded processors. Many readily extensible processors exist today both in academia (e.g., ) and industry (e.g., , , , ). The important motivation toward specialisation of existing processors versus the design of complete ASIPs is to avoid the complexity of a complete processor and toolset development. Instead, an
232087	15301	revolving around the synthesis of Application Specific Instruction-set Processors (ASIPs). This involved the automatic generation of complete instruction sets for specific applications (, , ). In that context, the goal is typically to design an instruction set which minimises some important metric (e.g., run time, program memory size, execution unit count). Permission to make digital
232087	15302	pass to the code. complexity of the special instructions. Our philosophy is different and we directly formulate as our goal to achieve a maximal gain per special instruction. In other works such as  or , the authors use approaches combining template matching (instruction selection, as it is called in compilers) and template generation (identification, in our parlance) for ASIPs. The main
232087	7884	0 4 88 stepsizeTable LD step 1 outp step outp step valpred ST step 3 delta 7 2 1 M0’’ SEL SEL SEL SEL SEL valpred SEL 0 32767 0 0 M0’ Figure 1: Motivational example from the adpcmdecode benchmark . SEL represents a selector node and results from applying an if-conversion pass to the code. complexity of the special instructions. Our philosophy is different and we directly formulate as our
232087	7884	where by no means one could use a computationally heavier model. 7. RESULTS The described algorithms were implemented within the MachSUIF framework  and tested on a subset of the MediaBench  suite benchmarks. Application C-code is compiled to MachSUIF intermediate representation and preprocessed with a classic if-conversion pass. In order to show the potentials of our algorithms with
232087	15304	similar from the identification perspective, although the overall goal and architectural context is rather different. Work in reconfigurable computing is often more in line with our goal (e.g., , , , ). Yet, identification algorithms are relatively simple and almost invariably target clusters producing a single result. Usually, clusters or subgraphs are somehow grown from their
232087	15305	the inner loop of our identification algorithm, where by no means one could use a computationally heavier model. 7. RESULTS The described algorithms were implemented within the MachSUIF framework  and tested on a subset of the MediaBench  suite benchmarks. Application C-code is compiled to MachSUIF intermediate representation and preprocessed with a classic if-conversion pass. In order
232087	15307	without incurring the area and energy cost of top-notch superscalar or multithreaded processors. Many readily extensible processors exist today both in academia (e.g., ) and industry (e.g., , , , ). The important motivation toward specialisation of existing processors versus the design of complete ASIPs is to avoid the complexity of a complete processor and toolset development.
232087	15308	the identification perspective, although the overall goal and architectural context is rather different. Work in reconfigurable computing is often more in line with our goal (e.g., , , , ). Yet, identification algorithms are relatively simple and almost invariably target clusters producing a single result. Usually, clusters or subgraphs are somehow grown from their output nodes by
232087	15308	in three respects: Firstly, prior work was mostly limited to instructions with a single output (with the exceptions of two outputs in  and several outputs only in very specific cases in ). Our technique identifies custom instructions with any number of outputs up to a user-specified constraint. Note that current VLIW architectures like ST200 and TMS320 can commit 4 values per cycle
15309	10128	make use of low-level node collaboration to reduce the energy cost of data transfer by aggregating data locally rather than sending all raw data to the application. For example, with LEACH , nodes form local clusters and all data within a cluster are aggregated by the cluster-head node before being transmitted to the base station. This limited form of low-level collaboration is also
15309	4875	are aggregated by the cluster-head node before being transmitted to the base station. This limited form of low-level collaboration is also found in the query-based technique of Directed Diffusion , in which nodes collaborate to set up routes as interests for particular data are disseminated through the network. Another approach to reducing energy dissipation is to turn nodes off whenever
15309	4875	the data-driven nature of these networks, and the many-to-one, many-to-some, or many6sto-many collection of the data. Sensor network routing protocols such as Rumor Routing , Directed Diffusion  and SPIN  provide lightweight, data-centric solutions tailored to typical sensor network traffic patterns. Although these protocols are effective in extending the lifetime of sensor networks,
15309	4875	we must also consider how the power costs of nodes are affected by their roles in the network (e.g., piconet masters or bridge nodes in Bluetooth scatternets, data aggregators in Directed Diffusion , coordinators in Span ). The power cost of using a node is a combination of the power to run the device, the power to transmit its data, the power to forward the data of other nodes in the set,
15309	5010	reducing energy dissipation is to turn nodes off whenever possible. As idle power can often be significant, this approach can greatly extend application lifetime. MAC-level protocols, such as PAMAS  and S-MAC  use this technique to reduce energy dissipation in the MAC protocol, often trading off latency in packet delivery for energy efficiency. Topology control protocols such as ASCENT ,
15309	15310	dissipation is to turn nodes off whenever possible. As idle power can often be significant, this approach can greatly extend application lifetime. MAC-level protocols, such as PAMAS  and S-MAC  use this technique to reduce energy dissipation in the MAC protocol, often trading off latency in packet delivery for energy efficiency. Topology control protocols such as ASCENT , Span , and
15309	15311	and S-MAC  use this technique to reduce energy dissipation in the MAC protocol, often trading off latency in packet delivery for energy efficiency. Topology control protocols such as ASCENT , Span , and STEM  use a similar technique of turning on and off sensors to maximize network lifetime while keeping the network fully connected. Other topology control protocols such as Lint
15309	5013	use this technique to reduce energy dissipation in the MAC protocol, often trading off latency in packet delivery for energy efficiency. Topology control protocols such as ASCENT , Span , and STEM  use a similar technique of turning on and off sensors to maximize network lifetime while keeping the network fully connected. Other topology control protocols such as Lint  aim to
15309	5013	Milan can continuously adapt to the specific features of whichever network is being used for communication (e.g., determining scatternet formations in Bluetooth networks, coordinator roles in Span , etc.) in order to best meet the applications’ needs over time. Figure 5 shows an overview of the interactions among Milan, the applications, and the sensors, together with a partial API. This
15309	5013	the power costs of nodes are affected by their roles in the network (e.g., piconet masters or bridge nodes in Bluetooth scatternets, data aggregators in Directed Diffusion , coordinators in Span ). The power cost of using a node is a combination of the power to run the device, the power to transmit its data, the power to forward the data of other nodes in the set, and the overhead of
15309	15312	technique to reduce energy dissipation in the MAC protocol, often trading off latency in packet delivery for energy efficiency. Topology control protocols such as ASCENT , Span , and STEM  use a similar technique of turning on and off sensors to maximize network lifetime while keeping the network fully connected. Other topology control protocols such as Lint  aim to determine the
15309	15313	Span , and STEM  use a similar technique of turning on and off sensors to maximize network lifetime while keeping the network fully connected. Other topology control protocols such as Lint  aim to determine the minimum transmit power necessary for a fully connected network, whereas protocols such as those described in  determine the optimal transmit power to minimize overall
15309	15314	fully connected. Other topology control protocols such as Lint  aim to determine the minimum transmit power necessary for a fully connected network, whereas protocols such as those described in  determine the optimal transmit power to minimize overall energy dissipation. In addition to the above two techniques, considerable energy can be saved by tailoring the routing protocol to the
15309	3706	fully connected. Other topology control protocols such as Lint  aim to determine the minimum transmit power necessary for a fully connected network, whereas protocols such as those described in  determine the optimal transmit power to minimize overall energy dissipation. In addition to the above two techniques, considerable energy can be saved by tailoring the routing protocol to the
15309	15315	of the sensors, the data-driven nature of these networks, and the many-to-one, many-to-some, or many6sto-many collection of the data. Sensor network routing protocols such as Rumor Routing , Directed Diffusion  and SPIN  provide lightweight, data-centric solutions tailored to typical sensor network traffic patterns. Although these protocols are effective in extending the
15309	15316	nature of these networks, and the many-to-one, many-to-some, or many6sto-many collection of the data. Sensor network routing protocols such as Rumor Routing , Directed Diffusion  and SPIN  provide lightweight, data-centric solutions tailored to typical sensor network traffic patterns. Although these protocols are effective in extending the lifetime of sensor networks, the gap between
15309	15317	Service discovery is useful for dynamic sensor networks to know what sensors and/or services are available; however, access to services remains object-based, similar to Corba. The Lime middleware  focuses on a different API (application programming interface), namely a shared memory scheme for mobile ad hoc components through a Linda-like tuple space . Neither Jini nor Lime consider the
15309	13607	to Corba. The Lime middleware  focuses on a different API (application programming interface), namely a shared memory scheme for mobile ad hoc components through a Linda-like tuple space . Neither Jini nor Lime consider the limited energy constraints of sensor networks, and their supporting protocols are heavyweight when compared to protocols tailored to sensor networks. Some
15309	15318	Some middleware acknowledge the changing properties of wireless networks and attempt to modify their own behavior to match the conditions detected within the network. For example, both Limbo  and FarGo  reorder data exchanges or relocate components to respond to changing network conditions such as bandwidth availability or link reliability. At a lower level, Mobiware  supports
15309	15319	acknowledge the changing properties of wireless networks and attempt to modify their own behavior to match the conditions detected within the network. For example, both Limbo  and FarGo  reorder data exchanges or relocate components to respond to changing network conditions such as bandwidth availability or link reliability. At a lower level, Mobiware  supports various levels
15309	15322	to allow the applications to adapt. For example, applications built on the Odyssey platform  can register for notification of changes in the underlying network data rate. Similarly, the Spectra  component of Aura  monitors the network conditions and the accessible computation resources, deciding where computation should be performed based on the network transmission required to
15309	15323	to adapt. For example, applications built on the Odyssey platform  can register for notification of changes in the underlying network data rate. Similarly, the Spectra  component of Aura  monitors the network conditions and the accessible computation resources, deciding where computation should be performed based on the network transmission required to complete them as well as the
15309	5672	specific data aggregation protocols of sensor networks, nor do they consider the details of the low-level wireless protocols. Among existing distributed computing middleware, QoS-Aware Middleware  provides the closest example of a middleware that can support sensor network applications. This middleware is responsible for managing local operating system resources based on application
15309	15324	the development of middleware specifically designed to meet the challenges of wireless sensor networks, focusing on the long-lived and resource-constrained aspects of these systems. Both the Cougar  and SINA  systems provide a distributed database interface to the information from a sensor network with database-style queries. Power is managed in Cougar by distributing the query among the
15309	15325	of middleware specifically designed to meet the challenges of wireless sensor networks, focusing on the long-lived and resource-constrained aspects of these systems. Both the Cougar  and SINA  systems provide a distributed database interface to the information from a sensor network with database-style queries. Power is managed in Cougar by distributing the query among the sensor nodes to
15309	15327	for standard networks because resource constraints are met on a per-sensor basis, but the techniques for collecting the current resource utilization are tailored to the sensor network. DSWare  provides a similar kind of data service abstraction as AutoSec, but instead of the service being provided by a single sensor, it can be provided by a group of geographically close 8ssensors.
15309	15328	as long as enough sensors remain in an area to provide a valid measurement. While these middleware for sensor networks focus on the form of the data presented to the user applications, Impala , designed for use in the ZebraNet project, considers the application itself, exploiting mobile code techniques to change the functionality of the middleware executing at a remote sensor. The key to
15309	15329	For example, in the personal health monitor, variables such as blood pressure, respiratory rate, and heart rate may be determined based on measurements obtained from any of several sensors . Each sensor has a certain QoS in characterizing each of the application’s variables. For example, a blood pressure sensor directly measures blood pressure, so it provides a quality of 1.0 1 in
15309	15331	nearly 11 Mbps, the network is able to support the transmission of all data from each of the sensor sets in FA from Table 1 in real-time. However, if other applications (e.g., video gait monitoring ) are running simultaneously on the network and the personal health monitor application can only utilize 100 kbps of the throughput, the network would not be able to support the transmission of data
15342	15343	field. At the level of detail in this section, prior work such as CHMA  is similar to HRMA , and MAC-SCC  and the MAC protocols implicit in the work of Li et al  and Fitzek et al  are similar to DCA . However, a final related channel hopping technology that is worth mentioning is the definition of FHSS channels in the IEEE 802.11  specification. At first glance, it
15342	15344	data rate is 54 Mbps. This low utilization can be explained by the IEEE 802.11a requirement that the RTS/CTS packets be sent at the lowest supported data rate, 6 Mbps, along with other overheads . 4.1.2 Overhead of an Absent Node SSCH requires more re-transmissions than IEEE 802.11 to prevent logical partitions. These retransmissions waste bandwidth that could have been dedicated to a node
15342	15345	IEEE 802.11 Power Save Mode (PSM) for ad-hoc networks. This implies a relatively stringent reliance on clock synchronization, which is particularly hard to provide in multi-hop wireless networks . In contrast, SSCH does not require tight clock synchronization because SSCH does not have a common control channel or a dedicated neighbor discovery interval. Secondly, synchronization traffic in
15342	15346	all the channels. One straightforward alternative scheme, which still only requires one radio, is to use one of the channels as a control channel, and all the other channels as data channels (e.g., ). Each node must then somehow split its time between the control channel and the data channels. Such a scheme will have difficulty in preventing the con4 3.5 3 2.5 2 1.5 1 0.5 0 Route Length (Hops)
15342	15346	a single channel at any given instance in time, and research that assumes more powerful radio technology, such as multiple NICs  or NICs capable of listening on many channels simultaneously , even if they can only communicate on one. Our work falls in to the former category; the SSCH architecture can be deployed over a single standards-compliant NIC supporting fast channel switching.
15342	15346	data rates is not commercially available. Another line of related work assumes technology by which nodes can concurrently listen on all channels. For example, Nasipuri et al  and Jain et al  assume wireless NICs that can receive packets on all channels simultaneously, and where the channel for transmission can be chosen arbitrarily. In these schemes, nodes maintain a list of free
15342	2290	Nodes: The per-flow throughput on varying the number of flows in the network. uniformly in a 200 × 200 m area, and set each node to transmit with a power of 21 dBm. The Dynamic Source Routing (DSR)  protocol is used to discover the source route between different source-destination pairs. These source routes are then input to a static variant of DSR that does not perform discovery or maintain
15342	2290	Previous work on multi-channel MACs has often overlooked the effect of channel switching on routing protocols. Most of the proposed protocols for MANETs rely heavily on broadcasts (e.g., DSR  and AODV ). However, neighbors using a multi-channel MAC could be on different channels, which could cause broadcasts to reach significantly fewer neighbors than in a single-channel MAC. SSCH
15342	15347	One Microsoft Way Redmond, WA 98052 jdunagan@microsoft.com past decade. One domain where this communication pattern naturally arises is fixed wireless multi-hop networks, such as community networks . Increasing the capacity of such wireless networks has been the focus of much recent research (e.g., ). A natural approach to increase the network capacity is to use frequency diversity
15342	15348	the current state of the field. At the level of detail in this section, prior work such as CHMA  is similar to HRMA , and MAC-SCC  and the MAC protocols implicit in the work of Li et al  and Fitzek et al  are similar to DCA . However, a final related channel hopping technology that is worth mentioning is the definition of FHSS channels in the IEEE 802.11  specification.
15342	15349	does not cover all related work, it does characterize the current state of the field. At the level of detail in this section, prior work such as CHMA  is similar to HRMA , and MAC-SCC  and the MAC protocols implicit in the work of Li et al  and Fitzek et al  are similar to DCA . However, a final related channel hopping technology that is worth mentioning is the
15342	15350	a single channel at any given instance in time, and research that assumes more powerful radio technology, such as multiple NICs  or NICs capable of listening on many channels simultaneously , even if they can only communicate on one. Our work falls in to the former category; the SSCH architecture can be deployed over a single standards-compliant NIC supporting fast channel switching.
15342	15350	of MAC protocol at high data rates is not commercially available. Another line of related work assumes technology by which nodes can concurrently listen on all channels. For example, Nasipuri et al  and Jain et al  assume wireless NICs that can receive packets on all channels simultaneously, and where the channel for transmission can be chosen arbitrarily. In these schemes, nodes maintain
15342	15351	work on multi-channel MACs has often overlooked the effect of channel switching on routing protocols. Most of the proposed protocols for MANETs rely heavily on broadcasts (e.g., DSR  and AODV ). However, neighbors using a multi-channel MAC could be on different channels, which could cause broadcasts to reach significantly fewer neighbors than in a single-channel MAC. SSCH addresses this
15342	15352	to exploiting frequency diversity. In the first category, we find that pseudo-random number generators have been used for a variety of tasks in wireless networking. For example, the SEEDEX protocol  uses pseudo-random generators to avoid RTS/CTS exchanges in a wireless network. Nodes 6.5 5.5 4.5 3.5 2.5 1.5 0.5 -0.5 Route Length (Hops)sbuild a schedule for sending and listening on a network,
15342	15354	Increasing the capacity of such wireless networks has been the focus of much recent research (e.g., ). A natural approach to increase the network capacity is to use frequency diversity . Commodity wireless networking hardware commonly supports a number of orthogonal channels, and distributing the communication across channels permits multiple simultaneous communication flows.
15342	15354	This technique allows control traffic to be distributed across all channels, and thus avoids control channel saturation, a bottleneck identified in prior work on exploiting frequency diversity . • We introduce a second novel technique to achieve good performance for multi-hop communication flows. The partial synchronization technique allows a forwarding node to partially synchronize with
15342	15354	the presence of a single NIC with a single half-duplex transceiver. The only other approach that we are aware of to exploiting frequency diversity under this assumption is Multichannel MAC (MMAC) . Like SSCH, MMAC attempts to improve capacity by arranging for nodes to simultaneously communicate on orthogonal channels. Briefly, MMAC operates as follows: nodes using MMAC periodically switch to
15342	15356	frequency diversity has proposed that each node be equipped 1 2 3 5 4 6 Figure 1: Only one of the three packets can be transmitted when all the nodes are on the same channel. with multiple radios . Multiple radios draw more power, and energy consumption continues to be a significant constraint in mobile networking scenarios. By requiring only a single standards-compliant NIC per node, SSCH
15342	15356	on one. Our work falls in to the former category; the SSCH architecture can be deployed over a single standards-compliant NIC supporting fast channel switching. Dynamic Channel Assignment (DCA)  and Multi-radio Unification Protocol (MUP)  are both technologies that use multiple radios (in both cases, two radios) to take advantage of multiple orthogonal channels. DCA uses one radio on a
15342	15356	detail in this section, prior work such as CHMA  is similar to HRMA , and MAC-SCC  and the MAC protocols implicit in the work of Li et al  and Fitzek et al  are similar to DCA . However, a final related channel hopping technology that is worth mentioning is the definition of FHSS channels in the IEEE 802.11  specification. At first glance, it may seem redundant that
8920780	2243	source utility is then maximized over transmission rates of all sources subject to the capacity constraints. The significance of the utility maximization model is that TCP Reno  and Vegas , as well as several other Internet congestion control algorithms can be interpreted within this model by choosing appropriate utility functions. This approach has been extensively studied in
8920780	2354	other Internet congestion control algorithms can be interpreted within this model by choosing appropriate utility functions. This approach has been extensively studied in several papers, such as , , , , . Solving the utility maximization problem directly requires coordination among possibly all sources, and hence is impractical for the networks. However, there exist
8920780	2354	objective (the primal problem) is to choose the source rates xs so as to: max ? Us(xs) (2) ms?xs?Ms s?S subject to capacity constraints: ? s?Sl xs ? cl This flow control problem is first posed in , and solved in  using techniques of constrained optimization. The problem can also be solved using a penalty function approach, see . Note that a unique maximizer exists, called the primal
8920780	8600	congestion control algorithms can be interpreted within this model by choosing appropriate utility functions. This approach has been extensively studied in several papers, such as , , , , . Solving the utility maximization problem directly requires coordination among possibly all sources, and hence is impractical for the networks. However, there exist decentralized solutions
8920780	8600	? s?Sl xs ? cl This flow control problem is first posed in , and solved in  using techniques of constrained optimization. The problem can also be solved using a penalty function approach, see . Note that a unique maximizer exists, called the primal optimal solution, since the objective function is strictly concave, and the feasible solution set is compact. Even though the objective
8920780	15360	control algorithms can be interpreted within this model by choosing appropriate utility functions. This approach has been extensively studied in several papers, such as , , , , . Solving the utility maximization problem directly requires coordination among possibly all sources, and hence is impractical for the networks. However, there exist decentralized solutions which
8920780	15360	(t) s ) (29) This amounts to updating, both prices (the dual variables), and rates (the primal variables) at the same time to achieve convergence to the global maximum of (2) subject to (3). In , this update along with the price update is called the primal-dual algorithm, and it can be shown that it is globally asymptotically stable in the continuous-time case. A slightly modified version
8920780	15363	yet. In , the continuous time version of REM has been shown to be globally stable, but for the original discrete-time case, a proof of stability is available only for the single-link case, see . The stability analysis of  is based on an invariance argument which does not generalize to multiple links. In this paper, we prove a similar stability result for a single-link network using a
8920780	15363	yet. In , the continuous time version of REM has been shown to be globally stable, but for the original discrete-time case, a proof of stability is available only for the single-link case, see . In this paper, we generalize this proof to the multi-link case using an appropriate Lyapunov function. To simplify notation, for each source s, we denote the restricted to  as (9) inverse
8920780	15363	l belongs. We have already shown that l cannot belong to the set I ? , which leaves us with two cases. 1 Multiple sources can be lumped into a single source with an equivalent utility function, see sCase I (l ? I + , J ? ): In this case, we have ¯p = p + ?b + ?(x ? c) ? 0, and ¯ b = 0, since b + y ? c ? 0. Thus, ? 1 (p + ?b) ? (x ? c) ? ?b (19) ? Substituting the state equation into ?V (b, p),
10039840	14072	actively participate. As Clark andsSchaefer claim, for knowledge co-construction to occur, participants must not only make a contribution, but must also get their contribution accepted by others . This involves a notion of building something together by engaging each of the group members in a common activity – the enhancing of the structure for knowledge sharing. The notion of a common
7481347	3260	that the total traffic in the network does not overwhelm the available resources. Traditional routing protocols make sure that the packets get to their destinations, while QoS routing protocols  make sure that the QoS traffic is well spread 1 This research is funded by the Development Foundation of Shanghai Education Committee (Grant No. 01F10). Shigang Chen Yong Tang Dept. of Com. & info.
7481347	3260	QoS traffic, because there may exist plenty of other paths that can support the required QoS when the shortest path cannot. In the recent years, there was a large body of research on QoS routing , which was designed primarily for a RSVP-like environment. Before a traffic stream is delivered, the sender or the receiver activates the QoS routing protocol to establish a routing path that has
7481347	4115	varied service requirements. The work on QoS support roughly falls in two broad categories: Integrated Services (IntServ) and Differentiated Services (DiffServ). At the heart of IntServ is RSVP . In RSVP, the required resources are reserved at every router along the path of a traffic stream (flow), and hence the performance of the traffic stream is guaranteed. Such a fine level of
7481347	15378	QoS traffic, because there may exist plenty of other paths that can support the required QoS when the shortest path cannot. In the recent years, there was a large body of research on QoS routing , which was designed primarily for a RSVP-like environment. Before a traffic stream is delivered, the sender or the receiver activates the QoS routing protocol to establish a routing path that has
15380	15381	most (least) popular, etc. Such higher-level patterns can help site owners better evaluate the Web site organization. 3.4 Integration of Usage Patterns with Web Content Information Recent studies  have emphasized the benefits of integrating semantic knowledge about the domain (e.g., from page content features, relational structure, or domain ontologies) in the Web usage mining process. The
15380	15382	for capturing theslatent or hidden semantic associations among co-occurring objects is Latent semantic analysis (LSA) . It is mostly used in automatic indexing and information retrieval , where LSA usually takes the (high dimensional) vector space representation of documents based on term frequency as a starting point and applies a dimension reducing linear projection, such as
15380	15383	factors and the two sets of objects. Due to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data
15380	15384	factors and the two sets of objects. Due to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data
15380	4531	to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data which, in this case, is comprised
15380	15385	to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data which, in this case, is comprised
15380	15386	who show a high propensity to buy versus whose who don’t. This, in turn, can lead to automatic tools that can tailor the content of pages for those users accordingly. Web usage mining techniques , which capture Web users’ navigational patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web
15380	15387	can be applied to the transaction data. The discovered patterns may then be analyzed and interpreted for use in such applications as Web personalization. The usage data preprocessing phase  results in a set of n pageviews, P = {p1,p2,... ,pn} and a set of m user sessions, U = {u1,u2,... ,um}. A pageview is an aggregate representation of a collection of Web objects (e.g. pages)
15380	15388	most (least) popular, etc. Such higher-level patterns can help site owners better evaluate the Web site organization. 3.4 Integration of Usage Patterns with Web Content Information Recent studies  have emphasized the benefits of integrating semantic knowledge about the domain (e.g., from page content features, relational structure, or domain ontologies) in the Web usage mining process. The
15380	419	parameters Pr(zk), Pr(ui|zk), Pr(pj|zk), while maximizing the following likelihood L(U, P )oftheobservations,sL(U, P )= m? n? i=1 j=1 w(ui,pj)logPr(ui,pj). Expectation-Maximization (EM) algorithm  is a wellknown approach to performing maximum likelihood parameter estimation in latent variable models. It alternates two steps: (1) an expectation (E) step where posterior probabilities are
15380	15390	factors and the two sets of objects. Due to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data
15380	15391	application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining systems use different data mining techniques, such as clustering, association rule mining, and sequential
15380	15391	most (least) popular, etc. Such higher-level patterns can help site owners better evaluate the Web site organization. 3.4 Integration of Usage Patterns with Web Content Information Recent studies  have emphasized the benefits of integrating semantic knowledge about the domain (e.g., from page content features, relational structure, or domain ontologies) in the Web usage mining process. The
15380	7762	reducing linear projection, such as Singular Value Decomposition (SVD) to generate a reduced latent space representation. Probabilistic latent semantic analysis (PLSA) models, proposed by Hofmann , provide a probabilistic approach for the discovery of latent variables which is more flexible and has a more solid statistical foundation than the standard LSA. The basis of PLSA is a model often
15380	7762	By conducting sensitivity analysis, we chose 30 factors in the case of CTI data and 15 factors for the Realty data. To avoid “overtraining”, we implemented the “Tempered EM” algorithm  to train the PLSA model. 4.2 Examples Usage Patterns Based on the PLSA Models Figure 1 depicts an example of the characteristic pages for a specific discovered task in the CTI data. The first 6
15380	15392	between the hidden factors and the two sets of objects. Due to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information
15380	4535	reducing linear projection, such as Singular Value Decomposition (SVD) to generate a reduced latent space representation. Probabilistic latent semantic analysis (PLSA) models, proposed by Hofmann , provide a probabilistic approach for the discovery of latent variables which is more flexible and has a more solid statistical foundation than the standard LSA. The basis of PLSA is a model often
15380	4535	as well as the following constraints on the two conditional probabilities: and l? ( m? k=1 i=1 l? ( n? k=1 j=1 Pr(ui|zk) ? 1) = 0, Pr(pj|zk) ? 1) = 0. Through the use of Lagrange multipliers (see  for details), we can solve the constraint maximization problem to get the following equations for re-estimated parameters: Pr(zk) = ?m ?n i=1 ?m ?n i=1 j=1 ?m ?n i=1 = ?m i=1 ?n ?m i ? =1 ?m ?m i=1
15380	4535	Pr(zk|u) values, as the primary task(s) performed by this user. For a new user session, unew, not appearing in the historical navigational data, we can adopt a “folding-in” method as introduced in  to generate Pr(task|session) viathe EM algorithm. In the E-step, we compute Pr(z|unew,p)= Pr(p|z)Pr(z|unew) ? z ? Pr(p|z ? )Pr(z ? |unew) , andintheM-step,wefixPr(p|z) and only update Pr(z|unew): ?
15380	9507	for the discovery of latent variables which is more flexible and has a more solid statistical foundation than the standard LSA. The basis of PLSA is a model often referred to as the aspect model . Assuming that there exist a set of hidden factors underlying the co-occurences among two sets of objects, PLSA uses Expectation-Maximization (EM) algorithm to estimate the probability values which
15380	15393	factors and the two sets of objects. Due to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval , text learning , and co-citation analysis . In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data
15380	15394	application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining systems use different data mining techniques, such as clustering, association rule mining, and sequential
15380	16238	, which capture Web users’ navigational patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining
15380	15395	Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining systems use different data mining techniques, such as clustering, association rule mining, and sequential pattern mining to extract
15380	15396	those users accordingly. Web usage mining techniques , which capture Web users’ navigational patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching [30,
15380	15397	most (least) popular, etc. Such higher-level patterns can help site owners better evaluate the Web site organization. 3.4 Integration of Usage Patterns with Web Content Information Recent studies  have emphasized the benefits of integrating semantic knowledge about the domain (e.g., from page content features, relational structure, or domain ontologies) in the Web usage mining process. The
15380	15398	those users accordingly. Web usage mining techniques , which capture Web users’ navigational patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching [30,
15380	15398	our approach with the standard clustering approach for the discovery of Web user segments. In order to compare the clustering approach to the PLSAbased model, we adopt the algorithm presented in  for creating “aggregate profiles” based on session clusters. In the latter approach, first, we apply a multivariate clustering technique such as k-means to user-session data in order to obtain a
15380	15398	admissions online. 4.3 Evaluation of User Segments and Recommendations We used two metrics to evaluate the discovered user segments. The first is called the Weighted Average Visit Percentage (WAVP) . WAVP allows us to evaluate each segment individually according to the likelihood that a user who visits any page in the segment will visit the rest of the pages in that segment during the same
15380	15399	those users accordingly. Web usage mining techniques , which capture Web users’ navigational patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching [30,
15380	15400	Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining systems use different data mining techniques, such as clustering, association rule mining, and sequential pattern mining to extract
15380	15402	25, 27], link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining systems use different data mining techniques, such as clustering, association rule mining, and sequential pattern mining to extract usage patterns from user
15380	15406	can be applied to the transaction data. The discovered patterns may then be analyzed and interpreted for use in such applications as Web personalization. The usage data preprocessing phase  results in a set of n pageviews, P = {p1,p2,... ,pn} and a set of m user sessions, U = {u1,u2,... ,um}. A pageview is an aggregate representation of a collection of Web objects (e.g. pages)
15380	15407	patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web site evaluation or reorganization , Web analytics and ecommerce data analysis , Adaptive Web sites , and Web pre-fetching . Most current Web usage mining systems use different data mining techniques, such as
15380	15408	who show a high propensity to buy versus whose who don’t. This, in turn, can lead to automatic tools that can tailor the content of pages for those users accordingly. Web usage mining techniques , which capture Web users’ navigational patterns, have achieved great success in various application areas such as Web personalization , link prediction and analysis , Web
8920786	16299	have expanded on Boserup’s model by incorporating other exogenous factors that also stimulate endogenous agricultural change. Binswanger and McIntire (1997) and Pingali, Bigot, and Binswanger (1987) argue that increased access to markets, as may result from development of roads or other infrastructure, also causes agricultural intensification. Lele and Stone (1989) argue that government
8920786	16299	interactions with livestock, crops, and forest. Yields and erosion parameters are given by the biophysical Erosion Productivity Impact Calculator (EPIC) model developed by Williams, Jones, and Dyke (1987). The model maximizes the aggregate utility of the whole microwatershed over a five-year planning horizon. 9 Utility is defined as the discounted value of future net monetary incomes plus the
14155713	4460	supporting such postulate is Michelson-Morley’s (M-M) celebrated experiment. However, this author has recently argued that M-M-type experiments are actually consistent with absolute space.  In this context, in this paper we explore, once again, the possible existence of absolute space , that is, the existence of a preferred frame of reference ? where APEIRON Vol. 5 Nr.3-4,
15472	15477	properties of these models mechanically. We have extended this tool to allow system behaviour to be specified by means of sets of scenarios described in the form of MSCs. This work is described in . Using these techniques, we are able to build models of the behaviour of systems, which include the user, built up from scenarios gathered during the requirements elicitation process. In this way
15472	15478	which at least approximates that which they would use in the final system. Animation has previously been used to make it easier to interpret the meaning of traces returned from model checking. A trace to deadlock for example may 5sbe illustrated in the problem domain by replaying the relevant sequence of actions as a graphical animation. In order to simulate web applications, and to
15472	15478	behaviour models, we need to develop techniques that enable us to convey the meaning of the model, in terms of the actual system that it is intended to represent, to an end user. As discussed in , there is a lot to be gained from using graphic animations to communicate the results of analysing formal models of systems. One example of an animation technique which can be used with LTSA is
15472	15482	a plugin to be used with LTSA’s extension mechanism, as with the Message Sequence Chart extensions. The benefits of the approach to simulation given here as compared to, for instance, that taken in  are that our simulation tools work with our existing behaviour modelling tools without having to change the representation in any way, and that the appearance of the interface to the simulation can
15483	15484	typically referred to as atoms. Thus, we can think of our signal as a molecule, and the forward transform decomposes it to its building atoms, where we try to use the fewest in this construction . From the numerical standpoint, the forward transform, defined as (P0), is a non-convex and highly non-smooth optimization problem, with many possible local minimum points. Prior work has
15483	15484	new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result - Worst-Case Analysis We consider the problem (P0) min ? ???0 subject
15483	15484	it is driven via the representations. A commonly used regularization in inverse problem forces sparsity of the representation of the unknown signal, and assumes independence in its coefficients . Such regularization is essentially the manifestation of a PDF on the unknown signal. Thus, generating representations following these rules, signals emerge with a PDF being a mixture of Gaussians.
15483	15484	better representations (with cardinality strictly smaller than 8) could be found in such cases. Thus, the range ???0 ?  is to be experimented on. In our experiment we cover the interval  disregarding the lower part being theoretically guaranteed by a known result. We can generate many such representations by first choosing the k non-zero locations at random with uniform
15483	15484	arithmetic accuracy threshold, a candidate representation is assumed found. We have conducted this experiment as described, and Figure 1 documents its results. Per every cardinality in the range  we performed 100 experiments and we present the relative number of experiments that ended with a perfect success (no sparser solution is found) and also the relative number of experiments that
15483	15488	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15488	8 column combinations from D. Nevertheless, it is interesting to see whether better representations (with cardinality strictly smaller than 8) could be found in such cases. Thus, the range ???0 ?  is to be experimented on. In our experiment we cover the interval  disregarding the lower part being theoretically guaranteed by a known result. We can generate many such representations by
15483	15489	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15489	So, our question focuses on the cases where uniqueness holds true, and ask whether the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness
15483	15490	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15490	So, our question focuses on the cases where uniqueness holds true, and ask whether the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness
15483	15492	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15492	So, our question focuses on the cases where uniqueness holds true, and ask whether the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness
15483	15493	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15493	So, our question focuses on the cases where uniqueness holds true, and ask whether the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness
15483	15493	rather than exact ones are appealing as well for many applications. A parallel study of the uniqueness of such representations is of great importance as well, extending prior results given in . 17sProbability of Success 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Basis Pursuit Ortho. Matching Pursuit 0 0 5 10 15 Cardinality of representation Figure 7: The probability of success (recovering at
15483	15494	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15494	So, our question focuses on the cases where uniqueness holds true, and ask whether the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness
15483	15496	study of this problem and methods to approximate its solution give promising new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result -
15483	15496	So, our question focuses on the cases where uniqueness holds true, and ask whether the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness
15483	15497	new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result - Worst-Case Analysis We consider the problem (P0) min ? ???0 subject
15483	15497	the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness property, showing that if the signal has a sparse enough representation, the pursuit
15483	15498	new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result - Worst-Case Analysis We consider the problem (P0) min ? ???0 subject
15483	15498	to discuss the more common case where Spark(D) ? N. This case refers to dictionaries generated from overcomplete wavelets, ridgelets, curvelets, many other types of frames, and amalgams of them . This is the more realistic scenario, paralleling the second experiment form Section 2. In this case we can guarantee uniqueness for representations satisfying ???0 < Spark(D)/2. This time the
15483	15498	the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness property, showing that if the signal has a sparse enough representation, the pursuit
15483	15498	Another promising direction for future research is the analysis of pursuit algorithms using the same probabilistic model drawn here, extending the results in  . Simulation results here and in  indicate that these algorithms are expected to perform far better than the worst-case bounds suggest. A similar analysis could shed light on this behavior. Approximate representations rather than
15483	15500	new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result - Worst-Case Analysis We consider the problem (P0) min ? ???0 subject
15483	15500	to discuss the more common case where Spark(D) ? N. This case refers to dictionaries generated from overcomplete wavelets, ridgelets, curvelets, many other types of frames, and amalgams of them . This is the more realistic scenario, paralleling the second experiment form Section 2. In this case we can guarantee uniqueness for representations satisfying ???0 < Spark(D)/2. This time the
15483	15500	the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness property, showing that if the signal has a sparse enough representation, the pursuit
15483	15501	new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result - Worst-Case Analysis We consider the problem (P0) min ? ???0 subject
15483	15501	the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness property, showing that if the signal has a sparse enough representation, the pursuit
15483	15503	new results, indicating that even though complicated, means exist to solve it at least in some cases using either greedy  or convex programming approaches . One aspect of these recent work is the result of uniqueness which will be the focus of this paper. 1.2 The Uniqueness Result - Worst-Case Analysis We consider the problem (P0) min ? ???0 subject
15483	15503	the pursuit algorithms succeed. Previous work analyzed this question for several variants of the greedy algorithm . Other work studied the Basis Pursuit algorithms . All these work concentrated on the worst-case scenario, just as described above with respect to the uniqueness property, showing that if the signal has a sparse enough representation, the pursuit
15483	15505	This is a general property for such random matrices, stemming from the fact that square random matrices are non-singular with probability 1 (see the seminal work by Edelman and later by Shen  on the probabilistic behavior of the extreme singular values of such matrices). Based on the known uniqueness result, every representation with less than Spark(D)/2 = 4.5 non-zeros must be unique.
15483	5282	to discuss the more common case where Spark(D) ? N. This case refers to dictionaries generated from overcomplete wavelets, ridgelets, curvelets, many other types of frames, and amalgams of them . This is the more realistic scenario, paralleling the second experiment form Section 2. In this case we can guarantee uniqueness for representations satisfying ???0 < Spark(D)/2. This time the
8920807	16358	obstruents devoice before the nasal a. this is the proof that the nasal is not in Coda, but in post-Coda position. b. in German, obstruents devoice in both final and internal Codas (e.g. Brockhaus (1995): Freund-e  &quot;friends&quot; vs. Freund  &quot;friend&quot; freund-lich  &quot;friendly&quot; c. 1. recall that in CVCV, a consonant in a Coda identifies as occurring before a governed empty
21262	15518	algorithms also rely heavily on table lookups for security. Of the five algorithms that were the finalists in the Advanced Encryption Standard (AES) effort (MARS , RC6 , Rijndael , Serpent , and Twofish ), all except RC6 used table lookups. For some of these algorithms, table lookups are used for optimization purposes – beyond achieving confusion. Rijndael, for instance, is the AES
21262	15521	by 4. The use of the load.ex.sc in this way completely eliminates any overhead associated with table lookups (Figure 2). A similar, but more restrictive approach has previously been described in . In that study, it was noted that if the tables were aligned to 1kB memory blocks, the effective address can very simply be calculated by a concatenation of the shifted index bits and the base
1819852	15524	from object-orientation of encapsulation, polymorphism, and inheritance. SimJAVA uses a basic discrete event simulation engine and extends this with Java's graphical user interface features. Silk, (Healy and Kilgore 1997) is a process-based language built in Java. Silk also uses object-oriented concepts to facilitate reuse of components and also takes advantage of the multithreading capabilities of Java. MOOSE
15545	4400	graph has to be processed as fast as possible.This scenario arises in many practical applications, including route planning for car traffic , database queries , Web searching , and time-table information in public transport .The algorithmic core problem consists in performing Dijkstra’s shortest path algorithm using appropriate speed-up techniques. Our initial
15545	15547	arises in many practical applications, including route planning for car traffic , database queries , Web searching , and time-table information in public transport .The algorithmic core problem consists in performing Dijkstra’s shortest path algorithm using appropriate speed-up techniques. Our initial interest in the problem stems from our previous work on
15545	15547	input instance.Distance-preserving algorithms were not in wide use in traffic information systems, mainly because the average response time was perceived to be unacceptable.However, the results in  showed that distance-preserving variants of Dijkstra’s algorithm are competitive in the sense that they do not constitute the bottleneck operation in the above scenario. These are the only
15545	15547	techniques of Dijkstra’s algorithm.The recent work in  investigates multi-criteria shortest path problems for computing Pareto optimal solutions in the above scenario.All these publications  are the only ones known to us regarding algorithms for wide-area railway traffic information systems. Related work is known for other traffic engineering systems, concerning mainly local public
15545	15548	a large number of on-line shortest path queries in a huge graph has to be processed as fast as possible.This scenario arises in many practical applications, including route planning for car traffic , database queries , Web searching , and time-table information in public transport .The algorithmic core problem consists in performing Dijkstra’s shortest path algorithm using
15545	15548	wide-area railway traffic information systems. Related work is known for other traffic engineering systems, concerning mainly local public transport , or private transport in wide-area networks . For various reasons (see e.g., ) the techniques in those papers cannot be directly applied to wide-area railway traffic information systems. Several of the approaches used so far in traffic
15545	15549	are hierarchical in the sense that the decomposition may be repeated recursively. Several theoretical results on shortest paths, regarding planar graphs  and graphs of small treewidth , are based on the same intuition. So far, however, there exists no systematic evaluation of hierarchical decomposition techniques, especially when concrete application scenarios are considered.In
15545	15549	are hierarchical in the sense that the decomposition may be repeated recursively. Several theoretical results on shortest paths, regarding planar graphs  and graphs of small treewidth , are based on the same intuition. So far, however, there exists no systematic evaluation of hierarchical decomposition techniques, especially when concrete application scenarios are considered.In
15545	4414	a large number of on-line shortest path queries in a huge graph has to be processed as fast as possible.This scenario arises in many practical applications, including route planning for car traffic , database queries , Web searching , and time-table information in public transport .The algorithmic core problem consists in performing Dijkstra’s shortest path algorithm using
15545	4414	wide-area railway traffic information systems. Related work is known for other traffic engineering systems, concerning mainly local public transport , or private transport in wide-area networks . For various reasons (see e.g., ) the techniques in those papers cannot be directly applied to wide-area railway traffic information systems. Several of the approaches used so far in traffic
15545	15554	shortest paths by edges.The techniques are hierarchical in the sense that the decomposition may be repeated recursively. Several theoretical results on shortest paths, regarding planar graphs  and graphs of small treewidth , are based on the same intuition. So far, however, there exists no systematic evaluation of hierarchical decomposition techniques, especially when concrete
15545	4430	queries in a huge graph has to be processed as fast as possible.This scenario arises in many practical applications, including route planning for car traffic , database queries , Web searching , and time-table information in public transport .The algorithmic core problem consists in performing Dijkstra’s shortest path algorithm using appropriate speed-up
8920818	15566	mechanism of molecular recognition and the regulation of gene expression. We have developed an electronically freely accessible thermodynamic database for protein-nucleic acid interactions, ProNIT , which contains several experimentally observed thermodynamic data along with experimental conditions, methods, sequence, structure and the other functional details. In order to understand the
1604537	15576	NSF Grant OSR-9553368, DARPA Grant DAAH04-96-1-0329 and GSA Grant ACT#: K96130308.sAssociation Rule Mining on Remotely Sensed Images Using Peano Count Trees 1. Introduction Association rule mining  is one of the important advances in the area of data mining. The initial application of association rule mining was on market basket data. Recently study on association rule mining has been
1604537	15578	we target on general association rule mining problems by considering both the support and confidence of the rules. The P-tree structure is related to some other structures including quadtrees  (and its variants point quadtrees  and region quadtrees ), and HHcode . The similarities between P-trees, quadtrees and HH-Codes are that they are quadrant based. The difference is that
1604537	15583	NSF Grant OSR-9553368, DARPA Grant DAAH04-96-1-0329 and GSA Grant ACT#: K96130308.sAssociation Rule Mining on Remotely Sensed Images Using Peano Count Trees 1. Introduction Association rule mining  is one of the important advances in the area of data mining. The initial application of association rule mining was on market basket data. Recently study on association rule mining has been
1604537	15583	on the user’s requirement. 4. Experiment Results and Performance Analysis In this section, we compare our work with the Apriori algorithm , and a recently proposed efficient algorithm, FP-growth , which does not have the candidate generation step. The experiments are performed on a 900-MHz PC with 256 megabytes main memory, running Windows 2000. We set our algorithm to find all the frequent
1604537	15584	Nitrate and Moisture to produce higher yield potential. 5. Related work and Discussions Remotely Sensed Imagery data belongs to the category of spatial data. Some works on spatial data mining are , including Association Rule Mining on Spatial data . A spatial association rule P-ARM 10sis a rule indicating that a certain association relationship exists among a set of spatial and possibly
1604537	15588	information to facilitate efficient association rule mining. Wavelets also provide a good way for compressing data, but unlike Ptrees, they do not facilitate pixel-by-pixel mining. The work in  presents the ideas of using AD-trees (All-Dimension trees) for machine learning with large datasets. AD-trees are a sparse data structure for representing the cached counting statistics for
8920822	15596	Texturebased features have been proven to be effective for tasks such as segmentation and similarity retrieval. The use of texture for content-based access has been explored by several researchers (, for example). An image can be considered as a mosaic of textures and texture features associated with the regions can be used to index the image data for searching and browsing. Several methods
8920822	15597	can be used to index the image data for searching and browsing. Several methods have been used thus far to extract texture features. The prominent ones are based on a) random field texture models , and b) multiscale filtering methods . Among these, a widely used feature  is that computed using multiscale Gabor filThis research was supported in part by the following grants and
8920822	15602	This choice is motivated by several factors. The Gabor representation has been shown to be optimal in the sense of minimizing the joint two-dimensional uncertainty in space and frequency , and thus are well suited for texture segmentation problems. Furthermore, Gabor filters approximate the characteristics of certain cells in the visual cortex of some mammals . The high
8920822	15605	of S.N. and R.E., we use an aerial image dataset with 90,744 subimages of 128×128 pixels. f? is computed for each subimage (using previously computed MPEG-7 features) and a standard VA file index  (with S bits per dimension) is constructed for the dataset. The indexing efficiency is quantified by the number of candidates (the smaller the better) obtained after VA filtering for K nearest
15606	15607	with these collections and to do so in a timely manner. As a result, content-based image retrieval (CBIR) from unannotated image databases has been a fast growing research area recently: see  for a recent extensive review on the subject. We consider a simple architecture of a typical CBIR system (Fig. 1), where there are two major tasks. The first one is feature Manuscript received
15606	15607	RETRIEVAL 147 hand, many current retrieval systems take a simple approach by using typically norm-based distances (e.g., Euclidean distance) on the extracted feature set as a similarity function . The main premise behind these CBIR systems is that given a “good set” of features extracted from the images in the database (the ones that significantly capture the content of images) then for two
15606	15607	to Histogram Methods Histograms have been used since the early days of image retrieval, especially for representing color features , as well as for texture or local geometric properties . In this section, we demonstrate that the histogram method can be interpreted through our statistical approach by using an appropriate model setup. Let us partition the range of image data into
15606	15608	In this work, we focus on the use of texture information for image retrieval. Some of the most popular texture extraction methods for retrieval are based on filtering or wavelet-like approaches ???. Essentially, these methods measure energy (possibly weighted) at the output of filter banks as extracted features for texture discrimination. The basic assumption of these approaches is that
15606	15608	have recently emerged as an effective tool to analyze texture information as they provide a natural partition of the image spectrum into multiscale and oriented subbands via efficient transforms ???. Furthermore, since wavelets are used in major future image compression standards  and are also shown to be prominent in searching for images based on color and shape , , a
15606	15617	according to perceived similarity. The detected perceptual criteria and rules for similarity judgment from this type of subjective experiments can be used in building image retrieval system . On the othersDO AND VETTERLI: WAVELET-BASED TEXTURE RETRIEVAL 147 hand, many current retrieval systems take a simple approach by using typically norm-based distances (e.g., Euclidean distance) on
15606	15619	using a method of moment matching, and the similarity function is again defined as weighted Euclidean distances on extracted model parameters. Independently of our work, Vasconcelos and Lippman  recently took a similar approach where they introduced a probabilistic formulation of the CBIR problem as a common ground for several currently used similarity functions. As an important case of
15606	15624	transforms –. Furthermore, since wavelets are used in major future image compression standards  and are also shown to be prominent in searching for images based on color and shape , , a wavelet-based texture retrieval system can be used effectively in conjunction with a compression system and retrieval systems using other image features (see Fig. 2). Using the assumption
15606	15626	the th subband, typically the following two values are used as features: and (10) (11) On the other hand, statistical approaches treat texture analysis as a probability inference problem (e.g., see ). A natural extension of the energy method is to model a texture by the marginal densities of wavelet subband coefficients. This is justified by recent psychological research on human texture
15606	15626	by matching the histograms of filter responses from a wavelet-liked transform. More accurate texture models can be obtained via a fusion of marginal distributions using minimax entropy principles  or by taking into account the joint statistics of wavelet coefficients across subbands . However, considering complexity as a major constraint in the image retrieval application, in this work
15606	15628	which suggests that two homogeneous textures are often difficult to discriminate if they produce similar marginal distributions of responses from a bank of filters . In fact, Heeger and Bergen  successfully synthesized many natural looking 2 This is an abuse of terminology since strictly speaking v norm is not an energy function. Sometimes it is chosen due to its simplicity. Results from
15606	15629	texture models can be obtained via a fusion of marginal distributions using minimax entropy principles  or by taking into account the joint statistics of wavelet coefficients across subbands . However, considering complexity as a major constraint in the image retrieval application, in this work we simply characterize texture images via marginal distributions of their wavelet subband
15606	13929	density of coefficients at a particular subband produced by various type of wavelet transforms may be achieved by adaptively varying two parameters of the generalized Gaussian density (GGD) , , , , which is defined as (12) where is the Gamma function, i.e., , . Here models the width of the PDF peak (standard deviation), while is inversely proportional to the decreasing rate
15606	13929	of interest to know the common range for the values of in GGDs for texture images. For typical natural images which are dominated by smooth regions, the values for are found to be between 0.5 and 1 . Fig. 7 shows the histogram of the estimated values of from our database of 640 texture images using the method described in Section III-B. The discrete wavelet transform of three levels using
15606	15631	coefficients at a particular subband produced by various type of wavelet transforms may be achieved by adaptively varying two parameters of the generalized Gaussian density (GGD) , , , , which is defined as (12) where is the Gamma function, i.e., , . Here models the width of the PDF peak (standard deviation), while is inversely proportional to the decreasing rate of the peak.
15606	15631	That means we model the wavelet coefficients using the following single parameter distribution family (19) This simplified model is often used in practical image processing problems like denoising . From the sample data sequence , the extracted feature is just the estimated parameter as given in (15) (20) The KLD between two PDFs from the family given in (19) is (21) On the other hand,
15606	15597	visually examining the images of retrieval results. However, this can only be based on a subjective perceptual similarity since there exists no “correct” ordering that is agreed upon by all people . Fig. 10 shows some examples of retrieval results to demonstrate the capability of our method. In Fig. 10(a), the query image is “leaves.” The system almost perfectly retrieves all images of the
15606	15597	scales and orientations using hidden Markov models. Furthermore, we can extend the statistical model for texture using the Wold theory which was shown to closely match human texture perception . As shown in Section II-B, the popular histogram method fits into our scheme. Thus beyond texture, color and local shape features can also be captured. Finally, assuming that different feature sets
15606	15634	effectively here for modeling the coefficients from the wavelet transforms and wavelet frames and can applied to other similar filtering schemes such as wavelet packets and Gabor transforms. In ???, we employed the statistical framework tosDO AND VETTERLI: WAVELET-BASED TEXTURE RETRIEVAL 157 more complex texture models that aggregate wavelet descriptors across scales and orientations
15637	15639	cost-effectiveness of test suites rather than on debugging processes. Selective regression testing is concerned with determining an optimal set of tests to run after a software change is made  . Although there are some similarities with the work described in this paper, the emphasis is quite different: Instead of selecting which tests to run, we analyse the set of tests that have failed,
15637	15640	obtain trace information on the messages being sent. The exact instrumentation mechanism to obtain the information depends on the implementation language. We used the concept of method-wrappers (), where the methods looked up in the method dictionary are replaced by wrapped versions, which can trigger some actions before or after a method is executed. Here the method wrapper simply stores
15637	15640	frrely available tool CodeCrawler  to visualize the information we obtained. We obtain the trace information by using AspectS , a flexible tool which builds upon John Brant’s MethodWrappers . Though AspectS obtains the traces in the same way as method-wrappers described before, we used AspectS because it lets us obtain more detailed information about the current state of the stack,
15637	15641	of the tested program whereas our approach can in principle be used without having prior versions, as we could also order the tests using only the coverage of the failed tests. Zeller et al.   use delta debugging to simplify test case input, reducing relevant execution states and finding failure-inducing changes. We focus on reducing failing tests from a set of semantically different
15637	15643	the likelihood that failures will occur early in test runs.The tests are prioritized using different criteria, the criterion which most closely matched our approach was total function coverage . Here a program is instrumented, and, for any test case, the number of functions in that program that were exercised by that test case is determined. The test cases are then prioritized according
15637	15644	tests while they were writing them. 1. MagicKeys 3 , an application that makes it easy to graphically view, change and export/import keyboard bindings in VisualWorks Smalltalk. 2. Van (Gîrba et al. ), a version analysis tool built on top of the Moose Reengineering Environment . 3. SmallWiki (Renggli ), a collaborative content management tool. 4. CodeCrawler (Lanza ), a language
15637	15646	or post-condition or invariant, thus relying solely on the assertions in their unit tests. This is a common behavior of Smalltalk developers today: The open source Smalltalk environment Squeak 4  in the version from February 2004 includes 1024 unit tests but only 23 pre- or post-conditions. Methods like XP suggest frequent testing and developing in small increments, so that developers can
15637	15647	as method-wrappers described before, we used AspectS because it lets us obtain more detailed information about the current state of the stack, when a method is entered. In Java we could use AspectJ . 4. Case studies We performed our experiments on the following four systems, which were created by four different developers, who were unaware of our attempts to structure their tests while they
15637	15648	in VisualWorks Smalltalk because • tools to wrap methods and assess coverage are freely available, • we have numerous case studies available, • we can build on the frrely available tool CodeCrawler  to visualize the information we obtained. We obtain the trace information by using AspectS , a flexible tool which builds upon John Brant’s MethodWrappers . Though AspectS obtains the traces
15637	15648	Van (Gîrba et al. ), a version analysis tool built on top of the Moose Reengineering Environment . 3. SmallWiki (Renggli ), a collaborative content management tool. 4. CodeCrawler (Lanza ), a language independent reverse engineering tool which combines metrics and software visualization. 4.1. Setup of the experiments In a first phase, we ordered the unit tests for each case study as
15637	15650	in parallel, thereby providing developers with constant feedback. The most popular unit testing framework used in TDD named XUnit  does not currently prioritize failed unit tests. Parrish et al.  define a process for test-driven development that starts with fine-grained tests and proceeds to more coarse-grained tests. They state that “Once a set of test cases is identified an attempt is
15637	15651	and export/import keyboard bindings in VisualWorks Smalltalk. 2. Van (Gîrba et al. ), a version analysis tool built on top of the Moose Reengineering Environment . 3. SmallWiki (Renggli ), a collaborative content management tool. 4. CodeCrawler (Lanza ), a language independent reverse engineering tool which combines metrics and software visualization. 4.1. Setup of the
15637	15652	easier to localize.” In their approach, tests are written beforehand with a particular order in mind, while in our approach we investigate a posteriori orderings of existing tests. Rothermel et al.  introduce the term “granularity” for software testing, but they focus on cost-effectiveness of test suites rather than on debugging processes. Selective regression testing is concerned with
15637	15653	on cost-effectiveness of test suites rather than on debugging processes. Selective regression testing is concerned with determining an optimal set of tests to run after a software change is made  . Although there are some similarities with the work described in this paper, the emphasis is quite different: Instead of selecting which tests to run, we analyse the set of tests that have
15637	15654	the emphasis is quite different: Instead of selecting which tests to run, we analyse the set of tests that have failed, and suggest which of these should be examined first. Test case prioritization  has been successfully used in the past to increase the likelihood that failures will occur early in test runs.The tests are prioritized using different criteria, the criterion which most closely
15637	15655	The test cases are then prioritized according to the total number of functions they cover by sorting them in order of total function coverage achieved, starting with the highest. Wong et al.  compare different selection strategies for regression testing and propose a hybrid approach to select a representative subset of tests combining modification based selection, minimization and
15637	15656	of the tested program whereas our approach can in principle be used without having prior versions, as we could also order the tests using only the coverage of the failed tests. Zeller et al.   use delta debugging to simplify test case input, reducing relevant execution states and finding failure-inducing changes. We focus on reducing failing tests from a set of semantically different
314614	15659	input. The key pair consists of the private key x and the public key Ps. The techniques given here can be applied to key pair generation for other IBS schemes including Blind Signature Scheme , Multisignature Scheme , Aggregate Signature , Bilinear Verifiably Encrypted Signature , ID-Based Blind Signature Scheme  and ID-Based Signature from Pairing  among others. A
314614	15660	x and the public key Ps. The techniques given here can be applied to key pair generation for other IBS schemes including Blind Signature Scheme , Multisignature Scheme , Aggregate Signature , Bilinear Verifiably Encrypted Signature , ID-Based Blind Signature Scheme  and ID-Based Signature from Pairing  among others. A summary of each of these schemes can be found in the
314614	8495	use this biometric based key string and an elliptic curve point embedding technique  to create the public key and corresponding private key. We then make use of a pairing based signature scheme  to perform signing and verification with these keys. We describe a possible attack on this system and suggest ways to combat it. Finally we describe how such a biometric signature scheme can be
314614	8495	with in the next section. 4 Key Pair Generation This section details how the biometric data is used in the generation of a key pair for use in an IBS scheme such as the BLS Short Signature Scheme . First we show how the biometric is embedded onto a point on the elliptic curve. Then, we show how that point is used as part of the key pair generation for the signature scheme. The “extracted”
314614	8495	Dutta et al . 5 Incorporating into an Identity Based Signature scheme An overview of the BLS Short Signature Scheme is given in this section. This scheme was proposed by Boneh, Lynn and Shacham  and consists of three stages. The first stage is the Key Generation stage. This stage was discussed above and should result in a point Pb and a key pair (Ps, x). The second stage is the Signing
314614	15661	on a fuzzy commitment scheme in . We now give a simplified outline of how such an extractor is constructed. A comprehensive account appears in  more on the other metrics can be found in . First we give the formal definition of a fuzzy extractor. Let M be a finite dimensional metric space consisting of biometric data points, with a distance function dis: M × M ? Z + , which
314614	15662	a legal dispute over whether a contract had been signed or not by a user. A biometric reading provided by the alleged signer would be enough to verify the signature. We make use of Fuzzy extractors  to generate a key string from a biometric measurement. We use this biometric based key string and an elliptic curve point embedding technique  to create the public key and corresponding private
314614	15662	mappings. 3 Generating Key data from Biometrics Using biometric data as a basis for cryptographic keys is problematic as biometric measurement is not perfectly reproducible. Recent work by Dodis  demonstrates how such data can be used to generate strong keys for any kind of cryptographic application. They use the notion of a fuzzy extractor to describe the process of extracting a random
314614	15662	enable the recovery of U from b ? the fuzzy extractor also outputs a public string V . The extractor is structured in such a way that the public value V does not leak any information about U. Dodis  describe three metrics to measure the variation in the biometric reading: Hamming Distance, Set Difference and Edit Distance. They then detail the construction of fuzzy extractors using these
314614	15662	Hamming Distance metric is based on previous work on a fuzzy commitment scheme in . We now give a simplified outline of how such an extractor is constructed. A comprehensive account appears in  more on the other metrics can be found in . First we give the formal definition of a fuzzy extractor. Let M be a finite dimensional metric space consisting of biometric data points, with
314614	15664	Encrypted Signature , ID-Based Blind Signature Scheme  and ID-Based Signature from Pairing  among others. A summary of each of these schemes can be found in the survey by Dutta et al . 5 Incorporating into an Identity Based Signature scheme An overview of the BLS Short Signature Scheme is given in this section. This scheme was proposed by Boneh, Lynn and Shacham  and consists
314614	8502	Blind Signature Scheme , Multisignature Scheme , Aggregate Signature , Bilinear Verifiably Encrypted Signature , ID-Based Blind Signature Scheme  and ID-Based Signature from Pairing  among others. A summary of each of these schemes can be found in the survey by Dutta et al . 5 Incorporating into an Identity Based Signature scheme An overview of the BLS Short Signature Scheme
314614	15665	may be more efficient for particular biometrics and applications. The fuzzy extractor construction using the Hamming Distance metric is based on previous work on a fuzzy commitment scheme in . We now give a simplified outline of how such an extractor is constructed. A comprehensive account appears in  more on the other metrics can be found in . First we give the formal
314614	15666	on a fuzzy commitment scheme in . We now give a simplified outline of how such an extractor is constructed. A comprehensive account appears in  more on the other metrics can be found in . First we give the formal definition of a fuzzy extractor. Let M be a finite dimensional metric space consisting of biometric data points, with a distance function dis: M × M ? Z + , which
314614	15672	for other IBS schemes including Blind Signature Scheme , Multisignature Scheme , Aggregate Signature , Bilinear Verifiably Encrypted Signature , ID-Based Blind Signature Scheme  and ID-Based Signature from Pairing  among others. A summary of each of these schemes can be found in the survey by Dutta et al . 5 Incorporating into an Identity Based Signature scheme An
15673	15678	to heuristic evaluation. Research finds that one evaluator is not enough to find all problems and the more expertise the evaluator has in usability and the field the more problems he will find . Jakob Nielsen recommends using three to five evaluators to improve the effectiveness of the method. We are only novice evaluators and therefore we will probably only find 40% of the problems on a
8920826	15721	midden; and nest maintenance work, the construction and clearing of chambers inside the underground nest. Tasks are interdependent; numbers engaged in one task depend on numbers engaged in another . Ants switch tasks, though not all transitions are possible. In harvester ants, task switching funnels ants into foraging and away from tasks inside the nest . An ant’s decision whether to
8920826	15721	for another 10 to 15 years. The behavior of older, larger colonies, of about 10,000 ants, is more stable to perturbation and more homeostatic than that of younger, smaller ones of about 3,000 ants . Because individual ants live only a year, this cannot be due to the experience of older ants. The simplest hypothesis is that individual decision rules are the same in young and old colonies, but
8920826	15724	of the nest. The rate at which one ant encounters others influences its task decisions. The pattern of interactions among ants as they move around can be seen as a kind of ad hoc, dynamical network . Deborah M. Gordon is from the Department of Biological Sciences, Stanford University, Stanford, CA 94305-5020; e-mail: gordon@ants.stanford.edu. © 2003 Wiley Periodicals, Inc., Vol. 8, No. 1
8920826	15736	mostly been used to predict the formation and shape of foraging trails (e.g., ). Versions of these models have been applied to more general AI problems such as the traveling salesman problem . Nest construction by wasps has also been studied theoretically and empirically . There have been few attempts to model formally the allocation of workers among different tasks. One approach to
8832677	15774	In the original Time Warp algorithm, every LP would store its entire state before executing each message. Other techniques, such as Incremental State Saving (Gomes 1996), Infrequent State Saving (Lin and Preiss 1991), Rollback Relaxation (Umamageswaran et al. 1998) and Lookback (Cheng and Szymanski 2003) are “state-based” approaches to reducing these costs. In this article, we investigate an alternative
8832677	15775	in distance. Applying the principle of executing the common case fast, several rollback algorithms, such as Incremental StateSaving (Gomes 1996) and Infrequent State-Saving (Line and Preiss 1991; Lin et al. 1993) and Lookback (Chen and Szymanski 2003) have reduced either the preparation Michael D. Peters Christopher D. Carothers Department of Computer Science Rensselaer Polytechnic Institute Troy, NY
8832677	15779	garbage collection. Other applications for reversible execution are in the areas of database transaction support, debugging support and checkpointing for high-availability software (Leeman, 1986; Sosic, 1994; Biswas and Mall, 1999). More recent work is concerned with source to source translation of popular high-level languages, such as C, to realize reversible programs. However, almost all of the
8832677	15780	LP would store its entire state before executing each message. Other techniques, such as Incremental State Saving (Gomes 1996), Infrequent State Saving (Lin and Preiss 1991), Rollback Relaxation (Umamageswaran et al. 1998) and Lookback (Cheng and Szymanski 2003) are “state-based” approaches to reducing these costs. In this article, we investigate an alternative computationbased technique called reverse computation.
14457605	15801	“B”; otherwise, respond “A.” (4) Humans rarely behave optimally, but appear to use the same strategy as the optimal classifier (see, e.g., Ashby & Maddox 1990, 1992; Busemeyer & Myung, 1992; Erev 1998). Ashby and colleagues proposed decisionbound theory, which assumes that the observer attempts to use the same strategy as the optimal classifier, but with less success due to the effects of
14457605	15801	of the change in the rate of reward, with larger changes in rate being associated with faster, more nearly optimal decision criterion learning (see, e.g., Busemeyer & Myung, 1992; Dusoir, 1980; Erev, 1998; Erev, Gopher, Itkin, & Greenshpan, 1995; Kubovy & Healy, 1977; Roth & Erev, 1995; Thomas, 1975; Thomas & Legge, 1970). To formalize this hypothesis, we construct the objective reward function,
14457605	15819	separately for each observer. All analyses were performed at the individual observer level because of concerns with modeling aggregate data (see, e.g., Ashby, Maddox, & Lee, 1994; Estes, 1956; Maddox, 1999; Maddox & Ashby, 1998; J. D. Smith & Minda, 1998). Method Observers. Eight observers were solicited from the University of Texas community. All observers claimed to have 20/20 vision or vision
14457605	15820	then respond “B”; otherwise, respond “A.” (5) Early applications of decision-bound theory freely estimated a suboptimal decision criterion, b, from the data (i.e., b replaced b o in Equation 5; see Maddox, 2002, for a review). One weakness of this approach is that no mechanism was formalized to guide decision criterion placement. Maddox and Dodd (2001) offered a formal theory of decision criterion
14457605	15820	placement in simultaneous base-rate/payoff conditions. GENERAL DISCUSSION Matching Versus Maximizing in Decision Criterion Learning All of our previous work with the hybrid model framework (see Maddox, 2002, for a review) assumed a COBRA. Even so, an important debate in the psychological literature is that between matching and maximizing strategies. This debate has not escaped the perceptual
14457605	15821	is a competition between reward maximization and probability matching (COBRM). 1 The longstanding interest in the matching– maximizing dichotomy makes this an important issue (see, e.g., Ashby & Maddox, 1993; Estes, 1976; Herrnstein, 1961, 1970; Herrnstein & Heyman, 1979; Williams, 1988). We compare versions of the hybrid model that instantiate COBRA with versions that instantiate COBRM. To anticipate,
14457605	15821	of the COBRA and COBRM hypotheses, we decided to partition the maximum likelihood fit summed over blocks into a separate fit value for each block. Previous work by Ashby and colleagues (Ashby & Maddox, 1993; Maddox & Ashby, 1993) indicated that observers have a tendency to shift from responding more in line with probability matching to responding more in line with maximization as they gain experience
14457605	15821	between matching and maximizing strategies. This debate has not escaped the perceptual categorization literature, but has not been addressed in our decision-criterion learning work (e.g., Ashby & Maddox, 1993; Maddox & Ashby, 1993). In this study, we have developed and tested a COBRM hypothesis. An advantage of the present formulation and quantitative comparison is that the two approaches (COBRA and
8920841	15845	achieves a significantly better recognition performance. Because mobile phone users typically use their phones in noisy environments, different noise reduction schemes for DSR have been proposed (Andrassy et al., 2001; Noe et al., 2001). These schemes, however, introduce extra computation burden on the front-end processor. Unlike Andrassy et al. (2001) and Noe et al. (2001) where noise reduction is performed at
8920841	15847	shape varies from speaker to speaker, spectral features, such as linear-predictive cepstral coefficients (LPCCs) (Rabiner and Juang, 1993) and mel-frequency cepstral coefficients (MFCCs) (Davis and Mermelstein, 1980), are often used. A set of speaker models is trained from the spectral features extracted from client utterances. A background model is also trained using the speech of a large number of speakers
8920841	15855	and parameterization of the decoded speech signals. However, it has been found that channel- and codec-distortion can degrade recognition performance significantly (Euler and Zinke, 1994; Lilly and Paliwal, 1996). To address this problem, the European Telecommunications Standard Institute (ETSI) has recently published a front-end processing standard in which feature vectors are extracted at the client-side
8920841	15856	it may not perform satisfactorily on telephone handsets with nonlinear characteristics. To overcome the limitation of the LMS algorithm, this paper incorporates a feature transformation algorithm (Mak and Kung, 2002; Mak et al., 2004; Tsang et al., 2002, 2003) into the back-end recognizer to enhance the robustness of the speaker verification system against handset variations. Experiments with and without using
8920841	15856	on the problems of transducer mismatches and robustness in recent years. 3 Stochastic Feature Transformation and Handset Identification Stochastic feature transformation (Sankar and Lee, 1996; Mak and Kung, 2002) is based on the assumption that clean feature vectors ˆxt can be recovered from distorted vectors yt using the transformation ˆxt = f?(yt) = Ayt + b, (1) where ? = {A, b} denotes the
8920841	15856	were used in the experiments reported here. Therefore, the values of S and D in Eqs. (5) through (8) were set to 64 and 12, respectively. 5.3 Enrollment Procedures Similar to our previous work (Mak and Kung, 2002; Mak et al., 2004; Yiu et al., 2003; Tsang et al., 2002), we trained a personalized 32-center GMM to model the characteristics of each client speaker in the system. 1 The feature vectors derived
8920841	15857	satisfactorily on telephone handsets with nonlinear characteristics. To overcome the limitation of the LMS algorithm, this paper incorporates a feature transformation algorithm (Mak and Kung, 2002; Mak et al., 2004; Tsang et al., 2002, 2003) into the back-end recognizer to enhance the robustness of the speaker verification system against handset variations. Experiments with and without using the LMS-based
8920841	15857	s)-th entry given by J?(yt)rs = ?f?(yt)s/?yt,r. For detailed derivation, see (Kung et al., 2004). In this work, the feature transformation was combined with a handset selector (Tsang et al., 2002; Mak et al., 2004) for robust speaker verification. Specifically, before verification takes place, we compute one set of transformation parameters for each type of handsets that claimants are likely to use. Then,
8920841	15857	experiments reported here. Therefore, the values of S and D in Eqs. (5) through (8) were set to 64 and 12, respectively. 5.3 Enrollment Procedures Similar to our previous work (Mak and Kung, 2002; Mak et al., 2004; Yiu et al., 2003; Tsang et al., 2002), we trained a personalized 32-center GMM to model the characteristics of each client speaker in the system. 1 The feature vectors derived from the SA and SX
8920841	15858	we collect the scores of 100 client speakers, each being impersonated by 50 impostors, to compute the speaker-independent equal error rate (EER) and to produce a detection error trade-off curve (Martin et al., 1997). Therefore, speaker-independent decision thresholds were used, and for each handset in an experimental setting, there were 300 client speaker trials (100 client speakers × 3 sentences per speaker)
8920841	15863	is trained from the spectral features extracted from client utterances. A background model is also trained using the speech of a large number of speakers to represent speaker-independent speech (Reynolds et al., 2000). Basically, the background models are used to normalize the scores of the speaker models to minimize nonspeaker related variability such as acoustic noise and channel effect. To verify a claimant,
8920841	15863	for training, i.e., 7 sentences per GMM. A collection of all SA and SX sentences uttered by all speakers in the customer set were used to train a 64-center 1 We chose not to use MAP adaptation (Reynolds et al., 2000) because we have enough data to create individual speaker models. Our recent study (Sit et al., 2004) also shows that when sufficient training data are available, the maximum likelihood approach
8920841	15863	used and Condition C was used for training and verification. ? 0.250 0.280 0.300 EER 7.31% 6.63% 7.00% (a) Approach I ? 0.139 0.140 0.143 EER 6.76% 5.85% 7.31% (b) Approach II GMM background model (Reynolds et al., 2000). The handset senh was used as the enrollment handset and its utterances were considered to be clean. The optimum values of ? in Eqs. (5) and (8) were determined empirically using the speech data
8920841	15864	researchers have focused on the problems of transducer mismatches and robustness in recent years. 3 Stochastic Feature Transformation and Handset Identification Stochastic feature transformation (Sankar and Lee, 1996; Mak and Kung, 2002) is based on the assumption that clean feature vectors ˆxt can be recovered from distorted vectors yt using the transformation ˆxt = f?(yt) = Ayt + b, (1) where ? = {A, b}
8920841	15866	telephone handsets with nonlinear characteristics. To overcome the limitation of the LMS algorithm, this paper incorporates a feature transformation algorithm (Mak and Kung, 2002; Mak et al., 2004; Tsang et al., 2002, 2003) into the back-end recognizer to enhance the robustness of the speaker verification system against handset variations. Experiments with and without using the LMS-based blind equalization were
8920841	15866	matrix with (r, s)-th entry given by J?(yt)rs = ?f?(yt)s/?yt,r. For detailed derivation, see (Kung et al., 2004). In this work, the feature transformation was combined with a handset selector (Tsang et al., 2002; Mak et al., 2004) for robust speaker verification. Specifically, before verification takes place, we compute one set of transformation parameters for each type of handsets that claimants are
8920841	15866	the values of S and D in Eqs. (5) through (8) were set to 64 and 12, respectively. 5.3 Enrollment Procedures Similar to our previous work (Mak and Kung, 2002; Mak et al., 2004; Yiu et al., 2003; Tsang et al., 2002), we trained a personalized 32-center GMM to model the characteristics of each client speaker in the system. 1 The feature vectors derived from the SA and SX sentence sets of the corresponding
15868	16739	processing or simply ignore them. However, KISS signaling units should be able to support LSRR by hardware1 . The destination field of the SETUP message is the final destination. An IP alert-option  can be used to notify routers along the path that they should examine the setup messages. Before forwarding a SETUP message, each enabled NIM replaces the IP-source field with it’s own interface
15868	16742	reservation  tie together the path selection and signaling mechanisms and mix the different call establishment tasks. Current signaling protocols are designed mainly for software implementation , , , , , , , , , , were some attempts were made for high-rate software implementations ,  As user population, communication volumes and connection rates
15868	16742	the rational which leads us to develop the KISS signaling paradigm. Currently, there are three main proposals for integrated network signaling protocols: IETF’s RSVP , , , MPLS’s LDP ,  and ATM’s PNNI . It seems that RSVP signaling for MPLS networks is the leading market solution. RSVP development correlates in time with the experimental MBONE . It is also optimized
15868	16743	tie together the path selection and signaling mechanisms and mix the different call establishment tasks. Current signaling protocols are designed mainly for software implementation , , , , , , , , , , were some attempts were made for high-rate software implementations ,  As user population, communication volumes and connection rates
15868	16743	rational which leads us to develop the KISS signaling paradigm. Currently, there are three main proposals for integrated network signaling protocols: IETF’s RSVP , , , MPLS’s LDP ,  and ATM’s PNNI . It seems that RSVP signaling for MPLS networks is the leading market solution. RSVP development correlates in time with the experimental MBONE . It is also optimized for a
15868	16748	selection and signaling mechanisms and mix the different call establishment tasks. Current signaling protocols are designed mainly for software implementation , , , , , , , , , , were some attempts were made for high-rate software implementations ,  As user population, communication volumes and connection rates increase faster than CPU performance,
15868	16748	section we describe the rational which leads us to develop the KISS signaling paradigm. Currently, there are three main proposals for integrated network signaling protocols: IETF’s RSVP , , , MPLS’s LDP ,  and ATM’s PNNI . It seems that RSVP signaling for MPLS networks is the leading market solution. RSVP development correlates in time with the experimental MBONE . It
15868	16748	unicast support should not be compromised for the support of the more complex multicast. The importance of a better unicast support was also acknowledged by the RSVP community. The RSVP-TE draft , suggests simplification of unicast reservations. This draft enables RSVP to open unicast simplex connections (LSP) on MPLS networks. Robustness is an important issue for a signaling protocol. A
15868	15878	While this paper focus mainly on the signaling part which is held back by major scalability issues, the other components should also be solved and optimized in an integrated way see for example , , . Moreover, certain mechanisms like multi-path reservation  tie together the path selection and signaling mechanisms and mix the different call establishment tasks. Current signaling
15868	15878	1/10 of RSVP implementation . This is not a reduced RSVP but a totaly new protocol. Its development was inspired by signaling protocols we reviewed including PNNI, RSVP, CR-LDP, SS7, OPENET  and more. In the following paragraphs we explain the KISS protocol design considerations. Maintaining complex data-structures in hardware is a challenging task. The only data-structures required by
15868	4075	by major scalability issues, the other components should also be solved and optimized in an integrated way see for example , , . Moreover, certain mechanisms like multi-path reservation  tie together the path selection and signaling mechanisms and mix the different call establishment tasks. Current signaling protocols are designed mainly for software implementation , ,
402036	15883	which are fitted to the data but do not have any direct biological interpretation. Their results contained the best overall compression on a range of DNA at that time. The new model of sequences (Allison et al., 1998) directly describes the ability of DNA to duplicate substrings, either in the forward direction or in the reverse direction from the complementary strand. Once a sequence contains two or more
402036	15892	Unfortunately the convergence is slow. Nevertheless this model has inspired a great many file-compression programs. Research has tended to concentrate on clever algorithms and data structures (Cleary and Teahan 1997), such as hash-tables and suffix-trees of ‘contexts’, to make the programs fast, as this is crucial in the areas of data communications and file-compression. As noted before, typical
402036	419	assumed that the machine’s parameters were known in advance, but usually they are not. Parameters are estimated by an expectation maximization (EM) process (Baum and Eagon 1967; Baum et al., 1970; Dempster et al., 1977). Initial parameter values are assumed and the algorithm makes a pass through the repeat graph. As it does so it computes the frequencies of the machine operations up to each node in the graph.
402036	15898	to the decoder at the receiver end. It is considered that data modeling is the hard part of data-compression and that the best model leads to the greatest compression. Arithmetic coders (Langdon 1984) are quite capable of encoding an event (in a sequence) in a non-integer number of bits and of approaching the calculated lengths arbitrarily closely. Over-fitting is a well-known problem in
15917	15724	distribution Pk but instead in the remaining degree: the number of edges leaving the vertex other than the one we arrived along. This new distribution q(k) is obtained from: (k + 1)Pk+1 q(k) = (1) < k > where < k >= ? k kPk. In a network with no assortative (or disassortative) mixing qc(j, k) takes the value q(j)q(k). If there is assortative mixing, qc(j, k) will differ from this value and
15917	15724	defined for complex networks, how they correlate with other statistical measures and what is their meaning and implications. III. ENTROPY AND INFORMATION By using the previous distribution q = (q(1), ..., q(i), ..., q(N)), an entropy measure H(q) can be defined: H(q) = ? N? q(k) log(q(k)) (4) k=1 The entropy of a network will be a measure of uncertainty (Ash, 1965). Within the context of
15917	15918	? k k2q(k) ? ?? k kq(k)? 2 of the distribution q(k), and hence the normalized correlation function is ? ? ? r = 1 ? 2 q ? ? ? jk(qc(j, k) ? ? ? jq(j) ? jk j j 10 1 10 1 k 2 ? -2.1 2 B D 10 2 10 2 3 (2) ? ??? (3) As defined from the previous equation, we have ?1 < r < 0 for DM and 0 > r > 1 for AM. Both biological ad technological nets tend to display DM, whereas social webs are clearly assortative.
15917	15918	the symmetric character of our system, no such distinction is made here. In figure 4 we can see the impact of heterogeneity on entropy. Specifically, we computed the entropy H(q; ?, ?) for ? ? (2, 3) and ? ? (0, 50) for a distribution Pk ? k ?? ?(k/?) using different scaling exponents ? and cut-offs ?. The impact of diversity (long tails) is obvious, increasing the uncertainty. As the scaling
15917	15921	a vertex with k edges leaving it provided that the vertex at the other end of the chosen edge has k ? leaving edges. This entropy is defined as: Hc(q|q ? N? N? ) = ? q(k)?(k|k ? ) log ?(k|k ? ) (7) k=1 k ??? =1 Since the conditional and joint probabilities are related through: ?(k|k ? ) = qc(k, k ? ) q(k ? ) the conditional entropy can actually be computed in terms of the two previous
15917	15922	transfer function 2 : N? N? I(q) = k=1 k ? qc(k, k =1 ? ) log qc(k, k ? ) q(k)q(k ? ) 2 The previous measures can be extended (with some care) into continuous distributions. In this case, we 5 (8) (9) (11)sSome limit cases are of interest here. The first corresponds to the maximum information transfer, which is obtained, for a given {qk}, when Hc(q|q ? ) = 0, i. e. when the conditional
15917	15924	function 2 : N? N? I(q) = k=1 k ? qc(k, k =1 ? ) log qc(k, k ? ) q(k)q(k ? ) 2 The previous measures can be extended (with some care) into continuous distributions. In this case, we 5 (8) (9) (11)sSome limit cases are of interest here. The first corresponds to the maximum information transfer, which is obtained, for a given {qk}, when Hc(q|q ? ) = 0, i. e. when the conditional probabilities
15917	15925	we have Pk = ?k,z, where z is a fixed number of links per node and ?ij the Kronecker’s delta function. For this ordered graph ?L, we have and thus q(k) = ?k,z?1 qc(k, k ? ) = ?k,z???1?k ? ,z?1 7 (12) (13) I(q) = H(q) = Hc(q|q ? ) = 0 (14) This is a trivial case, since the homogeneous character of the degree distribution implies zero uncertainty. The same situation arises for a Cayley tree
6898009	15953	of a highly heterogeneous problem, but also the local fluctuations which may be important in many applications. Recently there has been many contributions on multiscale numerical methods, including , , , , , , , , . Our work is in the spirit of that of Hou and Wu . Our model problem is a scalar elliptic partial differential equation which arises in many
6898009	15956	highly heterogeneous problem, but also the local fluctuations which may be important in many applications. Recently there has been many contributions on multiscale numerical methods, including , , , , , , , , . Our work is in the spirit of that of Hou and Wu . Our model problem is a scalar elliptic partial differential equation which arises in many applications
6898009	15958	heterogeneous problem, but also the local fluctuations which may be important in many applications. Recently there has been many contributions on multiscale numerical methods, including , , , , , , , , . Our work is in the spirit of that of Hou and Wu . Our model problem is a scalar elliptic partial differential equation which arises in many applications such
6898009	15959	problem, but also the local fluctuations which may be important in many applications. Recently there has been many contributions on multiscale numerical methods, including , , , , , , , , . Our work is in the spirit of that of Hou and Wu . Our model problem is a scalar elliptic partial differential equation which arises in many applications such as
6898009	15959	method in the spirit of  by solving (22) in a domain Q which is slightly larger than K. In the context of  this yields to a non-conforming finite element method which is analyzed in . In our framework, the oversampling method is still a conforming finite element method (see Remark 3.4). REMARK 3.2 In many applications, like composite materials, the coefficient matrix A ? is
6898009	15959	the definition of the finite element basis in the multiscale method of T. Hou and X.-H. Wu . REMARK 3.4 As explained in Remark 3.1 we can devise an oversampling method in the spirit of , . If the local oscillating test functions w ?,K i are computed from (22) in a domain Q which is larger than K, the composition with ?h l is still going to define a conforming finite element basis.
6898009	15961	problem, but also the local fluctuations which may be important in many applications. Recently there has been many contributions on multiscale numerical methods, including , , , , , , , , . Our work is in the spirit of that of Hou and Wu . Our model problem is a scalar elliptic partial differential equation which arises in many applications such as diffusion in
6898009	15961	the local boundary value problems (22). Since, from one cell K to the other, these problems are independent this may be done in parallel. This procedure is very similar to that introduced in . The hats used in our notation refer to exact solutions of boundary value problems : thus, ?w ? refers to the “true” oscillating test function and ?w ?,h refers to the collection over Th of all
6898009	15961	define it on a larger domain (even if only its restriction to K is used). For instance, in order to prevent boundary layer effects we can devise a so-called oversampling method in the spirit of  by solving (22) in a domain Q which is slightly larger than K. In the context of  this yields to a non-conforming finite element method which is analyzed in . In our framework, the
6898009	15961	column vector composed of values of uh at REMARK 3.3 In the case of piecewise linear finite elements P1 we recover the multiscale finite element method previously introduced by T. Hou and X.-H. Wu . Indeed, when the basis functions p K i belong to P1, by linearity the oscillating basis functions can be written A simple calculus shows that and, if we choose A ? K ? ?,K i (x) = p K i (x) +
6898009	15961	definition (22), we obtain ? ?? ?div{A ? grad ? ?,K i } = 0 in K ?? ? ?,K i = pi on ?K which is precisely the definition of the finite element basis in the multiscale method of T. Hou and X.-H. Wu . REMARK 3.4 As explained in Remark 3.1 we can devise an oversampling method in the spirit of , . If the local oscillating test functions w ?,K i are computed from (22) in a domain Q which is
6898009	2422445	? admits a second derivative, which is true if the finite element order is k ? 2. For k = 1 the argument must be slightly changed, but, since in such a case our method coincides with that in , , we do not give unnecessary details. The third term in the right hand side of (32) is ?{?(?hu ? )} ? ( ?w ? ? ?w ?,h )? L 2 (?) n ? ?u ? ? W 1,? (?)??( ?w ? ? ?w ?,h )? L 2 (?)
6898009	15963	the local fluctuations which may be important in many applications. Recently there has been many contributions on multiscale numerical methods, including , , , , , , , , . Our work is in the spirit of that of Hou and Wu . Our model problem is a scalar elliptic partial differential equation which arises in many applications such as diffusion in porous media, or
8912832	15967	24s4 Security 4.1 Introduction 4.1.1 Grid Security Infrastructure Security is of paramount importance in the Grid computing paradigm. The Globus Toolkit uses the Grid Security Infrastructure (GSI)  for secure access to Grid resources. Users of the Java CoG Kit thus need to interact with the GSI in order to access the Grid resources. This chapter starts with a brief discussion about the
8912832	15967	different organizations. This is necessary since it is not convenient for those organizations to abandon the existing infrastructure and switch to a new one. The Grid Security Infrastructure (GSI)  satisfies all the requirements mentioned above. GSI uses the Public Key Infrastructure (PKI), X.509 certificates, and Secure Sockets Layer (SSL) as its basis. It extends these standards for single
8912832	15968	authorities, proxies and gridmap authorization. The subsequent section explains the procedures to acquire the necessary credentials. A web-based credential management software called MyProxy  is then discussed. We then describe how a user can use the various tools that the Java CoG Kit provides for managing (creating, destroying, examining, etc.) certificates and proxies. A discussion
8912832	15968	and later retrieve a proxy whenever needed for authentication. Since proxies have a limited lifetime that can be controlled, the compromise of a proxy does not cause much damage. MyProxy  serves this purpose. A securely managed MyProxy server that is trusted by the user can provide an effective way of credential management. MyProxy is available from the following website: MyProxy
8912832	10323	Toolkit. This section assumes some knowledge of the fundamentals of information security and Public Key cryptography. If you are not familiar with these concepts, please refer to a book, such as . Working knowledge of Secure Sockets Layer (SSL)  is also assumed. The Grid infrastructure allows users to access computational and data resources that may span organizational, and perhaps
8912832	2981	their resources into the Grid environment. The Globus Toolkit provides two methods for accessing distributed data. As a data transfer protocol, it provides Grid File Transfer Protocol (GridFTP) , which is a common protocol independent of the underlying architectures. It supports GSI and Kerberos security. It also provides various features for high performance, reliable and restartable data
8912832	15969	It also provides various features for high performance, reliable and restartable data transfers, as mentioned in the next section. The other method, Globus Access to Secondary Storage (GASS) , allows applications to use standard file I/O interfaces (open, read/write, close) for distributed access. It defines a global name space using Uniform Resource Locators. It also allows the use of
8912832	16855	In these settings the user might be interested in discovering and monitoring the resources in a secure and efficient way. The Globus Toolkit supports a Monitoring and Discovery Service (MDS) to provide information about Grid resources. In short, MDS provides directory services for resources in the Grid. A directory service provides information about different entities in the
8912832	15971	and their users. Extensive documentation for MDS is available at Homepage : http://www.globus.org/mds Manual : http://www.globus.org/mds/mdsusersguide.pdf More technical details are available at  The structure of MDS is hierarchical . It consists of Grid Resource Information Service (GRIS), Grid Index Information Service (GIIS) and Information Providers (IPs) as shown in Figure 7.1.
15973	15974	s1 and outputs s2 might specify that if s2 = f(s1) in one execution (for some function f) then there must exist another execution in which s2 ?= f(s1). 2.2 Security Properties Alpern and Schneider  distinguish between properties and more general policies as follows. A security policy P is deemed to be a (computable) property when the policy has the following form. P (?) = ?? ? ?. ˆP (?)
15973	15974	liveness property. If the program has acquired a resource, we can always extend its execution so that it releases the resource in the next step. Other Properties Surprisingly, Alpern and Schneider  show that any property can be decomposed into the conjunction of a safety property and a liveness property. Bounded availability is a property that combines safety and liveness. For example,
15973	15976	policies of different levels of abstraction from the Software Fault Isolation policy for the Pentium IA32 architecture to the Java stack inspection policy for Sun’s JVM . Evans and Twyman  have implemented a very general enforcement mechanism for Java that allows system designers to write arbitrary code to enforce security policies. Such mechanisms may be more powerful than those
15973	15979	Neither of these questions can be answered effectively without understanding the space of enforceable security policies and the power of various enforcement mechanisms. Recently, Schneider  attacked this question by defining EM, a subset of safety properties  that has a general-purpose enforcement 1smechanism - a security automaton that interposes itself between the
15973	15979	we have taken eight steps without releasing the resource) that may be extended to sequences that are in the property (e.g., we release the resource on the ninth step). 2.3 EM Recently, Schneider  defined a new class of security properties called EM. Informally, EM is the class of properties that can be enforced by a monitor that runs in parallel with a target program. Whenever the target
15973	15979	including software fault isolation and Java stack inspection. However, they cannot enforce any of our other example policies (availability, bounded availability or information flow). Schneider  also points out that security automata cannot enforce safety properties on systems in which the automaton cannot exert sufficient controls over the system. For example, if one of the actions in the
15973	15979	operations of the automaton rather directly. 5 Future Work We are considering a number of directions for future research. Here are two. Composing Schneider’s security automata is straightforward , but this is not the case for our edit automata. Since edit automata are sequence transformers, we can easily define the composition of two automata E1 and E2 to be the result of running E1 on the
15973	15980	optimization techniques for security automata , certification of programs instrumented with security checks  and the use of run-time monitoring and checking in distributed  and real-time systems . Overview The remainder of the paper begins with a review of Alpern and Schneider’s framework for understanding the behavior of software systems
15973	15982	neither he nor anyone else has formally investigated the power of a broader class of runtime enforcement mechanisms that explicitly manipulate the program action stream. Erlingsson and Schneider  have implemented inline reference monitors, which allow arbitrary code to be executed in response to a violation of the security policy, and have demonstrated their effectiveness on a range of
15973	15983	on a range of security policies of different levels of abstraction from the Software Fault Isolation policy for the Pentium IA32 architecture to the Java stack inspection policy for Sun’s JVM . Evans and Twyman  have implemented a very general enforcement mechanism for Java that allows system designers to write arbitrary code to enforce security policies. Such mechanisms may be
15973	15984	classes of policies that they enforce. Other researchers have investigated optimization techniques for security automata , certification of programs instrumented with security checks  and the use of run-time monitoring and checking in distributed  and real-time systems . Overview The remainder of the paper begins with a review of Alpern and Schneider’s framework
15986	15987	. Packages that manipulate reduced ordered decision graphs are widely available and have become the most commonly used tool for discrete function manipulation in the logic synthesis community . Some of these packages are restricted to Boolean functions  (each non-terminal graph has exactly two outgoing edges) while others  can accept multi-valued attributes. 3 Minimizing
15986	7136	widely available and have become the most commonly used tool for discrete function manipulation in the logic synthesis community . Some of these packages are restricted to Boolean functions  (each non-terminal graph has exactly two outgoing edges) while others  can accept multi-valued attributes. 3 Minimizing Message Length and Encoding of RODGs Let ? be the description length of
15986	499	The level of a node is defined as the position of the variable tested at that node in the variable ordering used. Given an ordering, reduced ordered decision graphs are a canonical representation , for functions ? in ? that domain. This means that ? given ¤ a ?¥¤ function §©§?§???¤ ? : 1 2 and an ordering of the variables, there is one and only one representation ? for the function .
15986	15991	that derives an ordered decision graph with minimal description length from a decision tree built using standard techniques. The approach proposed here is very different from the one proposed in  that also used RODGs. Although this approach performs well for small problems, it requires far too much computation to be applicable to any problems with a larger number of attributes. Section 2
15986	15993	problems: the replication of subtrees required to represent some concepts and the rapid fragmentation of the training set data when high arity attributes are tested at a node. (See, for instance,  for a more detailed description of these limitations) Decision graphs (DG) have been proposed as one way to alleviate this problems, but the algorithms proposed to date for the construction of
15986	15993	limited success. This may have been caused by the fact that he used a non-canonical representation of Boolean functions (DNF expressions) which makes it hard to identify identical subtrees.  proposed a greedy algorithm that performs either a join or a split operation, according to which one reduces more the description length. He reported some good results in relatively simple problems
15986	7412	and stand for the Boolean and, or and exclusive-or of two Boolean functions, respectively. Consider now a decision tree created using one of the approaches proposed in the literature (for example, ). Let denote the variable tested ??? in node of the ????? decision tree, (the then node) denote the node pointed to by ??? the ??? arc leaving ??? node when attribute ????? is 1 and (the else node)
15997	14015	and communicate with a central authority. Proposed applications for sensor networks cover a wide range of areas, including environmental observation, health care, and security. Mainwaring et al.  describe an experimental, real-world application in habitat monitoring. Their research exposes the requirements and constraints of certain types of sensor network systems which partially motivate
15997	14015	we consider. For ease of analysis, we simply look at it from the inverse perspective: given a fixed recharging rate, how can we maximize the data rate? The experimental instance described in  uses one base station, located at a nearby ranger station, that collects data from the sensor network and relays it. If it is possible to provide power to a couple of more strategically placed base
15997	14015	of base stations on power consumption in these networks. III. COMPUTING THE RATE OF TRANSMISSION In our model, each sensor has the following characteristics: 1) Position (xv,yv) in the unit square  × . 2) Reachability radius rv ? 0: Sensor v can send messages to sensor w if the Euclidean distance between (xv,yv) and (xw,yw) is at most rv. 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE
15997	14015	cv = iv =1, rv = r. More formally, we consider the following decision problem, which we call BSP (for base station positioning): INPUT: A collection of vertex position pairs (x1,y1),...,(xn,yn) ???  × , a number of base stations 0 ? b ? n, a rate ? ?  PROBLEM: Decide whether there exists a layout of b base stations on top of the vertices that admits a routing strategy of rate at
15997	14015	each other’s transmission radius. We consider three classes of geometric graphs: the regular grid, the uniform random graph, and the preferential attachment graph. All graphs are contained in the ?? square of the Euclidean plane, which we call the unit square. In the regular grid, the vertices are spaced evenly at regular intervals in the unit square. The interval distances in the x and
15997	15998	that is needed for our model, is the simple description of the optimum solution afforded by the max-flow min-cut theorem. Moreover, existing fast implementations of maximum flow algorithms (e.g., ) can be used to implement the base station positioning algorithms. Power-efficient distribution and collection of data in sensor networks has also been studied for other network models and
15997	15998	only guarantee good worst-case behavior but also perform well in practice. For our empirical evaluation we used an implementation of the Goldberg-Tarjan algorithm  by Cherkassky and Goldberg . 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sIn what follows, we will assume that cv =1, iv =1, and rv = r for all v. In the sensor network world, this essentially means that all
15997	16000	for data collection with a single base station. Their model, however, does not explicitly address power consumption. Other approaches include minimizing packet length through data compression , selectively shutting off sections of the network while maintaining connectivity , exploiting the clustered structure of networks , , and varying the routing strategy with time .
15997	16001	explicitly address power consumption. Other approaches include minimizing packet length through data compression , selectively shutting off sections of the network while maintaining connectivity , exploiting the clustered structure of networks , , and varying the routing strategy with time . Another variation is online routing, where the rate of transmission and the message
15997	16002	include minimizing packet length through data compression , selectively shutting off sections of the network while maintaining connectivity , exploiting the clustered structure of networks , , and varying the routing strategy with time . Another variation is online routing, where the rate of transmission and the message sequence may not be known in advance , . The base
15997	16004	, selectively shutting off sections of the network while maintaining connectivity , exploiting the clustered structure of networks , , and varying the routing strategy with time . Another variation is online routing, where the rate of transmission and the message sequence may not be known in advance , . The base station positioning problem has also been studied in
15997	16007	of transmission and the message sequence may not be known in advance , . The base station positioning problem has also been studied in the context of cellular (UMTS) networks. Galota et al.  exhibit a polynomial time approximation scheme that maximizes the overall utility of base station positioning for a fairly comprehensive model, which includes parameters such as construction costs,
15997	16008	and ? are parameters of the model. One problem is to assign transmission ranges to nodes so as to minimize total power, under the constraint that the network is strongly connected. Kirousis et al.  show algorithms and lower bounds for this problem when the nodes lie either on a line or in three-dimensional space. Clementi et al.  derive an approximation algorithm for the planar version,
15997	16009	that the network is strongly connected. Kirousis et al.  show algorithms and lower bounds for this problem when the nodes lie either on a line or in three-dimensional space. Clementi et al.  derive an approximation algorithm for the planar version, though it is apparently not known if an efficiently computable exact solution is possible for this case. We are not aware, however, of any
15997	16010	a host of algorithms that do not only guarantee good worst-case behavior but also perform well in practice. For our empirical evaluation we used an implementation of the Goldberg-Tarjan algorithm  by Cherkassky and Goldberg . 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sIn what follows, we will assume that cv =1, iv =1, and rv = r for all v. In the sensor network world, this
15997	16013	uniform random graph, depending on the value of p, and models types of interaction that occur in more complex networks. It is inspired by proportional attachment models of the World Wide Web , , . In the case of the regular grid, we are able to give a theoretical analysis of the optimum layout of base stations, which is confirmed by the results of the experiments. Instances of the
15997	16014	random graph, depending on the value of p, and models types of interaction that occur in more complex networks. It is inspired by proportional attachment models of the World Wide Web , , . In the case of the regular grid, we are able to give a theoretical analysis of the optimum layout of base stations, which is confirmed by the results of the experiments. Instances of the other two
980522	16018	without modifying the theorem prover. One way to do this is by building over- and under-approximations for the formula. This technique has been successfully applied for three-valued model-checking . PVS (Prototype Verification System ) uses a completely different approach which involves constructing and proving additional formulas called type correctness conditions (TCCs). The validity of
980522	16020	without modifying the theorem prover. One way to do this is by building over- and under-approximations for the formula. This technique has been successfully applied for three-valued model-checking . PVS (Prototype Verification System ) uses a completely different approach which involves constructing and proving additional formulas called type correctness conditions (TCCs). The validity of
980522	16021	be dealt with is that most theoremprovers are based on classical logic. Various approaches have been advocated for modifying standard theorem-proving to accommodate logics with partial functions . However, we are interested in finding a method for supporting partiality without modifying the theorem prover. One way to do this is by building over- and under-approximations for the formula.
980522	16022	can accommodate partial functions is useful for a wide variety of applications, there is general disagreement on which logic should be used. An overview of the different approaches can be found in . Of the approaches which take partiality seriously as opposed to attempting a work-around, there are two main alternatives. The first allows terms to be undefined, but requires that all formulas be
980522	16022	be dealt with is that most theoremprovers are based on classical logic. Various approaches have been advocated for modifying standard theorem-proving to accommodate logics with partial functions . However, we are interested in finding a method for supporting partiality without modifying the theorem prover. One way to do this is by building over- and under-approximations for the formula.
980522	16025	combination of first-order theories. The tool takes a formula ? as an input and returns Valid or Invalid. CVC Lite is based on standard techniques for combining first-order decision procedures , and currently supports several theories, including uninterpreted functions, arrays, and linear real arithmetic. It also has some limited support for quantifiers. The input language of CVC Lite is
16026	16029	consider the data modelling technique NIAM . Naturally, conceptual data models that cannot be populated/instantiated are undesirable. As it turns out however the formalization of NIAM in  showed that populatability is a complex notion which has several gradations. These could only be expressed after an initial definition of what a population is had been given. Similarly, process
16026	5620	this matter in more detail, let us consider some concrete examples. In case of the formalization of techniques for conceptual data modelling, such as ER (Entity Relationship approach, see e.g. ) and NIAM (Natural language based Information Analysis Method, see e.g. ), possible mathematical domains are logic, set theory, and category theory  (each of these will be used in the
16026	5620	the importance of the Semantics Priority Principle. The subject of our case study is Object Role Modelling (ORM) . The reason for choosing ORM over the more popular Entity Relationship (ER)  family of data modelling techniques is that ORM is a richer modelling language. Meanwhile, ORM also has a well established history of formalization using different styles [Hal89, Hal90b, BHW91,
16026	16035	style, we have seen that there is no need to make such assumptions. There is, nevertheless, a trade-off between the Conceptualization Principle and the Primary Goal Principle involved. In , an alternative formalization using a set theoretic style is given that remedies the above drawback. In the next formalization style, the category theoretic approach, we will see an even cleaner
16026	16043	conceptual formalization, then category theory will be the most ideal instrument. However, if a wider audience needs to be addressed, a more mundane formalization may be more appropriate. Goguen  argued that category theory can provide help with dealing with abstraction and representation independence: in computing science, more abstract viewpoints are often more useful, because of the need
16026	16044	listed in table 1. The units of Process Algebra are atomic actions. The set of all atomic actions is called A. Although they are units of calculation, atomic actions need not be indivisible (see ). Starting with atomic actions, new processes can be constructed by applying sequential and alternative composition (“·” resp. “+”). The algebra that results (axioms A1-A5) is called basic process
16026	16052	by circles, while the transitions are represented by thick lines. The arrows capture the input and output relations. 3.4 Semantics of Task Structures through Predicate/Transition Nets In  two principal problems with Petri nets are stated. The first problem is that Petri net specifications easily become huge, complicated and unstructured. The second problem is their structural
16026	16054	style is most useful when developing and studying a theory of conceptual data modelling itself, but is much less suited for a novice audience due to the high abstraction level. As shown in  it could also serve as the basis for a data modelling shell, allowing analysts to define their own data modelling technique with required semantic features. The set theoretic formalization style
16026	5976	and analyze problem solving processes. In  they were extended and used as a meta-process modelling technique for describing the strategies used by experienced information engineers. In  they were extended again and a formal semantics in terms of Process Algebra was given. In figure 1, the main concepts of task structures are represented graphically. They are discussed
16026	5976	to the important ingredients of Process Algebra is given (for an in-depth treatment we refer to ), then the translation of task structures to Process Algebra is defined (based on ). This provides an example of denotational semantics. Although the name Process Algebra suggests a single algebra to describe processes, it actually refers to a whole family of algebras based on
16026	5976	on the other hand there were no synchronizers). The Process Algebra formalization has as advantage that it is relatively compact and may exploit equivalence theory developed for Process Algebra. In  the theory of bisimulation is used to demonstrate 16show task structures can be proven to be (in)equivalent (in this context: whether they can or cannot generate the same traces). When working on
16026	16056	can be made more elegant by allowing recursive decomposition and in addition to that recursive decomposition increases the expressive power of task structures (this is formally shown in ). As a rule, only dismiss those models as not being well formed to which you cannot possibly assign a formal semantics. Models that are initially well formed but “problematic” can then be formally
16026	16057	suited. Category theory can still be used, but typically, domains like Petri nets and higher-order variants thereof and algebraic approaches such as Process Algebra , CCS , and CSP  are particularly suitable. Petri netbased approaches provide formal verification techniques (such as support for the computation of invariants, see e.g. ), but a limited number of concepts,
16026	16057	(see e.g. ). One of its virtues is its ability to prove equivalence of process specifications. As an algebraic theory Process Algebra belongs to the same family as CCS  and CSP . First a short introduction to the important ingredients of Process Algebra is given (for an in-depth treatment we refer to ), then the translation of task structures to Process Algebra is
16026	16059	Hoare. In , a mathematical theory of programming is presented in three different styles (denotational, operational, and algebraic, which in our terminology corresponds to axiomatic). In  the use of logic is advocated as providing the basis for a systematic design methodology (which is illustrated with a number of formalization case studies). In , it is shown how an
16026	16061	the theory of bisimulation is used to demonstrate 16show task structures can be proven to be (in)equivalent (in this context: whether they can or cannot generate the same traces). When working on , the first author experienced great difficulties in using the Process Algebra formalization for proving properties (in particular deadlock freedom) of task structures that satisfied certain
16026	16061	to found for certain classes of task structures as this requires reasoning over equation sets of a specific form. For this purpose, the axiomatic semantics proves to be much more suitable (see also ). However, if one considers natural extensions to task structures, such as task decomposition, messaging, and time constructs, the axiomatic semantics becomes very hard to define properly and it is
16026	16062	types may also be objectified, which means that they may play roles in other relationship types. Many conceptual data modelling techniques offer concepts for expressing inheritance of properties . In the literature many forms of inheritance have been documented, and terminology is far from being standardized. In this section we only use the subtyping construct as it is used in an ORM
16026	10702	seems to provide a middle ground between the previous two. In  it was shown how this formalization style leads to an elegant characterization of complex uniqueness constraints, while in  it was used as the basis for the conceptual query language LISA-D. 5 Conclusions This paper presented a discussion on the how of formalization. Taking the need for elegant and concise
16026	16081	allowing analysts to define their own data modelling technique with required semantic features. The set theoretic formalization style seems to provide a middle ground between the previous two. In  it was shown how this formalization style leads to an elegant characterization of complex uniqueness constraints, while in  it was used as the basis for the conceptual query language LISA-D.
16083	9720	develop them. Currently, to develop a context-aware application, developers are required to either design their own application from scratch, directly interacting with devices, or use a toolkit . However, even with low-level toolkit support for acquiring context, experienced developers are still required to write a large amount of code to develop simple applications. A context-aware
16083	9720	rule set can be tested using the iCAP engine in run mode. The engine can either be set to simulate the context-aware environment, or be used in conjunction with a real context-aware environment . Users can interact with the engine to change the value of defined inputs, and evaluate the behavior of the rules being tested. With the engine, users are able to quickly design and test their
16083	16084	with the devices in a run mode. The behavior of created devices can either be simulated by this tool, or mapped to actual devices. We built iCAP using the Java 2 SDK version 1.4, on top of SATIN , a toolkit for building informal pen-based interaction systems. THE iCAP INTERFACE iCAP has one window with two main areas (see Figure 1). On the left is a tabbed window that is the repository for
16083	16085	different outputs is rare, thus only a single output sheet is currently supported. We implemented Pane and Myers’ matching scheme to allow users to visually specify the Boolean logic of each rule . Instead of traditional pull-down menus for executing commands, we use pie menus to better support pen interaction. In addition, we also support gestures for issuing common commands such as cut,
16083	16086	inspired by previous work in ubiquitous computing applications, specifically those that involve the development of rule based conditions. Some of these include AgentSheets  and Stick-e notes . These tools are designed for building applications, while iCAP is focused on helping developers rapidly prototype, test and iterate on their context-aware applications. FUTURE DIRECTIONS While we
16083	16087	WORK iCAP has been inspired by previous work in ubiquitous computing applications, specifically those that involve the development of rule based conditions. Some of these include AgentSheets  and Stick-e notes . These tools are designed for building applications, while iCAP is focused on helping developers rapidly prototype, test and iterate on their context-aware applications.
4511012	16098	configuration the musician would create a polyphonic saxophone able to play three part harmony. This kind of setup would suit instruments where the musician has one or two fingers that are idle . In the case of the trumpet, the free hand would enable six part harmony. In the case of a slide guitar, the slide could be outfitted with up to five ThumbTec input devices allowing the musician to
4511012	8960	could be outfitted with up to five ThumbTec input devices allowing the musician to play a wide variety of sliding chords. The vBow and Hyperbow are prime candidates for this technology as well,. 4. CONCLUSION The ThumbTec project indicates that, although research in alternate and instrument-inspired interfaces is very important, exploring other interface paradigms can lead to interesting
16101	16103	on truncated line metrics. Kleinberg and Tardos  gave a 2-approximation algorithm for the case of a uniform metric on the labels, and combined this with Bartal’s probabilistic tree embeddings  to obtain an O(log k log log k)-approximation for general metrics. Fakcharoenphol et al.  recently improved Bartal’s result, leading to an improved bound of O(log k). The earthmover LP was
16101	3460	Fakcharoenphol et al.  recently improved Bartal’s result, leading to an improved bound of O(log k). The earthmover LP was proposed independently by Charikar  and by Chekuri et al. . The first successful use of this LP was in , where it was used to give matching or improved algorithms for certain classes of distance functions d (convex and truncated linear), which had
16101	3460	a specialized LP for trees. Our rounding method also proves that the earthmover LP has no integrality gap when the input graph is a tree. This phenomenon was previously mentioned by Chekuri et al.  (a proof appears in their upcoming journal version), but our proof is very different. Interestingly, this implies, for 0-extension on tree graphs G, a distinction between the integrality ratio of
16101	16106	is known that a simple randomized algorithm based on the semi-metric LP relaxation matches this factor, and that the integrality gap for this LP is also 2(1 ? 1/k) . Calinescu et al.  strengthened the LP 3 Since several nodes may be mapped onto the same point in the containing metric, the induced distances between mapped nodes form only a semi-metric; hence the name. Our
16101	16107	is integral when the terminal metric is a tree (actually, a larger class of bipartite graphs that contains trees; here, a tree metric cannot contain Steiner nodes). Calinescu, Karloff and Rabani  gave an O(log k) approximation for 0-extension based on this LP relaxation. For the special case of input graphs that are planar, they gave an O(1) approximation algorithm. In fact, their algorithm
16101	16107	O(?) approximation. Fakcharoenphol et al.  improved the general result to O(log k/ log log k). On the other hand, the semi-metric relaxation is known to have an ?( ? log k) integrality gap . Finally, we note that Lee and 2 A stronger definition of decomposability that is sometimes used (padded decomposition) requires that the distance from a vertex to its cluster’s boundary is well
16101	16107	of lemma 2.6 does not use any property of the earthmover distance, and this grouping can be applied to the semi-metric relaxation as well. This leads to a much simpler proof of the Calinescu et al.  result on decomposable graphs. We omit the details here. Combining lemmas 2.5 and 2.6, we get Theorem 2.1. The algorithm described above gives an O(?) approximation algorithm to the 0-extension
16101	16107	edges are also fine in expectation. Thus we can improve the result for general graphs from O(log k) to O(log k/ log log k), matching the result in . It can be shown that the techniques of  imply an O(?) approximation to the problem when the terminal metric satisfies |B(x, 2r)|/|B(x, r)| ? 2? for every x and r. (Here B(x, r) is the set of points within radius r from x in the metric.)
16101	16109	our approximation guarantee is worse than the O(log k) currently known. However, there are applications where k ? n. For instance, in recognizing the position of a human body from an image (see ), the objects are the handful of rigid moving parts, and each part needs to be labeled with a six-tuple representing position, rotation and scale. 4 Our new algorithm for metric labeling uses the
16101	16110	case of input graphs that are planar, they gave an O(1) approximation algorithm. In fact, their algorithm extends to ?decomposable input graphs, giving an O(?) approximation. Fakcharoenphol et al.  improved the general result to O(log k/ log log k). On the other hand, the semi-metric relaxation is known to have an ?( ? log k) integrality gap . Finally, we note that Lee and 2 A stronger
16101	16110	k/ log log k)-decomposition, and the intergroup edges are also fine in expectation. Thus we can improve the result for general graphs from O(log k) to O(log k/ log log k), matching the result in . It can be shown that the techniques of  imply an O(?) approximation to the problem when the terminal metric satisfies |B(x, 2r)|/|B(x, r)| ? 2? for every x and r. (Here B(x, r) is the set
16101	16111	of a uniform metric on the labels, and combined this with Bartal’s probabilistic tree embeddings  to obtain an O(log k log log k)-approximation for general metrics. Fakcharoenphol et al.  recently improved Bartal’s result, leading to an improved bound of O(log k). The earthmover LP was proposed independently by Charikar  and by Chekuri et al. . The first successful
16101	16111	? T we have dT (u, v) ? d(u, v). In general, the trees may contain Steiner points not in the original metric space. Our algorithm uses the following two results. Theorem 3.1. (Fakcharoenphol et al. ) For every n-point metric d there is a probabilistic tree approximation with distortion O(log n) from which we can sample in polynomial time. Theorem 3.2. (Gupta ) For every tree T = (V ? ,
16101	16113	in the computer vision community. A polynomial-time algorithm for line metrics and a constant-factor approximation for uniform metrics were given by Boykov et al. . Gupta and Tardos  gave a constant factor approximation on truncated line metrics. Kleinberg and Tardos  gave a 2-approximation algorithm for the case of a uniform metric on the labels, and combined this with
16101	16114	3.1. (Fakcharoenphol et al. ) For every n-point metric d there is a probabilistic tree approximation with distortion O(log n) from which we can sample in polynomial time. Theorem 3.2. (Gupta ) For every tree T = (V ? , E, w) and a set of required vertices V ? V ? , there exists a tree T ? = (V, E ? , w ? ) such that for all u, v ? V , dT (u, v) ? dT ?(u, v) ? 8dT (u, v). Here is our
16101	16118	It is wellknown (see ) that any graph on n vertices is O(log n)-decomposable, and that for some graphs (expanders) this bound is the best possible. Klein, Plotkin and Rao  showed that planar graphs are O(1)-decomposable, and that more generally, graphs excluding a fixed minor of size r are O(r3 )decomposable. Fakcharoenphol and Talwar  improved the latter bound
16101	3465	14853. Research supported in part by NSF grant CCR0113371. Email: eva@cs.cornell.edu Kunal Talwar ¶ Éva Tardos ? 1 Introduction The metric labeling problem has been proposed by Kleinberg and Tardos  to model classification problems from several domains, ranging from categorizing web documents to machine vision, including image recovery and the stereo matching problem. Metric labeling models
16101	3465	approximation for uniform metrics were given by Boykov et al. . Gupta and Tardos  gave a constant factor approximation on truncated line metrics. Kleinberg and Tardos  gave a 2-approximation algorithm for the case of a uniform metric on the labels, and combined this with Bartal’s probabilistic tree embeddings  to obtain an O(log k log log k)-approximation
16101	3465	on V , which we probabilistically approximate with a tree, and then round. The crux is that our rounding method incurs no loss on the tree. Our use of tree approximations contrasts with that of , which instead approximates the label metric by a tree, then uses a specialized LP for trees. Our rounding method also proves that the earthmover LP has no integrality gap when the input graph is a
16101	3465	so now all its mass is concentrated on representative terminals close to u. 5. Round each vertex in Vs to one of the representative terminals using the rounding algorithm of Kleinberg and Tardos  for metric labeling on uniform metrics. Intuitively, this last step works since to the vertices in Vs, the metric on the representatives looks approximately uniform. 2.1 Formal rounding algorithm.
16101	3465	For u ? Vs, project the distributions xu from the clusters onto the representatives: x ?? ? ? i?Tl ui ? x? ui i = il 0 otherwise 2.8 (Rounding) Apply on (Vs, x ?? ) the rounding algorithm of  for metric labeling on the uniform metric. 2.2 Analysis. Let f be the final assignment output by our algorithm, i.e. vertex u is assigned to terminal f(u). For an edge uv, we will sometimes refer
16131	16134	mean. 1. Introduction In many recent works dedicated to pattern recognition the efforts are being shifted towards classifier fusion as a way of further improvement of the recognition rate - . Combining classifiers is now perceived as a universal and obvious advancement of the single–best strategy, often even paraphrased as “gather all and combine” . Very quickly it turned out that
16131	16138	preventing from coincident failures , . In the pattern recognition domain, diversity is claimed to be the property of the multiple classifier systems deciding about their performance , . In general terms diversity is a clear concept of variety, multiplicity and the hope was that diverse (here different) classifiers should produce more reliable and improved classification
16131	16140	responsible for the team strength is known under many names in the literature including: disagreement, diversity, independence etc, all denoting certain characteristics among classifier outputs =. More detailed analysis revealed that specific distributions of outputs corresponding to maximum error dispersion account for the optimal performance of the voting systems , . In 1sgeneral,
16131	16140	/ 2 MV j= 1 ij ? y = i ? M ?? 1 if ? y ? = ?M / 2 j 1 ij ? A more detailed definition of MV including the rejection rule, observed for equal number of opposite votes when M is even, can be found in . However, this work is 3snot concerned with a detailed study of MV itself and in further analysis, without any loss of generality, we assume odd M . In  we presented the majority voting error
16131	16142	outputs -. More detailed analysis revealed that specific distributions of outputs corresponding to maximum error dispersion account for the optimal performance of the voting systems , . In 1sgeneral, however, each combiner has its own characteristic features and phenomena explaining successful combination for one combiner may no longer be applicable for others. Nevertheless the
16131	16142	votes when M is even, can be found in . However, this work is 3snot concerned with a detailed study of MV itself and in further analysis, without any loss of generality, we assume odd M . In  we presented the majority voting error limits assuming that all classifiers perform at the same mean level. Recalling these limits for a specific mean classifier error e we have: 4 E min MV ? ? ? ?
16131	16143	classifier fusion. The problems with the diversity started to emerge with the attempts of measuring it. Focused mainly on the outputs disagreement, a majority of diversity measures investigated in - showed very weak correlations with combined performances as well as their improvements. In  Shipp and Kuncheva illustrated even an apparent conflict between diversity tendencies shown by
16131	16143	different sometimes quite complex combining methods. It is also reflected in many unsuccessful attempts to define a universally useful diversity measure indicating potential benefits of combining -. It is therefore our belief that any measure of diversity, which could be used as a certain criterion for selecting classifiers to be combined, or deciding whether one should use a combination
16131	16143	a given pool imposes exponential complexity of the process equal to the exhaustive evaluation of the combiner performance. However the evaluative complexity of non-pairwise measures discussed in  is exactly of the same order and the cost of the individual measure is commonly higher. Moreover, seemingly simple pairwise measures in addition to their quadratic evaluative complexity have to add
16131	16143	versions of the RE measure have been applied for all the combinations of 3, 5, 7, and 9 out of 11 classifiers. The same has been done with Q statistics measure and Double Fault measure discussed in - for comparison purposes. All these measures have been compared against the difference between the majority voting error and mean classifier error and the quality of a measure was evaluated by
16131	16147	shown by some measures as falling and by others as rising among AdaBoost generated classifiers as training progressed. An interesting finding emerged from our recent diversity investigations . A substantial gain in the correlation with majority voting error has been observed for measures operating on error coincidences with an asymmetry to change of the individual classifier outputs in
16131	16147	combination of classifiers in the first instance, should be designed in close connection with the combination method (i.e. majority voting, fuzzy templates, mixture of experts etc.) as indicated in  and . In an extreme case, the combination performance, which we advocate in this paper, could be used instead of some kind of “ universally useful diversity measure” . Trivially, as we intend
16131	16148	into the concept of diversity, the analysis of different error coincidence levels within a pool of classifiers to be combined showed further improvement in the correlation to majority voting error . Consequently the closer the measure to the definition of the combined error, the better its correlation with the combined errors and the better use of the measure. In this paper we attempt to take
16131	16148	shows superior quality in all aspects. Majority voting combiner is ideal for that purpose as measuring its error is often computationally cheaper than applying complex diversity measures -. We try to explain and illustrate that there is no point of using diversity measures to model majority voting error rather than just using this error itself. Moreover, we show that unlike diversity
16131	16148	of classifiers in the first instance, should be designed in close connection with the combination method (i.e. majority voting, fuzzy templates, mixture of experts etc.) as indicated in  and . In an extreme case, the combination performance, which we advocate in this paper, could be used instead of some kind of “ universally useful diversity measure” . Trivially, as we intend to show,
16131	16148	line of code in Matlab notation: E MV =mean(sum(Y’)>M/2); Further potential reduction of the individual costs of the measure is available through the approximation of the majority voting error. In  we showed a decomposition of the majority voting error into levels of general or exclusive coincidences. The k th G level of general coincidence L is just the summing, for all the k-combinations k
16131	16148	of these coincidences within the framework of set analysis. Additional savings are potentially available provided it is possible to estimate any of the two types of coincidence levels (see  for details). Recalling the experimental results from , we observed a substantial loss in the correlation with majority voting error if just one of the required levels is missing. In other
16159	16161	of a small hardware-secured trusted computing base) have been developed. They are, in turn, virtual machines with code verification , proof-carrying code , and inherently safe code formats . In virtual machines with code verification, the code is examined to ensure that the semantic gap between the source language and the virtual machine instruction format is not exploited. For
16159	12160	of a small hardware-secured trusted computing base) have been developed. They are, in turn, virtual machines with code verification , proof-carrying code , and inherently safe code formats . In virtual machines with code verification, the code is examined to ensure that the semantic gap between the source language and the virtual machine instruction format is not exploited. For
16159	12160	to make such a virtual machine efficiently implementable. The key here is our use of type separation and referentially-safe encodings on one hand (as previously demonstrated in the safeTSA project ), and of an intricate memory addressing scheme on the other hand. 5. ARCHITECTURE The central element of our architecture is a division of concerns between the proof-carrying code mechanism and the
16159	12160	be shorter and even faster to verify. In case of segments of programs that use only primitive types the proofs are implicit in the instruction set and doesn’t need any additional proofs. SafeTSA is a type-safe intermediate representation based on Static Single Assignment form (SSA). SafeTSA solves the problem of making SSA easily verifiable so that it can be used as a safe software
16159	872	proof typically re-establishes typing of data, for example, distinguishing Integers from Booleans and from pointers. Interestingly, current research on PCC (such as Foundational Proof Carrying Code ) appears to be directed solely at reducing the size of the trusted computing base on thestarget platform. Unfortunately, this increases the volume of the proofs that are required even further. We
16159	881	It does not provide any type safety and supports unsafe languages.Since the VM is very close to the real architecture it is able to achieve near to native speeds. Typed Assembly Language(TAL) is another frameworksfor verifying the safety of a program for a low level representation. TAL uses the type system of the source language to prove the safety of the program. It achieves this by
16159	16162	of a small hardware-secured trusted computing base) have been developed. They are, in turn, virtual machines with code verification , proof-carrying code , and inherently safe code formats . In virtual machines with code verification, the code is examined to ensure that the semantic gap between the source language and the virtual machine instruction format is not exploited. For
16159	16163	real machine code fast and efficient but also makes it an attractive compilation target. However the primary focus of most of these VMs is code optimizations rather than safe code. The LLVM project  proposes to optimize the program not only at compile time but also during link and run time. To achieve this they use a strongly typed SSA based intermediate representation(IR). Being SSA based and
16159	882	resident on desktop computers (outside of a small hardware-secured trusted computing base) have been developed. They are, in turn, virtual machines with code verification , proof-carrying code , and inherently safe code formats . In virtual machines with code verification, the code is examined to ensure that the semantic gap between the source language and the virtual machine
16159	882	programs comply with a safety policy defined by the system where the programs will execute. Typical policies are type, memory and control-flow safety, but in the framework described by Necula , any property of the program that can be expressed in first order logic constitutes a valid safety policy. Upon reception of an untrusted program, the code consumer examines the code and emits a
16159	16164	can be compressed to 12%– 20% of the size of x86 machine code (a factor of 30 reduction with respect to a previous scheme), but unfortunately this increases the proof checking time by a factor of 3 . Notice that this work compresses the proofs, but does not reduce the amount of facts that need to be proved. In some mobile code contexts, 20% space overhead or large checking times are considered
16159	16165	verification don’t exactly match those of the Java Language Specification, so that there are certain classes of perfectly legal Java programs that are rejected by all compliant bytecode verifiers. Further, due to the need to verify the code’s safety upon arrival at the target machine, and also due to the specific semantics of JVM’s particular security scheme, many possible optimizations
16167	16168	of highspeed switches and routers - employ an input-queued architecture where memories and switch fabric may operate at only the line speed. However, although many scheduling algorithms - proposed for the input-queued architecture achieve 100% asymptotic throughput, none of these algorithms match the performance of an output-queued switch. Also, when a switch is required to
16167	16168	is work-conserving for all traffic patterns and switch sizes. In , it was shown that a wide class of maximal size matching algorithms (including well-known scheduling algorithms such as iSLIP  and 2DRR ) can provide the same throughput performance of an output-queued switch with speedup equal to 2. Moreover, in , a speedup of 2 in a CIOQ switch was proven to be sufficient to
16167	4080	the inputqueued architecture because most of the schemes developed for providing QoS guarantees were based on the output-queued architecture. In these schemes, one of service scheduling algorithms - is placed at output ports and used to control the latency of packets which are immediately placed in output buffers upon arrival at input ports. The speedup of the switch fabric has been
16167	16175	Then, a natural question about a CIOQ switch arises, i.e., how much speedup is sufficient to match the performance of an output-queued switch? There have been many research works on this subject -. In , the authors showed that a CIOQ switch with VOQs operating under the lowest occupancy output first algorithm (LOOFA) and speedup of 2, is work-conserving for all traffic patterns and
16167	6268	question about a CIOQ switch arises, i.e., how much speedup is sufficient to match the performance of an output-queued switch? There have been many research works on this subject -. In , the authors showed that a CIOQ switch with VOQs operating under the lowest occupancy output first algorithm (LOOFA) and speedup of 2, is work-conserving for all traffic patterns and switch sizes.
16167	16177	authors showed that a CIOQ switch with VOQs operating under the lowest occupancy output first algorithm (LOOFA) and speedup of 2, is work-conserving for all traffic patterns and switch sizes. In , it was shown that a wide class of maximal size matching algorithms (including well-known scheduling algorithms such as iSLIP  and 2DRR ) can provide the same throughput performance of an
16167	16175	a natural question about a CIOQ switch arises, i.e., how much speedup is sufficient to match the performance of an output-queued switch? There have been many research works on this subject =. In , the authors showed that a CIOQ switch with VOQs operating under the lowest occupancy output first algorithm (LOOFA) and speedup of 2, is work-conserving for all traffic patterns and
16167	16175	(including well-known scheduling algorithms such as iSLIP  and 2DRR ) can provide the same throughput performance of an output-queued switch with speedup equal to 2. Moreover, in , a speedup of 2 in a CIOQ switch was proven to be sufficient to emulate an output-queued switch exactly using the stable marriage matching (SMM) algorithm which is well-known by the name of
16167	16175	and simultaneously, she prefers him to her assigned man. Because of its one-to-one assignment property, this stable marriage matching has been applied to scheduling for an input-queued switch as in . The stable residents/hospitals problem can be thought of as a marriage problem, in which polygamy is allowed by the hospitals, and covers the situations to find a stable many-to-one assignment
16167	16175	output-queued switch that is assumed to be fed with the same input traffic pattern as an MIOQ switch. A. Definitions In the rest of this paper, we use the following terms originally defined in , and revisit them in this section before proceeding. Definition 4: A “push-in first-out (PIFO) queue” is a queue in which arriving cells are placed at arbitrary location, and the cell at the head
16167	16175	PIFO queueing policy, regardless of 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004 X B-5 Bsincoming traffic pattern. Proof: The proof of Lemma 1 is almost identical to that of Theorem 4 in , and is omitted here for brevity. ? Lemma 2: In an MIOQ switch with a (2, 2)-dim crossbar scheduled by the SSA algorithm, the slackness L of a cell c waiting in the input side does not decrease
16167	16179	a speedup of only 2. Therefore, as a recent approach to achieve the comparable performance of an output-queued switch without any speedup, a parallel switching architecture(PSA) has been studied - which is composed of input demultiplexers, output multiplexers and parallel switches. In a PSA, an incoming stream of packets is spread packet-by-packet by a demultiplexer across the parallel
16167	16179	6 parallel switches. In , the authors adopt 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sthe co-ordination buffers instead of a complex centralized distribution algorithm used in  to avoid the mis-sequence problem among cells from different parallel switches. The conclusion is that the proposed PSA with no speedup can mimic a FIFO (First-In First-Out) output-queued switch
16167	16180	of 2, its finally-required operation rate is 6R/k. Therefore, to achieve the same performance of an output-queued switch without any speedup, the proposed architecture needs 6 parallel switches. In , the authors adopt 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sthe co-ordination buffers instead of a complex centralized distribution algorithm used in  to avoid the mis-sequence
7994989	16186	in the research literature, although some important advances have been made in the last few years. Recent methods proposed for this problem include the stochastic ruler method (Yan and Mukai, 1992; Alrefaei and Andradóttir, 1997), the method of Andradóttir (1995), the stochastic comparison method (Gong, Ho, and Zhai, 1992), ordinal optimization (Ho, Sreenivas, and Vakili, 1992), the stochastic branch-and-bound (Norkin,
7994989	16188	increasingly popular in recent years, and many theoretical advancements have also been made (Rosenthal, 1995). However, there is still a considerable gap between theory and practice in the field (Brooks and Roberts, 1998). Although many nice theoretical bounds have been derived for the convergence rate of Markov chains, such bounds normally contain constants that are not available a priori except for special cases.
7994989	16189	special cases. Therefore, such bounds are not directly applicable in practice. In fact, practitioners most often use convergence diagnostics to assess the convergence of a Markov chain simulation (Cowles and Carlin, 1996; Brooks and Roberts,s1998). Such convergence diagnostics usually consider some summary statistic of the Markov chain, and declare convergence when it appears to have settled down in steady state.
7994989	16190	is only a necessary condition for convergence, but not sufficient. Another method of bridging the gap between theory and practice is to use auxiliary simulation to estimate the required constants (Cowles and Rosenthal, 1996). This is the approach taken here. We use theoretical bounds as a basis for a stopping criteria, and then use auxiliary simulation to estimate a theoretical constant that is required for this
7994989	16192	stochastic ruler method (Yan and Mukai, 1992; Alrefaei and Andradóttir, 1997), the method of Andradóttir (1995), the stochastic comparison method (Gong, Ho, and Zhai, 1992), ordinal optimization (Ho, Sreenivas, and Vakili, 1992), the stochastic branch-and-bound (Norkin, Pflug, and Ruszczy?ski, 1996), and the nested partitions (NP) method (Shi and Ólafsson, 1997;1998). Under certain conditions, all of these methods have
7994989	16193	1997), the method of Andradóttir (1995), the stochastic comparison method (Gong, Ho, and Zhai, 1992), ordinal optimization (Ho, Sreenivas, and Vakili, 1992), the stochastic branch-and-bound (Norkin, Pflug, and Ruszczy?ski, 1996), and the nested partitions (NP) method (Shi and Ólafsson, 1997;1998). Under certain conditions, all of these methods have been shown to converge almost surely to an optimal solution, but a common
7994989	16196	its efficiency as an Markov Chain Monte Carlo (MCMC) sampler. The use of MCMC methods has become increasingly popular in recent years, and many theoretical advancements have also been made (Rosenthal, 1995). However, there is still a considerable gap between theory and practice in the field (Brooks and Roberts, 1998). Although many nice theoretical bounds have been derived for the convergence rate of
7994989	14897	(Gong, Ho, and Zhai, 1992), ordinal optimization (Ho, Sreenivas, and Vakili, 1992), the stochastic branch-and-bound (Norkin, Pflug, and Ruszczy?ski, 1996), and the nested partitions (NP) method (Shi and Ólafsson, 1997;1998). Under certain conditions, all of these methods have been shown to converge almost surely to an optimal solution, but a common difficulty is obtaining good stopping rules. This is of
5553	5553	. Since there is nothing in either the steepest-descent images or the Hessian that depends on ? , they can both be pre-computed. The inverse compositional algorithm is summarized in Figure 2. (See  for a schematic diagram of the algorithm.) The inverse compositional algorithm is far more computationally efficient than the LucasKanade algorithm. See Table 2 for a summary. The most time
5553	5553	goal of the Lucas-Kanade algorithm (see Equation (1)) is that we need to work in the linear subspace ??? ¥ ? ????????¡ . Working in this subspace can be achieved by using a weighted L2 norm  with: ¡¤£?????¥?© ??¡?£?????¥?? ? ???? ? 13 ? ??? ¡?£¦¥ ? ? ??? ¡??????? ??? (32) (33)s??? (assuming again that the vectors ??????? ? ¡¤£?????¥ ? are orthonormal) and minimizing: ?s¢¡¤£¦¥?? § ¡ ?
5553	5553	expression in Equations (33) and (34) are exactly the same thing. See  for the details. We can therefore use the inverse compositional algorithm with this weighted L2 norm (described in Part 2 ) to minimize the second term in Equation (31). The weighted steepest descent images are: ??????? ¡?£¦¥?© ??? ? ??¡¤£?????¥???? ???? ? ??? ¡?£¦¥ ¡ (see Equation (29) of ) and so can be computed:
5553	5553	? ? ¡???????¥?? because the inner product of two vectors projected into a linear subspace is the same as if just one ? ??? ¡¤£¦¥ ??????? of the two is projected into the linear subspace. Again, see  for more details. In summary, minimizing the expression in Equation (19) simultaneously with respect to ? and ? can be performed by first minimizing the second term in Equation (31) with respect to
5553	5553	is to use a robust error function instead of the “sum of squares” or Euclidean L2 norm. Robust extensions to the original inverse compositional algorithm are the subject of Part 2 of this series . The goal in  is to minimize: ? ????? ?s¢¡¤£¦¥?? with respect to the warp parameters ? where ? ¡?????????¥ and ??©?¡ ? ??? ? ? ????????? ??? ¥?? § ¡ ? ¡?£?????¥?¥?? ? ????? (50) is a symmetric
5553	5553	the various algorithms and their extensions in a consistent manner. Throughout the framework we concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed . We examine which of the extensions to the Lucas-Kanade algorithm can be applied to the inverse compositional algorithm without any significant loss of efficiency, and which extensions require
5553	5553	of warp parameters is ? and the number of pixels ins¡ computational cost of each iteration of the Lucas-Kanade ? ? ? ? ? algorithm is expensive step by far is Step 6. See Table 1 for a summary and  for the details. 4 is ? . The total ??? ¥ . The mostsIterate: The Lucas-Kanade Algorithm (1) Warp with to ????????????????? compute ? ??????????? (2) Compute ??????? the error image using ¡¤£?????¥
5553	5553	is a huge computational cost in re-evaluating the Hessian in every iteration of the Lucas-Kanade algorithm . If the Hessian were constant it could be precomputed and then re-used. In  we proposed the inverse compositional algorithm as a way of reformulating image alignment so that the Hessian is constant and can be precomputed. Although the goal of the inverse compositional
5553	5553	somewhat surprisingly, these ? two algorithms can be shown to be equivalent to first ? order in . They take (approximately) the same steps as they minimize the expression in Equation (1). See  for the proof of equivalence. 2.2.2 Derivation of the Inverse Compositional Algorithm Performing a first order Taylor expansion on Equation (11) gives: Assuming that ? where ??? ?s?¡ ??? ?
5553	5553	(7) ? Compute ? ? ? ? ??? ?????¤??????? (8) Compute ????????? (9) ?????????????????????????????¦????????????? Update the warp ??? ? ??????????? until Figure 2: The inverse compositional algorithm . All of the computationally demanding steps are performed once in a pre-computation step. The main algorithm simply consists of image warping (Step 1), image differencing (Step 2), image dot
5553	14049	use in the Lucas-Kanade algorithm , image alignment has become one of the most widely used techniques in computer vision. Besides optical flow, some of its other applications include tracking , parametric and layered motion estimation , mosaic construction , medical image registration , and face coding . The usual approach to image alignment is gradient descent. A
5553	14049	variation. Linear appearance variation has been considered by a number of authors, most notably by Hager and Belhumeur for illumination , by Black and Jepson for general appearance variation , and by Cootes and Taylor for non-rigid face modeling . As in Part 2, we distinguish two cases: (1) when the error function is the Euclidean L2 norm (the case that the error function is a
5553	14049	? ? ? and , the expression in Equation (18) can then model any possible gain and bias. ??? More can be used to model arbitrary linear illumination variation  or general appearance variation . If the expression in Equation (18) should appear (appropriately warped) in the input image §s¢¡?£¦¥???? ???? ????? ??? ¡¤£¦¥ , instead of Equation (1) we should minimize: ????? ¡¤£¦¥?? § ¡ ?
5553	1102	optical flow, some of its other applications include tracking , parametric and layered motion estimation , mosaic construction , medical image registration , and face coding . The usual approach to image alignment is gradient descent. A variety of other numerical algorithms have also been proposed , but gradient descent is the defacto standard. We propose a unifying
5553	1102	by a number of authors, most notably by Hager and Belhumeur for illumination , by Black and Jepson for general appearance variation , and by Cootes and Taylor for non-rigid face modeling . As in Part 2, we distinguish two cases: (1) when the error function is the Euclidean L2 norm (the case that the error function is a general weighted L2 norm is similar), and (2) when the error
5553	16200	? ? ??????? ????? ? ? ????? ? ? . ????? ? ? As a number of authors have pointed out, there is a huge computational cost in re-evaluating the Hessian in every iteration of the Lucas-Kanade algorithm . If the Hessian were constant it could be precomputed and then re-used. In  we proposed the inverse compositional algorithm as a way of reformulating image alignment so that the Hessian is
5553	16203	use in the Lucas-Kanade algorithm , image alignment has become one of the most widely used techniques in computer vision. Besides optical flow, some of its other applications include tracking , parametric and layered motion estimation , mosaic construction , medical image registration , and face coding . The usual approach to image alignment is gradient descent. A
5553	16203	the series, we cover image alignment with linear appearance variation. Linear appearance variation has been considered by a number of authors, most notably by Hager and Belhumeur for illumination , by Black and Jepson for general appearance variation , and by Cootes and Taylor for non-rigid face modeling . As in Part 2, we distinguish two cases: (1) when the error function is the
5553	16203	We then derive an efficient approximation to the simultaneous inverse compositional algorithm and also describe the extremely efficient “project out” algorithm proposed by Hager and Belhumeur . The project out algorithm first projects out the appearance variation and just solves for the warp parameters. Then in a second step, it solves for the 1sappearance parameters. We study the
5553	16203	? ? ??????? ????? ? ? ????? ? ? . ????? ? ? As a number of authors have pointed out, there is a huge computational cost in re-evaluating the Hessian in every iteration of the Lucas-Kanade algorithm . If the Hessian were constant it could be precomputed and then re-used. In  we proposed the inverse compositional algorithm as a way of reformulating image alignment so that the Hessian is
5553	16203	one” image. Given appropriate values ? ? ? and , the expression in Equation (18) can then model any possible gain and bias. ??? More can be used to model arbitrary linear illumination variation  or general appearance variation . If the expression in Equation (18) should appear (appropriately warped) in the input image §s¢¡?£¦¥???? ???? ????? ??? ¡¤£¦¥ , instead of Equation (1) we
5553	16204	Introduction Image alignment consists of moving, and possibly deforming, a template to minimize the difference between the template and an image. Since its first use in the Lucas-Kanade algorithm , image alignment has become one of the most widely used techniques in computer vision. Besides optical flow, some of its other applications include tracking , parametric and layered motion
5553	16204	of the robust appearance variation algorithms. 2 Background: Image Alignment Algorithms 2.1 The Lucas-Kanade Algorithm The original image alignment algorithm was the Lucas-Kanade algorithm . The goal of the Lucas-Kanade algorithm is to align a template images¢¡¤£¦¥ ¡???????¥?? is a column vector containing the pixel coordinates. Let ? set of allowed warps, where the pixel £ in the
5553	16204	matrix using Equation (9) and ? invert ? it ??? ?????¤?¢????? (7) Compute ? ? ? ? ? (8) Compute ??????? (9) ????????????? Update the parameters until ??????????? Figure 1: The Lucas-Kanade algorithm  consists of iteratively applying Equations (8) & (3) until the estimates of the parameters ? converge. Typically the test for convergence is whether some norm of the vector ??? is below a user
5553	16205	optical flow, some of its other applications include tracking , parametric and layered motion estimation , mosaic construction , medical image registration , and face coding . The usual approach to image alignment is gradient descent. A variety of other numerical algorithms have also been proposed , but gradient descent is the defacto standard. We propose a unifying
5553	16205	taking time . The only additional cost is inverting and composing it with . These two steps typically require operations. See . Potentially these 2 steps could be fairly involved, as in , but the computational overhead is almost always completely negligible. Overall the cost of the inverse compositional algorithm is ? ¡ ??? ? ? ? ¥ per iteration rather than ? ??? ¡ ? ? ? ? ??? ¥ ,
5553	16205	? ? ? and , the expression in Equation (18) can then model any possible gain and bias. ??? More can be used to model arbitrary linear illumination variation  or general appearance variation . If the expression in Equation (18) should appear (appropriately warped) in the input image §s¢¡?£¦¥???? ???? ????? ??? ¡¤£¦¥ , instead of Equation (1) we should minimize: ????? ¡¤£¦¥?? § ¡ ?
5553	16206	the most widely used techniques in computer vision. Besides optical flow, some of its other applications include tracking , parametric and layered motion estimation , mosaic construction , medical image registration , and face coding . The usual approach to image alignment is gradient descent. A variety of other numerical algorithms have also been proposed , but
5553	16206	? ? ??????? ????? ? ? ????? ? ? . ????? ? ? As a number of authors have pointed out, there is a huge computational cost in re-evaluating the Hessian in every iteration of the Lucas-Kanade algorithm . If the Hessian were constant it could be precomputed and then re-used. In  we proposed the inverse compositional algorithm as a way of reformulating image alignment so that the Hessian is
8920883	16210	interface nightmare. Must we put up with thousands of nagging ”May I?” prompts? Ka-Ping Yee of Berkeley has shown that the answer is ”No”; user actions implicitly specify the desired permissions. When I double-click on a Word document, I am telling the system that I want the process running Word to be able to read and write only this specific file. I don’t have to worry that a macro virus
16211	16213	Two technological trends making this feasible are the growth in embedded computers and wireless networking. About 98% of all microprocessors sold are embedded, and their percentage is growing . They are present in cell-phones, watches, stereos, microwaves, washing machines, wireless thermometers, cordless phones and answering machines. They exist in pocket video games, VCRs, DVD players,
16211	16216	of a more integrated system theory combining all these areas. For example, signal/image processing methods with information theoretic performance assessment and connections are already emerging . Networking is seeing the confluence of computer science with more traditional communications research conducted in Electrical Engineering departments.(INFOCOM, for example, is jointly organized by
16211	16217	of a more integrated system theory combining all these areas. For example, signal/image processing methods with information theoretic performance assessment and connections are already emerging . Networking is seeing the confluence of computer science with more traditional communications research conducted in Electrical Engineering departments.(INFOCOM, for example, is jointly organized by
16211	16218	malleability, we employ dedicated laptops (all running Linux) for each radio controlled car. For now, the controller for each car runs on its dedicated laptop. However, as described in the paper  describing the software infrastructure and middleware aspects of this project, the next phase of our software infrastructure development will involve automatic migration of code so that the
16211	16218	multiple actuators (the fifteen cars), and multiple computational resources, with loops closed over an ad hoc wireless network. We refer to systems of this type as Federated Control Systems . In principle, we should be able to replace the cars with airplanes, vision systems with GPS or other sensors, and have an air traffic control system. The architecture should be the same. Or,
16211	16219	layers of abstractions, giving a specific purpose to each layer and hopefully enabling it to perform at that layer. (There are many cross-layer design issues as well that deserve attention; see ). Services at a layer can be oblivious to lower layers, and hence can focus on that portion of the design which has been delegated to them. In order to interoperate, we must, of course, provide
16211	16223	for generating collision-free timed trajectories for each car along routessThe Convergence of Control, Communication, and Computation 11 which represent the high level goals of the system . The goals may be specified merely as a triple comprising an origination location, an intermediate waypoint, and a final destination on the track. The server must then be able to determine feasible
8920884	16228	Language” (OCL ). There are also many kinds of semantic interpretations of invariants and pre/postcondition constraints, for instance, axiomatic interpretations using Hoare-style triples , translations into algebraic specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using
8920884	16229	specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This research has been partially supported by the GLOWA-Danube project (07GWK04) sponsored by the German Federal Ministry of Education and Research, by the InOpSys project (WI 841/6-1) sponsored
8920884	16230	pre/postcondition constraints, for instance, axiomatic interpretations using Hoare-style triples , translations into algebraic specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This research has been partially supported
8920884	16232	a hierarchical specification as a suitable class of functions which can be applied to any realisation M of a subsystem specification and yields a realisation F (M) of the overall system (see e.g. ). Hence F plays the role of a user of M and therefore, for proving the correctness of F , it should be enough if F relies on the black-box views of the correct subsystem realisations M. We show
8920884	16233	interpretations of invariants and pre/postcondition constraints, for instance, axiomatic interpretations using Hoare-style triples , translations into algebraic specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This
8920884	16234	specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This research has been partially supported by the GLOWA-Danube project (07GWK04) sponsored by the German Federal Ministry of Education and Research, by the InOpSys project (WI 841/6-1) sponsored
8920884	16234	{hennicke,knapp,baumeist}@pst.ifi.lmu.de Even for a single formalism like OCL, two different styles of interpretations, in the following called the “implies”-style and the “and”-style (see also ), have been proposed. Given a pre-/postcondition constraint of the form C::op(. . .) pre: P post: Q, the “implies”-style () basically requires that if the precondition P is satisfied in
8920884	16234	is satisfied in the state ? ? after the execution of the operation. If the precondition is not satisfied in ? then the operation yields an arbitrary result. On the other hand, the “and”=style () considers relations (or state transitions) between pre- and poststates which simply do not contain any pair (?, ? ? ) where the precondition is not satisfied in the prestate ?. The semantic
8920884	16236	specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This research has been partially supported by the GLOWA-Danube project (07GWK04) sponsored by the German Federal Ministry of Education and Research, by the InOpSys project (WI 841/6-1) sponsored
8920884	16238	using Hoare-style triples , translations into algebraic specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This research has been partially supported by the GLOWA-Danube project (07GWK04) sponsored by the German
8920884	16238	the following called the “implies”-style and the “and”-style (see also ), have been proposed. Given a pre-/postcondition constraint of the form C::op(. . .) pre: P post: Q, the “implies”-style () basically requires that if the precondition P is satisfied in the state ? before the operation is performed then the postcondition is satisfied in the state ? ? after the execution of the
8920884	687	In this area, many formalisms have been developed which either are tailored to a particular programming language, like the assertion language of Eiffel  and the “Java Modeling Language” (JML ), or are programming language independent like Object-Z  and the “Object Constraint Language” (OCL ). There are also many kinds of semantic interpretations of invariants and
8920884	16241	specifications  or into (dynamic) firstorder logic , coalgebraic interpretations , functional interpretations  and there are semantic approaches using labelled transition systems . ? This research has been partially supported by the GLOWA-Danube project (07GWK04) sponsored by the German Federal Ministry of Education and Research, by the InOpSys project (WI 841/6-1) sponsored
16242	16247	FO 2 NEXPTIME complete  ??(TC complete  ? ) ? p ??(DTC 2 complete Prop 2 + ) NEXPTIME complete Th 4, 5 ??(TC, f) NEXPTIME complete Cor 6 ?? ? p 2 Undecidable Citation FO 2 (TC)  FO 2 (DTC)  ?(TC + ) Cor 9 ?(DTC + ) Th 8 ?(DTC ? ) Th 13 Fig. 1. Summary of the decidability and complexity, and the undecidability of the logics we study. The arity of all relation
16242	16247	that the arity of all relation symbols is bounded. 4 For example, consider the simple, decidable logic FO 2 . This is first-order logic restricted to having only two variables, x, y. Grädel et al.  prove that if we add the transitive-closure operator (TC) to FO 2 then the resulting logic is undecidable. In fact, they prove that even FO 2 (DTC) is undecidable. Here DTC — deterministic
16242	16248	has an EXPTIME-complete satisfiability problem  and the same has been shown true even for the more expressive guarded fixed-point logic, as long as the vocabulary remains of bounded arity . Guarded fixed-point logic can express reachability from a specific constant, or from some point of a specific color, and it can restrict this reachability to be along paths specified, for example,
16242	16248	a reference, and for results new to this paper we include the number of the relevant theorem. Decidable Complexity Citation µ calculus EXPTIME complete  Guarded Fixed Point EXPTIME complete  MSO(trees) non-elementary  FO 2 NEXPTIME complete  ??(TC complete  ? ) ? p ??(DTC 2 complete Prop 2 + ) NEXPTIME complete Th 4, 5 ??(TC, f) NEXPTIME complete Cor 6 ?? ? p 2
16242	4995	problems for monadic second-order logic with a single unary function symbol are decidable, 8 although their complexities are not elementary, even when restricted to first-order quantification . It is not hard to modify the proofs of Theorems 4 and 5 to apply to ?(TC, f). (For functions, the implication of Equation (1) is a biimplication, and thus the result goes through for positive and
16242	4990	, it is shown that this modeling can be improved so that it computes the most precise possible transformation summarizing each program step, through the use of decidable logics. Furthermore, in  we show that we can use a method we call “structure simulation” to significantly extend the sets of data structures that we can model with decidable logics over trees (monadic second-order logic)
16242	4990	in the former. As an example, to express reachability in dynamic, undirected graphs, as in , we need not only a spanning forest, but a record of all the remaining edges in the undirected graph . Fig. 1 summarizes results concerning the decidability and complexity of satisfiability for relevant logics. All the languages will be defined precisely in the next two sections. For previously
16242	10783	a tiling problem as in Definition 1, and let n be a natural number. It is an NEXPTIME-complete problem to test on input (T , 1 n ) whether there is a T -tiling of a square grid of size 2 n by 2 n . We will define a formula ?n that expresses exactly a solution to this tiling problem. There will be two constants: s, denoting the cell in the upper-left corner, and t, denoting the cell in the
16242	16251	Very generally, we model the properties of an infinite set of data structures that can be generated by the program we are analyzing, using a bounded set of first-order, three-valued structures . In , it is shown that this modeling can be improved so that it computes the most precise possible transformation summarizing each program step, through the use of decidable logics.
16242	5004	generally, we model the properties of an infinite set of data structures that can be generated by the program we are analyzing, using a bounded set of first-order, three-valued structures . In , it is shown that this modeling can be improved so that it computes the most precise possible transformation summarizing each program step, through the use of decidable logics. Furthermore, in
8920886	16273	Reckoning OMNI  Sonar, Infrared, Bump, Dead Reckoning the SWCS, the Notre Dame Computer-Controlled Power Wheelchair Navigation System (CPWNS) , and the Hephaestus Smart Wheelchair System  are the only systems that have been (or are being) developed as standalone units that can be added to existing wheelchairs. The CPWNS is intended for use with multiple brands of wheelchairs, but
8920886	16273	provide autonomous navigation between points. The investigative team developing the SWCS has previously worked on several smart wheelchair projects, including the NavChair , the Hephaestus , and the TinMan , along with research on computer vision and force-feedback joysticks for smart wheelchairs . As just described, the SWCS shares with the Hephaestus the goal of
8920886	16276	several smart wheelchair projects, including the NavChair , the Hephaestus , and the TinMan , along with research on computer vision and force-feedback joysticks for smart wheelchairs . As just described, the SWCS shares with the Hephaestus the goal of compatibility with multiple brands and models of wheelchairs, but the mechanisms for interfacing with wheelchairs have changed
8920886	16273	has shown that user trials of a smart wheelchair performed over a limited number of sessions are unlikely to produce significant differences in performance between aided and unaided navigation . This does not, of course, discount the performance of evaluating the SWCS with individuals with disabilities. However, we have chosen to delay user trials until the SWCS has been developed to the
16284	16285	essential mechanism of any file system. 6.3 Data Compression Data compression reduces the size of a file by taking out the redundancy in the file. This technique has been used in file systems ,  and web proxies . In general, however, the reduction factors achieved by data compression are smaller than those of operation shipping. This is because the former operates generically, while
16284	16288	execution ofP from a recent checkpoint, and will thereafter assume the role of P . For load balancing, a Unix process can migrate to another host to reduce the load imposed on the original host . For consistency guarantees, a previous Coda project proposed the notion of Isolation-Only Transaction. Users can delimit portions of executions using this notion. When an update conflict happens,
16284	16294	cached objects, and are also logged in a client-modify log (CML). The logging mechanism allows updates to be eventually propagated to the servers that maintain the primary replica of the objects , . This eventual propagation is called reintegration. A CML consists of records called CML entries, each is recording the effect of a mutating file-system operation. For example, a chmod
16284	16294	open, interspersed possibly with several write’s, and a final close operation on a file. Coda maps the whole sequence of operations to a single STORE record because it uses a session semantics . Second, a STORE’s associated data includes the content of the file being stored. In contrast, other record types do not include the content but only some directory attributes such as the owner or
16284	16294	2002 Traditionally, Coda uses a value-based approach for propagating STORE records. Container files, which represent the logged values of store operations, are shipped across the weak network . This is exactly the performance bottleneck that can be optimized out by operation shipping. In the next section, we will see how Venus can perform update propagation without shipping the bulky
16284	16294	that will cancel CML records that have no final effects. An example of these records are the nonfinal STORE records for a file that is updated many times before reintegration (see Chapter 6.3 of  for the formal treatment on this subject). For operation shipping, we need to slightly modify the standard procedure so as to preserve the needed information for validation. Specifically, an
16284	13825	research has demonstrated the feasibility and benefits of disconnected operation, in which a file-system client can continue to function even when it loses network connectivity to its server . They have also demonstrated that weak connectivity can be exploited. A key technique for the latter is to decouple the slow update propagation from the foreground processing of file-system
16284	13825	4.3, 4.4, and 4.5). To validate our approach, we have designed, implemented, and evaluated a prototype system. The main component of the system is an extended version of the Coda File System , , , . We also made minor extensions to a popular UNIX shell called the Bourne Again Shell (bash) and an image application called the GIMP. They serve as case studies showing how operation
16284	13825	still being actively developed and maintained by both the Carnegie Mellon University and a team of volunteer programmers around the Internet . It has been well-documented in the literature , , , , , , , , so here we only provide a very brief background. Coda uses a client—server model. In each Coda installation, there are many clients and a few servers. 1 On each
16284	16299	logging/shipping for some applications. The file system can also dynamically decide on the best mode of update propagation based on the network conditions, application execution time, etc. See  for some of these possible extensions.sLEE ET AL.: OPERATION SHIPPING FOR MOBILE FILE SYSTEMS 1413 Fig. 3. Logging of user operations. (a) The typical sequence of an ordinary (i.e., nonlogging)
16284	16299	image authoring , . Being an interactive application, GIMP has to be modified before it can participate in operation shipping. Fortunately, we found that the needed modification is moderate . It does not involve major changes in the internal logic of the application but only some minor alternations on the user-interface modules—each GIMP command being logged will need a few lines of
16284	16299	the surrogate, and the servers. The five phases are: 1. requesting, 2. replaying, 3. validation, 4. reintegration/aborting, and 5. finalization. Fig. 6 depicts an overview of the shipping stage;  gives more details. The mechanisms are very similar for both application-transparent and application-aware cases. We will first present the mechanism for the former, and then discuss the additional
16284	16299	better presentation in the figure. validation (Section 4.2). This modified procedure increases the overhead slightly but preserves the effectiveness of cancelation optimization (see Chapter 4.4 of ). 3.6 Surrogate The key roles of a surrogate 3 are to reexecute user operations and to reintegrate results with the servers on behalf of its weakly-connected client. There are three reasons why we
16284	16299	the range of 1.4 times to nearly 50 times. As we will discuss in Section 3, there are two types of operation shipping: application-transparent and application-aware. An early version of this paper  has presented some results for the first type. In this paper, we will present a complete picture, discuss both types of operation shipping, and report the experimental results of application-aware
16284	16299	on the surrogate. 5 EVALUATION We have performed two sets of controlled experiments for the application-transparent and application-aware cases respectively. The first set has been reported before (), so we may omit some details for the first set and focus more on the second set. 5.1 Experimental Setup The two sets of experiments were performed in two different time periods, so the hardware
16284	16299	speedups. For the application-transparent case, these ranged from 1.4 times to 26.3 times. (Due to space limitation, here we omit the individual numbers since they have been reported before ). For the application-aware case, the numbers are shown in Fig. 10. The speedups were the most substantial in the 9.6-Kbps network, where eight out of the 10 tests were accelerated by a factor
16284	16299	when we compressed the traffic for value shippingLV in Fig. 8 using the populargzip utility, which uses the Lempel-Ziv coding (LZ77), we could reduce the traffic by a factor of 2.7 to 8.1 (see ). These reductions were good but not as substantial as those achieved by operation shipping, which ranged from 12.0 to 245.7 times. Nevertheless, like delta shipping, datasLEE ET AL.: OPERATION
16284	16300	Coda project proposed the notion of Isolation-Only Transaction. Users can delimit portions of executions using this notion. When an update conflict happens, Coda will reexecute the transaction ,  to resolve the conflicts. Our work is different to these previous works in the specific goals and contexts. 6.2 Delta Shipping To reduce the network traffic for shipping a file, sometimes we
16284	16301	project proposed the notion of Isolation-Only Transaction. Users can delimit portions of executions using this notion. When an update conflict happens, Coda will reexecute the transaction ,  to resolve the conflicts. Our work is different to these previous works in the specific goals and contexts. 6.2 Delta Shipping To reduce the network traffic for shipping a file, sometimes we can
16284	16302	developed and maintained by both the Carnegie Mellon University and a team of volunteer programmers around the Internet . It has been well-documented in the literature , , , , , , , , so here we only provide a very brief background. Coda uses a client—server model. In each Coda installation, there are many clients and a few servers. 1 On each client, a cache
16284	16303	algorithms such as diff, which works for text files, and rsync, which works for binary files , or even file systems such as LBFS (, ). It is also used in web proxies , file archives , and source-file repositories , . However, delta shipping has several limitations. First, newly created files have no previous version (or we can say the delta of a newly created files is
16284	16304	behind utilities and algorithms such as diff, which works for text files, and rsync, which works for binary files , or even file systems such as LBFS (, ). It is also used in web proxies , file archives , and source-file repositories , . However, delta shipping has several limitations. First, newly created files have no previous version (or we can say the delta of a
16284	16304	of any file system. 6.3 Data Compression Data compression reduces the size of a file by taking out the redundancy in the file. This technique has been used in file systems ,  and web proxies . In general, however, the reduction factors achieved by data compression are smaller than those of operation shipping. This is because the former operates generically, while the latter exploits
16284	16305	4.4, and 4.5). To validate our approach, we have designed, implemented, and evaluated a prototype system. The main component of the system is an extended version of the Coda File System , , , . We also made minor extensions to a popular UNIX shell called the Bourne Again Shell (bash) and an image application called the GIMP. They serve as case studies showing how operation shipping
16284	16305	objects, and are also logged in a client-modify log (CML). The logging mechanism allows updates to be eventually propagated to the servers that maintain the primary replica of the objects , . This eventual propagation is called reintegration. A CML consists of records called CML entries, each is recording the effect of a mutating file-system operation. For example, a chmod operation is
16284	16306	They have also demonstrated that weak connectivity can be exploited. A key technique for the latter is to decouple the slow update propagation from the foreground processing of file-system requests . Unfortunately, even though update propagation is now a background activity, it is still a performance bottleneck in a weak network. Because it is traditionally done by shipping updated files in
16284	16306	actively developed and maintained by both the Carnegie Mellon University and a team of volunteer programmers around the Internet . It has been well-documented in the literature , , , , , , , , so here we only provide a very brief background. Coda uses a client—server model. In each Coda installation, there are many clients and a few servers. 1 On each client, a
16284	16311	1 INTRODUCTION MOBILE computers, unlike their stationary counterparts, are often at the mercy of weak connectivities—networks that are intermittent, low-bandwidth, expensive, or high-latency , . A mobile file system is a distributed file system that works well even with these unpleasant networks. Previous research has demonstrated the feasibility and benefits of disconnected
16284	16313	4.3, 4.4, and 4.5). To validate our approach, we have designed, implemented, and evaluated a prototype system. The main component of the system is an extended version of the Coda File System , , , . We also made minor extensions to a popular UNIX shell called the Bourne Again Shell (bash) and an image application called the GIMP. They serve as case studies showing how
16284	16313	It is still being actively developed and maintained by both the Carnegie Mellon University and a team of volunteer programmers around the Internet . It has been well-documented in the literature , , , , , , , , so here we only provide a very brief background. Coda uses a client—server model. In each Coda installation, there are many clients and a few servers. 1 On
16284	16313	a container file, which is a regular Unix file serving a double role as both the cache copy of the file and the logged value of the store operation. 1. Coda supports the use of replicated servers , . When an object is multiply replicated, a client needs to talk to multiple servers for processing the object. In the following, we use the plural form of the noun “servers,” which actually
16317	8152	devices with the traditional fixed Internet. The first scenario is just moving its first steps and still investigating solutions mostly at the network level, e.g., for multi-hop cooperative routing . At the opposite, the second scenario, which we will call wireless Internet in the following, already starts to exhibit research and commercial network-level solutions to support mobile
16317	16318	At the opposite, the second scenario, which we will call wireless Internet in the following, already starts to exhibit research and commercial network-level solutions to support mobile connectivity . However, the investigation of several aspects of service accessibility in the global and open wireless Internet, such as dynamic un/installation of infrastructure/service components, configuration
16317	16320	configuration management, service content adaptation, security, and interoperability, is still at its beginning and calls for flexible state-of-the-art middleware solutions at the application level . In addition, the wireless Internet stimulates research on several novel classes of services (and on methodologies for their development) where service results and the offered Quality of Service
16317	16320	middleware solutions for the wireless Internet. For a rapid overview of the most relevant recent approaches that propose MA-based partial solutions for pervasive computing, please refer to . 3. Mobile UbiQoS: Architecture and Middleware Components The provisioning of wireless Internet services to portable devices usually requires downsizing service contents to suit the specific limits
16317	16320	of access device capabilities, and security certificates, from directory-based name services can be too long to be directly controlled and managed by terminals with severe resource constraints . By focusing on where to operate VoD tailoring depending on the access device visualization capabilities and the installed software, typically by reducing frame rate/resolution and format
16317	16320	permit to define loosely coupled localities organized in a hierarchical way; this locality scenario naturally maps into discovery services with the visibility scope of one single domain . Figure 1. Portable devices roaming among MU network localities. The MU middleware consists of different components, as depicted in Figure 2. Shadow proxies represent portable devices over the
16317	16320	of MU-based location-dependent services without impacting on the application logic. In other words, migration is completely location-transparent and not visible to the client/server implementation . In the case of Wi-Fi connectivity, MU exploits the monitoring information that IEEE 802.11 service access points make available via standard Simple Network Management Protocol (SNMP) Management
16317	16322	technology enables the deployment of the MU middleware by enhancing the fixed network and by extending infrastructure services only when and where needed, without imposing any operation suspension . MU is the result of the extension to wireless portable devices of the existing ubiQoS middleware for the QoS tailoring, control and adaptation of VoD flows over best-effort global networks . In
16317	16324	to give direct location visibility to VoD clients when interested in developing novel locationaware service clients. Other details about the MU architecture and implementation can be found in . First experimental results show that the MU middleware reorganization time in response to a client device change of wireless locality is compatible with the challenging requirements imposed by the
16317	16324	access devices and the already admitted service sessions. In addition, MAs can simplify the QoS adaptation in response to modifications in the availability of network resources at provision time . For instance, MAs can dynamically migrate where needed to monitor network resources locally; this visibility of local monitoring indicators permits to trigger management operations to correct the
16317	16324	Even if not specific of the wireless Internet domain, the availability of these solutions is important and can significantly leverage the adoption of MAbased middlewares for pervasive computing . Due to the novelty of the MA technology, however, there are not many MA environments already employed to implement middleware solutions for the wireless Internet. For a rapid overview of the most
16317	16324	additionally note thatsthe server-based approach makes distributed caching infrastructures less effective because VoD flows are tailored since the beginning to fit the specific client requirements . On the opposite, client-based tailoring decentralizes the different charges: it is the client site that downscales the VoD flow before visualizing it, with a significant engagement of client
16317	16324	In addition, any active node can cache the traversing VoD flows before their possible local QoS downscaling, thus permitting the realization of an effective distributed caching infrastructure . MU extends significantly the applicability of the ubiQoS middleware to portable access devices with wireless connectivity and strict limits on local resources. 4. Implementation Insights about
16317	16330	to sense and control any new device entering the local MU domain. In the case of Bluetooth-based service access points (Bluetooth connectivity exploited in the so-called infrastructure mode ), PDLS uses the Java Bluetooth API specification to access the information about the devices currently connected to the network locality. The Java Bluetooth API is a novel specification to enable
16317	12908	when MU senses the client change of locality. Some first vendor-specific and technology-specific solutions for mobility prediction are at the state of the art of the wireless communication research . To the best of our knowledge, portable implementation solutions for mobility prediction and application-level APIs for their exploitation are still completely lacking. On the other hand, we start
438437	6538	of the page. The use of anchor text in Web search was first reported in 1994  and anchor text is believed to make a significant contribution to the effectiveness of the Google search engine . It has also been shown to be very effective on navigational (but not topic relevance) search tasks over enterprise-scale collections such as the TREC .GOV and WT10g collections   and in real
438437	16332	search engine . It has also been shown to be very effective on navigational (but not topic relevance) search tasks over enterprise-scale collections such as the TREC .GOV and WT10g collections   and in real enterprise intranets . 2. OKAPI BM25 Based on consistent results in TREC evaluations, the Okapi BM25 relevance scoring formula  can be said to embody a good model of relevance
438437	16332	of k1 with tf assuming a document of average length, N = 100000, n = 10 t, dl is the length of the document and avdl is the average document length 1 . k1 = 2.0, b = 0.75 are the constants used by  and in experiments reported here. Let us examine the three components of the BM25 formula and consider how applicable they are to anchor text. Term frequency: Anchor text exhibits very different
438437	16334	to be very effective on navigational (but not topic relevance) search tasks over enterprise-scale collections such as the TREC .GOV and WT10g collections   and in real enterprise intranets . 2. OKAPI BM25 Based on consistent results in TREC evaluations, the Okapi BM25 relevance scoring formula  can be said to embody a good model of relevance based upon term occurrences within text
438437	16335	an anchor text surrogate for the target. Here, the content of the anchor text surrogate will be called the “anchor text” of the page. The use of anchor text in Web search was first reported in 1994  and anchor text is believed to make a significant contribution to the effectiveness of the Google search engine . It has also been shown to be very effective on navigational (but not topic
438437	7287	such as the TREC .GOV and WT10g collections   and in real enterprise intranets . 2. OKAPI BM25 Based on consistent results in TREC evaluations, the Okapi BM25 relevance scoring formula  can be said to embody a good model of relevance based upon term occurrences within text documents. Here is a simplified version: wt = tf d × log( N?n+0.5 n+0.5 ) k1 × ((1 ? b) + b × dl avdl ) + tf
8920911	16338	genome sequences of the 150 organisms were obtained from the KEGG/GENES, JGI, and GenBank databases. Domain search was performed with the programs HMMER package and the Pfam database (release 7.2) . Cluster analysis was carried out by the ClustalW program. 3 Results and Discussion The survey of genome sequences of 150 organisms revealed the presence of 1252 GAF domains encorded by 1065 ORFs
6164	2868	while the control attributes are chosen to optimize the objective function. In more detail, we are given a very large collection of objects x1, . . ., xN, where each object xj has data attributes xj, . . ., xj. From the data attributes of an object xj, we precompute summary attribute(s) ?j = e(xj) and estimate the statistical attribute(s) ?j = f(xj, ?j). Finally, we define control
6164	7412	from partitioned data using decision trees and then aggregating the results. Fundamentally, our approach to this class of data mining problems is an outgrowth of the idea of inductive learning  and classification and regression trees (CART) . CART is a widely accepted tree-based methodology for performing classification and regression based on adaptation to a training set consisting of
16351	16352	massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since human operators may be unable to assess events quickly enough to respond effectively, there is increasing interest in “closing the loop” with tools to plan responses, and execute
16351	16352	as in PIER or Astrolabe. There has been much recent progress on statistical analysis tools that infer component relationships from histories of interaction patterns (e.g., from packet straces) . This work has demonstrated the value of pervasive instrumentation as a basis for problem determination and performance profiling.s3 Splice Architecture The design of Splice was guided by two
16351	16353	massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since human operators may be unable to assess events quickly enough to respond effectively, there is increasing interest in “closing the loop” with tools to plan responses, and execute
16351	16353	as in PIER or Astrolabe. There has been much recent progress on statistical analysis tools that infer component relationships from histories of interaction patterns (e.g., from packet straces) . This work has demonstrated the value of pervasive instrumentation as a basis for problem determination and performance profiling.s3 Splice Architecture The design of Splice was guided by two
16351	16354	massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since human operators may be unable to assess events quickly enough to respond effectively, there is increasing interest in “closing the loop” with tools to plan responses, and execute
16351	16354	as in PIER or Astrolabe. There has been much recent progress on statistical analysis tools that infer component relationships from histories of interaction patterns (e.g., from packet straces) . This work has demonstrated the value of pervasive instrumentation as a basis for problem determination and performance profiling.s3 Splice Architecture The design of Splice was guided by two
16351	16355	of initiatives for autonomic computing and adaptive enterprises at IBM and HP respectively. These trends combine in the idea of a “knowledge plane” for the Internet and other large-scale systems . This paper explores a new dimension of the emerging knowledge plane: the role of environmental sensors, physical location, and spatial and topological relationships with respect to support systems
16351	14532	for the instrumentation, and leveraging research in largescale sensor networks  and queries on massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since human operators may be unable to assess events quickly enough to
16351	16357	is to extend it to Internet-scale systems, often using a sensor metaphor for the instrumentation, and leveraging research in largescale sensor networks  and queries on massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since
16351	16357	moves through the overlay (e.g., TAG ). Other recent work has addressed massively distributed queries over current readings in sensors distributed across a wide area (e.g., Sophia  and PIER ). PIER uses an overlay to combine partial results from distributed query engines. Relative to these efforts, Splice focuses on queries over history and uses location to integrate environmental
16351	16358	massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since human operators may be unable to assess events quickly enough to respond effectively, there is increasing interest in “closing the loop” with tools to plan responses, and execute
16351	16358	as in PIER or Astrolabe. There has been much recent progress on statistical analysis tools that infer component relationships from histories of interaction patterns (e.g., from packet straces) . This work has demonstrated the value of pervasive instrumentation as a basis for problem determination and performance profiling.s3 Splice Architecture The design of Splice was guided by two
16351	5749	the art in three significant ways. The first is to extend it to Internet-scale systems, often using a sensor metaphor for the instrumentation, and leveraging research in largescale sensor networks  and queries on massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose
16351	5749	approaches appear in sensor networks that construct ad hoc overlays based on proximity, and combine and process information— including spatial information—as it moves through the overlay (e.g., TAG ). Other recent work has addressed massively distributed queries over current readings in sensors distributed across a wide area (e.g., Sophia  and PIER ). PIER uses an overlay to combine
16351	16359	the need to manage power and cooling in a data center. The cost of energy to power and cool the equipment of a large data center is significant (e.g., $72,000 per month for a 40,000 sq. ft. facility). Moreover, technology trends are driving increasing power density, in part to reduce costs for space and cabling. As a result, the infrastructure for power and cooling is critical to reliability,
16351	16362	is to extend it to Internet-scale systems, often using a sensor metaphor for the instrumentation, and leveraging research in largescale sensor networks  and queries on massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since
16351	16362	aspects of massive instrumentation. For example, others have addressed the design of scalable overlay structures to disseminate sensor information to distributed analysis components. Astrolabe  is one related project that has addressed this issue. While any structured overlay could be used, Astrolabe propagates information among 3 zones organized to reflect a logical administrative
16351	16363	a sensor metaphor for the instrumentation, and leveraging research in largescale sensor networks  and queries on massive sensor fields  for wide-area infrastructures such as PlanetLab  or the Grid . The second is to develop analysis tools to recognize patterns and diagnose anomalies in the data . Finally, since human operators may be unable to assess events quickly
16351	16363	it moves through the overlay (e.g., TAG ). Other recent work has addressed massively distributed queries over current readings in sensors distributed across a wide area (e.g., Sophia  and PIER ). PIER uses an overlay to combine partial results from distributed query engines. Relative to these efforts, Splice focuses on queries over history and uses location to integrate
8920916	16365	the last decade. In particular, methods based on second-order statistics have raised great interest since they can perform their tasks with relatively short data records . With a few exceptions  - , almost all the available literature is devoted to the linear channel case. However,itisofinterest to address the issue of nonlinear channels since real world systems such as satellite
8920916	16365	of a sensor array, when the continuous-time channel presents nonlinearities. It accommodates, polynomial approximations of nonlinear channels . The paper most in uencing our current work is . It considers the direct linear equalization of FIR Volterra channels using an approach similar to that in . The method is attractive because of its simplicity. Two of its notable features are
8920916	16365	rst issue motivating this paper is whether this squareness assumption can be dropped. Secondly, under the assumption that the length of the linear kernel exceeds the length of every other kernels,  shows that one can equalize the channel input. This raises the issue of whether one can estimate the channel itself, and if not then, what is the precise level of channel information that can be
8920916	16365	In resolving this issue we assume the knowledge of the input statistics and adopt an approach similar to that in . Third, in the event the linear kernel has the same length as another kernel,  has to resort to higher order methods to equalize the channel. Using the theory developed in this paper we show that this restriction on the length of the kernel is not necessary for second order
8920916	16375	of nonlinear channels . The paper most in uencing our current work is . It considers the direct linear equalization of FIR Volterra channels using an approach similar to that in . The method is attractive because of its simplicity. Two of its notable features are as follows. First it assumes that an associated channel matrix is square and nonsigular, and argues that this
8920922	16379	annotations are usually defined as individual instances of ontology classes, it is reasonable then to use ABox reasoning (the DL term for reasoning about individuals) to reason about annotations . However, developing a practical ABox reasoner for such annotations is difficult. This difficulty arises not only from the computational complexity of ABox reasoning, but also from the fact that
8920922	16381	of ontology based vocabulary to describe documents in “publish and subscribe” applications , to annotate data in bioinformatics applications  and to annotate web resources such as web pages  or web service descriptions  in Semantic Web applications. We believe that the existing one could be improved by adding some optimisations, providing extra functionalities and relaxing several
8920922	16382	given a KB K = ?T , A? and a concept C. It is the problem of finding all individuals a such that K |= a: C. It is clear that that tasks such as retrieval can be reduced to instance checking . 3 Instance Store An ABox A is role-free if it contains only axioms of the form x : C. We can assume, without loss of generality, that there is exactly one such axiom for each individual as x : C ?
8920922	6101	automated processes by adding “semantic annotations”—metadata (data about data) that describes their content. It is envisaged that the semantics in semantic annotations will be given by ontologies , which will provide a source of precisely defined terms (vocabularies) that are amenable to automated reasoning. A standard for expressing ontologies in the Semantic Web has already emerged: the
8920922	16383	the number of annotations might be extremely large. On the one hand, the task of reasoning with large amounts of individuals is difficult even for the state-of-art optimised ABox reasoner RACER . On the other hand, efficient management of very large volumes of data is well known to be a stronghold of relational databases. The approach of combining relational databases with Description
8920922	16385	data is well known to be a stronghold of relational databases. The approach of combining relational databases with Description Logics to deal with large amounts of data was, therefore, proposed in  which we call an Instance Store. The Instance Store provides an infrastructure for reasoning with large numbers of individuals. It combines a DL reasoner with a database in order to provide large
8920922	16385	of a general purposed role-free 1 DL reasoner over individuals that is sound and complete, has reasonable response times. This claim is supported by presenting a variety of empirical test results  contrasting the Instance Store with the other existing techniques such as RACER. 1 An ABox is said to be role-free if it does not contain role assertions, i.e., all the assertions are of the form a
8920922	16386	recently became a W3C proposed recommendation. One of the main features of OWL is that there is a direct correspondence between (two of the three “species” of) OWL and Description Logics (DLs) . This means that reasoning procedures from DLs can be used in order to infer useful facts from ontology based annotations in OWL. In particular, since annotations are usually defined as individual
8920922	16387	describe documents in “publish and subscribe” applications , to annotate data in bioinformatics applications  and to annotate web resources such as web pages  or web service descriptions  in Semantic Web applications. We believe that the existing one could be improved by adding some optimisations, providing extra functionalities and relaxing several assumptions. A severe restriction
8920923	16394	thousand unlabeled points per image! Recent publications demonstrate good performance in similar but less ambitious tasks by using statistical translation models , support vector machines  and sparse Bayesian kernel machines . Other publications use captioned images in order to learn representations of objects , but none of them explicitly perform data association.
8920923	16396	? (i) ) derives directly from the full conditional of ?k and consequently our acceptance rate is guaranteed to be high. We direct the reader to a couple excellent introductions to the M-H algorithm . We need to consider two cases for the Metropolised Gibbs sampler: when ?k = 0 and when ?k = 1. When kernel k is inactive, our proposal consists of flipping ?k to 1 with probability p(? ? k = 1 |
8920923	16397	and locating objects in scenes (a person is not just an elbow!), and there has been some success in learning principled representations of relations between parts  and global context . However, we believe it is still of general interest to explore the extent to which independent parts can function as a basis for robust object recognition. In this paper, we show that significant
8920923	16397	that there are often as many as a thousand unlabeled points per image! Recent publications demonstrate good performance in similar but less ambitious tasks by using statistical translation models , support vector machines  and sparse Bayesian kernel machines . Other publications use captioned images in order to learn representations of objects , but none of them
8920923	16399	peaked. This consideration exposes a major problem with mixture models — motivating a Bayesian kernel model — since the number of modes explodes factorially relative to the number of components . An additional advantage over cited methods is that we do not need to reduce the dimension of the descriptors through unsupervised techniques, such as vector quantization or principal component
8920923	16400	? (i) ) derives directly from the full conditional of ?k and consequently our acceptance rate is guaranteed to be high. We direct the reader to a couple excellent introductions to the M-H algorithm . We need to consider two cases for the Metropolised Gibbs sampler: when ?k = 0 and when ?k = 1. When kernel k is inactive, our proposal consists of flipping ?k to 1 with probability p(? ? k = 1 |
16402	17341	JULY 1998sFig. 1. Stylized mobility versus bit-rate plane classification of existing and future wireless systems. access scheme, studying TDMA , , , ,  and CDMA , , , . European third-generation research is conducted under the umbrella of the so-called UMTS  initiative, and so far, the following proposals have been submitted to the European Telecommunication
16402	17341	inflicted to other users in CDMA systems, HANZO: BANDWIDTH-EFFICIENT WIRELESS MULTIMEDIA COMMUNICATIONS 1347sFig. 4. Intelligent transceiver schematic. such as the American IS-95 system . Video codecs, such as the variable-rate MPEG-1  and MPEG-2  codecs, even more explicitly rely on the fluctuation of the source statistics. For example, when a new object is introduced in
16402	17341	video telephone systems suitable for the robust transmission of QCIF sequences over conventional mobile radio links, such as the pan-European GSM system , the American IS-54  and IS-95  systems, and the Japanese PDC system . In contrast to existing standard codecs, such as the ITU H.261 scheme and the MPEG-1 , MPEG-2 , and MPEG4  arrangements, our proposed video
16402	17341	PDC system , TDMA was proposed, assigning the whole bandwidth of a TDMA carrier to an MS for a fraction of the time, i.e., for the duration of a time slot. The American IS-95 CDMA system  uses all the system bandwidth all the time for all users, communicating with orthogonal signature codes. However, these systems employ contentionless bandwidth allocation, where the physical
16402	17342	to define the third-generation PCN, which is referred to as a PCS in North America. The European Community’s RACE program ,  and the consecutive framework referred to as the ACTS program ,  spearheaded these initiatives. In the RACE program, there were two dedicated projects, endeavoring to resolve the ongoing debate as regards the most appropriate multiple 1344 PROCEEDINGS OF
16402	17342	wide-band CDMA proposal  for the intelligent mobile terminal IMT 2000 emerging from NTT DoCoMo. These standardization activities are portrayed in more depth in . In the ACTS workplan , there are a number of projects dealing with multimedia source and channel coding, modulation, and multiple access techniques for both cellular and wireless LAN’s. These studies will design the
16402	17342	Consortium. VQ-based schemes were advocated by Ramamurthy and Gersho , as well as by Torres and Huguet . A major feature topic of the European Community’s Fourth Framework Program ,  on ACTS is video communications over a range of wireless and fixed links. In this section, initially we focused our attention on the design and performance evaluation of wireless video
16402	16444	, Klein et al. , and a number of other researchers at the University of Kaiserslautern in Germany, but the impressive individual contributions in this field are too numerous to mention ???. Let us now consider the issues affecting the choice of the appropriate modulation scheme. XI. MODULATION ISSUES A. Choice of Modulation In some of the European mobile systems, such as the
16402	17374	WIRELESS MULTIMEDIA COMMUNICATIONS 1347sFig. 4. Intelligent transceiver schematic. such as the American IS-95 system . Video codecs, such as the variable-rate MPEG-1  and MPEG-2  codecs, even more explicitly rely on the fluctuation of the source statistics. For example, when a new object is introduced in the scope of the camera, which cannot be predicted on the basis of
16402	17374	GSM system , the American IS-54  and IS-95  systems, and the Japanese PDC system . In contrast to existing standard codecs, such as the ITU H.261 scheme and the MPEG-1 , MPEG-2 , and MPEG4  arrangements, our proposed video codec’s Fig. 5. Simplified schematic of motion compensation . fixed but arbitrarily programmable bit rate facilitates its employment also in
16402	17374	that of the original image, which ensures bit-rate economy. The MCER frame can then be represented using a range of techniques , including SBC , , wavelet coding , DCT , , , , VQ –, or QT coding , , , . Some 1350 PROCEEDINGS OF THE IEEE, VOL. 86, NO. 7, JULY 1998sFig. 6. Simple video codec schematic. of these techniques will
16402	17374	cost-gain quantized DCT-based codec . C. DCT-Based Video Codec Our DCT-based video codec’s outline is depicted in Fig. 7. The DCT  has been popular in videocompression standards ,  since it exhibits a so-called energy compaction property, implying that upon transforming a correlated or predictable signal to the spatial frequency domain, most of its energy will be compacted to
16402	17374	solutions include SBC ,  or wavelet coding , which facilitate a flexible control over the allocation of bits in the spatial frequency domain. The MPEG standard codecs ,  and the H.261 and H.263 codecs scan and entropy code the DCT coefficients and also allow direct encoding of the more correlated video signal on a blockby-block basis. VQ – can be carried
16402	17410	This was followed by the equally significant development of the more robust, forward-adaptive, 15–ms-delay G.729 algebraic (A)CELP arrangement proposed by the University of Sherbrooke team , , AT&T, and NTT . Last, the standardization of the 2.4 kbits/s DoD codec led to intensive research in this very low rate range, and the MELP codec by Texas Instruments was identified  in
16402	16498	transmission. Quackenbush  suggested a transform-coded approach in order to allow for a higher flexibility in terms of allocating the bits available, which was proposed originally by Johnston  for 30-kHz-sampled high-fidelity audio signals, and reduced the bit rate required according to the lower sampling rate of 16 kHz. Ordentlich and Shoham proposed a low-delay CELP-based 32 kbits/s
16402	17427	SOURCE CODING A. Motivation and Background Motivated by the proliferation of wireless multimedia services , , a plethora of video codec schemes have been proposed for various applications ???, but perhaps the most significant advances in the field are hallmarked by the MPEG-4 initiative . The design of video-phone schemes centers around the best compromise among a number of
16402	16527	, Hubing , and Girod et al.  for a range of bit rates and applications, but the individual contributions by a number of renowned authors are too numerous to review. Khansari et al.  as well as Pelz  reported promising results on adopting the H.261 codec for wireless applications by invoking powerful signal-processing and error-control techniques in order to remedy the
16402	16529	to hostile wireless environments. Färber et al. – also contributed substantially toward advancing the state of the art in the context of the H.263 codec as well as in motion compensation , , as did Eryurtlu, Sadka, and Kondoz –. Further important contributions in the field were due to Chen et al. , Illgner and Lappe , Zhang , Ibaraki et al. ,
16402	16530	wireless environments. Färber et al. – also contributed substantially toward advancing the state of the art in the context of the H.263 codec as well as in motion compensation , , as did Eryurtlu, Sadka, and Kondoz –. Further important contributions in the field were due to Chen et al. , Illgner and Lappe , Zhang , Ibaraki et al. , Watanabe et
16402	16531	signal-processing and error-control techniques in order to remedy the inherent source-coding problems due to stretching its application domain to hostile wireless environments. Färber et al. – also contributed substantially toward advancing the state of the art in the context of the H.263 codec as well as in motion compensation , , as did Eryurtlu, Sadka, and Kondoz
16402	16531	The H.263 ITU Codec The H.263 codec was detailed in  and , while a number of transmission schemes designed for accommodating its rather error-sensitive bit stream were proposed in , , . As an illustrative example, in Table 2, we summarize the various video resolutions supported by the H.261 and H.263 ITU codecs in order to demonstrate their flexibility . Their
16402	16537	6 Improvements in Capacity Possible with Adaptive Modulation PRMA with 48 Slots  Fig. 27. Performance comparison of the proposed adaptive H.261 and H.263 transceivers over AWGN channels . probability of 0.1. The achievable capacity improvements for our DECT-like system are displayed in Table 7. In conclusion, adaptive modulation with PRMA gives the expected three- to fourfold
16402	16537	performance is expected. Let us now consider the expected performance of a similar intelligent multimode video system. XIV. VIDEO-PHONE SYSTEMS Below, we follow the approach of Cherriman et al. , and as an example, let us consider transmitting QCIF images, where the video codecs were programmed to generate 3560, 2352, and 1176 bits per frame. At a scanning rate of 10 frames/s, these coding
16402	16537	in Table 8, HANZO: BANDWIDTH-EFFICIENT WIRELESS MULTIMEDIA COMMUNICATIONS 1373sFig. 28. Performance comparison of the proposed adaptive H.261 and H.263 transceivers over Rayleigh channels . Table 7 Achievable Capacity Improvements for the Adaptive Modulation PRMA with 416 slots  where a BCH code represents a binary BCH code encoding bits to bits and capable of correcting errors
16402	16537	a Nyquist rolloff factor of 0.35. In the various operating modes investigated, the PSNR versus channel SNR curves of Figs. 27 and 28 were obtained for Table 8 FEC codes used for 4-, 16-, and 64-QAM  AWGN and Rayleigh channels, respectively. Since both the H.261 and H.263 source codecs have had similar robustness against channel errors, and their transceivers were identical, the associated
16402	16539	ITU Codec The H.263 codec was detailed in  and , while a number of transmission schemes designed for accommodating its rather error-sensitive bit stream were proposed in , , . As an illustrative example, in Table 2, we summarize the various video resolutions supported by the H.261 and H.263 ITU codecs in order to demonstrate their flexibility . Their uncompressed
16402	16539	more or less flat fading. Again, the residual fading can be equalized using a simple pilot-assisted equalizer. In closing, we note that a variety of OFDM-related aspects were investigated in ???. After this discussion on modulation techniques, let us briefly consider ways of reducing the BER using FEC techniques. XII. CHANNEL CODING The highest coding gain over AWGN channels is
16402	17473	redundancy, and the variance of the MCER becomes much lower than that of the original image, which ensures bit-rate economy. The MCER frame can then be represented using a range of techniques , including SBC , , wavelet coding , DCT , , , , VQ –, or QT coding , , , . Some 1350 PROCEEDINGS OF THE IEEE, VOL. 86, NO. 7, JULY
16402	17473	where the full transceiver performance over fading channels is also characterized. The above-mentioned range of fixed-rate video codecs is compared in terms of error resilience and video quality in . In –, a range of flexible reconfigurable multilevel transceivers were designed for the transmission of the VQ-, DCT-, and QT-coded video streams by allocating an addition physical speech
16402	17473	the corresponding real-time video quality at http://www-mobile.ecs.soton.ac.uk. 1354 PROCEEDINGS OF THE IEEE, VOL. 86, NO. 7, JULY 1998sFig. 10. Enhanced sample codebook with 128 8 2 8 vectors . variable-length coding techniques, work is also under way toward contriving more robust coding algorithms, such as those to be incorporated in the forthcoming MPEG4 scheme , . In the next
16402	16581	a throughput of up to 93% at the cost of a low MAC delay. A further alternative to support similar multirate multimedia users was also proposed by Brecht et al., which was termed MF-PRMA , . Here, we emphasize that most of the above statistical multiplexer schemes function also as multimedia packet multiplexers, supporting the delivery of multirate, multimedia traffic on a
16402	16581	run-length-coded variablerate video are extremely error sensitive, hence requiring higher integrity than speech and fixed-rate nonrun-lengthcoded video. Some of these aspects were also addressed in ??? and . The first-generation PLMR systems were designed for low traffic density, and the typical cell radius was often on the order of tens of miles. Even the second-generation GSM system
16402	16581	FL-DCC and multiplexed, for example, using the previously described PRMA or SPAMA schemes with speech and video signals. Multiplexing variable-rate multimedia sources was detailed, for example, in , , and . When the increased complexity of turbo codecs becomes acceptable due to the advances in low-voltage VLSI, the performance of wireless personal communicators can approach the
16402	12296	inspiring intensive further research in recent years –, in particular by Kamio et al. at Osaka University and the Ministry of Post in Japan –, as well as by Goldsmith and Chua  at the California Institute of Technology or by Pearce et al.  in the United Kingdom. The proposed schemes provide a means of realizing some of the time-variant channel capacity potential of
16402	12296	channels, while in , an unequal protection phasor constellation for signalling the current TS was proposed. The problem of appropriate power assignment was discussed, for example, in  and . In , a combined BER- and BPS-based optimization cost function was defined and minimized in order to find the required TS switching levels for maintaining average target BER’s of 1 10 and 1 10
16402	16647	more or less flat fading. Again, the residual fading can be equalized using a simple pilot-assisted equalizer. In closing, we note that a variety of OFDM-related aspects were investigated in ???. After this discussion on modulation techniques, let us briefly consider ways of reducing the BER using FEC techniques. XII. CHANNEL CODING The highest coding gain over AWGN channels is achieved
16402	8392	by a higher-rate, less powerful FEC code. A. Turbo Coding In recent years, significant advances have been made toward the Shannonian performance predictions with the introduction of the turbo codes  and using iterative decoding techniques, which will be briefly highlighted below. Turbo coding was proposed by Berrou et al. , where the information sequence is encoded twice, using RSC
16402	8399	one in order to achieve near-Shannonian performance. Hagenauer and Hoeher proposed to use the soft-output Viterbi  algorithm for the decoding of turbo codes in , while Hagenauer et al.  investigated also the feasibility of employing block codes as constituent codes, although most research is carried out in the context of convolutional codes. Robertson et al.  and Jung
16402	16539	WIRELESS MULTIMEDIA COMMUNICATIONS 1367sFig. 23. FFT-based OFDM modem schematic . Fig. 24. Frequency response in the bandwidth of w 2 ?H for the 512-channel OFDM system at 155 Mb/s . one single path in the impulse response. The worst case impulse response associated with the highest path length and delay spread is experienced in the farthest corners of the hall, which was
8920973	16687	of data integration must in addition be in a position to represent the intrinsic semantics of elements present in several distinct external data sources, including other application ontologies . Even where these data sources represent the same particulars in a given domain of reality, their view and understanding of these particulars may be different, and this results in several different
8920973	16692	topology, universals and particulars, space and time. It also contains modules for dealing with biological classes (natural kinds) and their instantiations , and also with granular partitions , as well as respecting the more general demands for a sound ontology recognized by the wider philosophical community . We have attempted to demonstrate empirically that BFO is situated to the
8920973	3678	kinds) and their instantiations , and also with granular partitions , as well as respecting the more general demands for a sound ontology recognized by the wider philosophical community . We have attempted to demonstrate empirically that BFO is situated to the task of providing a framework for mapping external application ontologies, terminologies, and databases onto a system like
8920973	17626	as a simple IS-A tree structure, with which is associated a more comprehensive first-order formalization, also available in a KIF representation in the Wonderweb Library of Foundational Ontologies . In its logical form, the expressiveness of the BFO theory may be exploited to inform models for information integration as well as to help in regimenting the core structure of LinKBase® itself.
8920973	17628	Users can compose their own scenarios of bean configuration according to their modelling focus. 2.3 Ontology Verification Mechanisms 2.3.1 Domain-Range RestrictionssUsually (for example in ) ontology management systems do not enforce domain and range restrictions at the modelling stage, but rather leave such restrictions do be dealt with by a reasoning engine where they can be used to
8920973	6469	algorithms are inherited through the hierarchy. The integration method maps the linktypes to BFO relations, such as dependence, inherence and also part-whole and other mereotopological relations . As described in 2.3.1 LinkFactory® has embedded within it a check, for each linktype instantiation, that verifies for each linktype compatibility of source (domain) and target (range) concepts
8920973	17632	linguists, informaticians, and physicians, and is currently being extended to a top-level formal ontology of biomedical categories such as function, site, system, and anatomical structure . 2.2 The LinkFactory® Ontology Management System LinkFactory® is a platform-independent ontology management system built with the goal of enabling the development of large and complex
16698	16701	to simulate hardware and low-level software so that more work can be done on the other dimensions of ubiquitous computing. UbiWise was born out of a desire to makesprogress on systems for devices  without waiting for the devices themselves. Ultimately we hope this leads to more ideas for hardware and more refined requirements for it. As concrete evidence of the potential we outline three
16698	16703	over a network as well. For this we would need to add a tag called <ProtocolClassName> to our device description file. Since we do other projects in nomadic computing based on Web protocols , we use an HTTP class here. Of course that makes our simulation especially simple as we can use the underlying desktop PC’s network infrastructure. Other choices would be easy to add as long as the
16698	16706	the data (e.g. images) she can produce. The links here are &quot;physical hyperlinks&quot;: bar-codes placed by the entrance to the room (see Fig.7 and 8) are mapped to a web page with the &quot;web/id&quot; service . While we don't know of a digital camera equipped with a tiny wireless web-browser and able to read bar-codes, these capabilities exist separately in other small devices. PDAs withsweb-browsers and
16698	16707	work in progress and all of the new code is released under the LGPL. 5.0 Related Work We don’t know of a general-purpose simulator directed at ubiquitous computing. Bylund and Espinoza  modified the Quake III Arena (Q3A) code to output the position co-ordinates of a player into the Context ToolKit . They demonstrated the potential of this tool in the development of a
16698	16708	general-purpose simulator directed at ubiquitous computing. Bylund and Espinoza  modified the Quake III Arena (Q3A) code to output the position co-ordinates of a player into the Context ToolKit . They demonstrated the potential of this tool in the development of a location-based “virtual note” project called GeoNotes. GeoNotes had a Java program reading the position co-ordinates and
16698	16709	called GeoNotes. GeoNotes had a Java program reading the position co-ordinates and providing a user-interface for placing virtual notes. This work convinced us to extend our initial efforts  to a more complete simulator that provided both three and two-dimensional views, each view trying to solve different problems. Two kinds of existing simulators inspired our work: computer-computer
354437	16723	and its history can be found in Pan  and Bürgisser et al. . An interesting new group theoretic approach to the matrix multiplication problem was recently suggested by Cohn and Umans . For the best available lower bounds see Shpilka  and Raz . Matrix multiplication has numerous applications in combinatorial optimization in general, and in graph algorithms in
354437	16726	for relatively dense matrices (i.e., m = n 1.68 ). The new algorithm is obtained using a surprisingly straightforward combination of a simple combinatorial idea, implicit in Eisenbrand and Grandoni  and Yuster and Zwick , with the fast matrix multiplication algorithm of Coppersmith and Winograd , and the fast rectangular matrix multiplication algorithm of Coppersmith . It is
354437	16735	An interesting new group theoretic approach to the matrix multiplication problem was recently suggested by Cohn and Umans . For the best available lower bounds see Shpilka  and Raz . Matrix multiplication has numerous applications in combinatorial optimization in general, and in graph algorithms in particular. Fast matrix multiplication algorithms can be used, for example, to
354437	16739	et al. . An interesting new group theoretic approach to the matrix multiplication problem was recently suggested by Cohn and Umans . For the best available lower bounds see Shpilka  and Raz . Matrix multiplication has numerous applications in combinatorial optimization in general, and in graph algorithms in particular. Fast matrix multiplication algorithms can be used,
354437	16742	(i.e., m = n 1.68 ). The new algorithm is obtained using a surprisingly straightforward combination of a simple combinatorial idea, implicit in Eisenbrand and Grandoni  and Yuster and Zwick , with the fast matrix multiplication algorithm of Coppersmith and Winograd , and the fast rectangular matrix multiplication algorithm of Coppersmith . It is interesting to note that a
8920986	16748	to perform and document a systematic analysis of evidence . But when not carefully applied, this process can sometimes make one more susceptible to deception.sDragoni, et al.  used a Bayesian approach in a decision aid for judicial proceedings to help assess witness deception. Abduction is used to eliminate information of low credibility and find maximally consistent
43155	16757	targeting of these limited and often declining financial resources in the future. There have been numerous studies on the role of government spending in the long-term growth of national economies (Aschauer 1989; Barro 1990; Tazi and Zee 1997). These studies found conflicting results about the effects of government spending on economic growth. Barro was among the first to formally endogenize government
16818	16819	as most of the axioms are drawn from well known sources. Among the other reasonably well axiomatized theories in the SUMO, I include the mereotopology ontology, which borrows liberally from , , and , and the ontology of holes, taken largely from . 7sRO approach therefore undergirds and supports the robust pragmatism of the AO approach. We can unabashedly embrace both. 4.2
16818	16820	of the axioms are drawn from well known sources. Among the other reasonably well axiomatized theories in the SUMO, I include the mereotopology ontology, which borrows liberally from , , and , and the ontology of holes, taken largely from . 7sRO approach therefore undergirds and supports the robust pragmatism of the AO approach. We can unabashedly embrace both. 4.2 Complementarity
16818	16822	— and hence its attendant expressive limitations — in mind. Consequently, AOs are usually expressed in the language of some computationally tractable sublogic of full firstorder logic (see, e.g., ). Such languages typically support: • Reasoning about classes and “slots” through the use of unary and (limited) binary predicates; • Conjunction and disjunction, but not negation; • Limited
16818	6469	as most of the axioms are drawn from well known sources. Among the other reasonably well axiomatized theories in the SUMO, I include the mereotopology ontology, which borrows liberally from , , and , and the ontology of holes, taken largely from . 7sRO approach therefore undergirds and supports the robust pragmatism of the AO approach. We can unabashedly embrace both. 4.2
16836	4171	of the independent variable t. In groundwater hydrology, T (t) has been used to represent the “operational time” a particle experiences, a time that passes more rapidly in high velocity zones . When modeling characteristics of a porous medium such as hydraulic conductivity, the subordinator may represent the number of depositional features encountered over a distance t. Since the
16855	17800	making these assessments. This dependency can lead to uneven results. For example, the recently released Congressional report of the Joint Inquiry into the Terrorist Attack of September 11, 2001  amplifies this by stating that “the quality of counterterrorism analysis was inconsistent, and many analysts were inexperienced, unqualified, under-trained, and without access to critical
16879	16883	or maintenance time constraints , or the best combination of aggregate data and indices . This approach is commonly referred to as practical (or partial or semi-eager ) pre-aggregation. Commercial OLAP systems now also exist that employ practical pre-aggregation, e.g., Informix MetaCube  and Microsoft Decision Support Services (Plato) . The premise
16879	16887	the materialized aggregatesswhen base data changes. With the goal of avoiding data explosion, research has focused on how to select the best subset of aggregation levels given space constraints  or maintenance time constraints , or the best combination of aggregate data and indices . This approach is commonly referred to as practical (or partial or semi-eager )
16879	16893	(Plato) . The premise underlying the applicability of practical pre-aggregation is that lower-level aggregates can be reused to compute higher-level aggregates, known as summarizability . Summarizability occurs when the mappings in the dimension hierarchies are onto (all paths from the root to a leaf in the hierarchy have equal lengths), covering (only immediate parent and child
16879	16893	(each child in a hierarchy has only one parent); and when also the relationships between facts and dimensions are many-to-one and facts are always mapped to the lowest levels in the dimensions . However, the data encountered in many real-world applications fail to comply with this rigid regime. This motivates the search for techniques that allow practical pre-aggregation to be used for a
16879	16893	relationships between facts and dimensions occur between bank customers and accounts, between companies and Standard Industry Classifications (SICs), and between students and departments . Non-strict dimension hierarchies occur from cities to states in a Geography dimension  and from weeks to months in a Time dimension. In addition, hierarchies where the change over time is
16879	16893	that summarizability is equivalent to the aggregate function (?) beingdistributive, all paths being strict, and the mappings between dimension values in the hierarchies being covering and onto . These concepts are formally defined below. The definitions assume a dimension ? ? ?? ? andanMOÅ ? Ë?????Ê . Definition 2 Given two categories, ? ?? such that ? ÈÖ??? , we say that the mapping from
16879	16897	covering. 3 Data Model Context and Concepts This section describes the aspects of a multidimensional data model that extend practical pre-aggregation. The full model is described elsewhere . Next, the data model context is exploited for defining properties of hierarchies relevant to the techniques. The particular data model has been chosen over other multidimensional data models
16879	16900	to the user, we believe is a novel feature. The only past research on the topic has been on how to manually, and not transparently to the user, achieve summarizability for non-covering hierarchies . The next section presents a real-world clinical case study that exemplifies the non-summarizable properties of realworld applications. Section 3 proceeds to define the aspects of a
16879	16900	between companies and Standard Industry Classifications (SICs), and between students and departments . Non-strict dimension hierarchies occur from cities to states in a Geography dimension  and from weeks to months in a Time dimension. In addition, hierarchies where the change over time is captured are generally non-strict. The mapping from holidays to weeks as well as organization
16879	16902	the materialized aggregatesswhen base data changes. With the goal of avoiding data explosion, research has focused on how to select the best subset of aggregation levels given space constraints  or maintenance time constraints , or the best combination of aggregate data and indices . This approach is commonly referred to as practical (or partial or semi-eager )
16879	16903	or maintenance time constraints , or the best combination of aggregate data and indices . This approach is commonly referred to as practical (or partial or semi-eager ) pre-aggregation. Commercial OLAP systems now also exist that employ practical pre-aggregation, e.g., Informix MetaCube  and Microsoft Decision Support Services (Plato) . The premise
52006	5620	Entity-Relationship models, database design, temporal databases, temporal data models, design criteria for temporal ER models, time semantics. 1 INTRODUCTION T HE Entity-Relationship (ER) model , in its different versions, with varying syntax and with some semantic variations, is enjoying a remarkable, and increasing, popularity in both the research community and in industry. The model is
52006	5620	be used for exemplification throughout. 2.1 Overview This section describes all the temporal ER models that we are aware of. We will assume that the reader is familiar with Chen’s standard ER model  and the various extensions of that model, e.g., subtyping (see, e.g., ). The models are presented in chronological order of their first publication. The description of the models all have, with
52006	5620	evaluating a model against this criterion, we will evaluate whether the model is upward compatible with respect to the ER model that it extends, if specified; otherwise, we will use Chen’s ER model  for models without superclass/subclass relationships and the EER model  for models with superclass/subclass relationships. Six models—RAKE, MOTAR, ERT, TempRT, and TERC+ —are upward compatible
52006	16911	criteria in the specific context of temporal extensions for concreteness. It is an interesting next step to explore the application of the criteria to other types of ER extensions, such as spatial , spatiotemporal, multimedia, or security ER models. Another promising direction deserving attention is the application of the criteria to extensions of modeling notations other than the ER model.
52006	6123	C8—Temporal Interpolation Functions. Temporal interpolation functions derive information about times for which no data is explicitly stored in the database (see, e.g.,  and (pp. 35–40 of ). For example, it is possible to record times when new salaries of employees take effect and then define an interpolation function (using so-called step-wise constant interpolation) that gives the
52006	5636	temporal consistency is presented. 2.7 The Entity-Relation-Time Model The Entity-Relation-Time (ERT) model exists in two versions, the original version , , and a recent refinement . We survey first the original model and then discuss the refinements at the end. The motivation for the development of the original ERT model was to meet the need for conceptual models of enhanced
52006	5636	relationship class instances. The graphical notation of objectified relationships is depicted in Fig. 17b. 2.7.3 Refining the Original ERT Model The original ERT model has recently been refined  in two respects. First, the definitions of temporal objects (entities or relationships) are given mathematically, by specifying what constraints are placed on the existence or validity periods of
52006	5636	t a, t b, ¡, t z are the ticks at which x exists/holds. Since the series of ticks usually is continuous, I x is called an interval although what actually has been defined is a set of intervals . This definition of “intervals” allows for the use of the usual set operators. To ensure a discrete bounded model, the possible ticks of an interval are limited to the finite set of ¥ = {0 … t },
52006	5636	I x will satisfy I x  ¥. In the original ERT model, a relationship class could only be marked with a T-mark indicating that the relationship was time-varying. The temporal marking is refined in  to include H-marks and TH-marks. In the following, interval i.e., ranges over all intervals associated with entitys482 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 11, NO. 3, MAY/JUNE
52006	5636	the temporal direction could be past and current (depending on what is E1 and E2). The characteristics of Hmarked relationships can be described using derived entity classes, for details see . Fourth, assume that instances of  I E1 and instances of E1 are involved in R over period IER 1 E2 are involved in the same relationship instance for IE2 and relationship R is TH-marked. Then, I
52006	5239	attribute types, i.e., provide temporal single valued, temporal multivalued, temporal composite, and temporal derived attribute types. These temporal attribute types may be snapshot reducible  with respect to their corresponding snapshot attribute types. This occurs if snapshots of the databases described using the temporal ER diagram constructs are the same as databases described by the
52006	16919	researchers and practitioners interested in temporal data modeling and data model design. Four studies are somewhat related to or complement the study reported here: 1) Theodoulidis and Loucopoulos  describe and compare nine approaches to specify and use time in conceptual modeling, here viewed as both semantic data modeling and requirement specification, of information systems. Their study
52006	5641	set of constraints for preserving temporal consistency is presented. 2.7 The Entity-Relation-Time Model The Entity-Relation-Time (ERT) model exists in two versions, the original version , , and a recent refinement . We survey first the original model and then discuss the refinements at the end. The motivation for the development of the original ERT model was to meet the need for
42459	16929	the scam web site as a trustworthy web site, e.g. a large bank, and asking users to login to their account, thereby collecting user accounts and passwords. See solutions to web-spoofing in . Some of the sensitive information collected in this manner may be of direct financial value, e.g. credit card numbers and other details, which the attacker abuses, among other things, to finance
6392777	16945	have also been applied in a number of sizing structural optimization problems . However, the relevant literature when probabilistic constraints are taken into consideration is rather limited . Optimization of large-scale ? Corresponding author. Tel.: +30-1-7721694; fax: +30-1-7721693. E-mail addresses: mpapadra@central.ntua.gr (M. Papadrakakis), nlagaros@central.ntua.gr (N.D. Lagaros).
6392777	16952	i=1 (13) J(w) is the Jacobian matrix of vector function E(w) and Hi(w) is the Hessian matrix of the component function Ei(w). In the present study the steepest descent and the Levenberg–Marquard  methods are used. 3.2. Improving generalization One of the problems that occur during neural network training is called over-fitting. The error on the training set is driven to a very small value,
6392777	16953	too small, the network will not adequately fit the training data. One approach to determine the regularization parameters in an automated fashion is the Bayesian framework proposed by David MacKay . In this framework, the weights and biases of the network are assumed to be random variables with specified distributions. The regularization parameters are related to the unknown variances
6392777	16965	optimization problems as well. The methods based on the use of penalty functions are employed in the majority of cases for treating constraint optimization problems. In a recent work by the authors  it has been observed that death penalty method employed by ES performs well for the problems considered. An extensive review of the constraint handling methods can be found in . 5.2.
6392777	16966	by the authors  it has been observed that death penalty method employed by ES performs well for the problems considered. An extensive review of the constraint handling methods can be found in . 5.2. Deterministic-based structural optimization using ES and NN After the selection of the suitable NN architecture and the training procedure is performed over a number (M) of data sets. In
16970	17922	of human perception of video quality as an important measure of performance. Moreover, user perception of quality is significantly influenced by the environment and the viewing device(e.g PDA) . For example, most people are able to differentiate between close video quality levels on laptop/desktop systems; however only few are able to differentiate between close quality levels on a
16970	16972	were removed from the iPAQ during the experiment and we measured power drawn by the device during MPEG streaming. The CPU architecture simulation was implemented using the Wattch/SimpleScalar  power simulator. We configured our simulated CPU to resemble a typical Intel XScale processor (widely used in today’s mobile devices, mostly due to their excellent MIPS/Watt performance): ARM core,
16970	16973	Dynamic Voltage Scaling  for MPEG streams have been widely researched. At the application and middleware levels, the primary focus has been to optimize network interface power consumption . A thorough analysis of power consumption of wireless network interfaces has been presented in . In , Shenoy suggests performing power friendly proxy based video transformations to reduce
16970	16974	Dynamic Voltage Scaling  for MPEG streams have been widely researched. At the application and middleware levels, the primary focus has been to optimize network interface power consumption . A thorough analysis of power consumption of wireless network interfaces has been presented in . In , Shenoy suggests performing power friendly proxy based video transformations to reduce
16970	16975	sufficient locality that can be exploited to reduce cache-memory traffic by 50 percent or 10 User 1 User 2 User 3 User 4 User 5smore through simple architectural changes. Dynamic Voltage Scaling  for MPEG streams have been widely researched. At the application and middleware levels, the primary focus has been to optimize network interface power consumption . A thorough analysis of
16970	16977	Dynamic Voltage Scaling  for MPEG streams have been widely researched. At the application and middleware levels, the primary focus has been to optimize network interface power consumption . A thorough analysis of power consumption of wireless network interfaces has been presented in . In , Shenoy suggests performing power friendly proxy based video transformations to reduce
16970	16978	optimized bursts of video by the proxy, along with control information. Since wireless network cards typically consume significantly more power in active vs sleep mode (about one order of magnitude ), our goal was to optimize the video burst sizes in order to maximize energy savings without performance costs. User 1 User N Proxy P Wired Wired HTTP/TCP/IP wireless 802.11b C Wireless device C
16970	16979	for realtime applications. Different solutions have been proposed at various computational levels: architecture (hardware) – caches/memory optimizations , dynamic voltage scaling(DVS) , dynamic power management (DPM) of system components (disks, network interfaces), efficient compilers techniques – and application/middleware based adaptations . However, an interesting
16970	16979	techniques will be incorporated into more future processors. 3.1.2 Integrated Dynamic Voltage Scaling A different knob for controlling power drawn by the processor is dynamic voltage scaling (DVS) . Voltage scaling is a method for trading-off processor speed against power, by lowering both voltage and operating frequency (power consumption when running at a high speed and voltage is much
16970	16981	sufficient locality that can be exploited to reduce cache-memory traffic by 50 percent or 10 User 1 User 2 User 3 User 4 User 5smore through simple architectural changes. Dynamic Voltage Scaling  for MPEG streams have been widely researched. At the application and middleware levels, the primary focus has been to optimize network interface power consumption . A thorough analysis of
16970	17934	dynamic voltage scaling(DVS) , dynamic power management (DPM) of system components (disks, network interfaces), efficient compilers techniques – and application/middleware based adaptations . However, an interesting disconnect is observed in the research initiatives undertaken at each level. Power optimization techniques developed at each computational level have remained seemingly
16970	16983	levels, the primary focus has been to optimize network interface power consumption . A thorough analysis of power consumption of wireless network interfaces has been presented in . In , Shenoy suggests performing power friendly proxy based video transformations to reduce video quality in real-time for energy savings. They also suggest an intelligent network streaming strategy for
16970	16984	for power and performance trade-offs for realtime applications. Different solutions have been proposed at various computational levels: architecture (hardware) – caches/memory optimizations , dynamic voltage scaling(DVS) , dynamic power management (DPM) of system components (disks, network interfaces), efficient compilers techniques – and application/middleware based adaptations
16970	16984	to the poor locality of the data stream, many MPEG implementations viewed video data as “un-cacheable” and completely disabled the internal caches during playback. However, Soderquist and Leeser  show that video data has sufficient locality that can be exploited to reduce cache-memory traffic by 50 percent or 10 User 1 User 2 User 3 User 4 User 5smore through simple architectural changes.
16970	16985	and adaptation is suggested for multimedia applications for optimal CPU gains. Dynamic transcoding techniques have been studied in  and objective video quality assessment has been studied in . 7 Conclusions & Future Work In this paper, we integrated low-level hardware optimizations with high level middleware adaptations for enhancing the user experience when streaming video onto
16988	16991	share of our public and private life. Consequently, much has been written about privacy in light of automated data processing , though less so in the context of ubiquitous computing . The following sections try to add a more differentiated view on the impact of ubiquitous computing on personal privacy by first examining why personal privacy is desirable, describing when we
16988	6286	share of our public and private life. Consequently, much has been written about privacy in light of automated data processing , though less so in the context of ubiquitous computing . The following sections try to add a more differentiated view on the impact of ubiquitous computing on personal privacy by first examining why personal privacy is desirable, describing when we
16988	17009	violation for many people. Applying ubiquitous computing technology in areas with primarily social borders – for example where a close social group interacts only among themselves, such as families  or co-workers – might seemingly alleviate some of the above concerns. Most participants share already close relationships and tend to know a great deal about each other, without needing a system to
16988	9771	computing systems into place will most certainly allow far greater possibilities for such border crossings in our daily routines. Consider the popular vision of a wearable memory amplifier , allowing its wearer to constantly record events of her daily life in a lifetime multimedia diary. While at first sight such a technology promises great help for those of us who tend to forget a
16988	17013	systems often cite the fact that information could both be gathered and stored locally (i.e., on the users belt, or within her shirt) as a turnkey solution for privacy conscious technologists . Border crossings, however, are not only about who does something, but what is happening. Even though a context-aware wearable system might keep its data to itself, its array of sensors
16988	8814	Ubiquitous computing systems, too, will need to posses such dependability. And with respect to the vision of invisible computing, meeting user expectations is certainly another important aspect . However, the implications we have seen in the previous sections also introduce additional requirements, such as persistence, manageability, control, and accountability. We will examine each of
16988	17025	violation for many people. Applying ubiquitous computing technology in areas with primarily social borders – for example where a close social group interacts only among themselves, such as families  or co-workers – might seemingly alleviate some of the above concerns. Most participants share already close relationships and tend to know a great deal about each other, without needing a system to
8921050	17048	It regulates metabolic pathway such as the fatty acid biosynthesis and degradation pathways, glyoxylate pathway and possible role in regulation of amino acid biosynthesis directly or indirectly . By use of cDNA microarray one can search for regulation of transcription factors and its impact on metabolism using microarray data analysis and sequence information. By comparing wild-type W3110
8921050	17048	and fadL, fadA genes, which are related to fatty acid degradation pathway, were up-regulated. aceA, aceK genes, related to glyoxylate shunt, were up-regulated which coincides with the literatures . fadR Sequence Motif Search in the Escherichia coli Whole Genome Through BLASTN using FadR DNA-binding consensus sequence, several possible FadR regulatory binding sites were identified. Among them
8921050	17048	in different media, respectively were obtained as well. Based on microarray experimental data, fadR regulation in fatty acid biosynthesis and degradation is observed and coincides with literatures . Data also reveals the regulation of fadR,regulating metabolism in different conditions. cDNA microarray has vast potential to decipher the complex genetic network of living organisms. It is
8921051	6101	5. The framework for semantic service discovery is in Section 6 and we conclude the paper in Section 7. 2 Why Ontology? An ontology is a formal explicit specification of a shared conceptualisation . It was developed in Artificial Intelligence area to facilitate knowledge sharing and reuse. Fig. 1 shows an example of a NaturallyOccurringWaterSource ontology . It shows common concepts or
8921051	17053	DAML+OIL for our knowledge representation. River subClassOf Lake Ocean Seas3 Related Work Using the ontology concept to enhance service discovery has now become a hot research topic. The work in  presents how a service registry can use an RDF-based ontology as a basis for advertising and querying for services. In , DAML project proposes a DAML+OIL-based language called DAML-S as a new
8921051	18016	enhance service discovery has now become a hot research topic. The work in  presents how a service registry can use an RDF-based ontology as a basis for advertising and querying for services. In , DAML project proposes a DAML+OIL-based language called DAML-S as a new service description language. DAML-S consists of the Service Profile ontology which describes functionalities that a Web
8921054	18024	recall rates due to the variability in handwritten words. Manmatha et al.  used an Euclidean distance map-based algorithm to index handwritten text using word image templates. Kolcz et al.  used the profile of the words to match handwritten words. Their method achieves a recall rate of 45% with no false alarms. Singer and Tishby  modeled on-line handwriting as modulated cycloidal
8921054	17064	on-line handwriting (see section 1.1). Recent advances in the processing of on-line handwritten data includes algorithms for (i) segmentation of handwritten text into lines, words and sub-strokes , (ii) character and word recognition , (iii) analysis of on-line document structure , and (iv) indexing and retrieval of on-line handwritten documents. Indexing is the problem of
8921054	17065	cycloidal motions of the pen tip to do recognition. The authors reported successful spotting of parts of a word in a small database of 30 words using dynamic time warping. Lopresti and Tomkins  used an edit distance measure to match a sequence of feature vectors extracted from segments of strokes in on-line handwritten data. The method achieves a recall rate of 95% with a precision of 4%
8921054	17065	Recall denotessPrecision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Legend writer1 writer2 writer3 writer4 writer5 writer6 writer7 writer8 writer9 writer10 Lopresti et al. Lopresti & Tomkins  --> Proposed Algorithm --> 0 0.2 0.4 0.6 0.8 1 Recall Figure 7. The precision vs. recall curves. the fraction of the number of correct retrievals to the total number of instances of the keyword in
8921054	17065	the PR curve for on-line documents. The accuracy of our algorithm was 92.3% at a recall rate of 90% averaged over the whole database (6, 672 words). This is significantly better than the results in . The indexing experiment was carried out on the passage of 30 pages. We discard short words (consisting of 3 characters or less) to avoid articles, prepositions etc. The word matching algorithm
8921054	17066	a document in a database using a handwritten keyword as a query. The process of comparing handwritten or spoken words in a document, without explicit recognition is referred to as ‘wordspotting’ . Handwritten data, which is captured (digitized) at the time of writing, encodes the dynamic information in the strokes 1 and is referred to as on-line handwriting (see section 1.1). Recent
8921054	17066	in the database (see figure 1(a)). Although the indexing process may seem functionally similar to retrieval, there are many aspects of indexing applications which make it different from retrieval . However, the critical factor in solving both the indexing and retrieval problems is the choice of a distance measure between words, which should have a small value (dissimilarity) for the
8921054	17066	sizes with 20% salt-and-pepper noise. In the case of handwritten documents, word-spotting systems attain considerably lower recall rates due to the variability in handwritten words. Manmatha et al.  used an Euclidean distance map-based algorithm to index handwritten text using word image templates. Kolcz et al.  used the profile of the words to match handwritten words. Their method achieves
8921054	17067	character templates to match individual characters to spot keywords in noisy printed text and attains a recall rate of 90% and precision rate of 97% on a set of 380 document images. O’Neill et al.  reported 90% recall and more than 95% precision in spotting printed words using moment features of the word pixels. The test set contained two words in 13 font sizes with 20% salt-and-pepper noise.
8921054	17068	on-line handwriting (see section 1.1). Recent advances in the processing of on-line handwritten data includes algorithms for (i) segmentation of handwritten text into lines, words and sub-strokes , (ii) character and word recognition , (iii) analysis of on-line document structure , and (iv) indexing and retrieval of on-line handwritten documents. Indexing is the problem of
8921054	17068	tip. Our word-spotting system operates on a database containing a number of handwritten pages. To carry out the matching, we need to segment a page into individual lines and words. Ratzlaff et al.  attempted the problem of text line identification in on-line documents. To identify the individual lines, the inter-line distance, d, is computed from the y-axis projection of the strokes in the
8921054	17069	text using word image templates. Kolcz et al.  used the profile of the words to match handwritten words. Their method achieves a recall rate of 45% with no false alarms. Singer and Tishby  modeled on-line handwriting as modulated cycloidal motions of the pen tip to do recognition. The authors reported successful spotting of parts of a word in a small database of 30 words using
17071	17072	For instance, even if components can dynamically acquire reciprocal knowledge, they may be thereafter forced to interact in a direct way to coordinate their activities with each other. Anthill  is a framework built to support design and development of adaptive peer-to-peer applications, that exploits an analogy with biological adaptive systems . Anthill consists of a dynamic
17071	9284	such information is absent, the routing simply reduces to flooding the network. Although its simplicity, this model captures the basic underling model of several different MANET routing protocols . The basic mechanism described in this section (tuples defining a structure to be exploited by other tuples’ propagation) is fundamental in the TOTA approach and provides a great flexibility. For
17071	17076	pattern depending on data (i.e., on the value of some tuples) found in the propagation nodes. More details on the TOTA programming model, with also code examples, can be found in a companion paper . 5 5. Application examples TOTA can be exploited to solve several problems typical of dynamic network scenarios, by simply implementing different tuples and propagation rules. 5.1. Routing on
17071	17077	dynamic network scenarios. Some approaches propose relying on shared data structures as the basis for uncoupled interactions andscontext-awareness. For instance, the Lime  and the XMIDDLE  middleware exploit tuple spaces and XML trees, respectively, as the basis for interaction in dynamic network scenario. Each device in the network owns a private data structure (e.g., a private
17071	939	tuple to be propagated, say, at most for 10 meters from its source. Taking this approach to the extreme, one could think at mapping the peers of a TOTA network in any sort of virtual overlay space , and propagating tuples accordingly to the virtual space topology. The spatial structures induced by tuples propagation must be maintained coherent despite network dynamism. To this end, the TOTA
17071	939	fundamental in the TOTA approach and provides a great flexibility. For example it allows TOTA to realize systems providing content-based routing in the Internet peer-to-peer scenario, such as CAN  and Pastry . 5.2. Gathering Information Building on the previous routing mechanism, another application that can be easily realized upon TOTA is a system to enable components to collect
17071	17080	connected nodes) and is of no support in a acquiring a more global perspective and in achieving complex distributed coordination patterns over a possibly large network. The approach proposed in  is explicitly targeted at facilitating the acquisition of contextual information in dynamic scenarios (i.e., MANETs). There, each node in the network can specify an interest for some contextual
17071	17080	in the previous section (i.e. query tuples create a structure to be used by answer tuples to reach the enquiring device). Also, in that way, we achieve in TOTA the same functionalities proposed in . Finally, it is rather easy to see that, by properly shaping tuples, we can achieve patterns of interactions analogous to the ones promoted by those middleware relying on virtually shared data
17071	5902	TOTA approach and provides a great flexibility. For example it allows TOTA to realize systems providing content-based routing in the Internet peer-to-peer scenario, such as CAN  and Pastry . 5.2. Gathering Information Building on the previous routing mechanism, another application that can be easily realized upon TOTA is a system to enable components to collect information in
17081	17086	service is based on. The OLAC versions of these standards, namely the OLAC Metadata standard and the OLAC Repositories standard, are designed to address the particular needs of language archiving (Bird and Simons 2003; Simons and Bird 2003a,b). ‘Metadata’ is structured data about data – descriptive information about a physical object or a digital resource. Library card catalogs represent a well-established type
17081	17094	(Pullum and Ladusaw 1986:168). In morphosyntax, the term ‘absolutive’ can refer to one of the cases in an ergative language, or to the unpossessed form of a noun as in the Uto-Aztecan tradition (Lewis et al. 2001:151), and a correct interpretation of the term depends on an understanding of the linguistic context. The existence of variable or unknown terms leads to problems for retrieval. Suppose that a
37297	17106	useful as a first stage of visual scene analysis. Our Primitives initialize a process of contextual integration that disambiguated locally ambiguous information in the artificial visual system . 1 Introduction Vision faces the problem of an extremely high degree of vagueness and uncertainty in its low level processes such as edge detection, optic flow analysis and stereo estimation. This
37297	17106	high energy. Condensation: Integration of information requires communication between Primitives expressing cross–modal , spatial (see, e.g., ) and spatial–temporal dependencies (see, e.g., ). This communication has necessarily to be paid for with a certain cost. This cost can be reduced by limiting the amount of information transferred from one place to the other, i.e., by reducing
37297	17106	‘good continuation’ increases by making use of the different modalities of the Primitives. Thirdly, orientation and position attributes habe been stabilized according to the temporal context. In  it is made use of the motion of an object to predict feature occurrences across frames and is has been shown that stereo processing can be stabilized by modifying the confidencessFigure 7: A:
37297	17106	in terms of visual Primitives. These Primitives are multi-modal and give a dense and meaningful description of a scene. Our Primitives are used as a first stage in the artificial visual system  where they initialize a disambiguation process. In this process confidences associated to our Primitive attributes adapt according to spatial and temporal context and in this way stabilize the
37297	17108	useful as a first stage of visual scene analysis. Our Primitives initialize a process of contextual integration that disambiguated locally ambiguous information in the artificial visual system . 1 Introduction Vision faces the problem of an extremely high degree of vagueness and uncertainty in its low level processes such as edge detection, optic flow analysis and stereo estimation. This
37297	17108	of the multi–modal Primitives. C: Extracted Primitives at positions of high energy. Condensation: Integration of information requires communication between Primitives expressing cross–modal , spatial (see, e.g., ) and spatial–temporal dependencies (see, e.g., ). This communication has necessarily to be paid for with a certain cost. This cost can be reduced by limiting the
37297	17108	in terms of visual Primitives. These Primitives are multi-modal and give a dense and meaningful description of a scene. Our Primitives are used as a first stage in the artificial visual system  where they initialize a disambiguation process. In this process confidences associated to our Primitive attributes adapt according to spatial and temporal context and in this way stabilize the
37297	17109	increases for higher semantic information, such as curvature. Furthermore, illumination variation heavily influences the measured grey level values and is hard to model analytically (see, e.g., ). Extracting information across image frames, e.g., in stereo and optic flow estimation, faces (in addition to the above mentioned problems) the correspondence and aperture problem which interfere
37297	17113	description of visual structures. Especially geometric as well as appearance based information is coded which both have shown to be useful in different contexts and for different tasks (see, e.g., ). Adaptibility of Primitives prevents hard decisions at an early stage of processing that are necessarily ambiguous (as discussed above). Finally, the condensed representation of our Primitives
37297	17114	Basic Filtering: The monogenic Signal The basic pre–processing stage of our Primitives is based on a rotation invariant quadrature filter, which is derived from the concept of the monogenic signal . Considered in polar coordinates, the monogenic signal performs a split of identity: it decomposes an intrinsically one-dimensional signal into intensity information (amplitude), orientation
37297	17114	reliable estimate. This holds also true for the feature attributes contrast transition, colour and optic flow. Contrast transition: Contrast transition is coded in the phase ? of the applied filter . The phase codes the local symmetry, for example a bright line on a dark background has phase 0 while a bright/dark edge has phase ? ????? (in fig. 7 the line that marks the border of the street in
37297	17114	structure. The local phase as additional feature allows to take the grey level information into account (as one parameter in addition to orientation) in a very compact way (see figure 5 and, e.g., ). In case of boundaries of objects, phase represents a description of the transition between object and background. 2.4 Color and Optic Flow Color ¦©???????????????§? is processed by integrating
37297	17115	response (Fourier transform of the kernel) are respectively given by: ??? £¥¤§¦©¨???? ¦?? ¨?? ??? ??? ¤?¦©????? ??????¦ ? ??? 1 In a global context, these features are related by the Riesz transform . ? ?????? ? ? ??? ? ? ??? ??? ? ? ? ¦?? ¨?? ??? ??? ??????¦ ? (2) ? ? ???? (1) ? ? ??? ? ? ??? ???s0.1 0.05 0 40 0.03 0.02 0.01 0 40 20 20 x 10 8 6 4 2 0 40 20 ?3 20 20 20 ?0.02 ?0.04 40 40 0 0.04
37297	17116	do not change under rotations (amplitude and phase), whereas the third coordinate directly reflects the rotation angle. This kind of quadrature filter, which is called spherical quadrature filter , is formed by triplet of filters: a radial bandpass filter and its two Riesz transforms. We construct the bandpass filter from difference of Poisson (DOP) filters, in order to get analytic
37297	17120	by three values ??????? represented by the Primitive, has intrinsic dimension ? . Our coding of intrinsic dimension as well as its application in our image representation is described in detail in . We want to express a local image patch corresponding to an intrinsically one–dimensional signal patch by a Primitive ? ??? ? ? ? ? ??? ? ? . ??? represents the confidence that the image patch,
37297	17120	confidences for the different intrinsic ??????? dimensions parametric description of a Primitive as ??? ? ? ? ??¦©¨?? ? ? ? ? ? ??¦?? ? ??? ? ??? ? ??? ??????????? ? ? ??? ? ? (see section 2.2 and ) we end up with a As mentioned above, in addition, to each of the parameters ? ? ??? ? ??? ? ??? ? there exist confidences ? ? ??¦?? that code the reliabilty of the specific sub–aspects that are
37297	17124	been used in different contexts. Firstly, an image patch also describes a certain region of the 3D space and therefore 3D attributes can be associated such as a 3D-position and a 3D-direction. In  a stereo similarity function is defined that makes use of multiple-modalities to enhance matching performance. It could be shown that the use of multiple modalities enhances stereo performance
17211	18187	several extensions and/or dialects of Java, which target the problems described in Section 1.1 by changing the language specication. BORNEO To address Java's problems, the Borneo language (Borneo , Darcy ) changes and extends Java so that all IEEE 754 features can be expressed and new numeric types can be easily created. Borneo allows either better hardware utilization than Java or
17211	18194	The following list does not claim to be complete, but is intended to cover the most important issues, especially in terms of run time behaviour. Detailed discussions of these topics can be found in , , .  The requirement of exact reproducibility of numerical results obtained on any platform was a great balk, which refused access to many
17211	18194	Computing in Java This section gives an overview of some of the most important attempts to introduce numerics to the Java platform. 1.2.1 Java Numerics The Java Grande Numerics Working Group  made several ocial proposals for extending Java's specication, many of them addressing the issues mentioned in Section 1.1. Furthermore there are proposals suggesting framework classes, for
17211	18194	084 fpe.html). The Complex Data Type The most obvious approach to address the lack of a complex data type is to introduce a new class. This was proposed by the Java Grande Numerics Working Group  and has been implemented by Visual Numerics . Wu  and Moreira et al.  show, that this approach delivers completely satisfactory performance, if the compiler is able to recognize complex
17211	17216	optimizing in terms of performance and program structure as well. This topic is obviously linked closely to the matters of operator overloading and the complex data type (Veldhuizen , Bacon ).  Currently, multidimensional arrays in Java are implemented as arrays of arrays. The introduction of true multidimensional arrays is necessary (). 1.2 Numerical Computing in Java This
17211	17216	can be explicitly programmed within the language. While the language maintains a uniform object reference semantics, eciency is obtained by making heavy use of unboxing and semantic expansion (see ). Kava can be compiled with a modication of the Jikes compiler (Bacon ), but it is still in an experimental state. Chapter 2 Performance Assessment 2.1 The Test Environment All performance
17211	17222	not claim to be complete, but is intended to cover the most important issues, especially in terms of run time behaviour. Detailed discussions of these topics can be found in , , .  The requirement of exact reproducibility of numerical results obtained on any platform was a great balk, which refused access to many performance relevant hardware features available nowadays.
17211	17222	, which allows array bounds checking to be turned o, gains on average 10 % performance speed-up using this feature. In extreme cases, the performance gain is around 30 % (Midki, Moreira, Snir ). 1.2 Numerical Computing in Java 5  The lack of operator overloading is mainly an issue of readability of code, but a very important one, especially since one of Java's main goals is to achieve
17211	17225	not claim to be complete, but is intended to cover the most important issues, especially in terms of run time behaviour. Detailed discussions of these topics can be found in , , .  The requirement of exact reproducibility of numerical results obtained on any platform was a great balk, which refused access to many performance relevant hardware features available nowadays.
17211	17225	lack of a complex data type is to introduce a new class. This was proposed by the Java Grande Numerics Working Group  and has been implemented by Visual Numerics . Wu  and Moreira et al.  show, that this approach delivers completely satisfactory performance, if the compiler is able to recognize complex objects, and is aware of its semantics in order to allow the optimization of
17211	17225	array package, capable of achieving almost Fortran-like performance using IBM's high performance compiler, which is not restricted to Java's rigidsoating-point conventions (Moreira et al. , Ninja ). BLAS, Linear Algebra Packages Jama  is the ocial Java Grande proposal of a numerical class library to be added to the Java specication. But there are many more packages available,
17211	17227	not claim to be complete, but is intended to cover the most important issues, especially in terms of run time behaviour. Detailed discussions of these topics can be found in , , .  The requirement of exact reproducibility of numerical results obtained on any platform was a great balk, which refused access to many performance relevant hardware features available nowadays.
17211	17227	Java 5  The lack of operator overloading is mainly an issue of readability of code, but a very important one, especially since one of Java's main goals is to achieve user-friendliness (Veldhuizen ).  The lack of some kind of lightweight classes, with less overhead, than Java's class model often forbids optimizing in terms of performance and program structure as well. This topic is obviously
8921110	7262	of data and knowledge and has a comprehensive compilation of related resources. KIF (Knowledge Interchange Format) (Genesereth et al., 1992) and KQML (Knowledge Query Manipulation Language) (Finin et al., 1994) are examples of important predecessors for contemporary initiatives of metadata and knowledge representation (Berners-Lee, 1998). 4.4 Personalization concepts There have been several attempts to
8921110	17243	to be “content-based,” in that they base their predictions on the contents of the artifacts about which they are concerned. One influence for content-based filtering is the work of Hammond et al. (Hammond et al., 1994), which proposed an information retrieval agent that was based on case-based reasoning in combination with semantic description of the resources to recommend. According to Smyth et al. (Smyth et
8921110	17246	2004 11sproducing and standardizing metadata for centuries” (Milstead et al., 1999). According to Jokela et al., in the future metadata will be the key element in managing content production (Jokela et al., 2000). The framework of metadata efforts (from a personalization perspective) can range from simple categorization (i.e. objective) to advanced knowledge models (i.e. subjective). According to Nilsson
8921110	17255	(Berners-Lee, 1998). 4.4 Personalization concepts There have been several attempts to categorize and create taxonomies for personalization both from a conceptual and technical viewpoint (Schafer et al. 1999). According to Adomavicius et al. (Adomavicius et al., 2003), personalization can be divided into three classes: 1. Provider-centric. 2. Consumer-centric. 3. Market-centric. The provider-centric
8832677	17261	of their respective transition systems. The comparison is illustrated by means of three examples: a thermostat, a railroad gate controller, and dry friction. 1 INTRODUCTION A hybrid automaton (Alur et al. 1995; Henzinger 2000b; Guéguen and Lefebvre 2001; Johansson et al. 1999) is one of the most popular formal models for hybrid system specification. This paper relates the hybrid automaton of Henzinger
8832677	17261	such as HyPa (Cuijpers and Reniers 2003), hybrid formalisms based on CSP (Jifeng 1994, Chaochen, Ji, and Ravn 1996), hybrid I/O automata (Lynch, Segala, and Vaandrager 2003), hybrid automata (Alur et al. 1995), and to work derived from hybrid automata, such as Charon (Alur et al. 2001) and Masaccio (Henzinger 2000a). In particular, the ? disrupt, choice, recursion, and reinitialization operators are
8832677	17262	(Jifeng 1994, Chaochen, Ji, and Ravn 1996), hybrid I/O automata (Lynch, Segala, and Vaandrager 2003), hybrid automata (Alur et al. 1995), and to work derived from hybrid automata, such as Charon (Alur et al. 2001) and Masaccio (Henzinger 2000a). In particular, the ? disrupt, choice, recursion, and reinitialization operators are inspired by HyPA (Cuijpers and Reniers 2003). Overall, ? is more expressive than
10168273	17281	the WISIWYS concept (what I see is what you see) not only for text/graphics application output as it is done in e.g. Timbuktu or the known X multiplexers (e.g. SharedX, Xmux, XTV) , , , , but also for multimedia applications such as multimedia authoring systems or film editors. JVTOS is also aimed at dynamic user participation, i.e. participants should be allowed to enter and
10168273	17283	MacOS, and the IBM PC with MS-Windows. JVTOS is issue of work package 4.2 within RACE 2060 project CIO . 2. Service Description JVTOS is a new telecooperation service for high-speed networks , , . It is structured into a set of four user-level services: Session Management, Application Sharing, Picturephone and Telepointing. The Session Management Service is the major control of
10168273	17283	such that they convert and distribute the data stream to the real interface among the machines of the other session members and collect and convert the data streams produced by these machines . 2.3. Telepointing Communication between JVTOS users which is based on sharing information generated by an application, is enhanced by providing globally visible pointing tools, termed telepointers
10168273	17285	and the IBM PC with MS-Windows. JVTOS is issue of work package 4.2 within RACE 2060 project CIO . 2. Service Description JVTOS is a new telecooperation service for high-speed networks , , . It is structured into a set of four user-level services: Session Management, Application Sharing, Picturephone and Telepointing. The Session Management Service is the major control of the
10168273	17287	and the IBM PC with MS-Windows. JVTOS is issue of work package 4.2 within RACE 2060 project CIO . 2. Service Description JVTOS is a new telecooperation service for high-speed networks , , . It is structured into a set of four user-level services: Session Management, Application Sharing, Picturephone and Telepointing. The Session Management Service is the major control of the entire
348605	17327	so many times we will refer to the level set and really mean where ?= 0. Because the free surface is the most important part of the level set, we only need to pay attention to a narrow band  of cells around it. We initialize the band around the free surface at every step, so our narrow band method is more accurately known as a sparse-field method . Figure 26 shows the popular
348605	17329	which in reality we can not carry out reliably anyway, when N is large, because of numerical error. One alternative is to use the conjugate gradient (CG) method (, sections 9.3 & 10.2; , section 2.3.1;  section 6.7) which minimizes 1 2 xT Ax ? x T b. (40) Setting x=A ?1 b gives the minimum value of equation 40, which is ?b T A ?1 b/2. Therefore, minimizing equation 40 and
348605	17329	viscosities, the condition number of the resulting matrix prevents an effective direct solve, so we solve the equation iteratively using the conjugate gradient method with a Jacobi preconditioner . The pseudo-code for a general preconditioned conjugate gradient is listed in table 4. The Jacobi preconditioner is simply the diagonal of the matrix, so its inverse is the reciprocal of each
348605	17330	QUAKE INSTALLED PROBABLY WILL NOT AFFECT WHETHER A SIMULATION WILL PRODUCE IDENTICAL RESULTS ON THE TWO MACHINES, Figure 4: Splashing fluid. BUT IT WILL BE DIFFICULT TO PROVE THIS.” T. C. Belding  The equations of motion that govern the movement of liquid are known and the Navier-Stokes equations. A rich history of solving the Navier-Stokes equations exists in computer animation [9, 25, 79,
348605	17331	and ease of programming (e.g., using less storage may need 34stypedef struct { byte numOff /* the number of off diagonal terms */ double diag /* value of the diagonal entry */ double offEntries /* value of the off diagonal entries */ int offIndices /* array index of the off diagonal terms */ } matrix-row Table 1: The matrix-row data structure used for the sparse matrix storage of A.
348605	17331	time steps for the information to propagate out that far. 6.4 Surface Extraction To extract triangles from the level set we use a simple exhaustive enumeration technique similar to marching cubes ( sec. 4.2). To avoid coding all 256 possible triangulations for cubes, and also to avoid dealing with the ambiguities from a naïve marching cubes implementation, we break each 94sFigure 33: A cube
348605	17331	16 triangulations for an implicit surface passing through a tetrahedron, and with similarities between the cases there are only 3. Most of the time we break the cube into 6 tetrahedra just like in  section 4.3.5, and shown in figure 33. The top right corner of figure 34 shows a surface extracted from a splash that has a tear forming in it. There are grid artifacts in that triangulation. To
348605	17332	Much of the material in this dissertation is drawn from the two publications, “Melting and Flowing”  and “Rigid Fluid: Animating the Interplay Between Rigid Bodies and Fluid” . I am extremely grateful to my coauthors, Greg Turk, Peter Mucha and Brooks Van Horn, without whose help those articles would not exist. Jessica Hodgins and the students in the Animation Lab–Alan
348605	17332	ranging from the absolute rigidity or slight bending of hardened wax to the splashing and sloshing of water. xvsThe coupling that ties together the rigid body and fluid solvers was presented in , and is known as the Rigid Fluid method. It is a technique for animating the interplay between rigid bodies and viscous incompressible fluid with free surfaces. Distributed Lagrange multipliers are
348605	17332	running a simulation we create detailed polygonal models of the fluid by splatting particles into a volumetric grid and then render these models using ray tracing with sub-surface scattering. In , I model the free surface with a particle level set technique . The surface is then rendered by first extracting a triangulated surface from the level set and then ray tracing that surface with
348605	17332	fluid method while describing possible future work. 5.7.1 Computation Time Tables 8 and 9 demonstrate the speed of the rigid fluid method on a Pentium 4 2.8GHz with 2GB RAM. All simulations in  were re-run to collect more useful timing information. Table 8 shows computation times for the first second, when most of the action takes place, including total CPU time (not including render and
348605	17333	staying in school and becoming fake doctors together would be a cool idea iiisACKNOWLEDGEMENTS Much of the material in this dissertation is drawn from the two publications, “Melting and Flowing”  and “Rigid Fluid: Animating the Interplay Between Rigid Bodies and Fluid” . I am extremely grateful to my coauthors, Greg Turk, Peter Mucha and Brooks Van Horn, without whose help those articles
348605	17333	that are needed to allow solids and fluids to interact: a rigid body solver, a fluid solver, and a mechanism for the coupling of the two solvers. The fluid solver used in this work was presented in . This Melting and Flowing solver is a fast and stable system for animating materials that melt, flow, and solidify. Examples of realworld materials that exhibit these phenomena include melting
348605	17333	but the small changes needed to couple this solver to interact with fluid will be. When simulating fluids, the fluid-air interface (free surface) is an important part of the simulation. In , the free surface is modelled by a set of marker particles, and after running a simulation we create detailed polygonal models of the fluid by splatting particles into a volumetric grid and then
348605	17333	that melt, flow and harden can be viewed always as a fluid, even when in solid form. 2sFigure 2: A lead and wood ball are thrown into a tank of water. The melting and flowing research presented in  does not use a level set to define the surface, instead Greg Turk extracted the surface models from the the simulation by splatting the marker particles into a high resolution volume and then he
348605	17333	ray tracing with subsurface scattering to render wax and paste-like materials. The splatting and rendering will not be covered in this thesis, and but I encourage the interested reader to refer to  for details. 1.2 Rigid Fluid The rigid fluid method described in chapter 5 continues with and expands on the idea that solids can be treated as fluid. Solids in the previous section were simulated
348605	536	and Gascuel also use Lennard-Jones style particle forces to create soft materials, but they maintain an explicit blending graph and perform particle size calculations in order to preserve volume . Stora et al. use particles and an approach to force calculations called smoothed particle hydrodynamics (SPH) in order to simulate the flow of lava . Their simulator models heat diffusion and
348605	17337	fluid by splatting particles into a volumetric grid and then render these models using ray tracing with sub-surface scattering. In , I model the free surface with a particle level set technique . The surface is then rendered by first extracting a triangulated surface from the level set and then ray tracing that surface with the Persistence of Vision Raytracer (http://povray.org).
348605	17337	torques because it can apply any force at any point on the rigid body, not just repulsion forces at the center of mass. The 56srigid fluid technique also incorporates free surfaces via level sets . 5.1 Rigid Fluid Domains The Navier-Stokes equations which govern the movement of an incompressible fluid are covered in chapter 3, and the extension to the implicit viscous Navier-Stokes equations
348605	17337	we must advance both F and R. We will describe the advancement of the fluid domain, followed by the advancement of the solid domain. We use the particle level set, ?, to identify the fluid region . In the level set implementation, all grid cells have a width of 1 and the time step is assumed to be 1 for simplicity, but the grid cells in C have a width of ?x and will be advanced by ?t. Also,
348605	17337	even if we use the 3 rd -order in time TDV-RK technique and the 5 th -order in space WENO technique, there will still be 1 st -order errors due to the coarse grid resolution. The particle level set  was created to compensate for some of the inaccuracies when advecting level sets by adding Lagrangian particles to the simulation and letting them correct the first order error with the inherent
348605	17337	we add vertices at the faces and center of each cube. First we get the interpolated level set values at those nodes then we use the standard error correction technique from the particle level set  to correct the level set value at the new vertices. The new vertices are then used to create a tetrahedral decomposition with 24 tetrahedra instead 6. There are 4 tetrahedra created for each face
348605	17339	time-steps even for moderate viscosity, and simply cannot animate highly viscous liquids , or they treat the fluid as inviscid and ignore the diffusive effects of viscosity all together . Many materials exhibit variable viscosity depending on properties such as temperature and water content. With the ability to simulate highly viscous fluid, the second goal of chapter 4 is within
348605	17339	and two-way coupling. More information on coupling in a more general animation framework can be found in . It is common in computer animation to see a ball splash into a pool of liquid . This is an example of one-way solid-to-fluid coupling where the motion of the ball is predetermined and the fluid motion is a secondary effect in response to the ball. In such simulations, the
348605	17339	on the liquid’s surface. Their technique is improved upon by adding another layer of particles outside the water. This new technique, called the particle level set, is used by Enright et al.  to create free surface flows with thin sheet splashes and better fluid volume preservation. Yngve et al. demonstrate the animation of explosions using CFD based on the equations for compressible,
348605	17339	a temperature field. Wrenninge wrote an overview of implementing a fluid solver for use in the visual effects industry . In his thesis he describes and uses the same basic implementation as  but improves the definition of solid boundaries by defining them as a static level set. A sketch of Wrenninge’s work, co-authored with Roble from Digital Domain, can be found in . Another
348605	17339	Foster and Fedkiw  improve on the technique described in those previous papers by allowing the fluid to move freely along the tangent of the solids. Enright et al. also use this improvement . One-way fluid-to-solid coupling is demonstrated by Chen et al.  and by Foster et al. , where solids are treated as massless marker particles that moved freely on the fluid’s surface. Wei et
348605	17341	or pudding, by adding elastic terms to the basic NavierStokes equations. The elastic terms are controlled by von Mises’s yield condition and a quasi-linear plasticity model. Fattal and Lischinski  control smoke animations by adding two terms to the standard flow equations: a smoke gathering term to prevent excessive diffusion, and a driving force to move smoke densities toward target smoke
348605	17342	and Stam improve upon this method using vorticity confinement to maintain the high frequency swirling in smoke, and by using clamped cubic interpolation to prevent the dissipation of fine features . Their improved technique allows solid boundaries, moving or stationary, but assumes a zero viscosity fluid . Shi and Yu  adapt a similar semi-Lagrangian smoke simulator to an adaptive
348605	17342	for the prevalence of finite differences methods used for computer graphics. 2.2 Previous Work in Solid Fluid Coupling Many researchers have demonstrated one-way solid-to-fluid coupling. In , the rigid bodies are treated as boundary conditions with set velocities. Foster and Fedkiw  improve on the technique described in those previous papers by allowing the fluid to move freely
348605	17342	Belding  The equations of motion that govern the movement of liquid are known and the Navier-Stokes equations. A rich history of solving the Navier-Stokes equations exists in computer animation , and this chapter will continue that tradition with the additional aim of supplying the reader with all the background material necessary to create their own fluid simulator. There are many choices
348605	17343	seamen down with the undertow. The work described in chapter 5 focuses on two-way coupling of rigid bodies and incompressible fluid. Two-way coupling of this type is in general a difficult problem , but with the rigid fluid method, two-way coupling between fluid and rigid bodies is a straightforward addition to a fluid and rigid body solver, and the extra computational cost scales linearly
348605	17343	A plethora of research on the coupling of solids and fluid exists in the physics and mathematics literature. Fedkiw uses the Ghost Fluid method to couple compressible fluids and deformable solids . Deformable solids have also been successfully treated by Peskin with the Immersed Boundary method . Two-way coupling between fluids and rigid solids is often accomplished in the computational
348605	17346	freely flowing (low viscosity). Other published methods for liquids in the graphics literature require small time-steps even for moderate viscosity, and simply cannot animate highly viscous liquids , or they treat the fluid as inviscid and ignore the diffusive effects of viscosity all together . Many materials exhibit variable viscosity depending on properties such as temperature and
348605	17346	and two-way coupling. More information on coupling in a more general animation framework can be found in . It is common in computer animation to see a ball splash into a pool of liquid . This is an example of one-way solid-to-fluid coupling where the motion of the ball is predetermined and the fluid motion is a secondary effect in response to the ball. In such simulations, the
348605	17346	including smoke in video games , spectral technique solvers , real time interactions . Foster and Fedkiw re-visit the Marker-And-Cell method, and improve upon it in several ways . First, they replace the forward Euler convection calculations with a semi-Lagrangian approach for greater stability. Second, and perhaps more important, they introduce and use the level set
348605	17346	Fluid Coupling Many researchers have demonstrated one-way solid-to-fluid coupling. In , the rigid bodies are treated as boundary conditions with set velocities. Foster and Fedkiw  improve on the technique described in those previous papers by allowing the fluid to move freely along the tangent of the solids. Enright et al. also use this improvement . One-way
348605	17346	Belding  The equations of motion that govern the movement of liquid are known and the Navier-Stokes equations. A rich history of solving the Navier-Stokes equations exists in computer animation , and this chapter will continue that tradition with the additional aim of supplying the reader with all the background material necessary to create their own fluid simulator. There are many choices
348605	17346	of implementation, ease of explanation, and popularity in the computer animation literature. This chapter describes one specific implementation of a fluid simulator, specifically a MAC method  with pressure projection  to enforce incompressibility. The chapter begins by describing the equations that govern the motion of fluid, and then it details one way to discretize those
348605	17346	Marker-And-Cell method of simulating fluids with free surfaces which was originally described by Harlow and Welch in 1965 . The MAC approach is described well in several other 17spublications , but in the hopes of making this a self-contained work and to fill in some deficiencies in the 3D portions of previous descriptions, I will describe one version of the method here. First I will
348605	17346	fluid on each side are discussed in section 3.2.3.5. This section covers the treatment of all cell faces that that have fluid on one side and either solid walls or empty air on the other. As in , we allow any of the cells of the MAC grid to be obstacles that fluid will not enter. In particular, the six sides of the simulation grid are treated as solid walls. We will discuss how this layer
348605	17346	Each surface-cell may have anywhere from one to six faces that are adjacent to air, and each case is treated in order to maintain the divergence-free property. These cases are enumerated in  for two dimensions and their extension to 3D is discussed next. To set the surface-face boundary conditions in a surface-cell we use the continuity condition expressed by equation 1. Throughout the
348605	17346	simplicity, we decided that even solid wax would be modeled with viscosity, a very high viscosity. The MAC method for simulating liquids was popularized in computer graphics by Foster and Metaxas , and it represented the surface/air interface that we knew we would need. The MAC method is a finite difference approach. Chapter 3 describes how to numerically solve the constant viscosity
348605	541	graphics by Foster and Metaxis. In a series of several papers, they demonstrate how the Marker-And-Cell (MAC) approach of Harlow and Welch  can be used to animate water , animate smoke , and be augmented to control the behavior of animated fluids . A major strength of their method is that liquid is no longer constrained to be a height field, as demonstrated by their animations
348605	8380	drawback to the signed distance representation is that it must be held in memory, so if many complicated rigid bodies are needed in the scene we suggest using a sparse data structure to store them . The advantage of this representation is constant time inside/outside tests. Negative values are inside the object and positive values are outside the object. Each center of the cell is tested, and
348605	17351	explosions in 3D by interpolating between several large 2D simulations with 3D Kolmogorov velocity fields. There are several fluid publications in press at the time of this writing. Goktekin et al.  animate viscoelastic fluids, such as hair gel or pudding, by adding elastic terms to the basic NavierStokes equations. The elastic terms are controlled by von Mises’s yield condition and a
348605	17351	process. A more long-term research question is how to allow the cracking of material that has melted and then hardened, perhaps through the inclusion of surface tension or viscoelastic forces . Many materials such as mud and lava crack while they harden, and it would be wonderful to animate this process. 55sCHAPTER V RIGID FLUID “NON-ULTRABUOYANT? I THINK I MADE UP A WORD. NOT THE BEST
348605	17354	densities (e.g., wood or lead) can be combined in the same animation. The rigid body solver used in this work is the impulse based solver, with shock propagation introduced by Guendelman et al. in . The rigid body solver allows for collisions ranging from completely elastic, where an object can bounce around forever without loss of energy, to completely inelastic where all energy is spent in
348605	17354	on the extension velocities and level set implementation. Once the level set position is updated, the rigid fluid method identifies the new R by moving the rigid bodies with the solver described in . The rigid bodies are polygonal objects, possibly concave, and their mass properties are computed directly from the polygonal representation . Before the rigid body solver can take a step it
348605	17354	Several steps are involved in calculating the discrete integrals from the above equations 81 and 82. First a volume and center of mass is stored for each piece of the solid that is contained 2 In  the updated rigid body velocity, v = v + ?tf, is used to check for interference. When the rigid body is wet we simply use the old velocity instead of an updated one. The most accurate way to update
348605	17354	due to collisions and are used in equation 70. Strictly speaking, the force, F, used in equations 68 and 69 has dimensions mass ? space/time 2 , but the impulse-based 67 ?srigid body solver we use  gives us impulse forces with dimensions mass ? space/time. We change the impulse forces to standard forces by dividing them by the current time step ?t. This chapter, and our code use standard
348605	17354	the density of the rigid body may vary with space or time, and keeping the stored per-cell quantities as general as possible helps simplify changes we may wish to make at a later date. Just as in , we have a signed distance function for each of our rigid bodies. Simple shapes like spheres and boxes use an exact formula, very complex shapes used a static level set, and shapes with few
348605	17356	is bent according to the variations in wave speed that the elevation profile induces. The simulation of breaking waves occurs at a particular sea floor elevation and wave velocity. Hinsinger et al.  uses an adaptive mesh and a procedural approach to animate and render deep water waves at interactive rates. Kass and Miller take a different approach to the simulation of fluids . Like most of
348605	17358	The free surfaces are delineated by a subdivision surface, and triangulated representations of that surface are used to quickly render caustic textures for realistic liquid lighting. Hong and Kim  use a VOF technique to simulate a twophase fluid flow with bubbles. A polygonal surfaces is extracted from the VOF indicator function and used to compute surface tension forces and in rendering for
348605	17359	of solid boundaries by defining them as a static level set. A sketch of Wrenninge’s work, co-authored with Roble from Digital Domain, can be found in . Another Sketch from Houston et al.  from Frantic Films describes a Unified Occlusion Field that stores slip conditions, velocities, and a level set boundary to represent all complex solid objects in the simulation grid. The melting
348605	17361	rendered (with subsurface scattering in the case of the wax bunny). The surface splatting technique was created and implemented by Greg Turk, and the subsurface rendering technique was created in  and implemented by Brooks Van Horn; details can be found in . Figure 25: A bunny from particles to triangles to final render. Considering the coarse resolution of the grid for the bunny
348605	17365	keyframes. They demonstrate significant speedups from the work in , and considerably more control including the animation of running humanoid figures made of smoke and water. Losasso et al.  simulate fluids on an octree data structure that is refined to capture high scale fluid movement near obstacles, liquid free surfaces, and areas with heavy smoke density. They create a symmetric
348605	17366	by adding two terms to the standard flow equations: a smoke gathering term to prevent excessive diffusion, and a driving force to move smoke densities toward target smoke states. McNamara et al.  use the adjoint method to reduce the complexity in solving optimal control problems to control both liquid and smoke animations with keyframes. They demonstrate significant speedups from the work
348605	17368	have used physically-based particle models to represent fluids. Miller and Pearce create solids, deforming objects and fluids by tuning the manner in which particles interacted with one another . Their particle forces are similar to Lennard-Jones forces: particles very close together repel one another, but at moderate distances they are attracted to each other, with the attraction falling
348605	17368	to this temperature. Several other graphics researchers have incorporated thermal diffusion and the resulting changes to viscosity into their material models, usually with a particle-based approach . Incorporating these effects into the MAC framework is straightforward, and we give details of how to do so now. The change in heat is governed by an equation that is very similar to the second
348605	17369	the rigid bodies with the solver described in . The rigid bodies are polygonal objects, possibly concave, and their mass properties are computed directly from the polygonal representation . Before the rigid body solver can take a step it must decide which solids are touched by the fluid and which are not. Those that are not touching fluid are updated exactly as in , but those
348605	17369	of the rigid fluid method, we actually took the intersection of the rigid body triangles and the cell, then used that new polygonal object to calculate an exact center of mass and volume for it . We note that this also gave us an accurate moment of inertia for that piece as well. As the rigid bodies we wished to simulate became more complicated, we realized that this approach was
348605	17369	the distance is known at each cell corner, a closed polygonal object could be created for that cell and a good guess for the cells center of mass and moment of inertia could be obtained from it . I did not implement this possible speedup, but it should work for most objects unless they have several small resolution features. Stam observed that the semi-Lagrangian technique used for
348605	17370	(SPH) in order to simulate the flow of lava . Their simulator models heat diffusion and variable viscosity, and they demonstrate animations that use up to 3,000 particles. Müller et al.  also use a SPH based approach to simulate free surface flows, and they animate up to 5,000 particles at interactive rates. Premoe et al.  use a Moving Particle Semi-implicit (MPS) technique to
348605	17372	the water. Their PDE’s govern the amount of fluid that passes between columns of water. O’Brien and Hodgins use a hybrid height field and particle-based representation to simulate splashing water . Several groups of researchers have used physically-based particle models to represent fluids. Miller and Pearce create solids, deforming objects and fluids by tuning the manner in which particles
348605	17372	of one-way coupling. In addition, they also propose tuning the velocity and pressure around objects to achieve two-way coupling, although they do not implement the idea. 11sO’Brien and Hodgins  demonstrate both types of one-way coupling by dropping a ball into water. First they calculate the forces from the ball colliding with the water to transfer the energy to the water and create waves
348605	17373	solids and fluid can have: one-way solid-to-fluid coupling, one-way fluid-to-solid coupling, and two-way coupling. More information on coupling in a more general animation framework can be found in . It is common in computer animation to see a ball splash into a pool of liquid . This is an example of one-way solid-to-fluid coupling where the motion of the ball is predetermined and
348605	17374	simulation, 35 × 28 × 38, we were impressed with the detail of the surface that the marker particle alone afforded us. However, another representation for the free surface, known as a level set , was gaining popularity in the computer graphics literature . A level set is a dynamic implicit surface. The term level set is a mathematical term that simply means the curve in 2D or
348605	17379	all that background out of the way, we can now get back to the point at hand–the technique that we use for reinitialization. For our level set implementation we chose a PDE-based reinitialization  instead of a fast marching method for two reasons: the PDE-based method is is O(N) instead of O(NlogN), so it should scale better to larger grid sizes, and the PDE-based reinitialization allows us
348605	17379	and TDV-RK techniques. Fast marching methods are notoriously difficult to extend to higher order. For most calculations we have a band width (? in ) of 4 voxels and a smooth band width (? in ) of 7 voxels, though these variables can change based on our needs. It should also be noted that the PDE-based method we use does not move the level set across cell nodes as the technique in
348605	17389	. In fact, when ? is a signed The signed distance property is maintained through a process known as reinitialization. One popular way to reinitialize the level set is with the fast marching method , which is an O(NlogN) technique based on sorting (the reason it is O(NlogN)). We will describe an O(N) PDE based scheme, but first we must introduce upwinding now because it is important to
348605	17389	considerably. Consider two ways that we can take a first derivative of ? in the x-direction: the forward difference operator, ? + x , and backward difference 2 Heavy-side and other functions  can be used to represent a level set, but the signed distance field is important for our implementation. 84soperator, ? ? x , at i, j,k are and ? + x = ?i+1, j,k ? ?i, j,k, (87) ? ? x = ?i, j,k ?
348605	17393	and suffers from two main drawbacks. First, the computational grid must be re-meshed when the elements get too distorted, an often costly procedure. The second drawback, pointed out by Singh et al. , is that two layers of elements are 13sneeded in the gap between solids as they approach one another. Researchers studying particulate suspension flows have introduced a two-way coupled computation
348605	554	and splashing. Stam uses a semi-Lagrangian method for fluid convection and an implicit integrator for diffusion so that large time steps can be used for animating smoke with no internal boundaries . Stam was also the first in computer graphics to use the now popular Chorin based pressure projection method  to satisfy the zero divergence condition. Fedkiw and Stam improve upon this method
348605	554	for the prevalence of finite differences methods used for computer graphics. 2.2 Previous Work in Solid Fluid Coupling Many researchers have demonstrated one-way solid-to-fluid coupling. In , the rigid bodies are treated as boundary conditions with set velocities. Foster and Fedkiw  improve on the technique described in those previous papers by allowing the fluid to move freely
348605	554	Belding  The equations of motion that govern the movement of liquid are known and the Navier-Stokes equations. A rich history of solving the Navier-Stokes equations exists in computer animation , and this chapter will continue that tradition with the additional aim of supplying the reader with all the background material necessary to create their own fluid simulator. There are many choices
348605	554	and popularity in the computer animation literature. This chapter describes one specific implementation of a fluid simulator, specifically a MAC method  with pressure projection  to enforce incompressibility. The chapter begins by describing the equations that govern the motion of fluid, and then it details one way to discretize those equations so that they may be solved on
348605	554	so care must be take with the implementation to make the incomplete Cholesky preconditioner worth the effort. Neither operator splitting nor implicit integration are new to computer graphics. Stam  used 44s/* Variables defined in table 3 except the matrix M and the vector z. */ ? = 0.0 b norm = ? b · b r = b - Ax /* matrix vector multiply is in table 2 */ ???(int i = 0; i < iter max ; i++){
348605	17395	to simulate flows on a Catmull-Clark surface inbeded in 3D space , and has a large body of work in fluid simulations for graphics including smoke in video games , spectral technique solvers , real time interactions . Foster and Fedkiw re-visit the Marker-And-Cell method, and improve upon it in several ways . First, they replace the forward Euler convection calculations with a
348605	17396	by solving an optimal control problem over every frame of the animation . Stam improves on his 2D fluid simulator by allowing it to simulate flows on a Catmull-Clark surface inbeded in 3D space , and has a large body of work in fluid simulations for graphics including smoke in video games , spectral technique solvers , real time interactions . Foster and Fedkiw re-visit the
348605	17397	fluid simulator by allowing it to simulate flows on a Catmull-Clark surface inbeded in 3D space , and has a large body of work in fluid simulations for graphics including smoke in video games , spectral technique solvers , real time interactions . Foster and Fedkiw re-visit the Marker-And-Cell method, and improve upon it in several ways . First, they replace the forward Euler
348605	17397	objects unless they have several small resolution features. Stam observed that the semi-Lagrangian technique used for advection is more accurate if it is used on a divergence free velocity field . Because of this observation we decided to solve the Navier-Stokes equations in the following order: 1. Add Body Force, 2. Viscous Diffusion, 3. Pressure Projection, 4. Advection, 5. Pressure
348605	17398	calculations in order to preserve volume . Stora et al. use particles and an approach to force calculations called smoothed particle hydrodynamics (SPH) in order to simulate the flow of lava . Their simulator models heat diffusion and variable viscosity, and they demonstrate animations that use up to 3,000 particles. Müller et al.  also use a SPH based approach to simulate free
348605	17398	to this temperature. Several other graphics researchers have incorporated thermal diffusion and the resulting changes to viscosity into their material models, usually with a particle-based approach . Incorporating these effects into the MAC framework is straightforward, and we give details of how to do so now. The change in heat is governed by an equation that is very similar to the second
348605	17405	to make them dangerous in the exciting field of computational fluid dynamics. One of the first steps in becoming less dangerous would be to understand stability. Chapter one of Trefethen’s book  would be a good place for the interested reader to start. This section, however, is an attempt to give the reader an intuitive feel for what stability is without resorting to a full von Neumann
348605	17405	schemes, higher-order explicit time steps (e.g., fourth-order Runge-Kutta) meet with similarly prohibitive stability criteria at large viscosities, at only marginally different threshold values . Lowering the time-step size is one possible fix to this problem, but this quickly leads to a prohibitively large number of time steps: even moderately viscous fluids with a viscosity of 10 require
348605	17407	simulations can be run in a small amount of memory. Treuille et al. add a level of user defined key-frame control to smoke by solving an optimal control problem over every frame of the animation . Stam improves on his 2D fluid simulator by allowing it to simulate flows on a Catmull-Clark surface inbeded in 3D space , and has a large body of work in fluid simulations for graphics
348605	17407	the adjoint method to reduce the complexity in solving optimal control problems to control both liquid and smoke animations with keyframes. They demonstrate significant speedups from the work in , and considerably more control including the animation of running humanoid figures made of smoke and water. Losasso et al.  simulate fluids on an octree data structure that is refined to
348605	558	et al.  use a combined VOF, CIP, and particle system to simulate fluid with splash–particles that are under the effect of gravity and foam–particles that stick to the fluid surface. Wei et al.  use Cellular Automata (CA) to simulate melting of volumetric solids at interactive rates. The transition between solid and liquid is controlled by the transfer of heat between cells. Wei et al.
348605	528	cells. Wei et al.  use a technique based on the CA model called a Lattice Boltzmann Model (LBM) to animate bubbles and feathers drifting in the air. A LBM approach is used again by Wei et al.  to simulate the microscopic behavior of fluid so that the averaged macroscopic movement obeys the Navier-Stokes equations. They also model the buoyancy forces of gas with a temperature field.
348605	17408	Belding  The equations of motion that govern the movement of liquid are known and the Navier-Stokes equations. A rich history of solving the Navier-Stokes equations exists in computer animation , and this chapter will continue that tradition with the additional aim of supplying the reader with all the background material necessary to create their own fluid simulator. There are many choices
348605	17410	to pay attention to a narrow band  of cells around it. We initialize the band around the free surface at every step, so our narrow band method is more accurately known as a sparse-field method . Figure 26 shows the popular Zalesak’s Disk , which will be used to demonstrate some concepts throughout this section. All points inside the water (blue in figure 26) have ?< 0 and all points
348605	17412	contact forces, they could be used to model the forces necessary for joints, or even human motion controllers. We would like to add these features to our rigid body solver so that simulated divers  could splash and interact with the water, and maybe learn to swim. As mentioned in section 5.7.3, we have not done much to improve the surface of the particle level set where the water and the
348605	17415	surface flows with thin sheet splashes and better fluid volume preservation. Yngve et al. demonstrate the animation of explosions using CFD based on the equations for compressible, viscous flow . Their method takes care to properly model the shocks along blast wave fronts, and also models the interaction between the fluids and solid objects. 8sFeldman et al. animate suspended particle
348605	17415	and spray from solid-to-fluid coupling. Once the ball is in contact with the water, its center of mass stays at the surface and follows the laws of one way fluid-to-solid coupling. Yngve et al.  demonstrate two-way coupling of breaking objects and compressible fluids in explosions, however their technique does not apply to incompressible fluids. To achieve fluid-tosolid coupling, Yngve et
348605	17415	by the two-way coupling with no user interaction or pre-defined motion. Moreover, the fluid-to-solid coupling in  and  use the pressure from the fluid to add forces to the solid. Just as in , dynamic forces due to the fluid momentum are not accounted for since the hydrostatic pressure forces act in a direction normal to the rigid body’s surface. Since many two-way coupled animations
348605	17415	for fluids moving at low speeds because their compression is negligible and the incompressibility allows for a convenient solution for the pressure (described below). Explosions are high speed , and so are jet planes moving at MACH speeds, and such phenomena are governed by other equations. Equation 2 describes the conservation of momentum, and it has several components. Reading from left
8921152	17423	recognition, and what might be called “language recognition,” distinguishing grammatical from ungrammatical “statements” in an artificially-generated language consisting of strings of symbols. All three are described in the relevant literature of perception and cognitive psychology. Since the existence of these effects is not controversial, our experiments have focused on understanding
8921152	17427	You will not be able to give another person such a stored certificate, even if you wished to do so. The novelty in the present work lies not in the use of pictures (e.g. “pictures replacing PINs”), but in suggesting that many natural characteristics of human memory can be exploited. Equally important is that these behaviors integrate naturally into cryptographic protocols. These give an
8921152	17427	APPROACHES Several other groups have exploited picture recognition for access control, generally viewing their techniques as a way of making passwords easier to recall. Thus Dhamija and Perrig had the user select a small, fixed, group of pictures, then pick them out of a larger group. Researchers at Microsoft used cued recognition of artificially generated Rorschach patterns to
8921152	18429	You will not be able to give another person such a stored certificate, even if you wished to do so. The novelty in the present work lies not in the use of pictures (e.g. “pictures replacing PINs”), but in suggesting that many natural characteristics of human memory can be exploited. Equally important is that these behaviors integrate naturally into cryptographic protocols. These give an
8921152	18429	as a way of making passwords easier to recall. Thus Dhamija and Perrig had the user select a small, fixed, group of pictures, then pick them out of a larger group. Researchers at Microsoft used cued recognition of artificially generated Rorschach patterns to generate passwords. Thesuser is shown a set of pictures and asked to assign a word to each, keeping it secret. Letters selected
8921152	17429	K and D). OBJECT RECOGNITION After finding that pictures with a clear central subject or theme were more easily recognized, we also explored using a standard database of 260 artist-drawn images  of common objects. Results of having two subjects train and test on these are shown in Figure 3. This was somewhat less successful than using photographs. A possible reason is that the familiarity
8921152	17430	a recognition protocol can still be designed with strings of letters. We studied recognition of previously seen pseudowords, generated by taking the list of common English words given in Wilson, and modifying them in one letter position using the program provided by van Heuven. A native English speaker then selected pseudowords which are pronounceable, and do not exist as valid words.
17446	17447	that to analyze soundness, the token-game semantics can safely be used, even though that semantics is not reactive. 1 Introduction Petri nets are a popular technique to formally model workflows . They offer a formal counterpart for the bubbles and arrows that people draw when modeling workflows. Their formal token-game semantics enables analysis of Petri net based workflow models. Under
17446	17447	what behaviors are allowed. By computing behavior of a workflow model using the token-game semantics, errors in a workflow model can be spotted before the workflow model is actually put to use (cf. ). A workflow model is put to use by feeding it to a workflow management system (WFMS). Heart of a WFMS is the workflow engine (WF engine), that does the actual management. WF engines are reactive
17446	17447	In particular, we will study in what respect and under what conditions these two semantics induce similar behavior. As an application of this result, we will show that the soundness property  is preserved when transforming a workflow net into a reactive workflow net, i.e., a workflow net with a reactive semantics. Thus, we give a justification why the token-game semantics can safely be
17446	17447	this semantics to the standard token-game semantics. In particular, we prove that under some conditions both semantics induce similar behavior. In Sect. 4 we recall the definition of a workflow net  and the soundness property. We present different interpretations for transitions in a workflow net. Using these different interpretations, in Sect. 5 we show how a workflow net can be transformed
17446	17447	reactive semantics. Note that this sequence has an intermediate stable state  not present in the original sequence. 4 Workflow Nets In this section we recall the definition of Workflow nets  and give different interpretations for transitions in Workflow nets. Definition. A Workflow net (WF net) is a Petri net with one input place i and one output place o such that: – Place i does not
17446	17448	execution of business processes. Many approaches aiming at providing support for workflow management are based on the use of Petri nets (e.g. ) or were mapped onto Petri nets (e.g. ). Even though these approaches cover the modeling and the analysis of business processes, they provide only limited support for the execution at run time. Reasons for that gap have been discussed
17446	17449	orthogonal criterion to distinguish choices, is the moment of choice, i.e., the moment one of the alternative transitions is executed. This distinction was made by Van der Aalst and his coworkers . They distinguish between implicit and explicit choices. An implicit choice (also deferred choice) is made the moment an external event occurs, hence it corresponds to a choice that consists of
17446	17451	than external transitions. So, if both the environment and the reactive system can do a transition, the reactive system will fire first. This corresponds to the perfect synchrony hypothesis , an assumption frequently made in the design of reactive systems: The reactive system is faster than the environment it controls. Note that the perfect synchrony hypothesis is an assumption, not a
17446	17453	are either returned to the shelf, the term is renewed, or a reminder is send. The outcome of the above mentioned choices depends either on the evaluation of external data or the event occurring. In  these choices have been clustered using the notation of a non-controllable choice. This notation suggests that the outcome of these choice depends on the environment. Choices whose outcome do not
17461	13476	et al., 2000a; Yangarber et al., 2000b). A related set of research uses labeled and unlabeled data in problem domains where the features naturally divide into two disjoint sets. Blum and Mitchell (Blum and Mitchell, 1998) present an algorithm for classifying web pages that builds two classifiers: one over the words that appear on the page, and another over the words appearing in hyperlinks pointing to that page.
17461	13476	is sufficient to learn the task. The separation into feature sets we use for the experiments in this paper is that of noun-phrases, and noun-phrase-contexts. 4.2.1. Cotraining Cotraining (Blum and Mitchell, 1998) is a bootstrapping algorithm that was originally developed for combining labeled and unlabeled data for text classification. At a high level, it uses a feature split in the data and starting from
17461	17462	and the partial-sentences they are embedded in can be used as two complementary sources of information about semantic classes. Similar methods have been used for named entity classification (Collins and Singer, 1999). Although a lot of effort has been devoted to developing bootstrapping algorithms for text learning tasks, there has been very little work in systematically applying these algorithms for
17461	17462	that correspond to organizations, people and locations from a set of corporate web pages. These three semantic classes are often identified using named entity recognizers (e.g. (Bikel et al., ; Collins and Singer, 1999)), but named entity recognition is usually limited to proper names, such as “John Smith” or “California”. Our goal is to identify all noun phrases including common ones such as “a
17461	2456	and label noun phrase instances that correspond to semantic categories of interest. 3. Data Set and Representation As our data set, we used 4392 corporate web pages collected for the WebKB project (Craven et al., 1998) of which 4160 were used for training and 232 were set aside as a test set. We preprocessed the web pages by removing HTML tags and adding periods to the end of sentences when necessary. 1 We then
17461	17464	maximum a posteriori parameters of a generative model for text classification (Nigam et al., 2000), using a generative model built from unlabeled data to perform discriminative classification (Jaakkola and Haussler, 1999), and using transductive inference for support vector machines to optimize performance on a specific test set (Joachims, 1999) are some examples that have shown that unlabeled data can
17461	17465	unlabeled data to perform discriminative classification (Jaakkola and Haussler, 1999), and using transductive inference for support vector machines to optimize performance on a specific test set (Joachims, 1999) are some examples that have shown that unlabeled data can significantly improve classification performance, especially with sparse labeled training data. For information extraction, Yangarber et
17461	17466	examples and adds all of them to the labeled set probabilistically (in the same way EM does for semi-supervised classification). This process iterates until the classifiers converge. Muslea et al. (Muslea et al., 2000) extend the co-EM algorithm to incorporate active learning and show that it has a robust behavior on a large spectrum of problems because of its ability to ask for the labels of the most ambiguous
17461	13480	ambiguous. Cotraining may harm its performance through its hard (binary 0/1) class assignment. 4.2.2. CoEM coEM was originally proposed for semi-supervised text classification by Nigam & Ghani (Nigam and Ghani, 2000) and is similar to the cotraining algorithm described above, but incorporates some features of EM. coEM uses the feature split present in the data, like co-training, but is instead of adding
17461	1258	for bootstrapping from very few examples for several text learning tasks. Using Expectation Maximization to estimate maximum a posteriori parameters of a generative model for text classification (Nigam et al., 2000), using a generative model built from unlabeled data to perform discriminative classification (Jaakkola and Haussler, 1999), and using transductive inference for support vector machines to optimize
17461	2473	to that page. Datasets whose features naturally partition into two sets, and algorithms that use this division, fall into the co-training setting (Blum and Mitchell, 1998). Meta-Bootstrapping (Riloff and Jones, 1999) is an approach to learning dictionaries for information extraction starting only from a handful of phrases which are examples of the target class. It makes use of the fact that noun-phrases and
17461	2473	between 0 and 1 of each noun-phrase and context belonging to the target class. This may reflect well the inherent ambiguity of many terms. 4.2.3. Meta-bootstrapping Meta-bootstrapping (Riloff and Jones, 1999) is a simple two-level bootstrapping algorithm using two features sets to label one another in alternation. It is customized for information extraction, using the feature sets noun-phrases and
17461	17467	each task as a binary classification task. Each noun phrase context consists of two items: (1) the noun phrase itself, and (2) and the context (an extraction pattern). We used the AutoSlog (Riloff, 1996) system to generate extraction patterns. By using both the noun phrases and the contexts surrounding them, we provide two different types of features to our classifier. In many cases, the noun
17461	13562	for learning new templates. They used an unlabeled corpus of 5,000 to 10,000 documents, and suggest extending the size of the corpus used, as many initial patterns are very infrequently occurring (Yangarber et al., 2000a; Yangarber et al., 2000b). A related set of research uses labeled and unlabeled data in problem domains where the features naturally divide into two disjoint sets. Blum and Mitchell (Blum and
8921164	5798	with both synthetic and real data (in both scenarios) are presented in Section 6. Finally, Section 7 has a brief summary and discussion of our results and future plans. 2. Background Atick et al.  proposed a method to use eigenheads to solve a shape from shading problem by leveraging the knowledge of object class, which was used to recover the shape of a 3D human face from single photograph.
8921164	5798	newly registered and aligned database of 3D faces faces to obtain our prior statistical shape model. As a consequence of this analysis, we can define all possible face geometries with eigenheads . This decomposition can be used to reconstruct a new or existing face through the linear combination of eigenhead basis functions. Therefore, our face model is given by ?s¢¡¤£¦¥¨§?©?????? ???????
8921164	17472	which was used to recover the shape of a 3D human face from single photograph. This line of research (including that of many others) ultimately culminated in the seminal work of Blanz & Vetter , who formulated an optimization problem to reconstruct a textured 3D face from one or more photographs in the context of inverse rendering. Though originally targeted to the computer graphics
8921164	17472	sufficiently high-quality inputs (and/or many frames), there is often no real need to estimate the texture-map. 1 Note that while PCA is typically applied to the 2D facial texture as well (as in  for example) an “eigenface” model is neither required nor in fact necessarily desired in our framework (see Sections 4.3 and 7). 4.1. Nonlinear Optimization ? Let be any arbitrary face ¡¤£¦¥ model
8921164	17473	Once we determines? , then, we compute the values of ¢ , ¤ and ¦ such that Eq.(5) is minimized. The needed parameters are obtained from an application of the full ordinary Procrustes analysis . ? ? 5.2 Monocular Sequence We consider a video sequence captured in front of a fixed video camera (eg. a ??? ? ????? ????? webcam). The user is required to start from a frontal view and then
8921164	17476	how we design the cost function ? in Eq.(2). The easiest way to measure difference of two binary images is the number of ‘on’ pixels when pixel-wise XOR operation is applied to the two images . In this case, ? ¡ ? ? ? ? ? ? ??? ????? ? ????? ? ¡¤£?¥?¥¨§?? ? ? ¡ ????? ¥ ????? (2) (3)s? ¡ ? ??? ¥¨§ ? ¡ ? ?¨? if otherwise. ¡ ? ? ? ¥¨§ ? ? ????? ? ¡¤£?¥?¡ ? ? ? ¥ ????? ? ??? If our goal ? §
8921164	17476	resulted in a similar convergence rate and a lower (1/10) cost required for original resolution data. Another way to expedite the optimization process is to employ a hierarchical approach . For example, 99% decimation in mesh resolution and a 75% reduction in image resolution resulted in only 30-40 seconds until convergence. As a result, it is likely better results can be obtained
8921164	16204	face and call it a mask in the following sentences. Since the first-order Taylor expansion of ? is non-linear with respect to ? , it is not trivial to apply ? the well-known Lucas-Kanade method  based on NewtonRaphson style iteration using spatial and temporal image gradients. We exploit a perspective projection, which is more proper model to deal with the sequences taken at relatively
8921164	17477	the image. Shape-from-Silhouette (SFS) techniques have been used to reconstruct 3D shapes from multiple silhouette images of an object without previous knowledge of the object to be reconstructed . The reconstructed 3D shape is called a visual hull, which is a maximal approximation of the object consistent with the object’s silhouettes. The accuracy of this approximate visual hull depends on
8921164	17479	best and most complete solution to this problem is to acquire/analyze/match a full 3D model of the face as represented, for example, by a 3D shape-mesh plus a 2D texture-map. While 2D view-based  and other appearance-based approaches definitely have merit, they suffer from a fundamentally limited representation (a collection of 2D appearance subspaces). We believe that an intrinsic 3D model
8921164	1112	2 through step 4 fors¢? all ’s ????? in database to get ¤??©? the correspondences among all the faces in the database. For step 3, a standard model for scattered data interpolation can be exploited . Note that, at step 4, we cannot get corresponding samples on the surface ??? of for some points on the boundary ??? ? of . It is likely that the two faces under consideration do not match exactly
8921164	1112	the reconstructed 3D face, we assign a color value which is determined from multiple texture images. To do so, we proceed as follows. Our approach is a view-independent texture extraction approach . Each vertex is projected to all image planes and tested if the projected location is within the silhouette area and if the vertex is visible (not occluded) at each projection. For all valid
8921164	17481	calibrated the eleven static cameras (Figure 3) by a standard technique using a calibration object . One could enhance this initial camera calibration by a technique that uses silhouette images . Figure 5 shows how our model face fits to real silhouette images of a Caucasian face. Note the similarity of alignment to the synthetic cases in Figure 4, demonstrating that our boundary-weighted
8921164	17482	the reconstructed 3D face, we assign a color value which is determined from multiple texture images. To do so, we proceed as follows. Our approach is a view-independent texture extraction approach . Each vertex is projected to all image planes and tested if the projected location is within the silhouette area and if the vertex is visible (not occluded) at each projection. For all valid
8921164	17483	of objects in motion) or a span of images in space (images captured from different viewpoints) or a combination of the two. The traditional approach is of course that of Structure-from-Motion (SFM) . It deals with the problem of recovering 3D points on a rigid object from 2D correspondences of points across images. SFM is a direct method to obtain the 3D points which are often not so dense or
8921164	17483	in real face, ? in a reference model face ? and is the number of feature points to be used. We already know ? . However,sis determined from a standard non-linear least square minimization technique . A Levenberg-Marquardt algorithm is applied to obtain the 3D point locations that ? correspond to feature points selected manually in a small number of (3-4) §?? texture images. ? We used in our
8921164	17483	that silhouette images can be easily acquired by a simple background subtraction technique. We calibrated the eleven static cameras (Figure 3) by a standard technique using a calibration object . One could enhance this initial camera calibration by a technique that uses silhouette images . Figure 5 shows how our model face fits to real silhouette images of a Caucasian face. Note the
8921164	17484	the reconstructed 3D face, we assign a color value which is determined from multiple texture images. To do so, we proceed as follows. Our approach is a view-independent texture extraction approach . Each vertex is projected to all image planes and tested if the projected location is within the silhouette area and if the vertex is visible (not occluded) at each projection. For all valid
8921164	14139	of objects in motion) or a span of images in space (images captured from different viewpoints) or a combination of the two. The traditional approach is of course that of Structure-from-Motion (SFM) . It deals with the problem of recovering 3D points on a rigid object from 2D correspondences of points across images. SFM is a direct method to obtain the 3D points which are often not so dense or
8921164	17485	However, our implementation of various stages is more robust and amenable to efficient realization (eg. in hardware), depending on the particular application scenario. Specifically, Vetter & Blanz  used a 3D variant of a gradient-based optical flow algorithm to derive the necessary point-to-point correspondence. Their method also employs color and/or texture information acquired during the
17486	17490	for Hybrid Probabilistic Programs Emad Saad Department of Computer Science New Mexico State University Las Cruces, New Mexico emsaad@cs.nmsu.edu Abstract. Hybrid probabilistic programs framework  is a variation of probabilistic annotated logic programming approach, which allows the user to explicitly encode the available knowledge about the dependency among the events in the program. In
17486	17490	of numerical and non-numerical , multi-valued logic , and probability theory . In probabilistic logic programming framework, Decktyar and Subramanian  have proposed the notion of hybrid probabilistic programs (HP P ). Hybrid probabilistic programs are built upon the idea of annotated logic programs introduced in , and extensively studied in
17486	17490	of the ordering (set inclusion order) employed in the hybrid probabilistic programs semantics. By recalling the definition of the set inclusion ordering in hybrid probabilistic programs framework , given the probabilistic intervals ,  ?  for a certain event e,  ?  iff  ? . This set inclusion order is known as the knowledge order. This
17486	17490	e how more likely that event might occur is more convenient for decision making and reasoning tasks. This intuition can be captured by employing the natural ordering ?t (truth order) described in  in logic programming with probability. The truth order ?t asserts that if ,  ?  are two probabilistic intervals for the events e1 and e2 respectively, then  ?t [b1,
17486	17490	that hybrid probabilistic programs can subsume Lakshmanan and Sadri  approach for probabilistic logic programming which was not possible with the semantics of hybrid probabilistic programs in . Additional advantage of the new semantics is that the TP operator is guaranteed to be always continuous. This is not the case in probabilistic annotated logic programming in general and hybrid
17486	17494	and facts in the logic programs. These formalisms include fuzzy set theory , possibilistic logic , hybrid (a combination of numerical and non-numerical , multi-valued logic , and probability theory . In probabilistic logic programming framework, Decktyar and Subramanian  have proposed the notion of hybrid probabilistic programs (HP P ). Hybrid
17486	17498	and facts in the logic programs. These formalisms include fuzzy set theory , possibilistic logic , hybrid (a combination of numerical and non-numerical , multi-valued logic , and probability theory . In probabilistic logic programming framework, Decktyar and Subramanian  have proposed the notion of hybrid probabilistic programs (HP P ). Hybrid
17486	17498	of a cyclic inference of the same hybrid basic formula with a slightly higher probability interval. This problem does not arise with h-programs that satisfy the finite termination property . Definition 20 (). Let P be an h-program. P is said to satisfy the the finite termination property iff (?F ? bfS(BP ))(? n < ?)(lfp(TP )(F ) = TP ? n(F )).sIf P satisfies the the finite
17486	17498	Implication-Based Approach To show how the probabilistic implication-based framework presented in  is subsumed by the new hybrid probabilistic programs framework, we follow the outline of . Assume that probabilities in h-programs can be represented by a pair of probabilistic intervals, where the various p-strategies used throughout this paper can be straightforwardly extended to cope
17486	17500	programs framework becomes more suitable for real-life applications. To illustrate the idea consider the following example. Example 1. Consider the following robot planning task adapted from . Suppose a robot’s grasping operation is not always successful because of the chance of the robot’s gripper to be wet. Suppose that most of the time the robot is able to grasp a block with a
17486	17501	values are attached to rules and facts in the logic programs. These formalisms include fuzzy set theory , possibilistic logic , hybrid (a combination of numerical and non-numerical , multi-valued logic , and probability theory . In probabilistic logic programming framework, Decktyar and Subramanian  have proposed the notion of hybrid
17486	17502	AI planning example that the new semantics allows us to obtain more intuitive and accurate probabilities. The new semantics of hybrid probabilistic programs subsumes Lakshmanan and Sadri  framework of probabilistic logic programming. The fixpoint operator for the new semantics is guaranteed to be always continuous. This is not the case in the probabilistic annotated logic
17486	17502	e how more likely that event might occur is more convenient for decision making and reasoning tasks. This intuition can be captured by employing the natural ordering ?t (truth order) described in  in logic programming with probability. The truth order ?t asserts that if ,  ?  are two probabilistic intervals for the events e1 and e2 respectively, then  ?t [b1,
17486	17502	programs framework requires changing its current syntax and semantics to adapt with this new ordering. The change includes using disjunctive composition functions similar to that are used in  for combing the probabilistic intervals associated to the same hybrid basic formula (an atom, a conjunction of atoms, or a disjunction of atoms) derived from different rules. As a consequence, the
17486	17502	intervals can be derived which reflects the correct solutions of the problems. Moreover, with the new semantics, it can be shown that hybrid probabilistic programs can subsume Lakshmanan and Sadri  approach for probabilistic logic programming which was not possible with the semantics of hybrid probabilistic programs in . Additional advantage of the new semantics is that the TP operator is
17486	17502	described in . Section 3 presents the new semantics for hybrid probabilistic programs. In section 4, we show how the new semantics of hybrid probabilistic programs subsumes Lakshmanan and Sadri  approach for probabilistic logic programming. Finally, section 5 presents some concluding remarks. 2 Preliminaries In this section we overview some of the basic definitions related to HPPs . Let
818767	17505	to produce and an example of that can be found in systems such as remote environment visualization and manipulation for monitoring and exploration in distant or hazardous locations. For instance in  they have developed an augmented virtual world that contains real world images as object textures that are created in an automatic way, these are called Reality Portals. Using Reality Portals with
818767	17510	stock exchange, news, etc.) or e-banking are particularly suitable for this kind of use . In the last years continuous interaction has been the interest of works such as those related in . As results of those studies we can see continuity as being particularly concerned with activity over a period of time. At a low level, this can involve real-time aspects of technologies such as
818767	17510	continuity is defined as an adaptability level of the user to change or learn new modes of operation. It is related to the similarity level between real and virtual interaction modes. In Dubois  two ergonomic properties of augmented reality systems are discussed: continuity and compatibility. At the perceptual level, the perceptual compatibility extends the observability property  to
818767	17510	these ergonomic properties when applied for an Augmented Reality system. Table 1. Ergonomic properties of observability, honesty, compatibility and continuity in Augmented Reality systems . Perceptual Level Observability Perceptual Compatibility Cognitive Level Honesty Cognitive Compatibility 1 concept 1 representation N concepts, 1 representation each Perceptual Continuity Cognitive
818767	17515	as a usability property has appeared in two fields: Mixed Reality systems and Multiplatform systems. Mixed Reality (MR) systems are systems that combine real and computer-based information. Milgram  defines the Reality-Virtuality continuum shown in Fig. 1. MR is the region between the real world and totally virtual environments. Augmented reality lies near the real world end of the continuum.
818767	17519	the real world end of the continuum. In AR systems, the perception that predominates is the real world augmented by additional capabilities or information provided by the computer system. Vallino  gives a list of 7 application domains where the use of AR reality systems has been investigated: medical domain, entertainment, military training, engineering design, robotics/telerobotics,
818767	17519	Trevisan, and Vanderdonckt term created by Milgram to identify systems which are mostly synthetic with some added real world sources such as texture mapping video onto virtual objects. Vallino  expects that this distinction will fade as the technology improves and the virtual elements in the scene become less distinguishable from the real ones. Therefore we can say that AR and AV are
818767	17519	stock exchange, news, etc.) or e-banking are particularly suitable for this kind of use . In the last years continuous interaction has been the interest of works such as those related in . As results of those studies we can see continuity as being particularly concerned with activity over a period of time. At a low level, this can involve real-time aspects of technologies such as
8718103	17541	a system. This provides a “wrap around” effect to facilitate testing. 3 OBJECTIVES One goal that all proper simulations have in common is to provide accuracy in the modeling of the problem domain (Page and Nance 1994). This becomes readily evident in application domains such as multi-billion dollar contracts that may rely on the results of an acquisition or acceptance simulation. Another area where model
8718103	17541	Event Simulation sub-community appears to have a narrow focus on the efficiency of a model, seemingly disregarding other important issues such as maintanability, portability, or extensibility (Page and Nance 1994). Mertens (1993) describes the Corps Battle Simulator (CBS), a discrete-event simulation used by the military. At the original design time in the early 1980s, the main focus was on performance, but
8921196	18609	costs, TCjit, for conducting interregional trade between the two regional markets at time t is modeled as a random variable with time varying mean transfer costs, ? jit and random component ejit: (3) TC jit = ? jit + e jit where ejit is normally distributed with mean zero and variance ?e 2 for all trade regime probabilities. Given the above formulation of autarky prices and transfer costs,
8921196	18609	policy change. Thus, the probabilities for the different trade regimes are determined simultaneously for the three periods: (1) period before the policy changes, (2) during the transitional period, (3) the period during the full effect of the policy changes. For example, a time path of structural change in a regime probability where the probability has increased as a result of policy change is
8921196	18609	in the TSP users guide (Hall and Cummins, 1999). These strategies include: (1) the choice of appropriate maximum likelihood estimation algorithm, (2) the choice of appropriate starting values, and (3) grid search on certain difficult parameters or full grid search on all parameters. In addition, graphical analysis of the relationship between spatial price differentials and the transfer costs
8921196	18609	(l) 0 6 35 0 5 0 0 Log likelihood Restricted -231.612 -259.456 -224.630 -297.220 -293.538 -292.277 -296.697 Unrestricted -230.150 -251.988 -223.589 -289.548 -285.519 -289.469 -292.484 LR Test ? 2 (3) Statistics 2.90 14.936 a 2.08 15.344 a 16.038 a 5.618 1.74 Observations 73 73 73 73 73 73 72sTable 4.3 Conditional Maximum Likelihood Estimates of EPBM for the Wheat Markets (1996:08 to 2002:08)
8921196	18609	period (l) 0 0 3 2 0 4 0 Log likelihood Restricted -289.931 -279.268 -71.106 -308.244 -33.041 -315.703 -312.793 Unrestricted -289.931 -278.385 -71.106 -308.244 -20.799 -313.480 -312.753 LR Test ? 2 (3) Statistics 0.000 1.766 0.000 0.000 24.484 a 4.446 7.818 Observations 73 73 73 73 73 73 72sPercentage loss or gain Percentage loss or gain Figure 4.1 Magnitude of Losses and Gains from Inefficient
8921196	18611	related due to changes in either market’s supply and demand conditions or the stochastic disturbance terms. as: In regime two, the spatial price differential is less than transfer cost and is given (5) P it ? Pjt < TC jit . This regime represents a market condition where no profitable arbitrage opportunities exist between the two markets. The two markets may be in autarky but prices are
8921196	18611	are stochastic and the 1 transfer cost between the two markets is independent of the direction of trade flows, we can redefine the conditions for regimes one, two and three given in equations (4), (5) and (6), respectively, as follows: o (9) P it ? Pjt ? ? ? ? jit = e jit ? 0 1 o (10) Pit ? Pjt ? ? ? ? jit = e jit ? u jit ? 0 1 o (11) P it ? Pjt ? ? ? ? jit = e jit + v jit ? 0 1 where ujit and
8921196	18612	are not transmitted across the markets. Finally, regime three is given as a condition where trade may or may not be occurring and the spatial price differential is greater than the transfer cost: (6) P it ? Pjt > TC jit . In this regime, the spatial arbitrage condition is violated and the markets are not efficient but may be integrated to some extent if some trade is occurring. In this regime,
8921196	18612	and the 1 transfer cost between the two markets is independent of the direction of trade flows, we can redefine the conditions for regimes one, two and three given in equations (4), (5) and (6), respectively, as follows: o (9) P it ? Pjt ? ? ? ? jit = e jit ? 0 1 o (10) Pit ? Pjt ? ? ? ? jit = e jit ? u jit ? 0 1 o (11) P it ? Pjt ? ? ? ? jit = e jit + v jit ? 0 1 where ujit and vjit are
8921196	18613	that transfer costs are unobservable but known to be related to o an (possibly biased) observable transfer cost estimate ? jit . Then, the unobservable transfer costs can be modeled as: 13so (7) TC jit = ? 0 + ?1? jit + e jit o where ? jit is the observable transfer cost estimate, ?0 and ?1 are unknown parameters and ejit is a random shock. 5 o The ? jit is also given as: o (8) ? = ? FR +
8921196	18613	& Jimma 26(100) -- 35(100) -- 61(100) -- Addis & Mekele 18(69) 24(92) 4(11) 23(66) 22(36) 47(77) Addis & Wollega 25(96) -- 35(100) -- 60(98) -- Dire Dawa & Nazareth 4(15) 26(100) 0(0) 35(100) 4(7) 61(100) Dire Dawa & Shashamane 23(89) 10(39) 35(100) 34(97) 58(95) 44(72) 43sTable 4.2 Conditional Maximum Likelihood Estimates of EPBM for the Maize Markets (1996:08 to 2002:08) EPBM Parameters
8921196	18615	direction of trade flows, we can redefine the conditions for regimes one, two and three given in equations (4), (5) and (6), respectively, as follows: o (9) P it ? Pjt ? ? ? ? jit = e jit ? 0 1 o (10) Pit ? Pjt ? ? ? ? jit = e jit ? u jit ? 0 1 o (11) P it ? Pjt ? ? ? ? jit = e jit + v jit ? 0 1 where ujit and vjit are non-negatively valued random variables that measure the deviation (if any)
8921196	18617	e v u ? v ?? ? ? e ? The likelihood function for ?t based on the joint probability density functions defined above for the different trade regimes over the entire study period is given as: T ? t= 1 (16) L =  ? . 1 1t 2 2t 1 The parameters can be obtained by maximizing the logarithm of the above likelihood function using numerical optimization. However, this is the
8921196	18620	should add up to one over the entire trade regimes, which requires the impositions of the following restrictions during the estimation procedure: (20) 0 ? ? 1 ? i (21) 0 ? + ? ? 1 ? i i 20s? i (22) ? = 1 ? i (23) ? = 0 In general, the EPBM represents an improvement over the standard PBM in that it allows tracing of the time path and a statistical test of structural change in spatial market
8921196	18625	& Dire Dawa 15(58) 26(100) 13(37) 35(100) 28(46) 61(100) Addis & Hosanna -- 26(100) -- 35(100) -- 61(100) Addis & Jimma 26(100) -- 35(100) -- 61(100) -- Addis & Mekele 18(69) 24(92) 4(11) 23(66) 22(36) 47(77) Addis & Wollega 25(96) -- 35(100) -- 60(98) -- Dire Dawa & Nazareth 4(15) 26(100) 0(0) 35(100) 4(7) 61(100) Dire Dawa & Shashamane 23(89) 10(39) 35(100) 34(97) 58(95) 44(72) 43sTable 4.2
8921196	18626	after the policy change is 26 and 35, respectively and figures in parenthesis are percentages of months with trade flows. * 26(100) 35(100) 31(89) 57(93) 57(93) Addis & Dire Dawa 15(58) 26(100) 13(37) 35(100) 28(46) 61(100) Addis & Hosanna -- 26(100) -- 35(100) -- 61(100) Addis & Jimma 26(100) -- 35(100) -- 61(100) -- Addis & Mekele 18(69) 24(92) 4(11) 23(66) 22(36) 47(77) Addis & Wollega 25(96)
691333	18647	thus in the equations of this Section, the track index t will be omitted. Gating is initially done for each IMM model separately, then the results are combined into a single validation gate . The predicted measurement and innovation covariance matrix for each model is ˆzk(?) = Hk(?)ˆx k|k?1(?); Sk(?) = Hk(?)P k|k?1(?)Hk(?) T + Rk(?). A validation gate is constructed around the
691333	18647	and P ? k|k?1 (?) is the corrected state prediction error covariance matrix. For a gating probability PG > 0.99 this is approximately equal to the state prediction error covariance matrix Pk|k?1(?). IMM Step 3: output combination estimates the state of the tracker by combining the state estimates for each model. The result is used as an output of the tracking filter at time k. ˆx k|k = P k|k
17622	17626	intrusions) and their costs (attention, time spent using the system, resources involved, etc.). These trade-offs can be formalized - see also  - and then applied to specific systems, as in . Here I just highlight some economic implications of the protocol presented in this paper. Firstly, the user has to spend more attention in the process, following its steps, and taking, for highest
17622	17626	(some only “exchanging” receipts once, some others exchanging them repeatedly), or using different, less secure, and more automated systems altogether. Similar conclusions are reached in  for traditional MIX-net systems. When the privacy concerns of individuals are distributed with enough variation, systems like the one proposed here might generate equilibria where users with the
17622	17627	strands in the cryptographic literature: MIX-nets (), but also onion routing (), and crowds (); ACID properties and privacy in ecommerce (e.g. ); as well as ANDOS protocols (e.g., ), group signatures , and the cocaine auction protocol (). While detailed properties, current known vulnerabilities, and extensions of the protocol in specific embodiments are discussed at
17622	17628	in this paper each user can take active steps to protect her privacy also in environments where other parties cannot be trusted. Furthermore, the protocol satisfies the ACID properties (see ) and therefore achieves reliability. The rest of this paper discusses the general idea behind this MIX-net variation by presenting first an informal description (Section 2) and then a formal, but
17622	17628	here therefore is related to several strands in the cryptographic literature: MIX-nets (), but also onion routing (), and crowds (); ACID properties and privacy in ecommerce (e.g. ); as well as ANDOS protocols (e.g., ), group signatures , and the cocaine auction protocol (). While detailed properties, current known vulnerabilities, and extensions of the protocol in
17622	17628	used in various applications (included anonymous messaging), it is most suited to areas where ACID properties and robustness 3 ACID stands for atomicity, consistency, isolation, and durability. See . 4 After the first round, the MIX third party knows that the customer must have received one of the sets of newly created receipts. The user can therefore repeat steps 3 to 7 n times, clouding
17622	6486	various possible uses of the protocol. Being “in control” comes at a cost, however, and the paper discusses the trade-offs arising from the proposed approach. 1 Introduction In 1981, David Chaum () introduced the concept of “MIX” - a third party that combines and forwards messages from several senders to several recipients, so that no relation between any particular sender and any particular
17622	6486	acts as a MIX, intermediating between parties who do not know or trust each other. The protocol presented here therefore is related to several strands in the cryptographic literature: MIX-nets (), but also onion routing (), and crowds (); ACID properties and privacy in ecommerce (e.g. ); as well as ANDOS protocols (e.g., ), group signatures , and the cocaine auction
17622	17630	and recipients even more: each message may go through several MIXes before reaching its final destination. Trust and reliability, however, are open issues in the MIX-net literature (see  and ). In the single MIX version of the protocol, the third party must be trusted not to reveal the correspondence between senders and recipients. In the cascade version of the protocol, collusion among
17622	17631	senders, and recipients even more: each message may go through several MIXes before reaching its final destination. Trust and reliability, however, are open issues in the MIX-net literature (see  and ). In the single MIX version of the protocol, the third party must be trusted not to reveal the correspondence between senders and recipients. In the cascade version of the protocol,
17622	930	can be externally observed. The MIX approach has been applied to “untraceable” digital pseudonyms (as discussed already in ), synchronous and asynchronous communication systems (see , , , and ), as well as electronic voting (see e.g. ). These applications rely not just on one MIX but on cascades of multiple MIXes forming a “MIX-net”. The MIX-net clouds the relation between
17622	930	between parties who do not know or trust each other. The protocol presented here therefore is related to several strands in the cryptographic literature: MIX-nets (), but also onion routing (), and crowds (); ACID properties and privacy in ecommerce (e.g. ); as well as ANDOS protocols (e.g., ), group signatures , and the cocaine auction protocol (). While detailed
17622	17632	recipient can be externally observed. The MIX approach has been applied to “untraceable” digital pseudonyms (as discussed already in ), synchronous and asynchronous communication systems (see , , , and ), as well as electronic voting (see e.g. ). These applications rely not just on one MIX but on cascades of multiple MIXes forming a “MIX-net”. The MIX-net clouds the
17622	17634	can be externally observed. The MIX approach has been applied to “untraceable” digital pseudonyms (as discussed already in ), synchronous and asynchronous communication systems (see , , , and ), as well as electronic voting (see e.g. ). These applications rely not just on one MIX but on cascades of multiple MIXes forming a “MIX-net”. The MIX-net clouds the relation
17622	17636	who do not know or trust each other. The protocol presented here therefore is related to several strands in the cryptographic literature: MIX-nets (), but also onion routing (), and crowds (); ACID properties and privacy in ecommerce (e.g. ); as well as ANDOS protocols (e.g., ), group signatures , and the cocaine auction protocol (). While detailed properties, current
17622	17637	also onion routing (), and crowds (); ACID properties and privacy in ecommerce (e.g. ); as well as ANDOS protocols (e.g., ), group signatures , and the cocaine auction protocol (). While detailed properties, current known vulnerabilities, and extensions of the protocol in specific embodiments are discussed at length in  and , here I offer an overview of why the
17638	9318	10.0 3.2 4.4 3.0 2.1 1.5 Source Present study (also see Alderman and Behrman 1999) Present study (also see Behrman, Kohler, and Watkins 1999) Renne (1997) Present study (also see Maluccio 2000) Foster and Rosenzweig 1995 Smith and Thomas 1997 Thomas, Frankenberg, and Smith 1999sTable 2?Reported reasons for men’s and women’s attrition in Kenyan (KICS) survey Reason for attrition: Working, moved to, or visiting
17660	17663	following proposition, which gives a lower bound on the number of feasible solutions for a random subset of elements in A. This proposition is based on the technique known as probabilistic method . In this technique, some combinatorial configuration is shown to exist with certainty, given the probability distribution of all possible configurations. We investigate the distribution of nodes in
17660	17665	develop an exact formula for the expected value of LMST in a finite graph with uniformly distributed arc costs. For the Steiner tree problem, which is a NP-hard variant of the MST, Bollobás et al.  proved that with high probability the weight of the Steiner tree is (1 + O(1))(k ? 1)(log n ? log k)/n when k = O(n) and n ? ?, where n is the number of vertices in a complete graph with edge
17660	17668	4sOther work in this area includes studies of the minimum spanning tree , quadratic assignment problem (QAP)  and, most notably, studies of the linear assignment problem (LAP) . More general work can be found in , where an analysis of parameters for random graphs was performed by Lueker. A nice introduction to probability topics in combinatorial optimization is given
17660	17668	Ramakrishnan , with experiments over several very large dense LAPs solved with an interior point algorithm. The Mézard-Parisi conjecture has been further strengthened by Coppersmith and Sorkin , who claimed that the expected value of the optimum k-assignment, for a fixed matrix of size n × m, is given by ? i,j?0, i+j<k 1 (m ? i)(n ? j) . Coppersmith and Sorkin also presented proofs of
17660	17670	the problem increases is still in its early stages. Recently, however, we have made some progress on the determination of asymptotic behavior for the expected value of optimum solutions for the MAP , and on the average number of local minima, given the distribution of costs. This work is discussed in the next two sections. 3 Expected Value of Optimum Solutions for the MAP One of the main
17660	17670	distribution. In this section we present results for the cases in which costs are taken from the uniform, exponential and normal distributions. The results in this section were first presented in . 3.1 Preliminary Results Initially, let us consider an enumeration method for MAP solution costs known as the index tree representation. This type of representation was introduced by Pierskalla
17660	17670	We investigate the distribution of nodes in an index tree representation, and show that among a specified number of such nodes there is at least one feasible solution. Proposition 2 (). Using an index tree to represent the cost coefficients of the MAP, randomly select ? different nodes from each level of the tree and combine these nodes from each level into set L. Then, L is
17660	17670	we assume ? = n d?1 /n! d?1 n (without the ceiling operation). The importance of determining the expected value of the ?-th order statistic is described in the next proposition. Proposition 3 (). Let z? u = nE, where E is the expected value of the ?th order statistic for each level of the index tree representation of the MAP. Then, z? u is an upper bound to the mean optimal
17660	17670	to derive some upper bounds on the asymptotic optimum value for instances with assignment costs taken from the exponential or uniform distributions. This is shown in the next theorems. Theorem 1 (). Given a d-dimensional MAP with n elements in each dimension, if the n d cost coefficients are independent exponentially distributed random variables with mean ? > 0, then z ? ? 0 as n ? ? or d ?
17660	17679	? 1.3679. More recently this result was improved by Olin  to the tighter value of 1.51. Finally, the conjectures of Mézard and Parisi have been solved in recent papers by Linusson and Wästlund  and Nair et al. . Their results have also confirmed the conjecture of Coppersmith and Sorkin. Concerning the MAP, the study of asymptotic behavior when the size of the problem increases is
17660	17685	a3 ? A3. That is, one wants to find a set of triples forming a match such that the preferences of all elements are satisfied. The 3DM problem can be easily reduced to the MAP with dimension d = 3 . 3sSome special cases of the MAP are, however, known to the solvable in polynomial time. For example, the most well known special case of the MAP (when d = 2) is the linear assignment problem
17660	17694	each element of {X1, X2, . . . , Xn}. The classic result proved by  is lim n?? Ln ? n = ?, with probability one, for a finite constant ?. This becomes significant, as addressed by Steele , because it is the key to Karp’s algorithm  for solving the TSP. Karp uses a cellular dissection algorithm for approximating the solution of the TSP. His result may be summarized by saying that
17660	17696	solutions when n is large. This points to the idea of using asymptotic techniques to develop effective solution algorithms. 4sOther work in this area includes studies of the minimum spanning tree , quadratic assignment problem (QAP)  and, most notably, studies of the linear assignment problem (LAP) . More general work can be found in , where an
17660	17696	length cij be an independent random variable drawn from the uniform distribution on , Frieze  showed that E ? ?(3) = ?? k=1 1 = 1.202 · · · as n ? ?. k3 This result was followed by , where the Tutte polynomial for a connected graph is used to develop an exact formula for the expected value of LMST in a finite graph with uniformly distributed arc costs. For the Steiner tree
17698	17733	less likely to look for new bugs. Instead, they can rerun lots of already-run regression tests—tests that the program has passed time and again and will probably pass time and again in the future.  Later in the project, testers can spent lots of time writing status reports, customer support manuals, and other documents that offer value to the company—but not bugs. Programmers and project
17742	17745	differences derive from the context in which queries are executed. The aim of the current proposal is essentially the same as that of the developers of systems such as Garlic  and Kleisli , i.e., to support declarative query formulation over distributed data stores and analysis tools. However, the development of service-based Grids provides certain opportunities for the developers of
17742	17746	development is relevant to both GSs and WSs. For example, in the Grid setting, applications can use GSs through toolkits, or Grid-enabled versions of parallel programming libraries such as MPI . In the WS setting, tools exist to support the generation of client stubs (e.g., AXIS ), but, more ambitiously, XML-based workflow languages have been developed to orchestrate WSs,
17742	17746	proposal from the authors . Polar* differs from the approach presented in this paper in that it is not service-based; in Polar*, Grid middleware is accessed using a Grid-enabled version of MPI . The absence of the service-based context in Polar* means that connection to external databases and computational services is much less seamless than in the OGSA setting. In the Web Services
17742	5714	from dynamic access to computational resources on the Grid. 1 Introduction The Grid is an emerging infrastructure that supports the discovery, access and use of distributed computational resources . Its name comes by analogy with the electrical power grid, in that the intention is that computational resources (by analogy with power generators) should be able to be accessed on demand, with the
17742	5714	the networking infrastructure now available, of course, and, more imGS portantly, the emergence, in both ebusiness and e-science, of a cooperNSnk ation model referred to as a virtual organisation . The service-based NSrc approach seems to many a good soRegistry lution to the problem of modelling a virtual organisation as a distributed system. GSs build upon and extend the service-oriented
17742	17748	of Grid applications means that services to support coordinated use of Grid resources are important, and considerable attention has been given to functionalities for managing data derivation (e.g., ) and replication (e.g., ). However, such higher-level Grid data management functionalities are still targeted principally at file-based data, and the only previous work on distributed query
17742	7746	resources are made shareable, and thus can be deployed flexibly to support changing user needs and system loads. How does the work presented here compare with other work on DQP, as surveyed in ? The principal differences derive from the context in which queries are executed. The aim of the current proposal is essentially the same as that of the developers of systems such as Garlic
17742	17751	been quick to integrate Web Service and data management products (e.g. ). However, we know of only one previous proposal for querying over collections of Web Services, viz. that of SkyQuery , which applies the classical wrapper-mediator architecture in a service-based setting. SkyQuery deploys WSs at each database store for handling metadata, performing queries, and cross matching
17742	17752	needed. The OGSI is currently undergoing a standardisation process through the Global Grid Forum . Although the initial emphasis in Grid computing was on file-based data storage , the importance of structured data management to typical Grid applications is becoming widely recognised, and several proposals have been made for the development of Grid-enabled database services
7664113	18797	means to better allign business and IT. The framework distinguishes five viewtypes: business, work, information, application and technology. RM-ODP (Reference Model of Open Distributed Processing) : provides a framework which identifies five viewtypes: Enterprise, Information, Computational, Engineering and Technology. The Model-Driven Architecture which is currently being developed by the
7664113	2422	Computational, Engineering and Technology. The Model-Driven Architecture which is currently being developed by the OMG (Open-Management Group) is inspired by the RM-ODP. Kruchten’s 4+1 framework : this framework originates from the domain of object-oriented modelling, and UML in particular. The framework identifies four perspectives on a system: Logical, Process, Development and Physical.
7664113	17763	on a system: Logical, Process, Development and Physical. This four perspectives are tied together by means of an integrating perspective (the “+1”): scenario’s. Soni, Nord and Hofmeister : this framework resembles the Kruchten framework, and identifies the following viewtypes: Conceptual, Module, Execution and Code. The is no fifth integrating viewtype. 6 ARCHITECTURE-DRIVEN In
19575	17771	contemporary projects, a chief item in the process of constructing, formulating and communicating about requirements is some sort of business domain model. The absence of explicit and well managed  business domain models 4 plays an important part in the failure of a significant number of system development projects. Given that business domain models are central in stakeholder communication,
19575	17771	by a diagramming technique such as ORM diagrams , ER diagrams  or UML class diagrams .s4.3 The process of domain modeling In general, the goals underlying (business) domain modeling are : 1. articulate clear and concise meanings of business domain concepts and 2. achieve a shared understanding of the concepts among relevant stakeholders. Based on the results reported in , we
19575	17771	goal in mind, it is referred to as an explicit conceptualization process. The above mentioned stream of activities called concept specification is such an explicit conceptualization process. In  a reference model for conceptualization processes is provided. This reference model distinguishes five streams of activities or phases: Assess domain and acquire raw material: Domain modeling
19575	17777	a number of actors involved with the environment, and the complete set of concepts they use in discourse. Let AC be the set of actors in the universe and let CO be the set of concepts. Following , we define a concept as the combination of a form and a meaning. The community of actors involved in an environment of discourse is provided by: Community : ED ??(AC) The set of concepts used in an
19575	5620	sections, we will mainly elaborate on this aspect. The way of modeling used for domain modeling is likely to be prescribed by a diagramming technique such as ORM diagrams , ER diagrams  or UML class diagrams .s4.3 The process of domain modeling In general, the goals underlying (business) domain modeling are : 1. articulate clear and concise meanings of business domain
8921207	17792	scheme which affect performance. We will introduce an algorithm which drawsonthispreviousworkbutissimpler. 3 Proposed algorithm Our algorithm is derived from Random Mutation Hill Climbing (, page 9), adapted to MM. We refer to it as SHC (Stochastic Hill Climber). 1. We submit to the Code maker a random guess constructed with 4 genes that we call the “Current Favourite Guess” (CFG). 2.
4053644	17797	our background in developing testing and analysis techniques for object-oriented programs, along with a fault model, suggests to us that aspect-oriented programming introduces new types of faults . Fault models for AOPs must be based on the nature of faults and failures in AOPs and the unique characteristics that make testing AOPs challenging. 3.1 The nature of faults and failures in AOPs
4053644	17800	in one module to reach a corresponding use in another module or within the same module. Denaro and Monga use model checking to verify properties of aspects suitable for formal verification . They do not consider the environment in which aspects will be used, but instead rely on the verified properties holding through system evolution. They illustrate their approach by verifying
4053644	17803	2.2 Related Work In aspect-oriented programming , aspects capture functionality that cross-cuts code modules. Other research extends aspects and their use to the design level (e.g., . Research on testing software built via aspect-oriented programming is scarce. We conducted an informal survey of the Figure 3. Sample before and after advice 5 (C) Copyright 2004. All Rights
4053644	17805	2.2 Related Work In aspect-oriented programming , aspects capture functionality that cross-cuts code modules. Other research extends aspects and their use to the design level (e.g., . Research on testing software built via aspect-oriented programming is scarce. We conducted an informal survey of the Figure 3. Sample before and after advice 5 (C) Copyright 2004. All Rights
4053644	17808	aspect-oriented programming has the possibility to make significant changes to the semantics of a core concern, thus creating new testing challenges. 2.2 Related Work In aspect-oriented programming , aspects capture functionality that cross-cuts code modules. Other research extends aspects and their use to the design level (e.g., . Research on testing software built via
4053644	17810	approach by verifying concurrency concerns for a set of aspects. A similar approach is used by Ubayashi and Tamai . An approach based on three-valued model checking is presented by Li, et al. . Their approach allows for reasoning about features and interactions that occur as the result of composition (i.e. weaving). While one, of course, can use existing black-box and white box testing
4053644	17811	aspect-oriented programming has the possibility to make significant changes to the semantics of a core concern, thus creating new testing challenges. 2.2 Related Work In aspect-oriented programming , aspects capture functionality that cross-cuts code modules. Other research extends aspects and their use to the design level (e.g., . Research on testing software built via
4053644	692	Given that the precondition is ensured, the client can reasonably assume that the method’s postcondition will be satisfied. This is a basic requirement for behavioral inheritance (i.e. subtyping) . Clients expect method postconditions to be satisfied regardless of whether or not aspects are woven into the concern. Hence the behavioral contracts of the concern should hold after the weaving
4053644	17813	our background in developing testing and analysis techniques for object-oriented programs, along with a fault model, suggests to us that aspect-oriented programming introduces new types of faults . Fault models for AOPs must be based on the nature of faults and failures in AOPs and the unique characteristics that make testing AOPs challenging. 3.1 The nature of faults and failures in AOPs
4053644	17814	criteria on the woven target that was used to test the core concern. However, this is difficult since the weave process of many aspect-oriented programming languages weave directly to byte code . We could analyze the byte code, except that it will be difficult to refer back to the source code of the core concern and/or the aspects. Also, compiler optimizations increase the difficulty.
4053644	17815	2.2 Related Work In aspect-oriented programming , aspects capture functionality that cross-cuts code modules. Other research extends aspects and their use to the design level (e.g., . Research on testing software built via aspect-oriented programming is scarce. We conducted an informal survey of the Figure 3. Sample before and after advice 5 (C) Copyright 2004. All Rights
4053644	17817	woven advice and the statements that are executed after the woven join point determine whether a pattern strengthserror will introduce a fault. Infections, invalid state caused by a program fault , induced by woven advice could be subsequently canceled by coincidental correctness. 3.3.2 Incorrect aspect precedence The order in which advice from multiple aspects are woven into a concern
4053644	17818	testing AOPs. It appears that the majority of work being done is on using aspectoriented methods to build testing tools. Zhao proposes an approach based on data-flow testing for unit testing AOPs . This approach combines testing of individual aspects and classes that have the potential to be affected by one or more aspects. Definition-use pairs (du-pairs) are computed to determine what
17819	12036	and contexts during analysis while on the other hand they should also provide facilities for easing the task of profile and context generation. A typical approach is presented by Budzik and Hammond  who describe Information Management Assistants which observe users while they interact with everyday applications and then anticipate their information needs using a model of the task at hand. The
17819	17821	context models which roughly determine the area of work in order to dissolve linguistic ambiguities in text retrieval. Natural-language aspects play an important role in their work: The system  observes interactions with everyday applications like word processors and web browsers, and mainly tries to find out the linguistic context of occupation. This approach is focused on the personal
17819	17822	ontology defining which information carriers exist (e.g., all document types in a hierarchical structure) and a domain ontology, describing e.g. the typical contents of certain document types . Profiles denote in which parts of the organizational memory a specific user is interested in and also provide the necessary textual knowledge to derive new instances from texts. This information
17819	17823	so-called logical objects of a document, e.g. title, author, chapter, etc. Afterwards, the generated word hypotheses are validated by an errortolerant dictionary look-up. For more detail, see e.g., , , . 4. TEXT CATEGORIZATION For automated information delivery it is important to know the type of documents a specific user is interested in. The personal view of a user on a document is
17819	15117	these examples, new documents can be assigned to the respective categories automatically. Most learning approaches for text categorization rely on statistical techniques with linear classifiers . A very attractive alternative are approaches that learn rules relying on text patterns. Until now, only a few rule learners have been applied to text categorization (examples are SWAP-1  and
17819	17828	. A very attractive alternative are approaches that learn rules relying on text patterns. Until now, only a few rule learners have been applied to text categorization (examples are SWAP-1  and Ripper ). Nevertheless, rule learners offer some practical advantages: • they produce very compact classifiers, • which are easy to understand and to modify by humans (if, e.g., a manual
17819	2475	rates and the corresponding maximum number of days the payment should be done. As in text categorization it is very interesting to learn patterns capturing the substance automatically by examples . Examples in information extraction are documents together with the information to be extracted. In parallel to text categorization, extraction patterns can also be learned using a
17819	17831	SUBSEQUENT INFORMATION EXTRACTION We will now explain shortly how information supply works when process contexts can be used for the textual analysis of new documents. More details can be found in  and to the best of our knowledge, no other approaches exist within that area. In this scenario, the first task is to assign the document to the correct process (that means, choosing one out of a
8921227	17868	similarity metric have been designed, the set of retrieved images often fits the user’s needs only partly. Typically, different users may categorise images according to different semantic criteria . Thus, if we allow different users to mark the images retrieved with a given query as “relevant” or “non-relevant”, different subsets of images will be marked as “relevant”, and the intersection of
8921227	17876	response based on some &quot;feedback&quot; from the user is widely recognised. A number of techniques aimed at exploiting such relevance feedback have been proposed in the literature ,,,,,,,,,,,. As discussed in Section 2, they are based on the fact that the user does not know the actual distribution of images in the feature space, nor the feature space
8921227	17876	Techniques developed for text retrieval systems should be suitably adapted to content based image retrieval, on account of differences in both feature number and meaning, and in similarity measures ,. Basically, relevance feedback strategies are motivated by the observation that the user is unaware of the distribution of images in the feature space, nor of the feature space itself, nor of
8921227	17878	based on some &quot;feedback&quot; from the user is widely recognised. A number of techniques aimed at exploiting such relevance feedback have been proposed in the literature ,,,,,,,,,,,. As discussed in Section 2, they are based on the fact that the user does not know the actual distribution of images in the feature space, nor the feature space itself, nor the
8921227	17878	space by a logistic regression model so that relevant images represented in the new feature space exhibit higher similarity values . A probabilistic feature relevance scheme has been proposed in , where a weighted Euclidean distance is used. A different perspective has been followed in  where relevance feedback technique based on the Bayesian decision theory was first proposed. The
8921227	17878	as relevant, and all other images in the top twenty as non-relevant. This experimental set up affords an objective comparison among different methods and is currently used by many researchers , ,. Tables 1 and 2 report the results of the proposed method on the two selected datasets in terms of average percentage retrieval precision and Average Performance Improvement (API). Precision
8921227	17878	obtained with other methods recently described in the literature are also reported, namely the RFM (Relevance Feedback Method)  and the PFRL (Probabilistic Feature Relevance Learning) . PFRL is a probabilistic feature relevance feedback method aimed at weighting each feature according to the information extracted from the relevant images. Thissmethod uses the Euclidean metric to
8921227	17879	on some &quot;feedback&quot; from the user is widely recognised. A number of techniques aimed at exploiting such relevance feedback have been proposed in the literature ,,,,,,,,,,,. As discussed in Section 2, they are based on the fact that the user does not know the actual distribution of images in the feature space, nor the feature space itself, nor the
8921227	17879	developed for text retrieval systems should be suitably adapted to content based image retrieval, on account of differences in both feature number and meaning, and in similarity measures ,. Basically, relevance feedback strategies are motivated by the observation that the user is unaware of the distribution of images in the feature space, nor of the feature space itself, nor of the
8921227	17879	the one containing images that are relevant to the user. A query shifting technique for CBIR based on the well known Rocchio formula developed in the text retrieval field  has been proposed in . The estimation of probability densities of individual features for relevant and non relevant images is used in  to compute a new query. The new query is determined by randomly drawing
8921227	17879	UCI repository. The MIT database was collected by the MIT Media Lab (ftp://whitechapel.media.mit.edu/pub/VisTex). This database contains 40 texture images that have been processed as described in . Images have been manually classified into fifteen classes. Each of these images has been subdivided into sixteen non-overlapping images, obtaining a data set with 640 images. Sixteen Gabor filters
8921227	17879	and all other images in the top twenty as non-relevant. This experimental set up affords an objective comparison among different methods and is currently used by many researchers , ,. Tables 1 and 2 report the results of the proposed method on the two selected datasets in terms of average percentage retrieval precision and Average Performance Improvement (API). Precision is
8921227	17881	&quot;feedback&quot; from the user is widely recognised. A number of techniques aimed at exploiting such relevance feedback have been proposed in the literature ,,,,,,,,,,,. As discussed in Section 2, they are based on the fact that the user does not know the actual distribution of images in the feature space, nor the feature space itself, nor the similarity
8921227	17881	used to modify the weights of the combination to reflect different feature relevance. Santini and Jain also proposed a parametrized similarity measure updated according to feedback from the user . Rather than modifying the similarity metric, Frederix et al. proposed a transformation of the feature space by a logistic regression model so that relevant images represented in the new feature
8921227	15607	of image retrieval techniques based on image content, where the visual content of images is captured by extracting low-level features based on color, texture, shape, etc., have been developed ,. Contentbased queries are often expressed by visual examples in order to retrieve from the database all images that are “similar” to the examples. The retrieval process is usually performed by a
8921227	15607	small for databases related to tasks where the semantic description of the images is reasonably well defined. For example, data bases of lithographs, frontal views of faces, outdoor pictures, etc. . For this kind of databases, a pair of imagessthat the user judges as being similar to each other is often represented by two near points in the feature space. However, no matter how suitable for
8921227	15607	near points in the feature space. Let us also assume that the user wishes to retrieve images belonging to a specific class, that is, she/he is interested in performing a so-called “category” search . As different users have different perceptions of similarity depending on the goal they are pursuing, for a given query, different users may identify different subsets of relevant images. According
17887	17888	methodological and/or development framework for the specification and exploitation of roles during planning and execution of tasks. 4. Formal Models There are a number of recent formal models  that capture important properties of roles. In  roles are related to relationship types via the predicate RoleOf(a, R), which means that a is one of the roles in relationship of type R. The
17887	17888	and conditions on roles are not specified explicitly, therefore it is not clear how agents are (re)assigned to roles. This latter approach is very close and extends the way roles are used in . Each role in  describes a major function together with the obligations, interdictions and permissions attached to it. Roles are associated with obligations towards other roles. Each obligation
17887	17889	simulations  and the robocup simulation league . Furthermore, this section refers to frameworks for implementing cooperative agents following the role-oriented agentprogramming paradigm . 4.1.1 The Karma-Teamcore framework. The Karma-Teamcore framework focuses on rapidly integrating distributed, heterogeneous agents and tasking them by providing wrappers that encapsulate general
17887	17889	in size and structure, dynamically (re)assign agents to roles, decide on the number of agents that should fill a role and deliberate on the roles that should structure the group. ROPE framework  as well as the work reported in  aim to this target by the use and exploitation of roles for specifying cooperation processes. Both works give a strong emphasis on the role concept. In ROPE a
17887	17893	methodological and/or development framework for the specification and exploitation of roles during planning and execution of tasks. 4. Formal Models There are a number of recent formal models  that capture important properties of roles. In  roles are related to relationship types via the predicate RoleOf(a, R), which means that a is one of the roles in relationship of type R. The
17887	17893	in an inheritance hierarchy. The main restriction is that an obligation exists between two agents in specified roles. However, each role can be filled by a number of agents. Summarizing the above,  and  follow the same approach regarding the formalization of roles. Cavedon and Sonenberg  are interested in the adoption of goals as a result of adopting roles and in the degree of roles’
17887	17894	and interaction. 3.1.4 AALAADIN Model. AALAADIN is not a specific agent methodology, but a meta-model for describing organizations of agents using the core concepts of group, agent and role . The methodology is supported by the MadKit platform. With AALAADIN one can describe multi-agent systems with different forms of organizations such as market-like or hierarchical organizations and
17887	17895	. Coordination policies and the way they are implemented in a MAS, impact agents’ collaborative activity and communication. Generic models that have been devised such as the SharedPlans model , the Joint Intentions  and Joint Responsibility models , provide the principles that underpin social activity and reasoning, and describe the necessary constructs for defining
17887	17898	impact agents’ collaborative activity and communication. Generic models that have been devised such as the SharedPlans model , the Joint Intentions  and Joint Responsibility models , provide the principles that underpin social activity and reasoning, and describe the necessary constructs for defining cooperative and individual behavior of agents in social contexts. Implemented
17887	17898	To address these concerns, implemented systems, driven by the high-level cooperation models that they implement, employ constructs and methods such as the intentional context, common recipes , fixed organizations with discrete roles interchanged among agents , and dynamic assignment of agents to pre-specified roles in conjunction with plan monitoring and repair . The aim is to
17887	17899	and the way they are implemented in a MAS, impact agents’ collaborative activity and communication. Generic models that have been devised such as the SharedPlans model , the Joint Intentions  and Joint Responsibility models , provide the principles that underpin social activity and reasoning, and describe the necessary constructs for defining cooperative and individual behavior of
17887	17900	design, do not provide the abstractions needed for agents’ robust planning and execution, and do not enable agents to deliberate on roles assignment. 3.1.3 AAII Methodology. The AAII methodology  operates at two levels of abstraction, the external and the internal viewpoint. From the external 9sviewpoint the system is decomposed into agents, modelled as complex objects characterized by
17887	17904	of agent technology has been increased substantially. Recent multi-agent systems (MAS) are deploying in more and more complex environments, including the RoboCup-Rescue  and RoboSoccer  domains, multi-robot space explorations, battlefield simulations  and information integration . The complexity of the environment in which a multi-agent system is deployed and the
17887	17905	methodological and/or development framework for the specification and exploitation of roles during planning and execution of tasks. 4. Formal Models There are a number of recent formal models  that capture important properties of roles. In  roles are related to relationship types via the predicate RoleOf(a, R), which means that a is one of the roles in relationship of type R. The
17887	17905	and no model is oriented towards implementation. An implementation oriented advance model of collaborative decision making and practical reasoning incorporating roles is the one proposed in . The model proposed aims to cover all the aspects of the processes involved for agents to reach a joint decision. A critical aspect of the model proposed is the social mental shaping mechanism,
17887	17905	to find a way to jointly achieve a state ?. This implies that an agent deliberates in order to decide whether it shall play a role. Role properties as there are conceived in the model proposed in  are summarized in the following table: Property Value Specification A role is viewed as a set of mental attitudes. Attitudes are either mandatory or optional Assignment Dynamic, based on the
17887	17906	cooperation models that they implement, employ constructs and methods such as the intentional context, common recipes , fixed organizations with discrete roles interchanged among agents , and dynamic assignment of agents to pre-specified roles in conjunction with plan monitoring and repair . The aim is to provide the means for systems to track the mental state of individual
17887	17906	coherent fashion. Roles have been used in the most well-known team architectures, that of the CMUnited  that won the RoboCup world championships RoboCup98 and RoboCup99 and that of FC Portugal  the winner of the RoboCup2000 world championship. A role in robotic soccer can be as simple as a position in the field. In the CMUnited  architecture an agent has a set of internal and external
17887	17906	Agents can change formation at run-time. This results in changing the characteristics of roles at run-time. However, agents within a formation can also inter-change roles in order to save energy . An agent undertakes one role in each formation. The next table summarizes the role properties in these systems. Property Value Specification A role is a specification of agent’s position and
17887	17908	team. In order to fulfill this objective, the team players must act in a well-coordinated and coherent fashion. Roles have been used in the most well-known team architectures, that of the CMUnited  that won the RoboCup world championships RoboCup98 and RoboCup99 and that of FC Portugal  the winner of the RoboCup2000 world championship. A role in robotic soccer can be as simple as a
17887	17909	more and more complex environments, including the RoboCup-Rescue  and RoboSoccer  domains, multi-robot space explorations, battlefield simulations  and information integration . The complexity of the environment in which a multi-agent system is deployed and the complexity of the tasks that is expected to perform can be assessed by studying the task-environment in three
17887	17910	systems (MAS) are deploying in more and more complex environments, including the RoboCup-Rescue  and RoboSoccer  domains, multi-robot space explorations, battlefield simulations  and information integration . The complexity of the environment in which a multi-agent system is deployed and the complexity of the tasks that is expected to perform can be assessed by studying
17887	17910	in implemented multi-agent systems in order to achieve coherence in teams of cooperative agents in domains where well-coordinated activity is required. Such domains include battlefield simulations  and the robocup simulation league . Furthermore, this section refers to frameworks for implementing cooperative agents following the role-oriented agentprogramming paradigm . 4.1.1 The
17887	17911	the principles that underpin social activity and reasoning, and describe the necessary constructs for defining cooperative and individual behavior of agents in social contexts. Implemented systems , aim to make explicit the cooperation model upon which agents’ behavior is based. The objective is to provide flexibility towards solving problems related to  “how individual agents should
17887	17911	common recipes , fixed organizations with discrete roles interchanged among agents , and dynamic assignment of agents to pre-specified roles in conjunction with plan monitoring and repair . The aim is to provide the means for systems to track the mental state of individual agents participating in the cooperative activity in a coherent and integrated way. The objective of this paper
17887	17911	systems (MAS) are deploying in more and more complex environments, including the RoboCup-Rescue  and RoboSoccer  domains, multi-robot space explorations, battlefield simulations  and information integration . The complexity of the environment in which a multi-agent system is deployed and the complexity of the tasks that is expected to perform can be assessed by studying
17887	17913	on the properties of roles mentioned above. 3.1 Roles in AOSE Methodologies Agent oriented software engineering is central to realizing agent technology as a new software engineering paradigm . Many researchers dealing with the invention of new software development techniques suitable for MAS have conceived multi-agent systems as organizations of agents that interact to achieve common
17887	17913	is intended to allow an analyst to go systematically from a statement of requirements to a design that is sufficiently detailed, that it can be implemented directly . Gaia, as it is stated in , encourages developers to think of building agent-based systems as a process of organizational design. An organization is considered to be a collection of roles that stand to certain relationships
17887	17914	3.1.2 Gaia Methodology. Gaia is intended to allow an analyst to go systematically from a statement of requirements to a design that is sufficiently detailed, that it can be implemented directly . Gaia, as it is stated in , encourages developers to think of building agent-based systems as a process of organizational design. An organization is considered to be a collection of roles that
17915	1835	landscape around the minimum can cause them to be pulled apart during global relaxation. Finally, if the output surface model is to be reconstructed from the input views by some sort of averaging , misaligned features can become blurred. For all these reasons, we would like the final pose to be both correct and well-constrained. Several methods have been proposed for evaluating and improving
17915	17918	stability of the final pose between two meshes. Once a set of point-pairs has been selected, the presence of sliding can be detected by analyzing the covariance matrix used for error minimization . The chosen point set can then be altered to provide the best constraints for the final pose. Guehring  addresses the problem of maximizing stability of the transform by assigningsweights to
17915	17919	by as much as a millimeter, while those aligned by our algorithm (Figure 10 (b)) stayed together. A system for global registration of meshes that uses our sampling is presented in a companion paper . We also investigated the influence of noise on the performance of our sampling strategy. Since the algorithm prioritizes the points based on their influence of the covariance matrix, it is
17915	17919	more difficult than the pairwise step, since we have to consider how sliding of a single scan pair will affect the entire system. We also discuss this issue in more detail in a companion paper . Acknowledgements. This work was supported by the National Science Foundation Research Grant IIS-0113427. The datasets used in this work were provided by the Digital Michelangelo and Forma Urbis
17915	17920	depends on the size of the overlap region between the two meshes, the resolution of the mesh, and the magnitudesof noise in the input data. In our experience with the Forma Urbis Romae dataset , for meshes that overlap bys¢¡¢£ , the number of points necessary for the eigenvectors to stabilize is on the order of several hundred. A2 For each ¤¥¤ ? ? , we need to determine whether it belongs
17915	17920	sphere” meshes for uniform, normal space and covariance sampling We have also applied our algorithm to real scan data. Figure 9(a) shows the sampling of two scans from the Forma Urbis Romae dataset . Similar to the “incised plane” example, these meshes exhibit translational sliding in the plane and rotational sliding around the vector perpendicular to the plane of the meshes. Most of the
17915	17922	distance metric. The point-to-plane error metric of Chen and Medioni  makes the ICP algorithm less susceptible to local minima than the point-to-point metric of Besl . Pottman and Hofer  show that if the two meshes are close to each other, the point-to-plane distance is the best approximation for the true distance between the two surfaces. This metric also has an advantage that it
17915	17923	We will call geometry that does not have enough constraints for good convergence “unstable.” In most 3D scanning systems, pairwise registration is usually followed by a global relaxation algorithm , which spreads the accumulated alignment error over a set of views. Since a single mesh usually has several partners in this set, poor pose for one mesh can easily be propagated to its partners.
17915	17923	to correctly align the vertical grooves (Figure 9(b)). We have performed some initial experiments with using the output of geometrically stable ICP in the global relaxation algorithm of Pulli  using the Forma Urbis Romae dataset. The results seem to suggest that scans that are aligned pairwise using our sampling strategy “hold together” better than those aligned using uniform sampling.
17915	17923	(a) (b) Figure 10: A visualization of residual error in the overlap portion of the pair of scans in Figure 9 after they and their partners have been processed by Pulli’s global registration . Meshes in (a) were aligned using uniform sampling. Meshes in (b) using our geometrically stable algorithm. Error is in mm, black corresponds to 0, white to 1. The maximum error in (a) is over 1
17915	17924	strategy and the choice of error metric to be minimized play a large role in both the rate of convergence and the accuracy of the resulting pose. A discussion of these issues can be found in . Poor alignment between a pair of meshes can come from several sources. Noise in the input data can cause ICP to Leslie Ikemoto Stanford University leslie@cs.stanford.edu Marc Levoy Stanford
17915	17924	by drawing a new set of sample points primarily from stable, i.e. “lock and key”, areas of the input meshes. This technique extends the normal space sampling proposed by Rusinkiewicz and Levoy . Unlike , our approach deals with both translational and rotational uncertainties in registration. 2. Geometric Stability of ICP In this section we describe a method based on 6x6 covariance
17915	17924	transformations which have small associated error change under the uniform sampling model. The two techniques that are the most similar to our approach are those of Simon  and Rusinkiewicz . Simon developed several hill climbing algorithms for selecting a set of points on one of the input meshes that has the best potential for constraining all transformations when another mesh is
17915	17924	each figure, the number and types of the instabilities are noted. A helix, which has one unstable screw motion, is missing, but helical shapes are not likely to arise in scanned data. Rusinkiewicz  proposed a technique called normalspace sampling that is aimed at constraining translational sliding of the input meshes. When drawing samples from a mesh, the algorithm tries to ensure that the
17915	17924	The first test case is two planar patches with two grooves forming an X (Figure 4). Each patch has independently added Gaussian noise. This test case is similar to the one used by Rusinkiewicz  for normal-space sampling. Figure 5 shows the convergence rates for aligning these patches using uniform sampling, normal-space sampling, and our covariance-based sampling. Both normal-space and
17915	17926	stability of the final pose between two meshes. Once a set of point-pairs has been selected, the presence of sliding can be detected by analyzing the covariance matrix used for error minimization . The chosen point set can then be altered to provide the best constraints for the final pose. Guehring  addresses the problem of maximizing stability of the transform by assigningsweights to
17915	17927	the noise by smoothing the meshes. This can have an undesirable side effect of smoothing away the features that provide the valid constraints. We can try to use other constraints, such as color . We can also add more points to be used for minimization of Equation 2. Just adding more points will not improve convergence, since they are as likely to come from the flat areas as from the parts
8921232	17932	Profile based retrieval refers to the ability of an information portal to aid users in effectively finding relevant resources, while taking their specific interests, defaults and needs into account . A pivotal role is played by what the searcher’s profile. It is this profile that enabled an information retrieval system to better tune its behaviour to the needs of the searcher. Some examples of
8921232	17933	of transfer protocols or storage formats. Currently available meta-data standards, standards for defining the structure of information resources and resources description frameworks, such as , provide a good starting point. However, in this research project we would like to go beyond these frameworks, aiming to provide more richer functionality with respect to dealing with heterogeneity
8921232	18981	development more relevant than a book on linear software development. In the proposed research, we will use knowledge representation languages and frameworks such as KQML , OIL  and DAML  from the DARPA as a base to develop a definition language for characterisation domains. The very existence of projects such projects also underlines the fact that RDF is not rich enough (yet) to
8921232	18982	on evolutionary software development more relevant than a book on linear software development. In the proposed research, we will use knowledge representation languages and frameworks such as KQML , OIL  and DAML  from the DARPA as a base to develop a definition language for characterisation domains. The very existence of projects such projects also underlines the fact that RDF is not
8921232	17934	as reported in  provides a semantic base to start from. This semantic base has been developed in terms of the concept of infons as used in Situation Theory  and logic. As reported in e.g. , logic is playing an increasingly important role to more fundamentally define the reasoning mechanisms driving information retrieval. For a profiling mechanism to be useful, it is imperative that
8921232	17935	Uniform resource access The availability of numerous information resources via the Internet, brings about a natural expectation of being able to search and access these resources in a uniform way . The present situation, however, is quite different. Users need to use different systems to find the resources they need. This requires the introduction of a mechanism by which highly heterogenous
8921232	17936	a fundamental understanding of the information retrieval problem and its relation to the searcher needs is required. The research, as conducted by one of the project proposers, as reported in  provides a semantic base to start from. This semantic base has been developed in terms of the concept of infons as used in Situation Theory  and logic. As reported in e.g. , logic is
8921232	18987	of transfer protocols or storage formats. Currently available meta-data standards, standards for defining the structure of information resources and resources description frameworks, such as , provide a good starting point. However, in this research project we would like to go beyond these frameworks, aiming to provide more richer functionality with respect to dealing with heterogeneity
8921232	17938	Profile based retrieval refers to the ability of an information portal to aid users in effectively finding relevant resources, while taking their specific interests, defaults and needs into account . A pivotal role is played by what the searcher’s profile. It is this profile that enabled an information retrieval system to better tune its behaviour to the needs of the searcher. Some examples of
8921232	17938	are needed to define and/or derive profiles. A user specific profile may, for example, be based on the searcher’s implicit or explicit feedback when using the information retrieval system . A role, or task, based profile may be defined in conjunction with the definition of the workflow or business process to which the role/task is associated. Alternatively, using data mining
8921234	17940	in Bayesian networks), by using a (?)log transformation. Another related systems is generated by setting A = ?IR ? {?}, ?, 0, min, +?. The GAI utility decomposition of (Bacchus and Grove, 95)  can then be expressed as a multiset of such Avaluations. Semiring-based CSPs. This elegant framework  generalises soft constraints formalisms such as weighted CSPs, fuzzy CSPs, probabilistic
8921234	17941	have many fewer non-zero tuples; the efficiency of the combination operation is related to the number of non-zero tuples in the input valuations. This idea is related to the notion of shrinking in . 5 Use of Upper and Lower Bounds The semiring relation ?A enables us to define a relation on semiring valuations. We define relation ? on A-valuations, by ? ? ? if ? and ? have the same scope U and
8921234	17941	which are not in P by 0. With appropriate semiring and choice of P , it can be shown that this does not affect the answer to certain kinds of queries. This is related to the notion of sinking in . i=2sConsider ?? = ?1 ? · · · ? ?k. Define M = {?i(ui) : i = 1, . . . , k, ui ? V?i } to be the set of all semiring values taken by any of the input valuations. Define M ? to be closure of M under
8921234	17942	we use the above proposition to generate an upper approximation of each message. The above algorithm generalises the upper bound obtained with the mini-buckets approach of (Dechter and Rish, 2003)  for the case of computing MPE (most probable explanation). Computing upper approximations has an added benefit in situations where ? is idempotent, as a pre-processing step. We can compute upper
8921234	17942	is applied to belief updating for a Bayesian network, based on semiring A = ?IR + , 0, 1, +, ??, this approximation then corresponds to the mini-buckets upper and lower bounds for belief updating . Setting some semiring values to 0 The element 0 is a lower bound for every other element a in the semiring, since 0 ? a = a. So a particular case of a lower bound is when we replace certain
8921234	2004	?. The semiring order ? is then a total order. We will consider P = Pa = {b ? A : b ? a}. Given the input semiring values are all bounded above by 1 (as is always the case for valuations structures ), condition (?) then holds. So if ?(v) ? P then ? ? (v) ? P . Furthermore, if ? ?T (t) is in P then there exists some v ? V extending t with ?(v) ? P . Hence ? ? (v) ? P , and so (? ? ) ?T (t) ? P
17947	10227	since situations where robots duplicate the effort of other robots can be avoided. An example of this is sharing map information so multiple robots do not explore the same area of an environment . But robots can only exchange information when robots they can communicate with each other. Maintaining wireless communication among a team of robots moving through an unknown environment can be a
17947	10227	One of the challenges faced in this task is to maintain communication between members of a team. Exploration tasks can be completed more efficiently when robots share information with each other . Communicating robots are also better able to address tasks requiring explicit coordination such as cooperative transport , where two or more robots must cooperate to move an object that
17947	17948	future. Previous work often relies on conservative coordination methods that successfully keep robots in communication with each other, but can over constrain the relative movement of the robots . These methods rely on maintaining a clear line of sight between communicating robots, which means that a line drawn between the two robots cannot intersect any other object. Since wireless
17947	17948	of connectivity may be acceptable given the increase in task performance. II. RELATED WORK The use of robot teams to explore an initially unknown environment has been the subject of much work . One of the challenges faced in this task is to maintain communication between members of a team. Exploration tasks can be completed more efficiently when robots share information with each other
17947	17948	that cannot be moved by a single robot. The problem of maintaining wireless communication among a team of robots has been addressed by constraining robots to be within “line of sight” of each other . 1sThis method of coordination is very successful at maintaining communication between robots but does not allow the robots to take advantage of the fact that wireless signals (such as those used
17947	17948	method is required to determine which pairs of robots must maintain direct communication with each other. Much work in multi-robot coordination uses leader-follower relationships between robots  to coordinate teams of robots. We will use leader-follower relationships to determine which pairs of robots must maintain wireless links to each other. In exploration and formation keeping tasks, a
17947	17948	leader. Previous work on maintaining communication in a team of mobile robots exploits leader-follower relationships to determine which robots need to maintain a clear line of sight to one another . Wagner and Arkin  propose an approach combining planning and reactive behavior to maintain communication in a team of robots performing a reconnaissance task. In this approach, they use plans
17947	495	of connectivity may be acceptable given the increase in task performance. II. RELATED WORK The use of robot teams to explore an initially unknown environment has been the subject of much work . One of the challenges faced in this task is to maintain communication between members of a team. Exploration tasks can be completed more efficiently when robots share information with each other
17947	17950	of connectivity may be acceptable given the increase in task performance. II. RELATED WORK The use of robot teams to explore an initially unknown environment has been the subject of much work . One of the challenges faced in this task is to maintain communication between members of a team. Exploration tasks can be completed more efficiently when robots share information with each other
17947	17951	of connectivity may be acceptable given the increase in task performance. II. RELATED WORK The use of robot teams to explore an initially unknown environment has been the subject of much work . One of the challenges faced in this task is to maintain communication between members of a team. Exploration tasks can be completed more efficiently when robots share information with each other
17947	17954	that cannot be moved by a single robot. The problem of maintaining wireless communication among a team of robots has been addressed by constraining robots to be within “line of sight” of each other . 1sThis method of coordination is very successful at maintaining communication between robots but does not allow the robots to take advantage of the fact that wireless signals (such as those used
17947	17954	schemes. Hand designed plans allow for sophisticated strategies, but require a priori map knowledge, and thus are not suitable for exploration tasks in unknown environments. Powers and Balch  describe a method called ValueBased Communication Preservation for moving a team of robots to a goal location while maintaining communication among the team members. Control decisions are based
17947	17956	method is required to determine which pairs of robots must maintain direct communication with each other. Much work in multi-robot coordination uses leader-follower relationships between robots  to coordinate teams of robots. We will use leader-follower relationships to determine which pairs of robots must maintain wireless links to each other. In exploration and formation keeping tasks, a
17947	17958	method is required to determine which pairs of robots must maintain direct communication with each other. Much work in multi-robot coordination uses leader-follower relationships between robots  to coordinate teams of robots. We will use leader-follower relationships to determine which pairs of robots must maintain wireless links to each other. In exploration and formation keeping tasks, a
17947	17961	of the artificial potential generated by a harmonic function results in the minimum hitting probability path to a goal location. Harmonic functions are resolution complete and free of local minima . In this work, a robot’s configuration consists of its coordinates in a planar world. For the purposes of computing harmonic functions, we will represent configuration space as a discrete grid
17947	17961	relaxation is used to compute the potentials at each grid square, and bilinear interpolation is then used to compute the gradient at the robot’s location. For more details see Connolly and Grupen . Due to issues with numerical precision, in rare cases the gradient of a harmonic function cannot be determined in some portions of the configuration space. This can occur for regions of space that
17947	17962	of the artificial potential generated by a harmonic function results in the minimum hitting probability path to a goal location. Harmonic functions are resolution complete and free of local minima . In this work, a robot’s configuration consists of its coordinates in a planar world. For the purposes of computing harmonic functions, we will represent configuration space as a discrete grid
17947	17963	harmonic function cannot be determined, ? is determined using the NF1 function. Our effectors always consist of single robots. The controllers used are similar to those described by Sweeney, et al. . Robot r uses the controller ? EXPr r for exploration. The sensory abstraction EXPr marks all unobserved grid squares as goal, all observed grid squares containing obstacles as obstacle, and all
17947	17964	of sight of robot i; if robot i is stationary, robot j is guaranteed to reach this region of space. D. Coordination Methods We will describe coordination methods in terms of null space compositions . Null space compositions use the 5s“subject to” (?) operator to compose controllers that descend artificial potentials. For controllers ?? and ??, ?? ? ?? (read “?? subject to ??”) means that the
17947	17965	of sight of robot i; if robot i is stationary, robot j is guaranteed to reach this region of space. D. Coordination Methods We will describe coordination methods in terms of null space compositions . Null space compositions use the 5s“subject to” (?) operator to compose controllers that descend artificial potentials. For controllers ?? and ??, ?? ? ?? (read “?? subject to ??”) means that the
17947	17966	On the other hand, Figure 11 shows that in the dense environment the network connectivity is effected significantly by the value of ?. This 2 To avoid problems arising from multiple comparisons , the statistical significance of the difference between the low point on the curve and the right most point on the curve is confirmed in an additional 25 trials that are independent of the 25
17967	17968	from an RDF repository. As real life case applications, web exhibitions generated from museum collection metadata are presented. 1. Two Views of the Semantic Web The notion of the Semantic Web 1  has two interpretations. From the machine’s viewpoint, the Semantic Web manifests itself as a distributed source of interpretable metadata concerning resources, such as web pages 2 , documents,
17967	17970	both the dynamic and static approaches have their own virtues and application possibilities. 5.2. Related Work Logic and dynamic link creation on the semantic web have been discussed, e.g., in . Our approach is different in it’s use of HTML templates and Prolog for describing the static HTML output. In the RDF Twig tool 11 the RDF to HTML transformation is based on XSLT. A problem here is
17967	17971	nature of logic programming. By using generic rules it is possible, in principle, to create tag definitions that will apply to any RDF repository. In contrast to view-based search systems, such as , the views are projected from the RDF(S) ontologies. The main benefit is that arbitrary mappings between view categories and data resources can be flexibly defined. The system infers the mapping
17967	17973	The archive contained 629 photographs about the promotion ceremonies of the University of Helsinki. The content of the archive was transformed into RDF(S) format in an other application project  and was used as it is by SWeHG. The domain knowledge consists of six ontologies with 329 promotion-related concept classes, such as “Person” and “Building”, 125 properties, and 2890 instances, such
17967	17974	its own. For example, artifact, material, and technique ontologies have been defined based on the Finnish MASA Thesaurus  of keywords used in several museums for indexing data. The ontology MAO  created based on MASA contains some 6600 classes organized in a taxonomy. There is also a location ontology that defines geographical concepts such as “country” and “town”. Their instances are
17967	17974	bottles. Before making a selection, the user’s guide was shown in the same frame. The classified index (“Hakemisto aiheittain”) is based on the RDFS taxonomy of the underlying cultural MAO ontology  that was used when creating the collection metadata. When selecting a concept, the rightmost frame shows links to its subconcepts together with links to RPages whose objects are directly related to
17967	17975	the web can be rendered to the human end-user as a searchable and browsable HTML web site or space. In this paper we present a new approach and tool named “Semantic Web HTML Generator” (SWeHG)  to address this problem (cf. figure 1). The idea is to specify the structure and the layout of an HTML web site in terms of a set of HTML templates using a tag language. The templates can be used
17967	17976	1) Resource pages (RPage) depict selected resources with their metadata. 2) Index pages (IPage) classify RPages along conceptual hierarchical classifications, that will be called facets or views . By using IPages, RPages can be found along different facets. 3) A home page (HPage) defines the entrance page to the HTML repository. 3. Specifying the Transformation Figure 3 depicts the RDF to
17967	17976	nature of logic programming. By using generic rules it is possible, in principle, to create tag definitions that will apply to any RDF repository. In contrast to view-based search systems, such as , the views are projected from the RDF(S) ontologies. The main benefit is that arbitrary mappings between view categories and data resources can be flexibly defined. The system infers the mapping
691958	18007	a common dominant plane. They tracked objects moving in this plane and from their trajectories they estimated the external parameters of the cameras in one coordinate system. Baker and Aloimonos  proposed a calibration method for a multi-camera network which requires a planar pattern with a precise grid. We propose a fully automatic calibration method which yields complete camera projection
691958	18008	environments with a varying number and quality of cameras used. 1 Introduction With decreasing prices of powerful computers and cameras, smart multi-camera systems have started to emerge . A complete multi-camera calibration is the inevitable step 1stowards the efficient use of such systems even though many things can be accomplished with uncalibrated cameras in virtual environments
691958	9693	environments with a varying number and quality of cameras used. 1 Introduction With decreasing prices of powerful computers and cameras, smart multi-camera systems have started to emerge . A complete multi-camera calibration is the inevitable step 1stowards the efficient use of such systems even though many things can be accomplished with uncalibrated cameras in virtual environments
691958	18010	in virtual environments and telepresence setups. To our best knowledge, no fully automatic calibration method, for multi-camera environments, exists. Very recent multi-camera environments  or , which are primarily designed for realtime 3D acquisition, use advanced calibration methods based on a moving plate . These calibration methods do not require a 3D calibration object with
691958	18011	of the possible working volume thus making the estimation unstable. Occlusions and very different, or even disjoint, fields of view were common problems when using the mobile version of our ViRoom  system. Calibration based only on the points visible in all cameras would be virtually impossible here. The filled points also take part in the estimation of the non-linear distortion. We will show
691958	18011	without external synchronization. One computer, a standard PC or a laptop running on Linux, often has to serve more than just one camera. The acquisition is synchronized via TCP/IP communication  which is naturally far less precise than external synchronization by a HW system. The working volume often contains furniture and computers and it cannot be completely darkened. The situation can
691958	18012	points by pairwise RANSAC analysis . 3. Estimate projective depths ? i j Section 2.1. and fill the missing points by the method described in 10sFigure 2: Immersive virtual environment BlueC  and our modification of a laser pointer. A small piece of transparent green or red plastic is attached to the laser pointer. The modification has been invented in order to get better visibility
691958	18012	possible application of our algorithm. Multiple cameras for immersive environments or telepresence virtual rooms often encompass the whole volume thus posing challenges in visibility. Our Blue-C  setups each 15s500 400 300 200 100 0 measured, o, vs reprojected, +, 2D points (camera: 4) 1000 ?500 0 100 200 300 400 500 600 700 500 0 measured, o, vs reprojected, +, 2D points (camera: 40) ?400
691958	18013	of the absolute conic constraint exist. In our implementation, we put the origin of the world frame to the centroid of the (unknown) reconstructed 3D Euclidean points, which is the approach used in . However, the formulation, where the origin of the world frame is in the first camera center , is equivalent. We extend the notation used in the previous 6ssections. As already mentioned,
691958	18016	makes the computation of epipolar geometry impossible . Moreover, given m ? 3 cameras, configuration is critical if all points and cameras lie in the intersection of two distinct ruled quadrics . This may happen, however, hardly in practice. Even though the projective structure and motion are estimated correctly there are still critical positions of cameras which make the Euclidean
691958	18017	and motion are estimated correctly there are still critical positions of cameras which make the Euclidean stratification impossible. Such positions are 9scalled critical motions of cameras . If all cameras and lenses are the same we shall consider the critical motions for self-calibration with constant internal parameters . In fact, in multi-camera systems there are several
691958	18017	each position, or (iii) translation along the optical axis, with arbitrary rotations around the optical axis, or (iv) motion with two viewing directions (orientation of optical axes) at most. See  for more thorough explanation. It should be noted that there is one more important motion which is not critical for our selfcalibration method but is critical for an alternative method based on
691958	18018	environments with a varying number and quality of cameras used. 1 Introduction With decreasing prices of powerful computers and cameras, smart multi-camera systems have started to emerge . A complete multi-camera calibration is the inevitable step 1stowards the efficient use of such systems even though many things can be accomplished with uncalibrated cameras in virtual environments
691958	18021	all cameras . We describe the Euclidean stratification in more detail in Section 2.2. 2.1 Projective reconstruction by factorization with filling the missing points Martinec & Pajdla’s method  was used for recovery of projective shape and motion from multiple images by factorization of a matrix containing the images of all scene points. This method can handle perspective views and
691958	18021	wide base-line stereo while the alternative with a sequence is more appropriate for video-sequences. In this paper, only the former alternative is explained, see Algorithm 1. For more details see . As noted in , any tree structure linking all images into a single connected graph can be used. This is especially advantageous when a large amount of occlusions is present in the data because
691958	18021	4s1. Set ? c p = 1 for all p’s corresponding to known points u c p . 2. For i ?= c do the following: If images i and c have enough points in common to compute a fundamental matrix uniquely (see  for details) then compute the fundamental matrix Fic , the epipole eic , and depths ?i p according to ? i p = (eic × ui p ) · (Ficuc p ) ?eic × ui p?2 ? c p if the right side of the equation is
691958	18021	Bt quickly becomes empty. This is why B is searched for as the closest 4D space to spaces Bt in the sense of the minimal sum of square differences of known elements. More details are reported in . Recently, new constraints on the consistent set of all camera matrices were found. They are more robust to both significant camera movement and occlusions. The new method is to appear in an
691958	18021	Bt = 1u 4 1 1 ?12 u12 ?13 u13 ?14 u14 0 ?2 1u21 ?22 u22 ?23 u23 0 u2 ? 4 It can be proven, that if Bt is of full rank (i.e. five, here) then B ? Span(Bt), which is exactly the constraint on B. See  for details how to construct the matrix Bt in a general situation. By also including image points with unknown projective depths, the spaces Bt, spanned by four-tuples of columns, become smaller,
691958	18023	cameras in virtual environments and telepresence setups. To our best knowledge, no fully automatic calibration method, for multi-camera environments, exists. Very recent multi-camera environments  or , which are primarily designed for realtime 3D acquisition, use advanced calibration methods based on a moving plate . These calibration methods do not require a 3D calibration object
691958	18024	containing the images of all scene points. This method can handle perspective views and occlusions jointly. The projective depths of image points are estimated by the method of Sturm & Triggs  using the epipolar geometry. Occlusions are solved by the extension of the method by Jacobs  for filling the missing data. This extension can exploit the geometry of the perspective camera so
691958	18024	have to be repeated, while the measurement matrix Ws is not complete. In what follows, we shall describe the two steps of the algorithm. Projective depth estimation We used Sturm & Triggs’ method  exploiting the epipolar geometry but other methods may be applied too. The method  was proposed in two alternatives. The alternative with a central image is more appropriate for wide base-line
691958	18025	and motion are estimated correctly there are still critical positions of cameras which make the Euclidean stratification impossible. Such positions are 9scalled critical motions of cameras . If all cameras and lenses are the same we shall consider the critical motions for self-calibration with constant internal parameters . In fact, in multi-camera systems there are several
691958	18026	equations fails, in the case where the optical centers of all cameras lie on a sphere and if the optical axes pass through the sphere’s center, a very natural situation in many multi-camera systems . The section about critical configuration and motions might be summarized in the following suggestions: To avoid numerical instability we should: (i) fill up the working volume with calibration
691958	18024	projective motion and the projective shape, respectively. If we collect enough noiseless points (u i j , vi j ) and the scales ?i j are known, then Ws has rank 4 and can be factored into P and X . The factorization of (3) recovers the motion and the shape up to a 4 × 4 projective transformation H: Ws = PX = PHH ?1 X = ˆPˆX , (4) where ˆP = PH and ˆX = H ?1 X. Any non-singular 4 × 4 matrix
691958	18027	environments with a varying number and quality of cameras used. 1 Introduction With decreasing prices of powerful computers and cameras, smart multi-camera systems have started to emerge . A complete multi-camera calibration is the inevitable step 1stowards the efficient use of such systems even though many things can be accomplished with uncalibrated cameras in virtual environments
691958	18027	of the possible working volume thus making the estimation unstable. Occlusions and very different, or even disjoint, fields of view were common problems when using the mobile version of our ViRoom  system. Calibration based only on the points visible in all cameras would be virtually impossible here. The filled points also take part in the estimation of the non-linear distortion. We will show
691958	14139	simultaneously. This modification will appear in a new version of the calibration package. Filling of missing elements in Ws The filling of missing data was first realized by Tomasi & Kanade  for orthographic camera. Jacobs  improved their method and we used our extension of his method for the perspective 4s1. Set ? c p = 1 for all p’s corresponding to known points u c p . 2. For i
691958	8858	four corners of the construction and the remaining 12 cameras are mounted on the aluminum scaffold that encompasses the CAVE. 11s4. Optimize the projective structure by using the Bundle Adjustment , if applicable. 5. Perform the rank 4 factorization of the matrix Ws to get projective shape and motion . 6. Upgrade the projective structures to Euclidean ones by the method described in
691958	18028	environments with a varying number and quality of cameras used. 1 Introduction With decreasing prices of powerful computers and cameras, smart multi-camera systems have started to emerge . A complete multi-camera calibration is the inevitable step 1stowards the efficient use of such systems even though many things can be accomplished with uncalibrated cameras in virtual environments
691958	18030	environments, exists. Very recent multi-camera environments  or , which are primarily designed for realtime 3D acquisition, use advanced calibration methods based on a moving plate . These calibration methods do not require a 3D calibration object with known 3D coordinates. However, they share the main drawback with the old classical methods. The moving calibration plate is
18031	6469	ought to be represented either as a part of an amputation of the foot, or alternatively, as an amputation of part of the foot. Depending on the context, these are two very different sorts of things. SNOMED here runs together endurants and occurrents. It runs together that element of parthood associated with the foot, an entity that endures in time, with that parthood associated with an
8816602	11564	the necessary ontology-based transformations on queries and results. We used this strategy effectively in a project that used DMOZ  information in a distributed knowledge-sharing application . Rather than convert the very large DMOZ data set to RDF, it was stored in a custom database layout and queries and query results were dynamically translated to RDF as needed. Using Nuin, we can
8816602	18043	by the use of ontologies, though it would seem that the use of an ontology representation has some impact on the solutions to most, if not all, of them. 3. OVERVIEW OF THE NUIN PLATFORM Nuin  is an agent platform we have created to assist agent designers to program deliberative agents, with a particular emphasis on BDI  agents. Nuin is founded on Rao’s AgentSpeak(L) , and
8816602	18044	design and maintenance). 4.4 Reconciling vocabularies In general, determining the correspondences between two (or more) ontologies is a very difficult task, requiring extensive human intervention . Once the mapping between two ontologies is defined, it is possible that translations between a value expressed in one ontology and a value expressed in anotherscan be automated. Some
8816602	1729	this trip is also in the class AirTravel because AirTravel is defined as the class that has vehicleType of class Airplane. Figure 7 shows a fragment of our ontology class hierarchy (using Protég?? ): Figure 7: section of ontology class hierarchy This approach highlights a particular difficulty with ontology development: when to uses classes vs. instances. We can define A320 as an instance of
8816602	7231	Nuin  is an agent platform we have created to assist agent designers to program deliberative agents, with a particular emphasis on BDI  agents. Nuin is founded on Rao’s AgentSpeak(L) , and extended to make a practical, Java™based programming tool. In this section, we briefly introduce some of the key features of Nuin, in order to provide some background for the solution
8816602	18047	in which an agent-based travel agent must co-operate with other agents to book a trip for a human client. We have been investigating the design and development of beliefdesire-intention (BDI)  agents for use in the Semantic Web . One outcome of this research is a BDI agent platform, Nuin, which has been designed ab initio to work with Semantic Web information sources. At the time of
8816602	18047	most, if not all, of them. 3. OVERVIEW OF THE NUIN PLATFORM Nuin  is an agent platform we have created to assist agent designers to program deliberative agents, with a particular emphasis on BDI  agents. Nuin is founded on Rao’s AgentSpeak(L) , and extended to make a practical, Java™based programming tool. In this section, we briefly introduce some of the key features of Nuin, in order
8921239	5458	N #+1 ) V (s #-1 -N |s # , y # -N+1 ). (B.6) Proof: See Sec. B.17. # Remark 43 (On the Markov Property of A Posteriori PMFs) Considering the corresponding (Forney-style) factor graph (normal graph)  of the hidden Markov model (see Fig. B.1), the statements in Lemma 42 are rather straightforward. S 3 S 2 S 1 S 0 Y 3 Y 2 Y 1 S-1 S-2 Y-1 Y 0 Figure B.1: Forney-style factor graph (normal graph)
18052	3269	done by extracting a skeleton from the Voronoi diagram, but this required vertex labelling and was only useable for polygon maps. We wished to take the crust algorithm of Amenta, Bern and Eppstein  and modify it to extract the skeleton from unlabelled vertices. We find that by reducing the algorithm to a local test on the original Voronoi diagram we may extract both a crust and a skeleton
18052	3269	with this approach are that the fringe points have to be labelled, and that only polygon-type maps can be produced. These limitations are overcome due to the work of Amenta, Bern and Eppstein , who showed that the “crust” of a curve or polygon boundary can be extracted from unstructured (and unlabelled) input data points if the original curve is sufficiently well sampled. Their intuition
18052	3269	skeleton) of a set of sample points from a smooth curve (Figure 6, after ) then by inserting the original vertices plus the Voronoi vertices into a Delaunay triangulation (Figure 7, after ) the circumcircles of this new triangulation approximate empty circles between the original smoothscurve and its medial axis. Thus any Delauany edge connecting a pair of the original sample points
18052	3269	12 in the input coordinates. A second idea was that Quad-Edges that failed the crust criterion were part of the skeleton or “anti-crust”. This term was mentioned briefly in the conclusions of , citing . This is based on the idea that the dual of a crust edge is a Voronoi edge that intersects the crust - and has been rejected. The remaining Voronoi edges form a “tree”
18052	3269	of constructing a second diagram. The results for Figure 6 are shown in Figure 8, and the results for Figure 9 are shown in Figures 11 and 12. Mis-assignments occur where the sampling conditions of  are not met - especially at acute angles. We later learned that Dominique Attali had previously given an angle condition on Delaunay circles that is equivalent to the local test . Our
18052	18056	of  are not met - especially at acute angles. We later learned that Dominique Attali had previously given an angle condition on Delaunay circles that is equivalent to the local test . Our expression is somewhat easier to compute, but gives the identical graph. The leaf vertices or &quot;hairs&quot; on this skeleton exist where there are three adjacent sample points whose circumcircle
18052	8843	We find that by reducing the algorithm to a local test on the original Voronoi diagram we may extract both a crust and a skeleton simultaneously, using a variant of the Quad-Edge structure of . We show that this crust has the properties of the original, and that the resulting skeleton has many practical uses. We illustrate the usefulness of the combined diagram with various applications.
18052	8843	correct maps could be generated directly from scanned input.sThe Voronoi diagram is implemented using the Quad-Edge structure and an incremental Voronoi algorithm that closely follows that given in  among others. In the rapid digitizing procedure of , the operator digitizes the interior of each polygon, inserting points around the perimeter, each with the polygon label. The Delaunay
18052	8843	led to the examination of the relationship between Voronoi edges and Delaunay edges in the original Voronoi/Delaunay construction. This was made simpler by the use of the Quad-Edge data structure , where two of the pointers refer to Delaunay vertices, and two to the dual Voronoi vertices. Our intuition was to apply the crust test to individual Quad-Edges on the original diagram, rather than
18052	8843	a local test - testing only the two Voronoi vertices that are the endpoints of the dual Voronoi edge. For a Delaunay edge (q, r) with dual Voronoi edge (a, b) the test is the standard InCircle test  on these four points. InCircle(q,r,a,b) determines whether point b is outside the circle through (q, r, a), assuming that this circle is oriented counterclockwise. If so, then that circle shows
18052	8843	that could apply to simple Delaunay/Voronoi structures as well as to polygon arcs. Since we were often working interchangeably with Delaunay/Voronoi representations, the Quad-Edge data structure of  became an obvious candidate. It has various attractions: it is a method for representing the edge connectedness of any connected graph on a manifold; it is symmetric in its storage of both the
18098	18123	The bias demonstrated here could have been due to either retinal or extra-retinal signals associated with the saccade. The fact that it occurs before the saccade points to an extra-retinal origin , but a role for retinal flow and smear due to the saccade cannot be excluded, since, even for trials with stimulus onset before the saccade, some retinal signals (e.g., due to the saccade target)
18098	18130	by Julesz . The ambiguity in this perceptual process is partly broken by the a priori hypotheses of rigidity (minimal relative motion ) and stationarity (minimal absolute motion ). However, as in the case of the Necker cube or the Mach book, there remains some ambiguity, leading to simultaneous reversals of depth and direction of motion. An example movie very similar to one
18098	18132	since the sign of relative depth is completely ambiguous (Fig. 2 offers an explanation of the ambiguity), the stimulus can be perceived as rotating in either direction about the vertical axis . The two solutions are oriented in opposite directions, and therefore have tilts that differ by 180 ? . 1 The subjects’ task was to report the perceived surface tilt, from which the perceived
18098	18133	since the sign of relative depth is completely ambiguous (Fig. 2 offers an explanation of the ambiguity), the stimulus can be perceived as rotating in either direction about the vertical axis . The two solutions are oriented in opposite directions, and therefore have tilts that differ by 180 ? . 1 The subjects’ task was to report the perceived surface tilt, from which the perceived
18098	18137	target) were present, and could have resulted in a retroactive bias. In order to probe the origin of the bias, an additional control experiment was performed with optically simulated saccades . Subjects maintained central fixation (as in the fixation condition above), while experiencing approximately the same flow on the retina as in previous saccadic trials. The results (Fig. 5) show
5486202	8175	as treestructured, while the WWW itself is an enormous, dynamic multigraph. Much work on attempting to extract information from Web pages makes explicit or implicit use of graph representations . It follows, then, that the ability to compare two graphs is basic functionality, as demonstrated in such applications as query-by-structure, wrapper generation for information extraction,
5486202	8175	databases of semistructured multimedia documents . Finally, a number of researchers have studied techniques for identifying smaller, coherent substructures within Web pages (e.g., lists, tables) . 3 Graph Probing The graph model we assume for HTML documents includes the standard tree-structured hierarchy generated when parsing the tags (the “contains”/“contained-by” relationship). In
5486202	8175	of graphs to find the desired matches. In addition to information retrieval, other possible applications of graph comparison via probing could include wrapper generation and maintenance (e.g., ) and analysis of HTML-coded tables (e.g., ). This would require retargeting our graph probing language to information extraction applications, a task that would be challenging but seems
5486202	18178	as treestructured, while the WWW itself is an enormous, dynamic multigraph. Much work on attempting to extract information from Web pages makes explicit or implicit use of graph representations . It follows, then, that the ability to compare two graphs is basic functionality, as demonstrated in such applications as query-by-structure, wrapper generation for information extraction,
5486202	18178	databases of semistructured multimedia documents . Finally, a number of researchers have studied techniques for identifying smaller, coherent substructures within Web pages (e.g., lists, tables) . 3 Graph Probing The graph model we assume for HTML documents includes the standard tree-structured hierarchy generated when parsing the tags (the “contains”/“contained-by” relationship). In
5486202	18178	of graphs to find the desired matches. In addition to information retrieval, other possible applications of graph comparison via probing could include wrapper generation and maintenance (e.g., ) and analysis of HTML-coded tables (e.g., ). This would require retargeting our graph probing language to information extraction applications, a task that would be challenging but seems
5486202	18179	as treestructured, while the WWW itself is an enormous, dynamic multigraph. Much work on attempting to extract information from Web pages makes explicit or implicit use of graph representations . It follows, then, that the ability to compare two graphs is basic functionality, as demonstrated in such applications as query-by-structure, wrapper generation for information extraction,
5486202	18179	error-tolerant embedding of trees to judge the similarity of XML documents . Likewise, Dubois et al. write about tree embedding for searching databases of semistructured multimedia documents . Finally, a number of researchers have studied techniques for identifying smaller, coherent substructures within Web pages (e.g., lists, tables) . 3 Graph Probing The graph model we assume
5486202	18184	under any isomorphism I, ifI(v) =v 0 then f(v) =f(v 0 ). One commonly used invariant is the degree of a vertex. In fact nauty, a successful software package for determining graph isomorphism (see ), relies on such vertex invariants. This observation can be seen as forming the basis for graph probing, a paradigm we have recently begun exploring for graph comparison . However, we desire
5486202	18185	set of features to the ones we use, they do not consider the approximate matching or subgraph problems. Papadopoulos and Manolopoulos discuss an idea that is philosophically quite similar to ours . However, they focus on a single invariant: node degree. It is clear this is not sufficient for catching all of the interesting differences that can arise between HTML documents. Moreover, their
5486202	18186	as treestructured, while the WWW itself is an enormous, dynamic multigraph. Much work on attempting to extract information from Web pages makes explicit or implicit use of graph representations . It follows, then, that the ability to compare two graphs is basic functionality, as demonstrated in such applications as query-by-structure, wrapper generation for information extraction,
5486202	18186	for which efficient comparison algorithms are known. Schlieder and Naumann consider a problem closely related to ours: error-tolerant embedding of trees to judge the similarity of XML documents . Likewise, Dubois et al. write about tree embedding for searching databases of semistructured multimedia documents . Finally, a number of researchers have studied techniques for identifying
5486202	18187	examine the subgraph matching problem as well. Valiente and Martínez describe an approach for subgraph pattern-matching based on finding homomorphic images of every connected component in the query . Again, the worst-case time complexity is exponential, but such features could also perhaps be incorporated in the heuristics we are about to present. Instead of trying to solve the problem for
8765534	4535	individual behavior. The resulting model is thus a distributed dynamic model which benefits from the recent technical developments in distributed parts based modelling of static vectorial data , with various applications including image decomposition, document modelling, information retrieval and collaborative filtering. Consistent generative semantics similar to the recently introduced
8765534	4535	the details of the identification of this model, which also highlights the close relationship between two existing related models, specifically the probabilistic latent semantic analysis (PLSA)  and LDA  as being instances of the same theoretical model and differing only in the estimation procedure adopted . 2.1 Parameter Estimation and Inference Exact inference within the LDA
8765534	4535	linearly with the number of non-zero state-transition counts. It is interesting to note that the MAP estimator under a uniform Dirichlet distribution exactly recovers the aspect mixture model of  as a special case of the MAP estimated LDA model. 2.1.1 Variational Parameter Estimation and Inference While being optimal in analyzing an existing data set, MAP estimators are notoriously prone to
8765534	9508	individual behavior. The resulting model is thus a distributed dynamic model which benefits from the recent technical developments in distributed parts based modelling of static vectorial data , with various applications including image decomposition, document modelling, information retrieval and collaborative filtering. Consistent generative semantics similar to the recently introduced
8765534	9508	sample point from a Dirichlet variable then taking derivatives with respect to the ?MAP k , a convergent series of updates ?t kn is obtained where the superscript denotes the t’th iteration. As in , for each observed sequence in the sample a MAP value for the variable ? is iteratively estimated by the following multiplicative updates ˜?kn = (?k?1)+? t kn |S| ? sm=1 n=1 · · · |S| ? s0=1 r
8765534	18194	individual behavior. The resulting model is thus a distributed dynamic model which benefits from the recent technical developments in distributed parts based modelling of static vectorial data , with various applications including image decomposition, document modelling, information retrieval and collaborative filtering. Consistent generative semantics similar to the recently introduced
8765534	18194	and yield the symbol transition probabilities Tm···0 = ?K k=1 Tm···0,k?k. The overall probability for a sequence sn under such a mixture, which we shall now refer to as a simplicial mixture , denoted as P (sn|T, ?) is equal to ? ? ? P (sn|T, ?)D(?|?)d? = ? d?D(?|?) |S| ? sm=1 · · · |S| ? s0=1 ? K? k=1 Tm···0,k?k ?r m···0 n Each sequence will have its own expectation under the Dirichlet
8765534	18196	Parameter Estimation and Inference While being optimal in analyzing an existing data set, MAP estimators are notoriously prone to overfitting, especially where there is a paucity of available data  and so the variational Bayes (VB) approach detailed in  can be adopted by considering Qn(?) = D(?|?n), where ?n is a sequence-specific variational free parameter vector. The above (2) can be
8765534	18196	using VB . This also provides an additional insight as to why LDA models improve upon PLSA, as they are in fact both the same model using different approximations to the likelihood, refer to  for an illustrative discussion on the weaknesses of MAP estimators. As a comparison to different structural models hidden Markov models with a range of hidden states were also tested on this data
8921246	18204	of the SynOD in the federated record server. Site-specific mapping to and from this reference structure will be the basis to manage the flexible exchange and automatic interpretation of the records . Enterprise Application Integration Hospitals are complex organizations. As such they have Enterprise Resource Planning (ERP) systems deployed to manage the hospital’s data and processes. ERP
18210	1989	and illustrates how to use it to solve constraint satisfaction problems. To write this course, we also took inspiration from many other tutorials and books on constraint programming, e.g., . Students that have been appealed by this introductory course are refered to these books to actually become experts! Finally, note that if this course has been designed for e-miage students, it is
18210	18212	this problem is very simple to describe and allows us to introduce the fact that there may exist different CSP modelings for a same problem. The second example is the stable marriage problem , which has more practical applications. Session 2 is a training session, where the student has to model 5 problems within the CSP framework: • The first problem involves computing the set of coins
18210	18214	and illustrates how to use it to solve constraint satisfaction problems. To write this course, we also took inspiration from many other tutorials and books on constraint programming, e.g., . Students that have been appealed by this introductory course are refered to these books to actually become experts! Finally, note that if this course has been designed for e-miage students, it is
18215	17968	to a favourable conclusion, the authors propose the usage of software agents, which roam the Semantic Web looking for information. The Semantic Web will be the natural habitat of software agents . It requires little imagination to apply these ideas to timetabling. 3. The different layers of the Semantic Web Berners-Lee proposed an architecture (Figure 1) for the Semantic Web . Some of
18215	18223	layer offers the opportunity to make ‘machine-processable’ statements. The technology behind it is called Resource Description Framework (RDF) . RDF allows to add formal semantics to the web . That is done by defining a data model, using three object types: • Resources: objects that can be identified by a URI • Properties: specific characteristics that can be used to describe a resource
18215	18223	the basic RDF model: RDF Schema (sometimes abbreviated as RDFS). RDF Schema is a data-typing model for RDF . Mark however that the functions in RDF Schema and XML Schema have are not the same : XML schema is used to control the syntax of an XML document, while RDF schema only considers the interpretation of RDF statements. 3.4 Fourth layer The next layer is the ontology layer. It is the
18215	1721	of RDF statements. 3.4 Fourth layer The next layer is the ontology layer. It is the layer in which at present most of the research concerning the Semantic Web takes place. Fensel et al.  explain why ontologies are nowadays popular: “ontologies are becoming popular largely because of what they promise: a shared and common understanding that reaches across people and application
18215	1721	areas where no standard ontologies exist or where people need to translate their own terminology into the standard. This translation service covers structural, semantic and language differences  (Figure 3). Although ontology languages (LOOM , POWERLOOM , KIF , CYCL , Ontolingua , etc) were created years before the term Semantic Web was invented, the two most important
18215	1721	<Auto> <Name>Daimler 230 SE</Name> <Preis>40.000DM</Preis> </Auto> Bestellinformation Product catalogue2 Business2 Figure 3: translation of structure, semantics and language. Example taken from  As part of the DAML programme, people from the academic world are working on DAML-S, an ontology of services. Services refer to dynamic web sites which allow actions or changes in the world .
18215	6101	understanding that reaches across people and application systems”. A commonly accepted definition of an ontology is: ”an ontology is a formal, explicit specification of a shared conceptualisation”. A conceptualisation is a way of thinking in a particular domain. This is typically expressed as a set of concepts (entities, attributes, processes), their definitions and their
18215	18225	their own terminology into the standard. This translation service covers structural, semantic and language differences  (Figure 3). Although ontology languages (LOOM , POWERLOOM , KIF , CYCL , Ontolingua , etc) were created years before the term Semantic Web was invented, the two most important languages are based on RDF Schema. The creators of both ontology languages
18215	7248	the standard. This translation service covers structural, semantic and language differences  (Figure 3). Although ontology languages (LOOM , POWERLOOM , KIF , CYCL , Ontolingua , etc) were created years before the term Semantic Web was invented, the two most important languages are based on RDF Schema. The creators of both ontology languages quickly realised that it was
18215	18226	these technologies are not based on languages like DAML, and thus lack expressiveness, they are for humans rather than for software agents. For a further discussion on this subject we refer to . 3.5 Fifth layer The fifth and the sixth layer are still under development. The logic layer provides a language for describing sets of deductions, which can be derived from a collection of data. It
18215	18228	how these technologies can be applied in timetabling. 4.1 Second layer At the essential, basic level we have to define a format or language for representing instances of a timetable. Burke et al.  mention requirements for a standard data format for timetabling. Such format should be: • general, expressing all kinds of timetabling problems, • complete, expressing these problems in full
18215	18228	another format. Almost every database can also generate XML files. This massive support from the software industry is an extra reason to express the elements of the language STTL  in XML. In , the authors note that: “Programs will be necessary to convert data in a non-standard format into the standard data format. In order to encourage the development of consistent conversion programs
18215	18228	can be incorporated back into the presented page.” This interface allows a programmer to manipulate the content of an XML document: elements can be added, deleted, moved, etc. Burke et al.  conclude with: “Many formats used by timetabling applications will have been designed with concerns for minimising data storage requirements or to facilitate the fast processing of data. This means
18215	18234	CONSTRAINT imposes imposes PRODUCT DEMAND satisfies Figure 4: OZONE top-level ontology adapted to timetabling tabling we propose the upper-level ontology that we developed based on the OZONE  scheduling ontology (Figure 4). This upper-ontology has 5 central concepts, namely:sUsing Web Standards for Timetabling 13 • ACTIVITY: a process that can be executed over a time interval. Resources
18215	18234	Homogeneous Resource Pool Heterogeneous Resource Pool Simple Capacity Pool Structured Capacity Pool Pool of large lecture rooms Teaching Unit Figure 5: UML example of an OZONE-based ontology  for timetabling resources.s14 P. De Causmaecker, P. Demeester and G. Vanden Berghe We opted not to use time as a (consumable) resource. 4.3.4 Relation with STTL The elements in Kingston’s STTL
18215	18235	from scratch. The hope is that, because these components are at a quite high level of abstraction, the assembly of the components can take place more easily than in other cases of software reuse.??? By incorporating domain ontologies, the domain-dependent information becomes explicit, accessible and editable. In the design phase a distinction is possible between the computational model and the
18215	18235	interrelated, these are strictly separated by using domain ontologies and problem-solving methods. This minimises the effort of updating the timetabling applications when requirements change . Another advantage is that any new algorithm with better performance can be plugged in as a problem-solving method. Finally, using ontology in system design makes it function as a breeding ground
18215	18237	use of an ontology. Montana  introduced a web-based timetabling application, the only similar approach we are aware of.s16 P. De Causmaecker, P. Demeester and G. Vanden Berghe Di Gaspero et al.  report on an Object-Oriented Framework called EasyLocal++ that is designed in 4 abstract layers. The top layers rely on services supplied by the lower levels, analogous to the Semantic Web (Figure
18215	18238	to be activated. We consider using EasyLocal++ as a starting point for the component based timetabling system. Although university and exam timetabling problems are occurring all over the world , there exists no generally satisfying solution strategy. That is because the demands, resources and constraints change from university to university. This bold statement is the standing ground for
18215	18244	to XML documents. The XML results can be presented in a browser by applying XSL and CSS. Each personnel scheduling problem has specific constraints and requires adapted approaches to solve it . In different sectors, we distinguish a different attitude towards cyclical schedules, overtime, locations, qualifications, etc. Just like in the university timetabling problem discussed in the
3560043	19347	time series data were used. A number of studies estimated price and income elasticities for some selected commodities. These studies include the work of Shalaby (1978), Mesilhy (1980), and Ali (1991). Three other studies provide only income or expenditure elasticity estimates: Ibrahim (1988); Ayaad (1994); and Ali and Adams (1996). The estimates of demand parameters in Egypt in the present
3560043	19378	line is obviously needy. However, households just above a poverty line may also be considered deserving of food subsidies. Recent head-count poverty measures in Egypt have varied widely. The INP (1996) study suggests that 22.9 percent of the Egyptian population were poor in 1995/96. Cardiff (1997) suggests that this µgure should be 44.5 percent in 1995/96, while IFPRI’s own study µnds that 26.5
3560043	19378	Sources: For Egypt, Table F.9; for pilot food price subsidy scheme in the Philippines, Garcia and PinstrupAndersen (1987); for general price subsidy in the Philippines, Subbarao, Ahmed, and Teklu (1996); for Brazil, Colombia, Indonesia, and India, World Bank (1984); and for Bangladesh, Ahmed and Billah (1994). and LE 3.34 to needy consumers. The wheat ×our subsidy system transfers LE 1.00 at a
3560043	19378	include the work of Shalaby (1978), Mesilhy (1980), and Ali (1991). Three other studies provide only income or expenditure elasticity estimates: Ibrahim (1988); Ayaad (1994); and Ali and Adams (1996). The estimates of demand parameters in Egypt in the present study differ from the previous studies mainly in two respects. First, the parameter estimates are based on the most recent primary data
3560043	19378	method, labeled the Food Characteristic Demand System (FCDS). It is based on the 1997 EIHS data. 35 The methodology and data requirements for estimation of the FCDS are discussed in detail in Bouis (1996). According to the FCDS, a household’s food acquisition behavior is motivated by (1) demand for energy (calories) to alleviate hunger; (2) demand for variety in the diet; (3) demand for food-group
18365	10543	used throughout this paper Pa(k) = P  Pb(k) = P  p = P  . Note that if the time series is to have two values, these three quantities must be in the range (0, 1). The mean µ is given by and the variance ? 2 by µ = E  = pa + (1 ? p)b. (1) ? 2 = E ? (Xt ? µ) 2? = E ? X 2? 2 2 t ? µ = p(1 ? p)(a ? b) . 2s2 On the ACF of a Weakly Stationary Time Series
18365	18366	Since the series has only two values, and The auto-covariance ?(k) is given by ?(k) = Pa(k) + Pb(k) ? 1 = Pa(k) ? p (1 ? p) = Pb(k) ? (1 ? p) . p ?(k) = E  Thus using (2) and (3), a ? µ = (1 ? p)(a ? b), (2) b ? µ = p(b ? a). (3) Pa(k) = 1 ? P  , Pb(k) = 1 ? P  . = P  (a ? µ) 2 + P  (a ? µ)(b ? µ) + P
8921279	18386	By specialization, the subclass may polymorphically redefine an operation (or statechart) to let be more appropriate to it and even substitute for the inherited behavior a new associated one . This linking type definition agrees with the common pathway process responsible for the production of thrombin. First, through positive feedback loops, thrombin quickly magnifies the hemostatic
18388	18389	is the processes of spotting tables in documents. Traditionally, this task comes in two basic forms - document image sourced tables (, ) and electronic text sourced tables including HTML (). The problem is extended to include the spotting of tables in other document encodings such as postscript, pdf, rtf, word, etc. In general, when considering tables on the web, the apMatthew Hurst
18388	18389	(TABLE,TH,TD, etc.). However, this is where we come to the first two distinguishing points. the presence of theTABLE tag in an HTML document does not necessarily indicate the presence of a table ( suggest less than 30 % of HTMLTABLEs are real tables in one particular domain). there are many other ways in which tables may be presented in web delivered documents - plain text (PRE), images,
18388	18389	requires the creation of accurate classification technology. Given any TABLE node in the HTML, the classifier must accept or reject it. Such a classifier may be built either via hand crafted rules () or using a machine learning approach. Experiments suggest that a machine learning approach using a naive bayse classifier () based on a feature set describing the set of tags below the
18388	18394	distribution of the text in a cell, authors occasionally split the text and place it in two or more adjacent cells. This problem may be accommodated by exploiting linguistic process as described in  where the content of the cell can be used to indicate continuity, if any, to other cells. errors spanning errors occur when the COLSPAN or ROWSPAN values are not correctly calculated. There are two
8921291	18400	such as dealing with a large number of attributes, were addressed. Biological data with many unknown values and multi-valued attributes were also used (&quot;gene-function&quot; data set). Independent work  highlighted the need to deal with several additional properties of data that are common in data-mining problems. Examples are a hierarchical structure of categorical attributes, a very small
8921291	18400	that contribute to a decision. The need for visualization becomes even clearer when unconventional data are concerned. Graph visualization formed an essential part of the successful completion of . 123sVisualization of the node information within the graph structure of protein-protein interactions strongly directed our investigation. In summary, it can be said that this thesis forms a solid
18414	18415	mappings between terms (e.g., nodes of graphs) by computing semantic relations (for example, equivalent or subsuming elements), instead of computing coefficients rating match quality in the  range. We are also interested in determining semantic relations by analyzing meaning (concepts, not labels as in syntactic matching) captured in the ontologies. In our effort to produce
18414	18415	reasoner that may be chosen for particular matching problems and using explanations generated by our system. Recently there has been work on verifying SAT solvers. A direct solution is provided in . They introduce a proof-producing infrastructure based on natural deduction for SAT engines (e.g., Chaff ). Another approach uses explanationssFigure 4: A graphical explanation in terms of
18414	18416	infrastructure based on natural deduction for SAT engines (e.g., Chaff ). Another approach uses explanationssFigure 4: A graphical explanation in terms of ”equivalent” inference systems  in order to provide explanations of potentially alternative deductive paths. The key distinctions of S-Match proofs are: • They are produced by a modified version of JSAT in S-Match that implements
18414	18417	subsumer); less general (?, subsumee); mismatch (?, disjoint); overlapping (?, 1 The current version of S-Match as described in this paper is a rationalized reimplementation of the CTXmatch system  with a few added functionalities.sthere may exist an instance of both classes). The relations form a partial order according to binding strength, with equivalence being stronger than subsumer or
18414	18418	illustrate the semantic matching approach. Then we describe the Inference Web (IW) infrastructure  for explanations in distributed, heterogeneous environments and its Proof Markup Language (PML). Using the matching example, we describe how the Inference Web explanations increase user understanding of semantic matching mappings, thereby increasing trust. This work is described in more
18414	18420	our goal is to be able to explain the mappings (whether they are complete, partial, or failed to be generated). In this paper, we present our approach to semantic matching as first introduced in , and implemented within the S-Match system  1 . An example is presented to illustrate the semantic matching approach. Then we describe the Inference Web (IW) infrastructure  for explanations
18414	18420	n2j, R ? >,withn1i ? G1, i=1,...,N1, n2j ? G2, j=1,...,N2 and R ? the strongest semantic relation holding between the concepts of nodes n1i,n2j. We define a mapping as a set of mapping elements . In the example, we would have a mapping element between the node labeled Europe in A1 and the node labeled Pictures in A2. Since Europe is below Images in A1, documents classified under it are
18414	18421	(whether they are complete, partial, or failed to be generated). In this paper, we present our approach to semantic matching as first introduced in , and implemented within the S-Match system  1 . An example is presented to illustrate the semantic matching approach. Then we describe the Inference Web (IW) infrastructure  for explanations in distributed, heterogeneous environments and
18414	18424	by a set of inference steps associated with a node set. This representation could be viewed as the web-ized distributed OWL version of one author’s previous work on explaining description logics . The IW Browser is used to present proofs and explanations. Exploiting PML properties, meaningful fragments of S-Match proofs can be loaded on demand. Users can browse an entire proof or they can
18414	4394	), we are not aware that any provide explanations. The DPLL procedure implemented in our approach, while unoptimized, includes the essence of the state of the art SAT engines such as Chaff , etc. Thus, one could consider using another optimized SAT reasoner that may be chosen for particular matching problems and using explanations generated by our system. Recently there has been work
18414	10178	approaches that use syntactic similarity measures or syntax driven approaches as syntactic matching since, while they may use syntactic context, they do not analyze term meaning directly, e.g., . We are interested in a semantic matching approach that generates mappings between terms (e.g., nodes of graphs) by computing semantic relations (for example, equivalent or subsuming elements),
18414	10178	the same concept?”. The IW proof defending the negative answer is shown in Figure 4. 5 Discussion While there are a number of other efforts in semi-automated ontology matching (see surveys in ), we are not aware that any provide explanations. The DPLL procedure implemented in our approach, while unoptimized, includes the essence of the state of the art SAT engines such as Chaff ,
18414	18425	the matching example, we describe how the Inference Web explanations increase user understanding of semantic matching mappings, thereby increasing trust. This work is described in more detail in . 2 Semantic Matching We will focus on class matching and motivate the problem by a simple catalog example shown in Figure 1. In this scenario, an agent may need to exchange documents stored
18414	18426	systems. 1 Introduction The amount of disparate online information is increasing as is the need for interoperability. This combination increases the need for managing semantic heterogeneity (e.g., ). Many solutions to the problem include “matching” terms in one information source to terms in another. We will view the information sources to be graph-like structures containing terms and their
18414	18426	the same concept?”. The IW proof defending the negative answer is shown in Figure 4. 5 Discussion While there are a number of other efforts in semi-automated ontology matching (see surveys in ), we are not aware that any provide explanations. The DPLL procedure implemented in our approach, while unoptimized, includes the essence of the state of the art SAT engines such as Chaff ,
18427	3193	In our model, the number of nodes in the network, n, is given exogenously. In other words, we are considering the behaviour of a publisher and an attacker in an existing network, such as Freenet  (for which n is around 10 000 nodes) or Free Haven . The publisher must decide on the number of these nodes, d, to which he wishes to deploy his document. To simplify the analysis, it is
18427	3194	exogenously. In other words, we are considering the behaviour of a publisher and an attacker in an existing network, such as Freenet  (for which n is around 10 000 nodes) or Free Haven . The publisher must decide on the number of these nodes, d, to which he wishes to deploy his document. To simplify the analysis, it is assumed that these d nodes are selected uniformly, at random,
18427	18429	tasks: breaking cryptography, finding collisions on hash functions, etc. When peer-to-peer systems came to be examined, the threat model employed in the literature was similar to the above . Here, we argue that peer-to-peer systems can be usefully considered with a more subtle threat model: participants in the system who have opposing interests can choose the level of resources they
18427	18429	and look at what they might mean in the context of a real censorship-resistance system. As stated in the introduction, our threat model is rather different from that specified in, for example, ,  or , because we consider not only whether it is possible for an attacker with given resources to compromise the system, but also whether a rational attacker would devote his
18427	182	(but not impossible) to retrieve a document from the system. 2 The Basic Model Although game theoretic models have been used before in the analysis of peer-to-peer networks, for example in , they have not been used to consider the kind of conflict that occurs in a system providing censorship resistance. In our model, the number of nodes in the network, n, is given exogenously. In
18427	18430	allows us to describe more precisely the viability of a peer-to-peer network. The scenarios to which this framework can be applied include distributed computation , decentralized backup , censorship resistant systems  and many others (even some terrorist organizations are peer-to-peer networks!). In this paper we focus on an attacker trying to censor a document
18427	18431	random but without replacement. This is perfectly consistent with a censorship resistant system which provides anonymity to the servers which are storing the document, such as the one described in . The goal of the publisher is to ensure that at least one copy of his document is available on a node that has not been corrupted. Thus we assume that our censorship resistance scheme has “perfect
18427	6501	look at what they might mean in the context of a real censorship-resistance system. As stated in the introduction, our threat model is rather different from that specified in, for example, ,  or , because we consider not only whether it is possible for an attacker with given resources to compromise the system, but also whether a rational attacker would devote his resources
7833994	8921298	rates faster than traditional agricultural commodities 25 . It is precisely this feature that is seen as an opportunity for developing countries to venture into processed food exports in a big way (Athukorala and Sen 1998) 26 . Indeed, fish exports from developing countries to developed country markets now often exceed the combined value of net exports of coffee, tea, cocoa, bananas and sugar (Delgado, Minot and
7833994	18472	low prices). Faced with risky environments, rural households often resort to selling their assets to smooth consumption (Rosenzweig and Wolpin 1993), but sometimes even this may not be feasible (Fafchamps et al 1998) 14 highlighting the need for credible insurance mechanisms 15 . Formal mechanisms are very difficult to implement particularly in large developing countries or where small farmers are numerous and
7833994	18472	Faso, this did not happen – possibly because widespread agricultural shock meant more contemporaneous decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in
7833994	19595	& Robilliard (2000) Focus is on domestic policy rather than trade liberalization per se. Uses CGE Model to evaluate domestic policy reform (price support versus direct payments) in Mexico Harris (2001) Off-farm employment effects could be positive. A three module (macro-module, farm sector production module & commodity-chain module) model focussing on agriculture Gerard, Marty, Lancon and
7833994	19595	calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to study poverty alleviation and gauge vulnerability to shocks (change in prices, real exchange rate, etc.) Lofgren (2001) 25sThe need to have detailed analysis of data on structure of household incomes, consumption bundles, output, etc. has been recognized but doing so led to no unequivocal pattern of change in real
7833994	19595	– more so in Latin America and Africa than in Asia. In the late 1990s, in Latin America, on an average, as much as 46% of rural 18 In a study of a proposed rice export tax in Thailand, Warr (2001) found that the resultant decline in domestic rice prices would also drive down wages of unskilled labour, which is employed extensively in the rice industry. The outcome for the rural (and urban)
7833994	19595	in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to participate in setting international standards, which are
7833994	19595	No Yes Wilson & Otsuki Data-based approach. Use a - These more stringent afflatoxin standards (2002); Wilson & gravity model to measure would reduce African food exports to EU by Otsuki (2001); impact of new EU afflatoxin 64% (US$ 670 million). Otsuki, Wilson & standard on food imports; - Similarly, if pesticide residue limits Sewadah () pesticide residue limits for (chlorpyrifos) in
7833994	18543	liberalization and poverty are many (McCulloch et al. 2001, Reimer 2002, etc.), as have been those focusing on specific issues – such as linkage between trade liberalization and wages (Wood 1995, Slaughter 1999), globalization and agro-industrialization (Reardon and Barrett 2000), etc. In contrast, few have put smallholders under the 3 The question of small farms in the developed countries has also become
7833994	19640	who supplement with offfarm income. Limited off-farm opportunities imply, retreat further into subsistence. Rojjanapo Qualitative study focussing on Thailand Liberalization has caused the number (2000) of small farms to decline while large farmers and agribusinesses expand their operation. This is mainly because market fluctuations, competition and high cost of purchased inputs have made
7833994	19640	is not Madagascar. Data on 825 rice-farming sales are unaffected. considered. households static disequilibrium model. Jayne et al Data-based Approach. Simulate Maize price increase as a result (2000) elimination of maize import tariffs in of import tariffs is a tax on rural Kenya using households survey of 24 poor & smallholders (52% being districts net-buyers). So liberalization has positive
7833994	19640	under NAFTA for Mexico Levy & van Wijnbergen (1992) Use microsimulation techniques on disaggregated household level data which is integrated into a CGE model – Madagascar. Cogneau & Robilliard (2000) Focus is on domestic policy rather than trade liberalization per se. Uses CGE Model to evaluate domestic policy reform (price support versus direct payments) in Mexico Harris (2001) Off-farm
7833994	19640	of gains Use CGE Model for Zimbabwe to examine income and equity effects of trade liberalization with and without complementary policies Bautista, Lofgren & Thomas(19 98) ;Bautista & Thomas (2000) 24sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Tariff liberalization yields larger income benefits to small-farm and &quot;other rural&quot; households relative to the more
7833994	19640	– public sector agricultural research organizations and multinationals 21 – few have studied the other important implication of such concentration in the seed industry. For the 21 Lesser et al (2000) for instance discuss the links between agricultural biotechnology, IPRs, national research organizations and the multinationals. 47ssmallholders, the important question is whether with seed
7833994	19652	in the  million livelihoods affected with (1996) Philippines corn imports from US resulting in 30% decline in market prices Watkins Qualitative study on various issues; Corn farmers in Mexico and (1997) addresses corn sectors in the Philippines cannot compete with US Philippines and Mexico. imports, small farmers will be destroyed. Nadal Qualitative study. Focus on corn sector 40% of corn farmers
7833994	19652	on nonagricultural households Study Approach CGE model for the Philippines. Studies 3 trade policy adjustments- general import surtax, import rationing and tariff reduction. Bautista & Thomas (1997) Emphasizes role of income diversification; also points to role of complementary policies. CGE Model for Malawi calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to
7833994	19652	based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”. Sarris (1997) too mentions low transmission coefficient of 0.24 and 0.58 in the short and long run. 30scartels prevented passing the 50% price reductions to consumers of corn products, although they would still
7833994	19652	which are seen as key areas for smallholder participation. 30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and
7833994	19653	in the  million livelihoods affected with (1996) Philippines corn imports from US resulting in 30% decline in market prices Watkins Qualitative study on various issues; Corn farmers in Mexico and (1997) addresses corn sectors in the Philippines cannot compete with US Philippines and Mexico. imports, small farmers will be destroyed. Nadal Qualitative study. Focus on corn sector 40% of corn farmers
7833994	19653	on nonagricultural households Study Approach CGE model for the Philippines. Studies 3 trade policy adjustments- general import surtax, import rationing and tariff reduction. Bautista & Thomas (1997) Emphasizes role of income diversification; also points to role of complementary policies. CGE Model for Malawi calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to
7833994	19653	based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”. Sarris (1997) too mentions low transmission coefficient of 0.24 and 0.58 in the short and long run. 30scartels prevented passing the 50% price reductions to consumers of corn products, although they would still
7833994	19653	which are seen as key areas for smallholder participation. 30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and
7833994	19657	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19657	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19657	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19657	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19657	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19658	& Robilliard (2000) Focus is on domestic policy rather than trade liberalization per se. Uses CGE Model to evaluate domestic policy reform (price support versus direct payments) in Mexico Harris (2001) Off-farm employment effects could be positive. A three module (macro-module, farm sector production module & commodity-chain module) model focussing on agriculture Gerard, Marty, Lancon and
7833994	19658	calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to study poverty alleviation and gauge vulnerability to shocks (change in prices, real exchange rate, etc.) Lofgren (2001) 25sThe need to have detailed analysis of data on structure of household incomes, consumption bundles, output, etc. has been recognized but doing so led to no unequivocal pattern of change in real
7833994	19658	– more so in Latin America and Africa than in Asia. In the late 1990s, in Latin America, on an average, as much as 46% of rural 18 In a study of a proposed rice export tax in Thailand, Warr (2001) found that the resultant decline in domestic rice prices would also drive down wages of unskilled labour, which is employed extensively in the rice industry. The outcome for the rural (and urban)
7833994	19658	in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to participate in setting international standards, which are
7833994	19658	No Yes Wilson & Otsuki Data-based approach. Use a - These more stringent afflatoxin standards (2002); Wilson & gravity model to measure would reduce African food exports to EU by Otsuki (2001); impact of new EU afflatoxin 64% (US$ 670 million). Otsuki, Wilson & standard on food imports; - Similarly, if pesticide residue limits Sewadah () pesticide residue limits for (chlorpyrifos) in
7833994	19659	in the  million livelihoods affected with (1996) Philippines corn imports from US resulting in 30% decline in market prices Watkins Qualitative study on various issues; Corn farmers in Mexico and (1997) addresses corn sectors in the Philippines cannot compete with US Philippines and Mexico. imports, small farmers will be destroyed. Nadal Qualitative study. Focus on corn sector 40% of corn farmers
7833994	19659	on nonagricultural households Study Approach CGE model for the Philippines. Studies 3 trade policy adjustments- general import surtax, import rationing and tariff reduction. Bautista & Thomas (1997) Emphasizes role of income diversification; also points to role of complementary policies. CGE Model for Malawi calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to
7833994	19659	based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”. Sarris (1997) too mentions low transmission coefficient of 0.24 and 0.58 in the short and long run. 30scartels prevented passing the 50% price reductions to consumers of corn products, although they would still
7833994	19659	which are seen as key areas for smallholder participation. 30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and
7833994	19664	from price rises. Alternatively, as Nadal (2000 a & b) claims, small corn farmers do not benefit from reductions in corn prices as buyers since tortilla industry 10 For instance, Quiroz and Soto (1995) conclude based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”.
7833994	19664	in an additional 80 cents for non-agricultural income for local enterprises, for selected countries in Africa it was estimated to be over two dollars (Delgado, et al 1998). Hazell and Hojjati (1995) found that the often prohibitive costs of trading in many rural areas in developing countries implies that much of the multiplier effect is driven primarily by household consumption demand and
7833994	19666	from price rises. Alternatively, as Nadal (2000 a & b) claims, small corn farmers do not benefit from reductions in corn prices as buyers since tortilla industry 10 For instance, Quiroz and Soto (1995) conclude based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”.
7833994	19666	in an additional 80 cents for non-agricultural income for local enterprises, for selected countries in Africa it was estimated to be over two dollars (Delgado, et al 1998). Hazell and Hojjati (1995) found that the often prohibitive costs of trading in many rural areas in developing countries implies that much of the multiplier effect is driven primarily by household consumption demand and
7833994	19669	in the  million livelihoods affected with (1996) Philippines corn imports from US resulting in 30% decline in market prices Watkins Qualitative study on various issues; Corn farmers in Mexico and (1997) addresses corn sectors in the Philippines cannot compete with US Philippines and Mexico. imports, small farmers will be destroyed. Nadal Qualitative study. Focus on corn sector 40% of corn farmers
7833994	19669	on nonagricultural households Study Approach CGE model for the Philippines. Studies 3 trade policy adjustments- general import surtax, import rationing and tariff reduction. Bautista & Thomas (1997) Emphasizes role of income diversification; also points to role of complementary policies. CGE Model for Malawi calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to
7833994	19669	based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”. Sarris (1997) too mentions low transmission coefficient of 0.24 and 0.58 in the short and long run. 30scartels prevented passing the 50% price reductions to consumers of corn products, although they would still
7833994	19669	which are seen as key areas for smallholder participation. 30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and
7833994	19674	in the  million livelihoods affected with (1996) Philippines corn imports from US resulting in 30% decline in market prices Watkins Qualitative study on various issues; Corn farmers in Mexico and (1997) addresses corn sectors in the Philippines cannot compete with US Philippines and Mexico. imports, small farmers will be destroyed. Nadal Qualitative study. Focus on corn sector 40% of corn farmers
7833994	19674	on nonagricultural households Study Approach CGE model for the Philippines. Studies 3 trade policy adjustments- general import surtax, import rationing and tariff reduction. Bautista & Thomas (1997) Emphasizes role of income diversification; also points to role of complementary policies. CGE Model for Malawi calibrated to a SAM (1997-98 Malawian integrated households survey). The aim is to
7833994	19674	based on an analysis of 78 countries that “in an overwhelming majority of cases, transmission of price signals in agriculture is either non-existent or low, by any reasonable standard”. Sarris (1997) too mentions low transmission coefficient of 0.24 and 0.58 in the short and long run. 30scartels prevented passing the 50% price reductions to consumers of corn products, although they would still
7833994	19674	which are seen as key areas for smallholder participation. 30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and
7833994	19682	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19682	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19682	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19682	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19682	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19684	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19684	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19684	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19684	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19684	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19685	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19685	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19685	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19685	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19685	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19688	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19688	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19688	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19688	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19688	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19689	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19689	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19689	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19689	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19689	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19690	& Tray (1988) Rural wages are endogeneously determined. multi-market model in semi-arid topics in India to evaluate the impact of trade liberalization on different indicators. Gulati & Kelley (1999) no wage link, but considers substitution possibilities. Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous “Immiserized Growth” hypothesis, price risk averse net-buyer small farmers
7833994	19690	Model for Morocco, different scenarios involving multi-period CGE Model focussing on agriculture for evaluating distributional impact of trade policies in India. Lofgren, El-Said & Robinson (1999) Storm (1999) 23sRemarks Findings: Does Liberalization Hurt Smallholders? No Yes Ambiguous Adversely impacts maize producers in Mexico, particularly small farmers, whose productivity levels are much
7833994	19690	that use modeling approach, often assume away problems of price transmission and structural and institutional constraints in smallholders’ environment. 9 For instance, Cashin, Liang and McDermott (1999) observe that low prices endure for more months than high prices For wheat, international price shocks have a median half-life of 44 months. There is a probability of 50% that prices prevail below
7833994	19690	decision by households to sell livestock and lower the efficacy in smoothing consumption (Fafchamps et al 1998). 15 For detailed discussions on social risk management, see Siegel and Alwang (1999). 37sconsiderable subsidies bringing to the fore questions of sustainability. Insurance arrangements in developing countries have tended to be informal often bundling credit and insurance depending
7833994	19690	30 Broader issues on SPS get excellent treatment in Jensen (2002), which offers a developing country perspective of the agreement, Josling (1997) of measuring the impact and Bureau et al (1999) of trade considerations and SPS. See Henson et al (2001), which surveys developing countries to identify problems in coping with SPS standards and the issue of developing countries’ ability to
7833994	19696	East & North Africa 34 24 Europe & Central Asia 78 25 Latin America & Caribbean Number of Poor (in millions) % of population in rural areas Source: World Development Indicators (1998), Karanja (2002) It is thus evident that smallholders are important – because a large number of livelihoods depend on small farms, because they constitute a large share of the rural poor and because they account
7833994	19696	benefited by avoidance of repeat processing of loan documents every year and improvement in recovery. The Union Budget 2001-02 set a target to cover all eligible farmers in 3 years. Source: NABARD (2002) Box 2—Columbia: Repos for Livestock financing Colombia’s National Agricultural and Livestock Exchange (BNA) designed an innovative livestock securitization programme in 2000. Under the programme,
7833994	19696	marginal operational differences exist in Zimbabwe (on national stateowned radio) and Mozambique (local stations against payment) as well. All appear to have benefited farmers. Source: Shepherd (2002) 39sTraders have now been able to have their own market information networks (Chaudhury and Banerji, 2001 cited in World Bank, 2002). The concern here is one of sustainability. With MIS being
7833994	19696	public relations image at little opportunity cost, since neither Kenyan nor highland Mexican farmers would have purchased the technologies without the donation. Source: World Development Report (2002) 48sTECHNOLOGY That agriculture has become a hi-tech industry has been recognized for some time now (Josling 1999). Already, bio-technological advances dramatically affect farm-input industries (ex.
7833994	19696	increased by 15 percent between 1976 and 1985. Milk production for La Serenisima jumped by almost 50 per-cent despite a 6 percent decrease in dairy farm areas of suppliers. Source: World Bank (2002) 23 The AATF is a public-private initiative to link needs of resource-poor small farmers in Sub-Saharan Africa to potential technological solutions by acquiring royalty free licenses or agreements
18574	18575	is satisfied. Note that this problem may be infeasible, that is, there does not exist a set of beamforming weights such that each user’s SINR value is larger than the required minimum threshold. In , iterative algorithms are proposed to minimize total transmitted power subject to the constraint that SINR of each user is satisfied for downlink transmissions in a single cell network. In , the
18574	18577	In , iterative algorithms are proposed to minimize total transmitted power subject to the constraint that SINR of each user is satisfied for downlink transmissions in a single cell network. In , the problem of joint beamforming and base station assignment is considered, where each user can be served by any base station in the network. An algorithm that assigns each user to the optimal
18574	18579	manner, and vary in the criteria that determine the order in which users are inserted. This problem is extended to be combined with other multi-user access schemes such as TDMA, OFDM and CDMA in . A common assumption in these studies is infinite packet backlog for any user, i.e., there is always a packet to be served at the queue for each user. The major drawbacks of these works are the
18574	18579	However, these users may not be spatially separable and may not be served together. On the other hand, we may choose a set of compatible users such that the total throughput is maximized (see ,  for examples of such scheduling algorithms). Therefore, there exists a tradeoff between serving the longest queues first and maximizing total instant system throughput. In order to balance the
18574	18579	this may lead to smaller overall queue sizes, and hence in the long run the system may remain stable for a larger set of arrival vectors, potentially at the price of larger delay jitter. In , an algorithm that attempts to maximize the total system throughput is presented. The basic idea is to search through all the users and select the user that is most compatible with already
18574	18580	scheduling policy the user throughput requirements are satisfied and, hence, the long term total system throughput is maximized. Similar queueing systems have been used to model other scenarios in , , , and was first proposed in  for a multihop radio network where the SINR requirement demands that two links can be active simultaneously only if they are separated by certain minimum
18574	18580	some policy is identified. However, the complexity of an optimal scheduling policy increases exponentially with the number of users, and no practical sub-optimal scheduling policy is proposed in , , . In this paper, we follow a similar approach as in , and propose scheduling policies of polynomial complexity that achieve sub-optimal performance for our problem. This paper is
18574	18582	policy the user throughput requirements are satisfied and, hence, the long term total system throughput is maximized. Similar queueing systems have been used to model other scenarios in , , , and was first proposed in  for a multihop radio network where the SINR requirement demands that two links can be active simultaneously only if they are separated by certain minimum required
18574	18582	is identified. However, the complexity of an optimal scheduling policy increases exponentially with the number of users, and no practical sub-optimal scheduling policy is proposed in , , . In this paper, we follow a similar approach as in , and propose scheduling policies of polynomial complexity that achieve sub-optimal performance for our problem. This paper is organized as
18574	18582	A ?A, is that there exists a scheduling policy that achieves A ? D := ? ? (2) S?S ?S R?S cSRR T 1I×1 where cSR, S ?S, R ? S, are nonnegative numbers such that ? R?S cSR =1for all S ?S. Proof: See  for a proof. D. Optimal scheduling policy In this subsection we are interested in finding an optimal scheduling policy that satisfies (1) for each A ?A. In particular, we consider the following
18574	18583	prove the existence of a stationary distribution of X(t) and, hence, the stability of the system, we use the following theorem. 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004sTheorem 1: (, ) For a given Lyapunov function L(X(t)), if there exists a compact region ? of ? J and a constant ?>0 such that 1) E < ? for all X(t) ?? J 2) E ?
294522	18587	favor of new models that explicitly replicate these observed statistics. 1 Examples of these generators include the INET AS-level topology generator , BRITE , BA, AB , GLP, PLRG , and the CMU power-law generator . Each of the aforementioned degree-based topology generators uses one of the following three probabilistic generation methods. The first is preferential
294522	18587	expected node degree sequence follows a power-law, the generated graph’s node degree distribution will exhibit the same power law. The third generation method, the Power Law Random Graph (PLRG) , also attempts to replicate a given (power law) degree sequence. This construction involves forming a set L of nodes containing as many distinct copies of a given vertex as the degree of that
294522	18587	http://www.caida.org/analysis/topology/ router-level-topology.xml. 2 It is believed that the PLRG and GRG models are “basically asymptotically equivalent, subject to bounding error estimates” .sconnected nodes form a central cluster. When using these models to represent the Internet, the presence of these highly connected central nodes in these networks has been touted its “Achilles’
294522	18590	absence of concrete examples of such alternate models, degree-based methods have remained popular representations for large-scale Internet structure. This paper follows the previous arguments of  in favor of the need to explicitly consider the technical drivers of network deployment and growth. In spirit, it delivers for degree-based networks a similar message as  did for the random
294522	15921	the CMU power-law generator . Each of the aforementioned degree-based topology generators uses one of the following three probabilistic generation methods. The first is preferential attachment  which says (1) the growth of the network is realized by the sequential addition of new nodes, and (2) each newly added node connects to some existing nodes preferentially, such that it is more
294522	18593	, and proposed topology generators are often evaluated on the basis of whether or not they can reproduce the same types of macroscopic statistics, especially power law-type degree distributions . Yet, from our viewpoint, this perspective is both incomplete and in need for corrective action. For one, there exist many different graphs having the same distribution of node degree, some of
294522	18593	in favor of new models that explicitly replicate these observed statistics. 1 Examples of these generators include the INET AS-level topology generator , BRITE , BA, AB , GLP, PLRG , and the CMU power-law generator . Each of the aforementioned degree-based topology generators uses one of the following three probabilistic generation methods. The first is
294522	18593	and evaluating network topologies have been dominated by graph-theoretic quantities and their statistical properties, e.g., node-degree distribution, expansion, resilience, distortion and hierarchy . However we claim here that these metrics are inherently inadequate to capture the essential tradeoffs of explicitly engineered networks. Node degree distribution. In general, there are many
294522	7172	The use of this type of random graph model was later abandoned in favor of models that explicitly introduce non-random structure, particularly hierarchy and locality, as part of the network design . The argument for this type of approach was based on the fact that an inspection of real networks shows that they are clearly not random but do exhibit certain obvious hierarchical features. This
294522	7172	We believe the answer to this question lies in the absence of a concrete methodological approach for understanding and evaluating structures like the Internet’s router-level topology. Building on , this work presents such an approach and illustrates it with alternate models that represent a clear paradigm shift in terms of identifying and explaining the cause-effect relationships present in
294522	18594	California Institute of Technology doyle@cds.caltech.edu components, whether they be machines in the router-level graph  or entire subnetworks (Autonomous Systems) in the ASlevel graph . A particular feature of network connectivity that has generated considerable discussion is the prevalence of heavy-tailed distributions in node degree (e.g., number of connections) and whether or
294522	18596	the key features of preferential attachment and is amenable to rigorous mathematical analysis, we refer to  and references therein. The second generation method is due to Chung and Lu  who considered a general model of random graphs (GRG) with a given expected degree sequence. The construction proceeds by first assigning each node its (expected) degree and then probabilistically
294522	18596	a more traditional interpretation as likelihood and relative likelihood, respectively, associated with the general model of random graphs (GRG) with a given expected degree sequence considered in . The GRG model is concerned with random graphs with given expected node degree sequence ? = (?1, · · · ?n) for vertices 1, · · · , n. The edge between vertices i and j is chosen independently with
294522	19725	of topology-related network measurements, significant efforts by the networking community are yielding an emerging picture of the large-scale statistical properties of these topologies . The development of abstract, yet informed, models for network topology evaluation and generation has followed the work of empiricists. The first popular topology generator to be used for
294522	19725	campus networks. Similar observations are found when examining (where available) topology-related information of global, national, or regional commercial ISPs. In view of recent measurement studies , it is important to recognize that the use of technologies at layers other than IP will affect what traceroute-like experiments can measure. For example, the use of shared media at Layer 2 (e.g.
294522	18597	The use of this type of random graph model was later abandoned in favor of models that explicitly introduce non-random structure, particularly hierarchy and locality, as part of the network design . The argument for this type of approach was based on the fact that an inspection of real networks shows that they are clearly not random but do exhibit certain obvious hierarchical features. This
294522	18598	followed the work of empiricists. The first popular topology generator to be used for networking simulation was the Waxman model , which is a variation of the classical Erdös-Rényi random graph . The use of this type of random graph model was later abandoned in favor of models that explicitly introduce non-random structure, particularly hierarchy and locality, as part of the network design
294522	18598	in that it can generate graphs with a power law node degree distribution if the given expected degree sequence ? conforms to a power law, or it can generate the classic Erdös-Rényi random graphs  by taking the expected degree sequence ? to be (pn, pn, · · · , pn). As a result of choosing each edge (i, j) ? E(g) with a probability that is proportional to ?i?j, in the GRG model, different
294522	7175	California Institute of Technology doyle@cds.caltech.edu components, whether they be machines in the router-level graph  or entire subnetworks (Autonomous Systems) in the ASlevel graph . A particular feature of network connectivity that has generated considerable discussion is the prevalence of heavy-tailed distributions in node degree (e.g., number of connections) and whether or
294522	18599	to it. This assumption allows for good bandwidth utilization of higher level routers 6 . While 6 We also tried choosing the traffic demand between routers as the product of their degrees as in , and qualitatively similar perforother performance metrics may be worth considering, we claim that maximum throughput achieved using the gravity model provides a reasonable measure of the network
294522	18600	node degree, they have often been abandoned in favor of new models that explicitly replicate these observed statistics. 1 Examples of these generators include the INET AS-level topology generator , BRITE , BA, AB , GLP, PLRG , and the CMU power-law generator . Each of the aforementioned degree-based topology generators uses one of the following three probabilistic
294522	3026	they have often been abandoned in favor of new models that explicitly replicate these observed statistics. 1 Examples of these generators include the INET AS-level topology generator , BRITE , BA, AB , GLP, PLRG , and the CMU power-law generator . Each of the aforementioned degree-based topology generators uses one of the following three probabilistic generation
294522	18602	considerable discussion is the prevalence of heavy-tailed distributions in node degree (e.g., number of connections) and whether or not these heavy-tailed distributions conform to power laws . This macroscopic statistic has greatly influenced the generation and evaluation of network topologies. In the current environment, degree distributions and other large-scale statistics are popular
294522	18602	were reported by Faloutsos et al. . Since then, the identification and explanation of power laws has become an increasingly dominant theme in the recent body of network topology literature . Since the GT-ITM topology generators fail to produce power laws in node degree, they have often been abandoned in favor of new models that explicitly replicate these observed statistics. 1
294522	18603	considerable discussion is the prevalence of heavy-tailed distributions in node degree (e.g., number of connections) and whether or not these heavy-tailed distributions conform to power laws . This macroscopic statistic has greatly influenced the generation and evaluation of network topologies. In the current environment, degree distributions and other large-scale statistics are popular
294522	18605	is the essence of the so-called scale-free network models, which have been a popular theme in the study of complex networks, particularly among researchers inspired by statistical physics . However, this emphasis on power laws and the resulting efforts to generate and explain them with the help of these degree-based methods have not gone without criticism. For example, there is a
294522	18606	from five connections (in Los Angeles) to twelve connections (in New York). Abilene maintains peering connections 5 Of the approximate 80,000 - 140,000 terabytes per month of traffic in 2002 , Abilene carried approximately 11,000 terabytes of total traffic for the year . Here, “carried” traffic refers to traffic that traversed an Abilene router. Since Abilene does not peer with
294522	18607	replicate these observed statistics. 1 Examples of these generators include the INET AS-level topology generator , BRITE , BA, AB , GLP, PLRG , and the CMU power-law generator . Each of the aforementioned degree-based topology generators uses one of the following three probabilistic generation methods. The first is preferential attachment  which says (1) the growth of
294522	9918	vulnerable to attacks that target the high-degree hub nodes . It has been similarly argued that these high-degree hubs are a primary reason for the epidemic spread of computer worms and viruses . The presence of highly connected central nodes in a network having a power law degree distribution is the essence of the so-called scale-free network models, which have been a popular theme in the
294522	18609	of topology-related network measurements, significant efforts by the networking community are yielding an emerging picture of the large-scale statistical properties of these topologies . The development of abstract, yet informed, models for network topology evaluation and generation has followed the work of empiricists. The first popular topology generator to be used for
294522	7186	and evaluation of network topologies. In the current environment, degree distributions and other large-scale statistics are popular metrics for evaluating how representative a given topology is , and proposed topology generators are often evaluated on the basis of whether or not they can reproduce the same types of macroscopic statistics, especially power law-type degree distributions
294522	7186	and evaluating network topologies have been dominated by graph-theoretic quantities and their statistical properties, e.g., node-degree distribution, expansion, resilience, distortion and hierarchy . However we claim here that these metrics are inherently inadequate to capture the essential tradeoffs of explicitly engineered networks. Node degree distribution. In general, there are many
294522	7186	degree-preserving rewiring is a means for moving within a general “space of network graphs,” all having the same overall degree distribution. Expansion, Resilience, Distortion. Introduced in , these metrics are intended to differentiate important aspects of topology. Expansion is intended to measure the ability of a node to “reach” other nodes within a given distance (measured by hops),
294522	7186	topology. For each of these three metrics, a topology is characterized as being either “Low” (L) or “High” (H). Yet, the quantitative values of expansion, resilience, and distortion as presented in  are not always easy to interpret when comparing qualitatively different topologies. For example, the measured values of expansion for the AS-level and router-level topologies show a relatively big
294522	7186	with measured topologies. In contrast, it could be argued that Tiers generates topologies whose expansion values match that of the measured router-level graph reasonably well (Figure 2(g) in ), but Tiers is classified to have “Low” expansion. Such problems when interpreting these metrics make it difficult to use them for evaluating differences in topologies in a consistent and coherent
294522	18611	were reported by Faloutsos et al. . Since then, the identification and explanation of power laws has become an increasingly dominant theme in the recent body of network topology literature . Since the GT-ITM topology generators fail to produce power laws in node degree, they have often been abandoned in favor of new models that explicitly replicate these observed statistics. 1
294522	18612	The use of this type of random graph model was later abandoned in favor of models that explicitly introduce non-random structure, particularly hierarchy and locality, as part of the network design . The argument for this type of approach was based on the fact that an inspection of real networks shows that they are clearly not random but do exhibit certain obvious hierarchical features. This
294522	18612	previous arguments of  in favor of the need to explicitly consider the technical drivers of network deployment and growth. In spirit, it delivers for degree-based networks a similar message as  did for the random graph-type models  that were popular with networking researchers in the early 1990s. While  identified and commented on the inherent limitations of the various constructs
294522	18612	We believe the answer to this question lies in the absence of a concrete methodological approach for understanding and evaluating structures like the Internet’s router-level topology. Building on , this work presents such an approach and illustrates it with alternate models that represent a clear paradigm shift in terms of identifying and explaining the cause-effect relationships present in
12121133	419	algorithm. Although the details of EM are beyond the scope of this paper, the resulting algorithm is easily described (for a description of EM and applications to filling in missing values, see  and ): 1. Build the models using only the labeled data (as in Section 2). 2. Use the models to probabilistically label the unlabeled images. 3. Using the images with the
12121133	18620	is interesting to note that with the more complex models, such as the dependency trees or local dependence networks, even with the same amount of labeled data, unlabeled data improved performance.  have reported similar performance degradation when using a large number of labeled examples and EM with a naive-Bayesian model to classify text documents. They describe two methods for overcoming
12121133	18620	the effective incorporation of unlabeled data into image classification procedures; it should be possible to use unlabeled data in any of these tasks. The closest related work is presented in . They used naive-Bayes methods to classify text documents into a pre-specified number of groups. By using unlabeled data, they achieve significant classification performance improvement over using
18623	483	chemical and physical properties. These sensor nodes will perform significant signal processing, computation, and network self-configuration to achieve scalable, robust and long-lived networks . More specifically, sensor nodes will do local processing to reduce communications, and consequently, energy costs. Akyildiz et al.  provide a comprehensive overview of different aspects of
18623	4875	required for multihop communication (adaptive duty cycling). The second is data reduction through in-network processing (also called data aggregation), whereby correlations in data are exploited  to reduce the size of data, and correspondingly communication cost. The large number of nodes expected in sensor network deployments, and the unpredictable nature of deployment conditions introduce
18623	4875	efficient as well, since sensor networks are expected to have bursty traffic. We describe different approaches to such routing schemes in the context of sensor networks. Early Diffusion schemes , and TinyDB construct opportunistic routing trees to route data to the sink or base-station. Recent research in localization schemes have, however, made it likely that cheap and precise location
18623	4875	of a closed polygonal region in clock-wise edge order, in this case,s¢¡¤£¦¥§¡¤£©¨?¡¤£¦? the nodes inside a target region, which is a common primitive in data-centric sensor network applications . The protocol has two distinctive features: It uses an energy-aware neighbor selection heuristic to forward packets. Nodes with more remaining energy are preferred over depleted nodes to avoid
18623	4875	readings from some sensors could be dumped to debug the collaborative event detection algorithm between nearby nodes. Dump can be implemented as an generic application over Directed Diffusion or other data management frameworks . Because the amount of data per node may be large, dump should be invoked only at small spatial scales (i.e., from a few nodes), and only when there is a
18623	18625	required for multihop communication (adaptive duty cycling). The second is data reduction through in-network processing (also called data aggregation), whereby correlations in data are exploited  to reduce the size of data, and correspondingly communication cost. The large number of nodes expected in sensor network deployments, and the unpredictable nature of deployment conditions introduce
18623	18625	Study: Building a Distributed Storage Framework We consider the application of the networking schemes described above in the context of a distributed data storage and querying framework, Dimensions . This system is targeted at long-term scientific deployments, such as micro-climate monitoring , to obtain data about previously unobservable phenomena for detailed analysis by experts in
18623	18625	of the methodology used in Dimensions to tackle these problems, and then discuss different ways in which networking techniques fit into such a system. 7 Dimensions Architectural Overview Dimensions  constructs progressively lossy hierarchy of wavelet summaries of sensor data and distributes them around the network. These summaries are generated in a multi-resolution manner, i.e. there are
18623	15310	backbones, and enable multi-hop routing while powering down radios for power conservation? Two techniques that have been explored to solve this problem are Adaptive duty-cycling (S-MAC , ASCENT , SPAN ), where the set of nodes whose radios are powered down are carefully chosen such that a network backbone is continually maintained, while non-backbone nodes can put their
18623	15310	proposed three sleep schemes to improve the 802.11 PS mode. None of them requires node synchronizations. The cost is more frequent beaconing and more wake-up packets before broadcasts. S-MAC  is a contention MAC with integrated low-duty-cycle operation that supports multi-hop operation. More details of S-MAC are described in Section 2.2. The last catagory of adaptive duty cycling is
18623	15310	protocol. In ASCENT, each node makes the decision only based on locally measured packet loss and connectivity information. More details of ASCENT are described in Section 2.3 2.2 S-MAC S-MAC  explores design trade-offs for energy-conservation in the MAC layer. It reduces energy consumption on radio from the following sources: collision, control overhead, overhearing unnecessary traffic,
18623	15310	periodicity of communication is fixed, since summaries are generated every epoch. These requirements lend the problem to be particularly well-suited for a scheduling-based protocol such as S-MAC . The advantages of 23susing SMAC for long messages can be exploited to send the summaries efficiently. Routing queries does not require large data transfers, but have two features: (a) they can be
18623	18626	backbones, and enable multi-hop routing while powering down radios for power conservation? Two techniques that have been explored to solve this problem are Adaptive duty-cycling (S-MAC , ASCENT , SPAN ), where the set of nodes whose radios are powered down are carefully chosen such that a network backbone is continually maintained, while non-backbone nodes can put their
18623	18626	proposed three sleep schemes to improve the 802.11 PS mode. None of them requires node synchronizations. The cost is more frequent beaconing and more wake-up packets before broadcasts. S-MAC  is a contention MAC with integrated low-duty-cycle operation that supports multi-hop operation. More details of S-MAC are described in Section 2.2. The last catagory of adaptive duty cycling is
18623	18626	protocol. In ASCENT, each node makes the decision only based on locally measured packet loss and connectivity information. More details of ASCENT are described in Section 2.3 2.2 S-MAC S-MAC  explores design trade-offs for energy-conservation in the MAC layer. It reduces energy consumption on radio from the following sources: collision, control overhead, overhearing unnecessary traffic,
18623	18626	cost. It enables each node to adaptively switch mode according to the traffic in the network. The overall gain on energy savings is much larger than the performance loss on latency and throughput . 2.3 ASCENT: Adaptive Self-Configuring sEnsor Networks Topologies To motivate the need for ASCENT, consider a habitat monitoring sensor network that is to be deployed in a remote forest. Deployment
18623	15311	and enable multi-hop routing while powering down radios for power conservation? Two techniques that have been explored to solve this problem are Adaptive duty-cycling (S-MAC , ASCENT , SPAN ), where the set of nodes whose radios are powered down are carefully chosen such that a network backbone is continually maintained, while non-backbone nodes can put their radios to
18623	15311	network provides for energy savings. The basic idea is to only power on a small number of nodes that are sufficient to maintain network connectivity. Examples include GAF  SPAN  and ASCENT . GAF utilizes geographic location information, and divides the network into fixed square grids. Within each grid, nodes are equivalent from the routing point of view, so only one node needs to be
18623	15311	are: The use of adaptive techniques that permit applications to configure the underlying topology based on their needs while trying to save energy to extend network lifetime. Results presented in  show that ASCENT always improves network lifetime (defined as time till 90% of network runs out of power), and for large densities is a factor of 3 better than a network not running the protocol.
18623	15311	higher cost. For the second case, a network backbone would need to be continually maintained, since user queries can be injected into the network anytime and from anywhere. A protocol like ASCENT , that maintains a network routing topology continually, would enable such querying, while being robust to environmental dynamics. Debugging the network deployment The following example can
18623	5013	enable multi-hop routing while powering down radios for power conservation? Two techniques that have been explored to solve this problem are Adaptive duty-cycling (S-MAC , ASCENT , SPAN ), where the set of nodes whose radios are powered down are carefully chosen such that a network backbone is continually maintained, while non-backbone nodes can put their radios to sleep. To
18623	5013	that a dense network provides for energy savings. The basic idea is to only power on a small number of nodes that are sufficient to maintain network connectivity. Examples include GAF  SPAN  and ASCENT . GAF utilizes geographic location information, and divides the network into fixed square grids. Within each grid, nodes are equivalent from the routing point of view, so only one
18623	15312	To load-balance, and thus prevent nodes from dying, the active subset of nodes are adaptively cycled, based on parameters such as available energy, radio coverage, etc. Wakeup on demand (STEM , Wake-on-Wireless ): This technique uses nodes with multiple radios, a low-power radio (such as a mote radio ) that is used exclusively to wake up the high power radio (such as 802.11) when
18623	15312	time a node wakes up, it broadcasts a beacon including its own ID. If other nodes want to talk to this node, they need to wake up and listen until receiving the beacon. Another example is STEM . STEM uses two radios operating in different channels, one for data transmission and the other for node wake-up. When there are no data to send, nodes turn off their data radio completely, and put
18623	15312	at different levels, and (b) to route queries in a drill-down manner. In the first case, latency is not a concern, since this is a background, slow-running process. Thus, a scheme like STEM  would not be necessary. Further, the periodicity of communication is fixed, since summaries are generated every epoch. These requirements lend the problem to be particularly well-suited for a
18623	18627	and thus prevent nodes from dying, the active subset of nodes are adaptively cycled, based on parameters such as available energy, radio coverage, etc. Wakeup on demand (STEM , Wake-on-Wireless ): This technique uses nodes with multiple radios, a low-power radio (such as a mote radio ) that is used exclusively to wake up the high power radio (such as 802.11) when the need arises, much
18623	5746	such as available energy, radio coverage, etc. Wakeup on demand (STEM , Wake-on-Wireless ): This technique uses nodes with multiple radios, a low-power radio (such as a mote radio ) that is used exclusively to wake up the high power radio (such as 802.11) when the need arises, much like a paging channel in cellular networks. Such a technique is especially useful in ad-hoc
18623	5746	reduction techniques. 3sThe second challenge is resource constraints on embedded sensor nodes. The capabilities of these devices can range from Ipaq-class devices, to highly constrained mica motes . Traditional database methods are resource-intensive, whereas data management schemes for sensor networks need to function on resource-limited nodes. Geographic Routing in Sensor Networks Routing
18623	5746	a potentially more efficient global optimization of in-network query processing. TinyDB is a query processing system with small footprint intended for highly resource-constrained mote sensor nodes . TinyDB provides a declarative interface for data collection and aggregation inspired by selection and aggregation facilites in database query languages. TinyDB’s aggregation service, Tag (Tiny
18623	9607	on data about the physical world, hence their use is expected to be highly data-centric. While many attributebased naming mechanisms have been proposed over an IP framework for the internet (eg: ), sensor networks pose unique challenges due to their resource constraints. Foremost among these is the disparity between the amount of data generated by sensors and the amount of data that a
18623	9607	approaches bear mention due to their relation to naming sensor data. The Intentional Naming System is an attribute-based name system operating in an overlay network over the Internet . Its use of attributes as a structuring mechanism and a method to cope with dynamically locating devices is similar in spirit to 12sScheme Type Platform Query Specification Language Directed
18623	18629	on data about the physical world, hence their use is expected to be highly data-centric. While many attributebased naming mechanisms have been proposed over an IP framework for the internet (eg: ), sensor networks pose unique challenges due to their resource constraints. Foremost among these is the disparity between the amount of data generated by sensors and the amount of data that a
18623	18629	ad-hoc routing protocol reverse multicast tree - all leaves are data sources DataSpace describes an attribute based naming mechanism for querying physical objects that produce and store local data . The DataSpace is divided into smaller administrative and logical datacubes, which are logically grouped into dataflocks. Query results may involve aggregation of more specific queries addressed to
18623	13119	nodes always keep their radios on, since the radio is a major energy consumer. Measurements have shown that a typical radio consumes the similar level of energy in idle mode as in receiving mode . It is important that nodes are able to operate in low duty cycles. Current research on adaptive duty cycling can be broadly divided into three catagories: general schemes of sleep and wake-up, low
18623	18632	focuses on how to set up the communications between two nodes that are normally in sleep mode. These schemes are not tightly coupled with other protocols such as MAC and topology control. Piconet  is such an example. In Piconet, each node randomly goes into sleep mode, and periodically wakes up for a short period time. Every time a node wakes up, it broadcasts a beacon including its own ID.
18623	3696	MACs naturally enable low-duty-cycle operations on nodes, since they only need to turn on their radio during their own time slots for sending and receiving. Typical examples include Bluetooth  and LEACH . In both protocols, nodes form clusters, and TDMA is used for intra-cluster communications. The major disadvantage of TDMA protocols is the scalability, i.e.,, it is difficult 6sto
18623	18633	is difficult 6sto dynamically change the frame size or the number of slots when the number of nodes changes. For example, Bluetooh may have at most 8 active nodes in a cluster. Sohrabi and Pottie  proposed a self-organization protocol for wireless sensor networks. Each node maintains a TDMA-like frame, in which the node schedules different time slots to communicate with its known neighbors.
18623	18635	time. 802.11 assumes all nodes are within one hop. In multi-hop operation, the PS mode may have problems in clock synchronization, neighbor discovery and network partitioning, as pointed out in . Tseng et al.  proposed three sleep schemes to improve the 802.11 PS mode. None of them requires node synchronizations. The cost is more frequent beaconing and more wake-up packets before
18623	18636	benefits that a dense network provides for energy savings. The basic idea is to only power on a small number of nodes that are sufficient to maintain network connectivity. Examples include GAF  SPAN  and ASCENT . GAF utilizes geographic location information, and divides the network into fixed square grids. Within each grid, nodes are equivalent from the routing point of view, so
18623	5749	provides a declarative interface for data collection and aggregation inspired by selection and aggregation facilites in database query languages. TinyDB’s aggregation service, Tag (Tiny Aggregation ), operates on a routing tree structure, where the root is typically a base-station or other egress point to users, and leaves of the tree consititute all nodes in the network. SELECT AVG(volume),
18623	2305	then geographic queries can be leveraged to constrict the data dissemination to the relevant region and to reduce routing control overhead. Geographic Routing protocols such as GPSR (Karp et al ) or GEAR (Yu et al ) can be used to optimize the process of finding sources by using geographic information to constrain the search process. Since geographical routing does not have to be tied
18623	2305	a restricted flooding search to navigate around holess. One drawback of this mechanism is the difficulty in determining an appropriate scope for the search. Greedy Perimeter Stateless Routing (GPSR ), elegantly avoids this problem by deriving a planar graph out of the original network graph. When a packet reaches a region where greedy forwarding is impossible, GPSR recovers by forwarding the
18623	2305	Given such a sub-graph, perimeter forwarding can be done in a straghtforward manner using the Right Hand traversal rule. A detailed treatment of the planarization procedure can be obtained from . The complete GPSR algorithm combines greedy forwarding on the full network topology, with the perimeter forwarding on the subgraph when greedy forwarding becomes impossible. By keeping state only
18623	18640	can be leveraged to constrict the data dissemination to the relevant region and to reduce routing control overhead. Geographic Routing protocols such as GPSR (Karp et al ) or GEAR (Yu et al ) can be used to optimize the process of finding sources by using geographic information to constrain the search process. Since geographical routing does not have to be tied to any particular data
18623	18640	to circumvent communication holes. We discuss this scheme in more detail in Section 4.2. While other geographic protocols address point-to-point routing, GEAR (Geographic and Energy-Aware Routing)  studies the problem of forwarding a packet to all a communication hole is when greedy forwarding reaches a local maximum where the current node is closer to the destination than any of its
18623	18640	thus limiting the receive power expended by nodes. GEAR (Geographic and Energy-Aware Routing) has been used to extend Directed Diffusion when node locations and geographic queries are present . 4.2 Greedy Perimeter Stateless Routing (GPSR) GPSR offers two benefits. First, it is stateless, and requires propagation of topology information only within a single hop neighborhood. Such a
18623	18642	those proposals share two common design principles: localized network state exchange and innetwork aggregation. For example, the coverage problem in wireless sensor networks is studied in . By exchanging sensor coverage information within a local neighborhood, these techniques detect maximal breach path and maximal support path, along which there is poorest and best coverage of
18623	18644	coverage information within a local neighborhood, these techniques detect maximal breach path and maximal support path, along which there is poorest and best coverage of sensors, respectively. In , node failure information is collected using similar techniques. By limiting state exchange of nodes to only their local vicinity, these protocols successfully monitor collective network states
18623	18645	limiting state exchange of nodes to only their local vicinity, these protocols successfully monitor collective network states without communicating individual node state over long distance. eScan  takes advantage of in-network aggregation to construct an aggregated map of the remaining energy levels for different regions in a sensor field. Instead of extracting individual node states, the
18623	18645	with low communication overhead. Some protocols are designed to self-regulate monitoring activity when network workload is heavy or at least provide a “knob” for adaptive aggregation. In eScan , resolution (how close nodes are geographically nearby) and tolerance (how similar their energy levels are) can control the extent of aggregation. In STREAM 19sFig. 6. Monitoring wireless sensor
18623	18645	be even more robust than generic applications because they might be the last resort when a massive failure happens. 5.2 A Monitoring Architecture for Wireless Sensor Networks In this section, eScan and digest are illustrated as two examples of sensor network monitoring solutions. These tools can be combined into a coherent architecture for monitoring sensor networks . This
18623	18645	the entire network, or throughout a significant section of the network. Thus, this class of tools has a significantly greater spatial extent than dumps. One example of a scan is the eScan . To compute an eScan, a special user-gateway node initiates collection of node state, for instance residual energy supply level, from every node in the system. Instead of delivering the raw data to
18623	18645	less communication cost as compared to centralized collection. From such a global view, users are able to isolate those nodes upon which they can invoke tools such as dump. Simulation studies in  reveal that good aggregation benefit (˜15% data reduction) is obtained with low distortion (˜5%) in the energy scans collected from a 400 node randomly placed network Clearly, the energy cost of
18623	18647	and tolerance (how similar their energy levels are) can control the extent of aggregation. In STREAM 19sFig. 6. Monitoring wireless sensor Fig. 7. Representation and Aggrenetworks gation of eScans. , topology mapping of sensor networks can be collected at different levels of detail, which is decided by the parameters of “virtual range” and “resolution factor”. Note that none of techniques
18623	18648	than generic applications because they might be the last resort when a massive failure happens. 5.2 A Monitoring Architecture for Wireless Sensor Networks In this section, eScan and digest are illustrated as two examples of sensor network monitoring solutions. These tools can be combined into a coherent architecture for monitoring sensor networks . This architecture is
18623	18648	low distortion (˜5%) in the energy scans collected from a 400 node randomly placed network Clearly, the energy cost of collecting an eScan can be significant, and our third class of tools, digests, can help alert users to error conditions (partitions, node deaths) within the network. A digest is an aggregate of some network property in small size (say a few bytes). For example, the size of
18623	18648	design a networked debugging scheme to tackle such problems? A suite of debugging tools such as eScan can be appropriately used to solve the problems. For the above instance, periodic digests can provide clues to the engineer to help debug the problem. Geographic Data Forwarding The first step in constructing the distributed storage structure is the selection of cluster nodes that are
18623	14015	above in the context of a distributed data storage and querying framework, Dimensions . This system is targeted at long-term scientific deployments, such as micro-climate monitoring , to obtain data about previously unobservable phenomena for detailed analysis by experts in the field. Consider the following sample deployment: A medium scale wireless sensor network (many
18623	14015	summarized research in networking techniques for sensor networks, focusing on representative approaches. As the sensor network community moves out of its infancy, and embark on real deployments , many of these networking schemes that have been developed are being put to test in real environments. Each application domain will, no doubt, introduce novel challenges, and involve
18623	18650	the distributed storage structure is the selection of cluster nodes that are responsible for data storage at different resolutions. Dimensions builds on prior work in Data Centric Storage (DCS ) that constructs a Distributed Hash Table (DHT ) to hash the name of a certain event key (dataName) to a location within the network. Each node in the network uses the same hash function, and
18623	18650	first concern, i.e. to forward data to the destination location obtained from the hash function. To address the second problem, the perimeter forwarding strategy in GPSR is cleverly used in DCS () to consistently deliver data addressed to a location to the node closest to the target location. Thus, the storage application does not need to deal with this translation. Drill-down Queries
18623	18651	of cluster nodes that are responsible for data storage at different resolutions. Dimensions builds on prior work in Data Centric Storage (DCS ) that constructs a Distributed Hash Table (DHT ) to hash the name of a certain event key (dataName) to a location within the network. Each node in the network uses the same hash function, and thus global consensus on the mapping between
8796899	18661	27 28 29 30 19 20 21 22 23 24 13 14 15 16 17 18 7 8 9 10 11 12 1 2 3 4 5 6 Fig. 1. Layout of a 36 cell network We use two models for the behavior of the users at the edge of a network. Edge effects  describe the break in the symmetry of the cells at the edge of the network. The cells at the edge exhibit a different behavior than the cells towards the center. This is because the cells at the
8796899	2722	model: We use the random walk mobility model for the user movement except in the case of the preferential mobility hotspots, where we use a modified version of the random waypoint mobility model (, ). In the original mobility model, the choice of the destination is completely random. However, in our study, we need to be able to mimic the movements of users going towards a hotspot. To
8796899	2722	We use the random walk mobility model for the user movement except in the case of the preferential mobility hotspots, where we use a modified version of the random waypoint mobility model (, ). In the original mobility model, the choice of the destination is completely random. However, in our study, we need to be able to mimic the movements of users going towards a hotspot. To
8921413	18720	Martin Rajman Artificial Intelligence Laboratory Information and Communication Sciences Faculty EPFL, Lausanne July 9th, 2004 Abstract This paper exposes the Rapid Dialogue Prototyping Methodology , a methodology allowing the easy and automatic derivation of an ad hoc dialogue management system from a specific task description. The goal of the produced manager is to provide the user with a
8921413	18720	. . . . . . . . . . . . 19 4.3 Further experimentation . . . . . . . . . . . . . . . . . . . . . . . 19 5 Conclusions and Further Work 20 2s1 Introduction The Rapid Dialogue Prototyping Methodology  (or RDPM) aims at creating task-oriented dialogue models according to the following general idea: the target dialogue model is a finite-state model that can be easily and systematically derived
18722	18723	in experiments where changes in emphasis induce changes in the duration of the segments of the target words. There is already evidence that segment lengthening, e.g. under focus, is non-linear (see Heldner 2001) and we expect similar effects for the chanted call. This should also help to explore some more anchor points of the tune timing. Furthermore, we will transfer the chanted call pattern to other
18726	943	of relay nodes until they reach one or more destinations—have received a good deal of attention from the research community in recent years. Examples include computed-routing systems such as Chord , CAN , Tapestry  and many others; rendezvous/indirection systems such as i3 ; publish-subscribe systems such as Siena  and intentional multicast . All of these systems assume the
18726	943	we also simulated a network in which servers are arranged in a regular logical topology, with well-defined routes between nodes in the overlay. Examples of this type of overlay include Chord , CAN , Tapestry , and Pastry . We use Tapestry  as the representative structured overlay. In Tapestry each node is assigned a unique ID, which is structured as a number written in some
18726	943	Infrastructure (i3)  is another generic overlay service that can also subsume a number of existing services. It achieves such power by routing over distributed hash table (such as Chord ) using rendezvous points. To get to a destination, a packet may have to travel throughout multiple rendezvous points. Our speccast solution trades band0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE
18726	939	nodes until they reach one or more destinations—have received a good deal of attention from the research community in recent years. Examples include computed-routing systems such as Chord , CAN , Tapestry  and many others; rendezvous/indirection systems such as i3 ; publish-subscribe systems such as Siena  and intentional multicast . All of these systems assume the existence of
18726	939	also simulated a network in which servers are arranged in a regular logical topology, with well-defined routes between nodes in the overlay. Examples of this type of overlay include Chord , CAN , Tapestry , and Pastry . We use Tapestry  as the representative structured overlay. In Tapestry each node is assigned a unique ID, which is structured as a number written in some base b.
18726	9796	reach one or more destinations—have received a good deal of attention from the research community in recent years. Examples include computed-routing systems such as Chord , CAN , Tapestry  and many others; rendezvous/indirection systems such as i3 ; publish-subscribe systems such as Siena  and intentional multicast . All of these systems assume the existence of an efficient
18726	9796	a network in which servers are arranged in a regular logical topology, with well-defined routes between nodes in the overlay. Examples of this type of overlay include Chord , CAN , Tapestry , and Pastry . We use Tapestry  as the representative structured overlay. In Tapestry each node is assigned a unique ID, which is structured as a number written in some base b. The number of
18726	9697	years. Examples include computed-routing systems such as Chord , CAN , Tapestry  and many others; rendezvous/indirection systems such as i3 ; publish-subscribe systems such as Siena  and intentional multicast . All of these systems assume the existence of an efficient underlying unicast addressing and routing system (viz., the Internet) on which they introduce their own
18726	9697	It is similar to content distribution and publish/subscribe systems in which content is published to the overlay and the data is then retrieved via the overlay (examples include systems like Siena , Intentional Naming System , and Gryphon ). We simulated the broker overlay as a set of servers. Clients that have content to publish inform a nearby broker (server) who advertises the
18726	9697	the conjunction, for each outgoing interfaces. VI. RELATED WORK The work most closely related to speccast deals with socalled publish-subscribe systems. A number of such systems have been proposed , , . Speccast resembles such systems in its use of an association between predicates and nodes to define the semantics of message delivery. However, their problem statement (as defined, for
18726	9607	systems such as Chord , CAN , Tapestry  and many others; rendezvous/indirection systems such as i3 ; publish-subscribe systems such as Siena  and intentional multicast . All of these systems assume the existence of an efficient underlying unicast addressing and routing system (viz., the Internet) on which they introduce their own higher-level address space and
18726	9607	and publish/subscribe systems in which content is published to the overlay and the data is then retrieved via the overlay (examples include systems like Siena , Intentional Naming System , and Gryphon ). We simulated the broker overlay as a set of servers. Clients that have content to publish inform a nearby broker (server) who advertises the content on behalf of the client.
18726	9607	predicates: delay vs. number of disjuncts. The pub-sub model gives the receiver ultimate control over comes to it, while the speccast model gives the sender control. Intentional multicast in INS  solves a problem similar to our speccast problem by routing in a shared-tree overlay. INS is a special case of the system that we simulate in the case of idealized broker overlay. Because of the
18726	18727	in Figure 1. It is therefore possible for a solution to achieve network load ratios less than one. This effect has been observed before in “shared tree” multicast protocols such as CBT and PIM=SM .) • Delay. We define the delay cost of a routing solution to be the sum, over all nodes d satisfying p, of the number of edges traversed on the path from n to d, when that solution is used. We
18726	18727	load when speccast is used for multicast delivery. The results confirm expectations: Because speccast constructs a shared distribution tree, it performs much like a core-based multicast tree , reducing the network load (ratio to SPF-T below 1) at the expense of longer delays (stretch > 1). Figures 10 and 11 show the delay for multi-unicast services with and without locality. 3 As the
18726	9608	systems in which content is published to the overlay and the data is then retrieved via the overlay (examples include systems like Siena , Intentional Naming System , and Gryphon ). We simulated the broker overlay as a set of servers. Clients that have content to publish inform a nearby broker (server) who advertises the content on behalf of the client. Similarly, a client
18726	9608	for each outgoing interfaces. VI. RELATED WORK The work most closely related to speccast deals with socalled publish-subscribe systems. A number of such systems have been proposed , , . Speccast resembles such systems in its use of an association between predicates and nodes to define the semantics of message delivery. However, their problem statement (as defined, for example, by
18726	940	servers are arranged in a regular logical topology, with well-defined routes between nodes in the overlay. Examples of this type of overlay include Chord , CAN , Tapestry , and Pastry . We use Tapestry  as the representative structured overlay. In Tapestry each node is assigned a unique ID, which is structured as a number written in some base b. The number of digits needed in
18726	18730	conjunction, for each outgoing interfaces. VI. RELATED WORK The work most closely related to speccast deals with socalled publish-subscribe systems. A number of such systems have been proposed , , . Speccast resembles such systems in its use of an association between predicates and nodes to define the semantics of message delivery. However, their problem statement (as defined, for
18726	9610	such systems in its use of an association between predicates and nodes to define the semantics of message delivery. However, their problem statement (as defined, for example, by Carzaniga and Wolf ) is different in that their predicates apply to datagrams rather than nodes. That is, they postulate a set D of datagrams, and a set F of predicates on datagrams. Associated with each node is a
18726	18731	information about the “density” of atomic propositions, i.e. the fraction of nodes that satisfy each one. This idea has previously been explored in the context of a still another pub-sub system . The study explores different approaches of using multicast for broker-based publish-subscribe systems, and considers the effect of locality (“regionalism of attributes”). One of the main results
18732	18734	queries coming form the network, etc. As a result, an opportunity for developing a software layer which will manage effectively databases in p2p network is considered. Local Relational Model (LRM)  provides us with a foundation to base our research on. This research formally establishes the validity of multiple local schemas and proposes a theoretical method to establish mappings between
18732	18734	the space of available data within the P2P network as an open collection of possibly overlapping and inconsistent databases. To meet these challenges is designed a Local Relational Model (LRM) . The LRM has an assumption that the set of all data in a P2P network consists of local (relational) databases, each with a set of acquaintances (peers). In the p2p network, peers are fully
18732	18734	from our approach due to fact that they rely on a building a mediated schema, while we are aiming to move out the central management to local data coordination. Another vision work is designed in  that intended to replace the concept of a global schema in the conceptual view of the database. The paper presents Local Relational Model (LRM), a data management mechanism that allows for
18732	19867	method to establish mappings between these schemas as well as propagating queries throughout the p2p. In order to meet these challenges, the LRM introduces new concept as database coordination . The database coordination gives a possibility to the peers to manage semantic interdependences among databases in the p2p paradigm. Although the proposed coordination techniques are well defined,
18732	19867	research problems that stretch today’s multi-database solutions beyond their limits. Therefore, a similar work that supports database coordination is outlined in “Making Peer Database Interact” . The database coordination is presented as a possibility which allows for databases to inter-operate in a modality coherent with the p2p nature. The coordination can effectively manage at runtime
18732	18736	specific to p2p architectures, the others address the implementation issues. However, concerning the implementation they hardly have any results. The paper “What can database do for Peer-to-Peer?”  is one of the first papers that discuss database management issues in a P2P environment from a database research perspective. The work is focused on the question what databases can do for p2p
18732	18738	used for p2p data integration. Any of solutions to this problem concerning query or update processing can be applied to our system. Similar work to our proposal is presented in the Hyperion project . They proposed an architecture of database management systems (PDBMS) that assumes total absence of any central authority or control, no global schema, transient participation of peer databases,
18732	18740	GAV and LAV style to map between peer schemas. In fact, the idea of mediating between different databases using semantic relationships is not new. The similar idea for mediation is discussed in . In the paper is presented a Peer Mediator System (PMS), a decentralized mediation architecture based on the p2p paradigm. In the PMS, mediators integrate data sources and other mediators through
18732	1177	P2P network vs. Distributed Data Management Databases in a p2p system resemble heterogeneous distributed databases, often called multi-databases system. The multi-database management systems (MDBMS) enable data sharing among heterogeneous local databases and thus provide interoperability required by diverse applications. Integration is therefore performed by multiple software sub-systems. A
18732	18742	and management of the logical metadata that enables data sharing and coordination between independent, autonomous peers. A p2p distributed data sharing system, called PeerDB is described in . PeerDB is a system built on top of BestPeer , a generic and self-configurable p2p architecture developed at the University of Singapore. The system distinguishes itself from existing p2p
18732	18743	enables data sharing and coordination between independent, autonomous peers. A p2p distributed data sharing system, called PeerDB is described in . PeerDB is a system built on top of BestPeer , a generic and self-configurable p2p architecture developed at the University of Singapore. The system distinguishes itself from existing p2p systems in several ways: First, it is a full-fledge
18732	18746	ECA rules on the way in which peers exchange and share data. An interesting approach that combines the advantages of Piazza and LRM, by having a common virtual superpeer schema is presented in . This work allows peers to reuse the existing integration of other nodes that may act as a bottleneck in the systems. In particular, they show how any peer can combine the different integrations of
18747	19884	or strengthening. Capacity development or capacity strengthening are UNICEF’s—as well as this paper’s—preferred terms. CAPACITY GAPS Different types of capacity gap have been identified. Matta (2000) refers to the chasm that separates the view of what “should be done” from “the client’s ability and motivation to do it” as the Type-I Capacity Gap, i.e., “what we think is right for you” versus
18747	19884	CAPITAL Social capital essentially refers to the ability of individuals to secure benefits as a result of membership in social networks or other social structures. The World Development Report (2000) of the World Bank provides examples, from a poverty perspective, that illustrate how social networks and organizations are key assets in the portfolio of resources drawn on by the poor to manage
18747	19884	that its members are usually excluded—by discrimination or lack of resources—from places where major decisions regarding their welfare are made. Many case studies in the World Development Report (2000) have shown that social capital is an important ingredient of community capacity, which not only enhances project sustainability, but provides the energy for successfully scaling up programs. 12
18747	19884	proactive “learning-by-doing” approach to capacity development that may be adopted in parallel with more conventional and specific capacity development initiatives (see Section 7). Matta (2000) argues for such an approach in calling for “capacity building through results.” Engaging key actors in a series of rapid results initiatives, he suggests, would immediately involve them in an
18747	19884	at any level from the household up to national or international levels. The analysis of capacity should ideally start at the household and community level and be progressively broadened (UNDP (2000) refers to this broadening as “zooming out”). The framework can also be represented as intersecting checklists to 23sguide information gathering and analysis so as to understand better whether and
18747	19906	to form action plans, with outsider facilitation. Local people set their own agenda and mobilize to carry it out in the absence of outsider initiators and facilitators. Source: Adapted from Pretty (1995) in Cornwall (1996). Relationship of research or action to local people ON local people FOR local people FOR/WITH local people WITH local people WITH/BY local people BY local people Communities are
18747	19906	to country, depending on the political and administrative culture of government, and the degree to which local community cultures already encourage participation. Table 2, taken from Shrimpton (1995), illustrates how qualitative data on aspects of community involvement can be organized to measure the degree to which a program empowers local communities. Community capacity is determined by the
18747	19906	aware problems program and dimension or program progress, is aware of of problem, program progress/benefits. program but no feedback to dimension of progress/benefits. progress. Source: Shrimpton (1995). VHC. process and program progress. a VHC = village health committee. b CW = community worker. c IS = information system.sDecentralization gives local governments and communities greater power to
18747	19906	program, “how” did the program achieve this effect in this village? Shared presentations and feedback sessions—where the findings are shared with respondents. Source: Adapted from Young and Jaspars (1995) and Dongier (2000).s85 ANNEX 3: MODEL MONITORING PROFORMA The following is one example of a typical core proforma for monitoring a community-based nutrition program—in this case, one that includes
18747	18778	results from a few measurements, although usually requiring a large sample. Deciding program content, however, requires more advanced analysis, including small-scale but in-depth studies (Mock and Mason 2000). Indicators appropriate to program design may be grouped into the categories of outcome, process, and context. Outcomes refer to population level changes in behavior and health/nutrition status,
18747	18778	users.sAn MIS is essentially a system of collecting, analyzing, and using key monitoring data to improve the management and ultimately effectiveness of a program (see Tontisirin and Gillespie 1999; Mock and Mason 2000). The two main principles for the use of information for action are first, to collect the minimum, feasible amount of data required to inform and improve decisions leading to action, and second,
18747	18778	issues of program coverage, targeting, design, management, and implementation—have already been covered in three papers of the earlier RETA 5671 (Tontisirin and Gillespie 1999; Mock and Mason 2000; Mason et al. 1999). INSTITUTIONAL CAPACITY ANALYSIS The quality of nutrition-relevant institutional analysis is generally fairly poor. In the World Bank’s 1999 review, Bank project designs were
18747	18797	responsible for implementing them. The principles behind successful community-driven nutrition programming in Asia have been described in an earlier Regional Technical Assistance (RETA) project (Tontisirin and Gillespie 1999), where a dual programming model was adopted—namely direct action in the form of communitybased nutrition programs, backed up by supportive or enabling sectoral policy and programs. This paper
18747	18797	as primary information users.sAn MIS is essentially a system of collecting, analyzing, and using key monitoring data to improve the management and ultimately effectiveness of a program (see Tontisirin and Gillespie 1999; Mock and Mason 2000). The two main principles for the use of information for action are first, to collect the minimum, feasible amount of data required to inform and improve decisions leading to
18747	18797	support functions of institutions—including issues of program coverage, targeting, design, management, and implementation—have already been covered in three papers of the earlier RETA 5671 (Tontisirin and Gillespie 1999; Mock and Mason 2000; Mason et al. 1999). INSTITUTIONAL CAPACITY ANALYSIS The quality of nutrition-relevant institutional analysis is generally fairly poor. In the World Bank’s 1999 review, Bank
18747	18797	Plan and a greater emphasis was placed on effective resource allocations through targeting and the integration of micro-level program implementation with macro-level policy (described in Tontisirin and Gillespie 1999).sIn sum, the means may be as important as the ends with respect to policy. The process, sometimes prolonged, of building collaboration between sectors and between community organizations and local
18747	18797	an era of progressive decentralization, a central challenge for nutrition programs is finding a balance of approaches that work. The nature of communitygovernment partnerships has been described (Tontisirin and Gillespie 1999) but how can the grassroots and the center be brought together effectively? What balance of top-down (or center-derived) versus bottom-up (community-derived) planning and action is optimal for
18805	18807	Nicholson is currently with SAC Capital Management, LLC, and Malik Magdon-Ismail is currently with Rensselaer Polytechnic Institute. 1 Here we present a mathematical paradigm for generalization  in which each hypothesis is modeled as a bin containing colored marbles indicating agreement or disagreement with the target function. This `Bin Model' relates a classication problem to a physical
18805	18815	if the hypothesis can generalize from the training set to the entire space X . Many attempts have been made to assess the generalization performance of a learning process. The Prediction Error , VC analysis , and Exhaustive Learning paradigm  are some of the signicant results along these lines. The Prediction Error approach gives a general form for the out-of-sample error
18805	18815	of formulating the impact of model complexity and data noise on the gap between in- and out-of-sample error. The prediction error criterion is an asymptotic result, and for the nonlinear case , it is assumed that the input density is discrete with support only at input points. These are required for mathematical feasibility, but may limit the accuracy of the approximation in practice.
18805	2249	of sample error, overtting 1 Introduction Learning from examples is one of the standard techniques for dealing with unstructured or mathematically ill-dened problems (see for instance  for an introduction). The task at hand is to extract relevant information from asnite set of examples to develop predictive models. If only a small amount of data is available (as is often the case
18805	18820	A \clever&quot; learning algorithm might be able tosnd a hypothesis thatsts the training set well and has a small in-sample error. However, in-sample error can have no bearing on the out-of-sample error. Typically, two possible scenarios may occur in a learning process as illustrated in Figure 2. In thesrst scenario, the out-of-sample error decreases as the in-sample error decreases, all the way
18823	18842	quantifiers, which take relations rather than sets as inputs, remains an active area of research (cf. Higginbotham and May, 1981; van Benthem, 1989; May, 1989; Keenan, 1992; Westerst?ahl, 1994; Hella et al., 1996; Moltmann, 1996) 13 handbook.tex—July 5, 2004sII Quantification and Scope The relational theory of determiners which we examined, if all too briefly, in Section (I) explains some of the important
18823	18876	is one that shows that the quantifier most defined in (10) cannot be defined by any combination of (ISOM) ?1? quantifiers. This shows that we really do need at least type ?1, 1? quantifiers (cf. Väänänen, 1997). They also investigate the delicate issue of whether we need to go beyond ?1, 1?. We saw that more should be interpreted as taking three arguments. Whether we will also need to consider what are
18823	20038	too much in it to cover in any exhaustive way. There are, fortunately, two very good more specialized surveys to which interested readers may turn for more details and more references: Westerst?ahl (1989) and Keenan and Westerst?ahl (1997). I.1 Generalizing Quantifiers Before examining the variety of natural language quantification, we should return to the basic question of what quantifiers are. The
18823	20038	? ? Y ? |,thenQM (X, Y ) ?? QM ?(X? ,Y ? ). There has been a great deal of mathematical work investigating these quantifiers; much of it elegantly discussed in van Benthem (1986) and Westerst?ahl (1989). ISOM (or PERM) does appear to capture the idea that quantifiers are general, and so not about any objects in particular. It is a further question whether this makes them genuinely logical
18823	20038	generalized quantifiers to satisfy ISOM by definition.) I.6 Glimpses Beyond We have now seen the beginnings of generalized quantifier theory, but only the beginnings. The surveys of Westerst?ahl (1989) and Keenan and Westerst?ahl (1997) discuss a number of extensions of the theory, and applications of generalized quantifier theory in linguistics. Among the results they discuss is one that shows
18823	20038	of linguistics. The basics can be found in many syntax books. For a recent survey, see Szabolcsi (2001). Though many logical form theories take the syntax of logical form to determine scope, May (1985, 1989) considers a theory in which it does not completely do so. 22 handbook.tex—July 5, 2004sthe order in which the quantifiers are moved does on the logical form approach. Hendriks (1993) shows that
8921427	18880	common meaning is intended. 2.2 Continuants All real world entities of the sort which we encounter in the healthcare domain fall into one of two exclusive categories of continuant and occurrent . Continuants are entities which continue to exist through time; they preserve their identity from one moment to the next even while undergoing a variety of different sorts of changes. Continuants
8921427	18880	promisee on the other hand. Moreover, the dependence is in each case one-sided: the promiser and promisee are persons; they themselves do not require the existence of the promise in order to exist . What we can say, however, is that the roles those persons play within the promise-making context and the promise itself are mutually dependent on one another. Two or more objects are mutually
8921427	18883	The first consequence of our view of faithfulness is that the information in the medical record itself is not about what was “true” of the patient but what was observed and believed by clinicians . Every Act-instance is committed to accurately and truthfully representing who documented what and when and where it was documented, but is not likewise committed to the truth of the content of the
8921427	10054	provided fifty years earlier by the German philosopher Adolf Reinach in his , which has strong claims to be the work in which the theory of speech acts was first set forth in a systematic way.  4.1 The PromisesReinach treats what he called the theory of social acts as a branch of ontology – it is part of a general ontological theory of the structures of social interaction. For the
8921451	12687	classes of TCTL and TPTL do not admit punctuality (see  and ), which is useful in our setting. As an example, let A be the automaton in Fig.3. We note that A j= (l 1 ; I 1 ):(:h):(l 2 ; )::, which means that if l 2 is performed exactly one time unit after l 1 , then h has been performed in the meantime. This is a leak of information, which cannot be expressed without considering
8921451	12687	In the case of discrete time we must calculate the exact k which gives this certitude. As an example, if we assume dense time then the automaton in Fig.5 satises (l 1 ; ):h 1 ^ h 2 :(l 2 ; ):, which requires that there is a run where both h 1 and h 2 appear between l 1 and l 2 . If we assume discrete time, then the property is satised only if we choice a k less than 1 3 , since (l 1
8921451	18978	request r c causes A c to reach state c 1 . When the page is in the cache (state c 2 is active and u  K holds), the time elapsed between a request r c and an answer a c is in the interval T 1 = . Now, the only observable actions for e are r e and a e , since it cannot observe interactions between the browser and the cache and between the browser and w. In  it is shown that, when the
8921451	18980	Conclusions In this section, before drawing some conclusions, we discuss three choices: 1) the choice of introducing TIFL instead of exploiting existing timed temporal logics, like TCTL and TPTL , to express informationsow properties; 2) the choice of solving the problem A j=swithout exploiting the well known techniques of reachability and emptiness ; 3) the choice of using dense time
8921451	18981	each possible sequence of h 1 ; : : : ; hn , thus having a formula exponential w.r.t. n. The third reason is that the decidable classes of TCTL and TPTL do not admit punctuality (see  and ), which is useful in our setting. As an example, let A be the automaton in Fig.3. We note that A j= (l 1 ; I 1 ):(:h):(l 2 ; )::, which means that if l 2 is performed exactly one time unit
8921451	18982	c is in the interval T 1 = . Now, the only observable actions for e are r e and a e , since it cannot observe interactions between the browser and the cache and between the browser and w. In  it is shown that, when the user visits the site e, e can attack the browser and infer whether it has recently visited some web page in w or not, thus violating the privacy of the user. In fact,
8921451	18986	must enumerate each possible sequence of h 1 ; : : : ; hn , thus having a formula exponential w.r.t. n. The third reason is that the decidable classes of TCTL and TPTL do not admit punctuality (see  and ), which is useful in our setting. As an example, let A be the automaton in Fig.3. We note that A j= (l 1 ; I 1 ):(:h):(l 2 ; )::, which means that if l 2 is performed exactly one
8921451	18987	actions have been or have not been performed. So, TIFL permits to describe behaviors giving rise to informationsow. Automata for the specication of security problems have been used for example in . Introducing specic logics for specic problems of security is known in the literature . By these automata and logics one can describe systems with the assumption of discrete time. This
8921451	18988	actions have been or have not been performed. So, TIFL permits to describe behaviors giving rise to informationsow. Automata for the specication of security problems have been used for example in . Introducing specic logics for specic problems of security is known in the literature . By these automata and logics one can describe systems with the assumption of discrete time. This
8912162	19004	since, and was similar to a previous system developed by Bruce Blumberg and others described in the paper “Multi-Level Direction of Autonomous Creatures for Real-Time Virtual Environments” . Improv connected together many technologies to achieve its goals. The geometric models were governed by degree of freedom which could be used to limit the rotate, position and degree of movement
8912162	19005	technology Behavioral Animation techniques are generally used in a reactive context. It stems from the works of Rodney Brooks and especially his seminal paper “Intelligence without representation” . At any one point (a frame of an animation for example) a reactive system can only make judgements based on what it “knows” at this point in time. As a result there is no sense of continuity, which
8912162	7444	for picking involve using methods of attention focussing, emotional engines and personality amongst other things. More discussion on data-driven and goal-driven arbitration can be found in  and  and more discussion on the c4 architecture in .sChapter 2. Literature survey 69 2.4 Off the shelf AI-in-a-box As the need for competent AI in games ever increases so companies have
8912162	19015	hit. It incorporated some very high fashionable technologies such as neural networks and genetic algorithms, but they worked together well and the end results were revolutionary. Several papers  cover the academic descriptions and implication of the Creatures series and an internal memo  will interest the readers as it is written form the point of view of describing the highly
8912162	19016	It incorporated some very high fashionable technologies such as neural networks and genetic algorithms, but they worked together well and the end results were revolutionary. Several papers  cover the academic descriptions and implication of the Creatures series and an internal memo  will interest the readers as it is written form the point of view of describing the highly technical
8912162	19016	which the cell must be perturbed to fire. SVRule (State Variable Rule) Genetically defined functions which governs how input values affect the output values of a cell. More in depth discussion see . The dendrites which connect cells themselves also have many parameters, governing for how long they fire, their weightings, and how they interact with SVRules. The lobes in the Norns brains are
8912162	19016	partly of parents genes (up to the cross over point) and partly of the other parents. Genes are 8-bit values, and “can safely be mutated into an 8-bit value, without fear of crashing the system”. When splicing occurs between parents it is only done so at gene boundaries. If crossovers happen part way through genes then undesired effects can result, as values inside a gene might well be
8912162	19019	it introduce a fast algorithm for learning controllers that enables either physics-based models or their neural network emulators to synthesise motions satisfying prescribed animation goals. See  or  for a full description Terzopoulos’ article “Artificial Life for Computer Graphics”  is a very interesting study of the state of art in 1999 of technologies relating to artificial life
8912162	19019	Tu, and is credited with the co-development of several animated video sequences which can be found on his website. His work has progressed in a number of direction including NeuroAnimator (in conjunction with Radek Grzeszczuk) a system for “replacing the numerical simulation and control of model dynamics with a dramatically more efficient alternative.” A novel approach to creating
8912162	19019	a fast algorithm for learning controllers that enables either physics-based models or their neural network emulators to synthesise motions satisfying prescribed animation goals. See  or  for a full description Terzopoulos’ article “Artificial Life for Computer Graphics”  is a very interesting study of the state of art in 1999 of technologies relating to artificial life and,
8912162	19024	the same architecture controls the character Duncan (the main character) is also used to control the sheep, however much of the architecture is deactivated to maintain lower resource overheads. See  ???sChapter 5. Conclusions and Further Ideas 133 of combining different smaller AI technologies together would provide flexible and customisable solutions. 5.4.3 Supporting technologies genetic
8912162	19026	emotional engines and personality amongst other things. More discussion on data-driven and goal-driven arbitration can be found in  and  and more discussion on the c4 architecture in .sChapter 2. Literature survey 69 2.4 Off the shelf AI-in-a-box As the need for competent AI in games ever increases so companies have started to produce “Brain in a box” type solutions much like
8912162	19030	including “It knows what you’re going to do: adding anticipation to a Quakebot”  looking at adding anticipation of a players moves to a Quake bot as well as several other projects such as . SOAR has also been used for non-Quake projects including “Building Advanced Autonomous AI systems for Large Scale Real Time Simulations”. “Soar is a theory, implemented as a software
8912162	19032	of characters you interact with is SOAR backed. A series of interfaces were developed to allow SOAR to be distributed, for it and the visualisation to be run on the same, or distributed, machines . The project was based around “ hope that complex AI characters will lead to games where the human player is faced with challenges and obstacles that require meaningful interactions with the
8912162	19032	games. Whilst there are no doubt computational overheads (the sheer number of states expanded in the simple example in  would seem to point to substantial overheads), from the graphs in  reasonable refresh rates and speeds are still achievable so long as the total number of characters on screen at one time is kept reasonable. The possibility of “Level of Detail” style updates is
8912162	19034	be developed specifically of the task at hand, although 3rd party planners have been used by researchers for instance John Laird used the SOAR planner  to implement a predictive agent for Quake . Planners are often based on the Sense, Model, Plan, Act model of reasoning. In the world of computer games the sensing and modelling can be done relatively easily (unlike in a “real world”
8912162	19034	RPROP learning algorithm . Whilst the ability to learn is one of the greatest attractions of neural networks, it also raises many questions, primarily “do we wish our NPC’s to 3 John Laird in  gives an excellent example of advanced players behaviors including learning from previous experience. See the Dennis Fong story in sChapter 2. Literature survey 37 learn?” This would seem an
8912162	19034	field of Artificial Intelligence is the SOAR architecture  which he has used to implement various other projects including “It knows what you’re going to do: adding anticipation to a Quakebot”  looking at adding anticipation of a players moves to a Quake bot as well as several other projects such as . SOAR has also been used for non-Quake projects including “Building Advanced
8912162	7581	for Large Scale Real Time Simulations”. “Soar is a theory, implemented as a software architecture, that seeks to describe and realise the fundamental, functional components of intelligence.” problem spaces as a single framework for all tasks and subtasks to be solved objects with attributes and values as the single representation of temporary knowledge production rules as the single
8912162	19039	algorithms]and I have tried using NNs for pattern recognition etc”. A most interesting use of GA’s is described in “Using a genetic algorithm to tune first-person shooter bots” whereby a genetic algorithm is used to find the optimal settings for the fuzzy logic controlled of an NPC in a First Person Shooter 2.3.6 Blackboarding Originally used primarily as information
8912162	19042	over time. These characters aren’t attempting to be intelligent in their behavior, but rather to use carefully-crafted statistical models to engage their audience of users.”  As described in  Improv’s basic architecture is built on a three level abstraction system. At the lowest level is the geography level, which is manipulated in real-time. Above that is an Animation Engine, which
8912162	17573	racing lines almost at will, to add a bit of character to the drivers”  without loosing the ability for the NPC to follow the track. The network was trained using RPROP learning algorithm . Whilst the ability to learn is one of the greatest attractions of neural networks, it also raises many questions, primarily “do we wish our NPC’s to 3 John Laird in  gives an excellent example
8912162	19050	or their neural network emulators to synthesise motions satisfying prescribed animation goals. See  or  for a full description Terzopoulos’ article “Artificial Life for Computer Graphics”  is a very interesting study of the state of art in 1999 of technologies relating to artificial life and, particularly relating to its use to simulate realistic motion in films and movies. He
8912162	19050	work done in physical realistic modelling to create an more realistic effect.sChapter 2. Literature survey 42 Figure 2.9: Artificial life, and its place in the development of realistic animations sChapter 2. Literature survey 43 Animators, even at the cutting edge at the time had spent time modelling how creatures moved, and animat researchers such as Reynolds had spent their time modelling
8912162	19050	(see Grand and Cliff in this chapter) and concludes with hopeful statement that researchers might go on to creatures “that are self creating, self-controlling, self-animating and self-evolving”  Terxopoulos appears to have moved his research area away form autonomous agent in the latter years, however his contribution especially in the area of perception and the work with Tu should not be
8912162	20218	as the single learning mechanism There are many papers about SOAR and its working, though for an overview of how SOAR works, and many of the key concepts the instructional video  found in sChapter 2. Literature survey 50 SOAR has been used for both academic and military research and for limited use in games. Haunt 2 (which is based on a 1980 project called Haunt which “was the first
8912162	19052	The MIT Synthetic Characters Group did various experiments with a pack of virtual wolves. One of these is discussed in the paper “Social Behavior, emotion and Learning in a Pack of Virtual Wolves” . When the wolves perform any action, it does so in an “emotional style”. The emotion at a given time is dependant not only on the external stimuli, the internal registers of the wolves but also of
8912162	19053	may well be redefined to encompass more than simple reactive behaviorial models such as is seen in the flocks of birds and fish as developed by Reynolds. The work by Tu on physically realistic fish  took the field of behaviorial animation one step further. Whilst the fish were self animating in the same sense as Reynolds’ project they were far more advanced than simple reactionary entities.
8912162	19053	about the MIT group can be found on their website, . 2.2.3 Tu Xiaoyuan Tu’s PhD Dissertation “Artificial Animals for Computer Animation: Biomechanics, Locomotion, Perception, and Behavior” won the ACM Doctoral Dissertation Award in 1996. Her work was based around “physics-based modelling, modelling and control of reactive behaviour and behavioural animation” .It used a
8912162	19053	that “during the last decade, much attention in the graphics community has centred on realistic low-level motion synthesis, with only a few researchers pursuing the modelling of realistic behavior” . Terzopoulos also states that the links between artificial life and computer animation were forged in 1986 by Craig Reynolds with his “Boids” experiments. Researchers became aware of the need to
8912162	20224	that they play. Any game that involves simple or repetitive tasks is unlikely to result in an interesting gaming experience. There are the notable exceptions, for example the 1980’s game Frogger  involved simple movements and “characters” (logs, cars, etc) with very simple movement patterns, but in general users are likely to become frustrated with the limited methods of increasing the
8912162	20227	in 1999 a developer of an up and coming sports games “planned to include an option to reset the AI should the player feel it had become feeble-minded (or too strong a player, as the case may be)” . Whilst the most interesting property of neural networks is their ability to learn, continual learning throughout a game is unlikely to be their more useful implementation. Andre LaMothé (CEO of
19059	19063	overlapping space and less 3D perspective distortion in between them and therefore the alignment estimation between the frames is more accurate. Our work on panorama stitching has been described in  and has several advantages over other approaches . We briefly summarize this method here. The basic processing scheme has two phases: the alignment phase and the stitching phase. During the
9651571	19122	Kupiec uses pre-specified suffixes and then learns statistically the POS predictions for unknown word guessing (see ). The XEROX tagger comes with a list of built-in ending-guessing rules (see ). Brill builds 1 Sofia University “St. Kliment Ohridski”, Sofia, Bulgaria e-mail: preslav@rocketmail.com Preslav Nakov 1 more linguistically motivated rules by means of tagged corpus and a lexicon
9651571	19123	builds 1 Sofia University “St. Kliment Ohridski”, Sofia, Bulgaria e-mail: preslav@rocketmail.com Preslav Nakov 1 more linguistically motivated rules by means of tagged corpus and a lexicon (see ). He does not look at the affixes only but optionally checks their POS class in a lexicon. Mikheev proposes a similar approach that estimates the rule predictions from a raw text (see ). Daciuk
9651571	19124	(see ). He does not look at the affixes only but optionally checks their POS class in a lexicon. Mikheev proposes a similar approach that estimates the rule predictions from a raw text (see ). Daciuk uses finite state transducers. 3 SYSTEM OVERVIEW 3.1 Unknown word tokens and types identification MorphoClass is interested in the identification and morphological classification of the
9651571	19124	class — it is determined by the last word the compound is made of. Third, we try to guess the class looking at the stem ending. We implemented a Mikheev-like ending-guessing rules (see ). We selected a confidence level of 90%, considered endings up to 7 characters long that must be preceded by at least 3 characters and whose frequency is at least 10. We trained the model over 8,5
9651571	19125	is based on both lexicon-based and suffixbased morphology. First, for each stem we check whether it is present in our stem lexicon. (We built it using the free lexicon of the Morphy system (see )). If so, we reject it since the unknown word could not have a known stem: all words the known stems could generate are already known. Second, we check whether the stem could be a compound by
257702	19127	access-control mechanisms. These systems complement each other. The ability of voting algorithms to tolerate failures or slow nodes has led to their recent adoption in storage systems. FarSite  is a distributed serverless file system that uses voting-based algorithms to tolerate Byzantine failures. Self-* is also a serverless file system that uses voting-based erasure-coding algorithms
257702	19130	20% to 80% of traditional arrays, even with three-way replication. Commodity hardware is, of course, far less reliable than its enterprise counterparts. Using the reliability figures reported in , we expect the mean time between failures of a typical network switch to be 4 years, and that of a typical brick to be 4 to 30 years, depending on the quality of disks and the internal disk
257702	19130	Failures are assumed to be independent. We assume a disk mean time to failure (MTTF) of 57 years, based on manufacturers’ specifications and a brick (enclosure) MTTF of 30 years, based on data from . The time to repair a failure depends on the failure type and is based on the time required to copy the data to spare space — we assume that spare space is always available. Based on this, we pick
257702	19130	MTTDL, we need at least 3 bricks per logical block using replication. The primary reasons the system requires such high a degree of replication are the use of failure-prone commodity components , and the size of the system. A FABsMTTDL (years) 1E+06 1E+05 1E+04 1E+03 1E+02 2-replication 3-replication 2,4 erasure coding 0 256 512 768 1024 Logical capacity (TB) Figure 10: Mean time to first
257702	19132	the functionality of array controllers across bricks while maintaining the consistency semantics of a single disk. The idea of distributed, composable disk arrays was pioneered by TickerTAIP  and Petal . Petal uses a master-slave replication protocol, which cannot tolerate network partitioning. In addition, it has a period (?30 seconds) of unavailability during fail-over, which can
257702	19133	brick failures and recoveries smoothly without disturbing client requests. 2. RELATED WORK Today’s standard solution for building reliable storage systems is centralized disk arrays employing RAID , such as EMC Symmetrix, Hitachi Lightning, HP EVA, and IBM ESS. To ensure reliability, these systems incorporate tightly synchronized hardwarelevel redundancy at each layer of their functionality,
257702	19135	function NewTimestamp generates a locally monotonically increasing timestamp by combining the real-time clock value and the brick ID (used as a tie-breaker). clocks with sub-millisecond precision . Being able to abort requests, however, offers two benefits. First, it allows for an efficient protocol—a “read” request can complete in a single round as opposed to two in previous algorithms [5,
257702	19135	usually in less than an hour (Section 5). However, simulation results with real workloads show that the timestamp-table size increases by at most 4MB per brick per hour even after brick failure . It is extremely unlikely that the number of timestamps will exceed what a brick can store in memory. 4.4 Improving the efficiency of voting One of the criticisms of majority voting is its
257702	19136	is, only by sometimes aborting requests can an algorithm properly linearize requests whose coordinators could crash in the middle. A theoretical treatment of this issue appears in separate papers . 4.2 Erasure coding FAB also supports generic m,n Reed-Solomon erasure coding. Reed-Solomon codes have two characteristics. First, they generate n ? m parity blocks out of m data blocks, and can
257702	13054	is a distributed serverless file system that uses voting-based algorithms to tolerate Byzantine failures. Self-* is also a serverless file system that uses voting-based erasure-coding algorithms . OceanStore  is a wide-area file system that uses voting to File/DB servers ... FAB bricks ... Admin frontend iSCSI frontend Paxos Volume layouts & seggroups Coordinator RPC Status monitor
257702	19137	in addition to replication. Recently, LeftHand Networks  and IBM  have proposed FAB-like storage systems, but no details about them have been published. Network-attached secure disks (NASD)  let clients access network-attached disks directly and safely. Both FAB and NASD try to build scalable distributed storage, but with different emphases: FAB focuses on availability and reliability
257702	19138	the same block must all return the old block value or all return the new value, until the block is overwritten by a newer “write” request. Prior approaches, e.g., Gifford’s use of two-phase commits  cannot ensure a quick fail-over, and Ling et al.’s use of end-to-end consistency checking  conflicts with our goal of leaving the client interface (iSCSI) unchanged. FAB takes an alternative
257702	19140	is a distributed serverless file system that uses voting-based algorithms to tolerate Byzantine failures. Self-* is also a serverless file system that uses voting-based erasure-coding algorithms . OceanStore  is a wide-area file system that uses voting to File/DB servers ... FAB bricks ... Admin frontend iSCSI frontend Paxos Volume layouts & seggroups Coordinator RPC Status monitor
257702	4921	storage systems. Specifically, our contributions are: New replication and erasure-coding algorithms: We present asynchronous voting-based algorithms that ensure strictly linearizable accesses  to replicated or erasure-coded data. They can handle any non-Byzantine failures, including brick failures, network partitioning, and slow bricks. Existing algorithms , in contrast, not only
257702	4921	in the handling of the failure of the participants in the middle of a “write” request: the new value may end up on only a minority of bricks. A storage system must ensure strict linearizability —it must present a single global ordering of (either successful or failed) I/O requests, even when they are coordinated by different bricks. Put another way, after a “write” coordinator fails,
257702	19143	accesses, and require a special protocol to run on each client. Consistent reconfiguration has been studied in viewstamped replication , which uses two-phase commits to update data and Paxos  to transition views. More recently, RAMBO  proposed the idea of concurrent active views and background state synchronization. This idea is used in FAB as well, but whereas RAMBO is based on
257702	19143	and seggroups are called the global metadata, because they are replicated on every brick and are read by the request coordinator. Following the approach pioneered by Petal , we use Paxos , an atomic broadcast protocol, to maintain the consistency of the global metadata across bricks. Paxos allows bricks to receive exactly the same sequence of metadata updates, even when updates are
257702	19143	agree on a single sequence of primary views; i.e., it ensures that disjoint, concurrent views (a “split-brain” situation) never happen. We use dynamic voting  1 —a protocol similar to Paxos , but optimized for view agreement—for this purpose. The participants of the dynamic voting protocol are the set of bricks that store blocks in the seggroup, plus at least two additional witness
257702	19145	of array controllers across bricks while maintaining the consistency semantics of a single disk. The idea of distributed, composable disk arrays was pioneered by TickerTAIP  and Petal . Petal uses a master-slave replication protocol, which cannot tolerate network partitioning. In addition, it has a period (?30 seconds) of unavailability during fail-over, which can cause clients
257702	19145	Volume layouts and seggroups are called the global metadata, because they are replicated on every brick and are read by the request coordinator. Following the approach pioneered by Petal , we use Paxos , an atomic broadcast protocol, to maintain the consistency of the global metadata across bricks. Paxos allows bricks to receive exactly the same sequence of metadata updates,
257702	19145	replication protocol to the masterslave protocol, the traditional method for replicating data across a network. We have built a variation of FAB that runs a masterslave protocol similar to Petal’s . In this protocol, the dynamic voting protocol is used to let bricks agree on the single master for each seggroup. Each I/O coordinator forwards the request to this master. For read requests, the
257702	13025	with a subset (quorum) of bricks that store the data. Voting allows FAB to tolerate failed bricks and network partitioning safely without blocking. It also enables performance decoupling ???tolerating overloaded bricks by simply ignoring them, as long as others are responsive. This is especially effective in systems like FAB, in which brick response times fluctuate due to the
257702	13025	as a high-throughput local-area storage system. It tolerates only stopping failures, but it ensures consistent data accesses without changing the clients or exploiting file-system semantics. Ling  and Huang  use voting to build a high-throughput storage system, but they support only replication, with only single-client accesses, and require a special protocol to run on each client.
257702	13025	is overwritten by a newer “write” request. Prior approaches, e.g., Gifford’s use of two-phase commits  cannot ensure a quick fail-over, and Ling et al.’s use of end-to-end consistency checking  conflicts with our goal of leaving the client interface (iSCSI) unchanged. FAB takes an alternative approach, performing recovery lazily when a client tries to read the block after an incomplete
257702	13025	3-way replication. Interestingly, for both DB and 64KB-random-write workloads, FAB outperforms the master-slave protocol. This is due to the performance decoupling effect of the voting protocols ???specifically, FAB can ignore slow bricks by collecting replies only from a majority. Performance decoupling is especially effective in a disk-bound system like FAB in which disk accesses,
257702	19147	enough to detect out-of-order requests with older timestamps. This period is conservatively chosen to be larger than the maximum clock skew plus the maximum possible scheduling delay on any brick . Another improvement can be made by observing that a single “write” request usually updates multiple blocks, and that each of the blocks affected will have the same timestamp. We thus organize the
257702	19148	protocol lets bricks in a seggroup agree on a single sequence of primary views; i.e., it ensures that disjoint, concurrent views (a “split-brain” situation) never happen. We use dynamic voting  1 ???a protocol similar to Paxos , but optimized for view agreement—for this purpose. The participants of the dynamic voting protocol are the set of bricks that store blocks in the seggroup,
257702	19149	linearizable accesses  to replicated or erasure-coded data. They can handle any non-Byzantine failures, including brick failures, network partitioning, and slow bricks. Existing algorithms , in contrast, not only lack erasure-coding support, but also could break consistency when a brick that coordinates a request crashes in the middle. A new dynamic quorum reconfiguration algorithm:
257702	19149	on each client. Consistent reconfiguration has been studied in viewstamped replication , which uses two-phase commits to update data and Paxos  to transition views. More recently, RAMBO  proposed the idea of concurrent active views and background state synchronization. This idea is used in FAB as well, but whereas RAMBO is based on single register (logical block) emulation, FAB
257702	19149	10]. Being able to abort requests, however, offers two benefits. First, it allows for an efficient protocol—a “read” request can complete in a single round as opposed to two in previous algorithms , skipping the round to discover the latest timestamp. Second, abortion enables strict linearizability—that is, only by sometimes aborting requests can an algorithm properly linearize requests whose
257702	19150	function NewTimestamp generates a locally monotonically increasing timestamp by combining the real-time clock value and the brick ID (used as a tie-breaker). clocks with sub-millisecond precision . Being able to abort requests, however, offers two benefits. First, it allows for an efficient protocol—a “read” request can complete in a single round as opposed to two in previous algorithms [5,
257702	19151	but they support only replication, with only single-client accesses, and require a special protocol to run on each client. Consistent reconfiguration has been studied in viewstamped replication , which uses two-phase commits to update data and Paxos  to transition views. More recently, RAMBO  proposed the idea of concurrent active views and background state synchronization.
257702	19152	m data blocks, and can reconstruct the original data blocks from any m out of n blocks. Second, they provide a simple function, which we call Delta, that enables incremental update of parity blocks . Using this function, when writing to a logical block X, the new value of any parity block can be computed by xor(old-parity, Delta(old-x, new-x)), where oldparity is the old parity block value,
257702	19156	is piggybacked on every message exchanged between bricks; this gives a timely view of the status of a small set of bricks. Second, we use a variation of the gossip-based failure detector  to advertise the status to a random brick every three seconds; this gives an older, but more comprehensive, view of the system. Finally, the backend modules are responsible for managing and
257702	19158	mechanisms. Voting-based replication is not new, but it has seen little use in high-throughput systems, because of concerns about inefficiency, as reading data must involve multiple remote nodes . Insthis paper, we show that voting is indeed practical and often necessary for reliable, high throughput storage systems. Specifically, our contributions are: New replication and erasure-coding
257702	19158	what a brick can store in memory. 4.4 Improving the efficiency of voting One of the criticisms of majority voting is its inefficiency, because “read” requests must contact multiple remote nodes . This problem, however, does not apply to FAB for two reasons. First, we apply an “optimistic read” technique for the common case scenario of reading from a logical block that is already
433844	13613	of complex eventdriven behaviours, containing parallelism, event broadcasting, state hierarchy, interrupts and non-determinism. They have been used in structured analysis since the mid-1980s  and in object-oriented analysis since the early 1990s . They take a central place in the UML , where they can be used, among others, to specify object life cycles. Correspondence and offprint
433844	13613	of these semantics, without the execution algorithms and the examples, are presented in a previous paper .s248 R. Eshuis et al. 3.1. Semantic Choices Statecharts were introduced by Harel  to model the behaviour of activities in the structured analysis approach Statemate . They have been adapted in many object-oriented design notations, including the UML , but with an
19163	5677	excluded from the answer to the posed query. 4.1 Classification Of Queries Classes of queries related to sensor networks have been identified broadly as traditional SQL-like queries and aggregates  or probabilistic range queries for moving objects . In this paper, we follow a classification of queries similar to the former case. However, we do not claim that queries covered in this
19163	5677	the correct answer over the true unknown readings, and is also more accurate than the answer on the noisy (uncleaned) readings. Our approach for obtaining the eligible set bears similarities with  for dealing with uncertainty in data due to lag of instantaneous updates. The major differences in our approach lie in excluding all sensors with probability ? from the set, generalization to the
19163	5677	models. Indexing techniques in GADT can then be used over our resultant more accurate uncertainty models. Uncertainty in sensor databases due to lag of updates has been addressed recently in . Due to continuous changes in sensor values and limited network bandwidth and energy, the database state may lag the state of the real world, and therefore, the data inside the database is
19163	19173	area of sensor networks, both static sensors and sensors on moving objects. For example, some research has focused on data centric approaches, routing, storage of sensor data, and fault tolerance . In general, in-network processing was proved to be more energy-efficient theoretically and experimentally , since valuable communication energy is saved. This motivated the recent work
19163	4875	area of sensor networks, both static sensors and sensors on moving objects. For example, some research has focused on data centric approaches, routing, storage of sensor data, and fault tolerance . In general, in-network processing was proved to be more energy-efficient theoretically and experimentally , since valuable communication energy is saved. This motivated the recent work
19163	7748	operation as in traditional databases. Recent work on query processing in sensor databases has focused on data gathering using network primitives, in-network aggregation, and query languages . The emphasis of these approaches is to take into consideration the resource constraints of sensors such as bandwidth and energy. We argue that errors is also a serious limitation of sensors as
19163	7748	in a distributed fashion , and on designing and implementing database functionality . Also, generic architecture for queries over streaming sensors has been proposed in . All these research efforts take into consideration the severe resource constraints of sensor networks, especially, energy and communication constraint, and their unattended deployment potentially
19163	7748	about the capabilities and the limitations of each sensor. The database system should be able to turn the sensors on/off or control their rate using proxies, similar to the ones proposed in . The underlying networking functionality should allow for such a scenario. Users may define specific quality requirements on the answer to their queries, as part of the query, e.g., a confidence
19163	7748	and evaluation, and resource consumption. Unfortunately, a large part of existing work on query processing in sensor networks has only focused on homogeneous clean data from all sensors , even though there are three dimensions of possible sensor data: homogeneity, uncertainty, and sampling.s8. ACKNOWLEDGEMENT We would like to thank Professor David Madigan for helpful discussions on
19163	19180	in general, has received attention in literature especially uncertainty due to incomplete information. Parsons surveys most of the work done in this area from both AI and database perspectives . Uncertaintyshas been handled using fuzzy logic and fuzzy theory. However, our focus is on noise which cannot be handled using such an approach since there is no fuzziness or vagueness involved
19163	18651	area of sensor networks, both static sensors and sensors on moving objects. For example, some research has focused on data centric approaches, routing, storage of sensor data, and fault tolerance . In general, in-network processing was proved to be more energy-efficient theoretically and experimentally , since valuable communication energy is saved. This motivated the recent work
19163	19181	operation as in traditional databases. Recent work on query processing in sensor databases has focused on data gathering using network primitives, in-network aggregation, and query languages . The emphasis of these approaches is to take into consideration the resource constraints of sensors such as bandwidth and energy. We argue that errors is also a serious limitation of sensors as
19163	19184	our cleaning functionality works on every single sensor. Even if the readings of a set of sensors are combined (aggregated) into a single more robust reading to reduce the effect of noise , our approach can still work on this single reading, thus yielding more accurate results. Based on our proposed uncertainty models and using a statistical approach, we introduce several algorithms
19163	19184	and for querying these models. General modeling of sensor streams and defining abstractions to represent sensor networks as databases were studied by Gehrke et al. as part of their Cougar project . They have also studied indexing and retrieval of noisy sensors in GADT . Specifically, they proposed abstract data types (ADT) and data structures for “indexing” noisy sensors that are
19163	18648	userdefined queries. Existing networks are used for monitoring of several physical phenomena such as contamination, climate, building structure, and so on, potentially in remote harsh environments . They also found several interesting applications in industrial engineering such as monitoring the quality of food, especially perishable items, as well as real life applications such as
19163	18648	level is that a point estimation of the resultant posteriors can be obtained. Consequently, traditional approaches to in-network query processing and aggregation can be used with error bounds . Performing the cleaning at the sensor and the query processing at the database level has no advantages. This is clearly due to the fact that communicating the noisy reading (a single value) to the
19163	18648	approaches, routing, storage of sensor data, and fault tolerance . In general, in-network processing was proved to be more energy-efficient theoretically and experimentally , since valuable communication energy is saved. This motivated the recent work on computing aggregates in sensor networks by processing the query in-network hierarchically, in a distributed fashion
19163	18648	problems “efficiently”, in terms of the available resources, and “online”. Existing research on missing values in sensor networks either focused on providing low-level networking solution such as , or customized solutions that work for specific applications such as . In both cases, the problem persists though less severely. Hence, a general purpose solution for this problem as well as
8921478	5216	preferred. Brooks' Subsumption Architecture is an architecture that uses these three design principles. 3.2 | Brooks' subsumption architecture Rodney Brooks introduced the subsumption architecture (Brooks, 1991) in which the robot is controlled by different layers, each responsible for achieving a certain (sub)task, such as avoiding obstacles or moving towards a prey. Each layer in the subsumption
8921478	19193	between the effectors of one robot and the sensors of another robot is by necessity bodily. The importance of the body for cognitive behavior has been stressed by several researchers (e.g. Clark, 1997; Chiel & Beer, 1997; Kelso, 1995; Thelen & Smith, 1994; Clancey, 1997; Resnick, 1994; Varela, Rosch & Thomson, 1991). It becomes immediately apparent when one considers the behavior of a predator
8921478	19193	interactive behavior might be very useful to simple creatures. 1.2 | Niche environments The importance of the environment for the effect it has on behavior has been indicated by many researchers (Clark, 1997; Beer, 1995; Clancey, 1997; Resnick, 1994; Thelen & Smith, 1994). To test the second assumption that specific specialized behavior of a robot can be useful in a distinct set of possible
8921478	19213	Certain resources used in a design might be inessential to the structure and removing them will make the system, easier to develop, more robust, and more efficient. Statelessness is (as defined by Werger, 1999) a measure of the amount of internal state maintained by a system. A system that maintains state has to keep track of every change in the environment to keep it synchronized. In even quite simple
8921490	1871	Computer Science, The University of Arizona 1. Introduction Version 9.0  is the current version of Icon, superseding Version 8.10. Version 9.0 contains new features, enhanced graphics faciliies , and significant changes to the implementation. These changes are described in more detail in an associated document . This report provides the information necessary to install Version 9.0 of
8921490	19225	Version 8.10. Version 9.0 contains new features, enhanced graphics faciliies , and significant changes to the implementation. These changes are described in more detail in an associated document . This report provides the information necessary to install Version 9.0 of Icon under VMS. The installation process for Version 9.0 is similar to that for Version 8.10. In addition to Icon itself,
8921491	11590	Research in anaphora resolution has been very intense during several periods in the past and present. Several anaphora resolution modules have been implemented in the past (Mitkov et al., 2002; Grosz et al., 1995; Kennedy and Boguraev, 1996), and a number of NLP applications use anaphora resolution components. In particular, several questionanswering systems have implemented anaphora resolution in several
8921491	19232	resolution has been very intense during several periods in the past and present. Several anaphora resolution modules have been implemented in the past (Mitkov et al., 2002; Grosz et al., 1995; Kennedy and Boguraev, 1996), and a number of NLP applications use anaphora resolution components. In particular, several questionanswering systems have implemented anaphora resolution in several ways (Jong-Hoon et al., 2001;
8921491	19232	a state-of-theart answer-extraction system such as ExtrAns. In particular it uses syntactic information, such as that produced by Link Grammar, rather than resorting to parser-free approaches like (Kennedy and Boguraev, 1996). We will see in the following sections that the type of syntactic information required for the anaphora resolution algorithm is different from the one provided by Link Grammar, but it is possible
8921491	19233	Ontologies). The resolution algorithm we are going to present in this paper is an adaptation of a purely syntactic approach. The theory behind the anaphora resolution module of ExtrAns is based on (Lappin and Leass, 1994), but it has been fine-tuned in several ways for the answer extraction task. Its major advantagesDocument PreProcessing Linguistic Analysis parsing pruning anaphora resolution MLF Generator Figure
8921491	19233	structure, in contrast with (Grosz, 1981). Thus, the result is computed in relatively short time and uses less resources. Lappin and Leass’ anaphora resolution model has the following components (Lappin and Leass, 1994, 536): • An intrasentential syntactic filter for ruling out anaphoric dependence of a pronoun on an NP on syntactic grounds. • A morphological filter for ruling out anaphoric dependence of a
8921491	19233	are listed in Table 2. The values are the same as Factor type Weight Cataphora -275 Parallel roles 35 Recency 100 Table 2: Weight values for dependent salience factors in the original algorithm (Lappin and Leass, 1994) except for the case of cataphora, where we decided to increase the penalty by 100 units (up from -175). 2.9 Equivalence classes Coreference between the pronoun and the noun is signaled by
8921491	19233	the repair kit to the repair area until it is ready to use”. So we have 26 pronouns correctly resolved out of 33, which results in an accuracy of 79%. This is lower than the result reported by (Lappin and Leass, 1994) of 86%. But, of course, our data is too small to be representative. We then examined in detail what happened for each pronoun type: It: Out of 28 anaphoric it, 21 were correctly solved. Thus, the
8921491	19237	specifically at technical documentation. After an initial application to the Unix manpages (Mollá et al., 2000), ExtrAns was used in the Aircraft Maintenance Manual (AMM) of the Airbus A320 (Rinaldi et al., 2002), and currently we are targeting the Linux HowTos. ExtrAns translates documents and questions into a flat semantic representation using a comprehensive linguistic analysis. The system resolves
8921491	8143	Figure 1. Document sentences and questions are syntactically processed by Link Grammar, a parsing system that consists of a robust dependency-based parser and a wide-coverage grammar for English (Sleator and Temperley, 1993). In the current version of ExtrAns, anaphora resolution is restricted exclusively to pronominal cases since it is less clear how the explicit resolution of definite noun phrases and especially
19240	2254	represents the Bloom filter F(A) of the set A. Note the false positive in the set B ? F(A) that server sB sends back to server sA. Fig. 4. Network architecture and protocol overview A Bloom filter  is a hash-based data structure that summarizes membership in a set. By sending a Bloom filter based on A instead of sending A itself, we reduce the amount of communication required for sB to
19240	6538	for all of the documents containing the keyword. A horizontally partitioned index divides this list among several nodes, either sequentially or by partitioning the document identifier space. Google  operates in this manner. A vertically partitioned index assigns each keyword, undivided, to a single node. Figure 3 shows a small sample index partitioned horizontally and vertically, with K1
19240	6538	and a satisfactory number of responses . Freenet provides no search mechanism and depends instead on well-known names and well-known directories of names. Web search engines such as Google  operate in a centralized manner. A farm of servers retrieves all reachable content on the Web and builds an inverted index. Another farm of servers performs lookups in this inverted index. When the
19240	19242	set and search trace. The document set totals 1.85 GB of HTML data, comprising 1.17 million unique words in 105,593 documents, retrieved by crawling to a recursion depth of five from 100 seed URLs . The searches performed are read from a list of 95,409 searches containing 45,344 unique keywords. The search trace is the IRCache log file described in Section 1. Note that the results presented
19240	19243	key. The means for choosing the key is left for the application built on top of the DHT to determine. For example, the Chord File System, CFS , uses hashes of content blocks as keys. Freenet , which shares some characteristics of DHTs, uses hashes of filenames as keys. In each case, users must have a single, unique name to retrieve content. No functionality is provided for keyword
19240	19243	semijoin reductions. We dealt with DHT-based systems in Section 1. The others, we describe here. The first generation of peer-to-peer systems consists of Napster , Gnutella , and Freenet . Napster and Gnutella both use searches as their core location determination technique. Napster performs searches centrally on well-known servers that store the metadata, location, and keywords for
19240	14671	generation of DHTs request documents using an opaque key. The means for choosing the key is left for the application built on top of the DHT to determine. For example, the Chord File System, CFS , uses hashes of content blocks as keys. Freenet , which shares some characteristics of DHTs, uses hashes of filenames as keys. In each case, users must have a single, unique name to retrieve
19240	14671	hash range. Thus, individual machines may be assigned disproportionate numbers of keywords (recall that keywords are assigned to the host whose ID is closest to it in the hash range). Virtual hosts  are one technique to address this potential limitation. Using this approach, a node participates in a peer-to-peer system as several logical hosts, proportional to its request processing capacity.
19240	2264	represents the Bloom filter F(A) of the set A. Note the false positive in the set B ? F(A) that server sB sends back to server sA. Fig. 4. Network architecture and protocol overview A Bloom filter  is a hash-based data structure that summarizes membership in a set. By sending a Bloom filter based on A instead of sending A itself, we reduce the amount of communication required for sB to
19240	2264	Given optimal choice of hash functions, the probability of a false positive is p f p = .6185 m/n , (1) where m is the number of bits in the Bloom filter and n is the number of elements in the set . Thus, to maintain a fixed probability of false positives, the size of the Bloom filter must be proportional to the number of elements represented. Our method for using Bloom filters to determine
19240	19245	O(lgn) steps, for networks of n hosts. They keep churn costs  – the costs associated with managing node joins and departures – logarithmic with the size of the network. Using consistent hashing  they divide load roughly evenly among available hosts. Finally, they perform replication to ensure availability even when individual nodes fail. Our design uses a DHT as its base; thus, it does not
19240	19246	request routing, churn costs, load balancing, and unavailability. Most DHTs route requests to nodes that can serve them in expected O(lgn) steps, for networks of n hosts. They keep churn costs  – the costs associated with managing node joins and departures – logarithmic with the size of the network. Using consistent hashing  they divide load roughly evenly among available hosts.
19240	6542	is the order in which results are presented to the user. Many documents may match a given set of keywords, but some may be more useful to the end user than others. Google’s PageRank algorithm  has successfully exploited the hyperlinked nature of the Web to give high scores to pages linked to by other pages with high scores. Several search engines have successfully used words’ proximity
19240	939	under one kilobyte of bandwidth. Keywords: search, distributed hash table, peer-to-peer, Bloom filter, caching 1 Introduction Recent work on distributed hash tables (DHTs) such as Chord , CAN , and Pastry  has addressed some of the scalability and reliability problems that plagued earlier peer-to-peer overlay networks such as Napster  and Gnutella . However, the useful keyword
19240	3198	of bandwidth. Keywords: search, distributed hash table, peer-to-peer, Bloom filter, caching 1 Introduction Recent work on distributed hash tables (DHTs) such as Chord , CAN , and Pastry  has addressed some of the scalability and reliability problems that plagued earlier peer-to-peer overlay networks such as Napster  and Gnutella . However, the useful keyword searching
19240	188	capacity. We use three distributions for network speeds: one with all modems, one with all backbone links, and one based on the measurements of the Gnutella network performed by Saroiu et al . This last heterogeneous set contains a mixture of modems, broadband connections (cable/DSL) and high-speed LAN connections. Our CPU speed distribution is roughly a bell curve, with a mean of 750
19240	188	that a peer-to-peer search infrastructure scales with the number of participating hosts. Unless otherwise specified, the results presented in this section all assume the heterogeneous distribution  of per-peer network connectivity and the default distribution of CPU power described in Section 4. Caching and Bloom filters are both initially turned off. As shown in Figure 7(a), increasing the
19240	943	using under one kilobyte of bandwidth. Keywords: search, distributed hash table, peer-to-peer, Bloom filter, caching 1 Introduction Recent work on distributed hash tables (DHTs) such as Chord , CAN , and Pastry  has addressed some of the scalability and reliability problems that plagued earlier peer-to-peer overlay networks such as Napster  and Gnutella . However, the
19240	19249	Garcia-Molina suggest techniques to reduce the number of nodes contacted in a Gnutella search while preserving the implementation-specific search semantics and a satisfactory number of responses . Freenet provides no search mechanism and depends instead on well-known names and well-known directories of names. Web search engines such as Google  operate in a centralized manner. A farm of
19250	19252	that available on many graphics cards. Alternatively, the surface patches can be packed into texture space via a triangle packing algorithm in the manner described by Stalling  or Carr et al. . However, the packing problem becomes complex since CFD meshes are usually composed of many scalene triangles as opposed to the equilateral and isosceles triangles often found in computational
19250	19253	According to Stalling , applying LIC to surfaces becomes particularly easy when the whole surface can be parameterized globally in two dimensions, e.g., in the manner of Forssell and Cohen , . However, there are drawbacks to this approach. Texture distortions are introduced by the mapping between parameter space and physical space and, more importantly, for a large number of
19250	19255	Space vs. Object Space vs. Image Space One approach to advecting texture properties on surfaces is via the use of a parameterization, a topic that has been studied in depth, e.g., by Gorla et al.  or Levy et al. . According to Stalling , applying LIC to surfaces becomes particularly easy when the whole surface can be parameterized globally in two dimensions, e.g., in the manner of
19250	19255	shown how valleys and ridges on the surface can be detected and emphasized by lines . They have studied how texture, aligned with principal directions, can aid in understanding surface shape . Another approach to visualizing surface features is releasing particles on the surface and moving them according to a principal direction . Inspired by these results, we have studied if ISA
19250	19256	is either minimal or ??? maximal. We denote these ??? curvatures ????? ? by and . Interrante et al. have presented a variety of methods to visualize surface shape based on differential geometry . They have shown how valleys and ridges on the surface can be detected and emphasized by lines . They have studied how texture, aligned with principal directions, can aid in understanding
19250	19257	al. have presented a variety of methods to visualize surface shape based on differential geometry . They have shown how valleys and ridges on the surface can be detected and emphasized by lines . They have studied how texture, aligned with principal directions, can aid in understanding surface shape . Another approach to visualizing surface features is releasing particles on the surface
19250	19258	Heidrich et al.  exploit pixel textures to accelerate LIC computation. Jobard et al. introduced a Lagrangian-Eulerian texture advection technique for 2D vector fields at interactive frame rates , . The algorithm produces animations with high spatio-temporal correlation. Each still frame depicts the instantaneous structure of the flow, whereas an animated sequence of frames reveals the
19250	7198	computational time required to generate the results, lack of flow orientation (upstream vs. downstream), and applicability to 3D flow. Two new algorithms, namely, those introduced by Laramee et al.  (named here as ISA, Image Space Advection) and IBFVS (Image Based Flow Visualization for Curved Surfaces) by Van Wijk  have recently been introduced and overcome the computation time hurdle by
19250	7198	for two reasons: (1) not all of the vectors have to be projected (Sec. IV-D), thus saving computation time and (2) ISA may use the vectors from 3D world space in order to construct a velocity mask . The ISA approach yields the following benefits: (1) the vector field and polygon mesh are decoupled, thereby freeing up expensive computation time dedicated to polygons smaller than a single pixel
19250	7198	Flow diagrams of the two texture-based flow visualization algorithms, side-by-side. On the left is the ISA pipeline, on the right is IBFVS. The edge detection process of the original ISA algorithm  is not included in order to highlight the major differences between the algorithms. Fig. 4. The five component images, plus a sixth composite image, used for the visualization of surface flow:
19250	7198	. Figure 4 (top, right) shows a sample (? blended image ) and Figure 4 (bottom, left) shows a sample (? noise image ). More details about the noise injection process can be found in previous work , . H. Image Overlay Application For both ISA and IBFVS the rendering of the advected image and the noise blending may be followed by an image overlay. An overlay enhances the resulting
19250	7199	sample of a 3D flow field. For simplicity we assume that this velocity is confined to the surface (possibly by projection), i.e., . The more general case is discussed by Laramee ??? ??? ?s? et al.  and Van Wijk . Given a model, viewing, and perspective transformation, the mesh can be projected on the screen. We denote this by £???? , where £ ? is a 2D point on the screen and where ? ¡?£?©
19250	7199	blood vessels. An abnormal cavity has developed that hinders the optimal distribution of blood. ISA and IBFVS have also been applied to the visualization of surface topology  and isosurfaces . Fig. 13. A close-up, top view of the surface of the intake port mesh shown in Figure 12. Here we illustrate user-supported zooming with automatic, on the fly recalculation of the flow texture. B.
19250	19259	Space vs. Image Space One approach to advecting texture properties on surfaces is via the use of a parameterization, a topic that has been studied in depth, e.g., by Gorla et al.  or Levy et al. . According to Stalling , applying LIC to surfaces becomes particularly easy when the whole surface can be parameterized globally in two dimensions, e.g., in the manner of Forssell and Cohen
19250	19259	the problem of advecting texture properties across triangle edges would have to be addressed. To by-pass this, the surfaces could be divided into patches which could be stored into a texture atlas . However, much computation time would be spent generating texels which cover polygons hidden from the current point of view because packing does not consider the current viewing projection. B.
19250	3923	by the mapping between parameter space and physical space and, more importantly, for a large number of surfaces, no global parameterization is available, such as isosurfaces from marching cubes  and most unstructured surface meshes used for CFD simulations. Figures 1 top, 2 and 12 are examples of surfaces for which a global parameterization is not easily derived. Another approach to
19250	19260	directions, can aid in understanding surface shape . Another approach to visualizing surface features is releasing particles on the surface and moving them according to a principal direction . Inspired by these results, we have studied if ISA and IBFVSsIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 10 Fig. 14. Medical visualization: blood flow at the surface of the junction of
19250	19262	in 2D. Cabral and Leedom  present a parallel processing implementation of LIC. Max and Becker were early to introduce the idea of moving textures in order to visualize vector fields . Heidrich et al.  exploit pixel textures to accelerate LIC computation. Jobard et al. introduced a Lagrangian-Eulerian texture advection technique for 2D vector fields at interactive frame rates
19250	19262	the vector field, both ISA and IBFVS compute the texture coordinates used to advect the textures in image space. Both ISA and IBFVS use backward coordinate integration (introduced by Max and Becker ): ? ????? ? ? ? ?s??? ¡ ? ? ¥¨§¨©???§ (12) k = k + 1 Static Case wheres? represents a magnitude and direction after projection to the image plane. In other words, the texture is advected over the
19250	19264	Another approach to advecting texture properties on surfaces would be to immerse the mesh into a 3D texture. Then the texture properties could be advected directly according to the 3D vector field . This would have the advantage ofsIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 4 simplifying the mapping between texture and physical space and would result in no distortion of the
19250	19265	steady-state flow. In addition, we have not seen much work in the area of texture-based flow visualization on surfaces in general since the introduction of UFLIC (Unsteady Flow LIC) by Shen and Kao  in 1998, which also used a parameterization. One open question concerning the performance times of ISA and IBFVS arises in the case of programmable graphics hardware. In the case of surfaces the
19250	19266	Traditional visualization of boundary flow using texture mapping first maps one or more 2D textures to a surface geometry defined in 3D space. The textured geometry is then rendered to image space . Here, we alter this classic order of operations. First we project the surface geometry and its associated vector field to image space and then Manuscript received February 26, 2004 Robert S.
19250	19266	on complex, non-parameterized surfaces. The challenge of visualizing time-dependent vector fields on surfaces at fast frame rates remains unsolved in surveys of the research literature , , . However, several techniques have been proposed to successfully resolve parts of the problem. In the next two sections we describe the two main categories of approaches for dense representations on
19250	19266	white noise on triangle meshes in 3D space and extended LIC for visualizing a vector field on arbitrary surfaces in 3D. Stalling provided a helpful overview of LIC techniques applied to surfaces  in 1997. In particular, a useful comparison of parameterized vs. non-parameterized surfaces is given. B. 2D, Unsteady Flow Much work has been done in order to speed up texture-based flow
19250	19266	to advecting texture properties on surfaces is via the use of a parameterization, a topic that has been studied in depth, e.g., by Gorla et al.  or Levy et al. . According to Stalling , applying LIC to surfaces becomes particularly easy when the whole surface can be parameterized globally in two dimensions, e.g., in the manner of Forssell and Cohen , . However, there are
19250	19266	required would exceed that available on many graphics cards. Alternatively, the surface patches can be packed into texture space via a triangle packing algorithm in the manner described by Stalling  or Carr et al. . However, the packing problem becomes complex since CFD meshes are usually composed of many scalene triangles as opposed to the equilateral and isosceles triangles often found in
19250	7208	as a function of time, referred to as the Lagrangian step, while the color distribution of the image pixels is updated in place (Eulerian step). Image Based Flow Visualization (IBFV) by Van Wijk  is one of the most recent algorithms for synthesizing dense, 2D, unsteady vector field representations. It is based on the advection and decay of textures in 2D. Each frame of the visualization is
19250	7208	visualization on surfaces, we present the framework upon which ISA and IBFVS are built, with emphasis on the synthesis of dense textures in two dimensions. More details are given by Van Wijk . A. Texture Synthesis in 2D Suppose we have an unsteady, two-dimensional vector fields¢¡¤£¦¥¨§¨©???????? withs?¡?£¦¥¨§¨©???? ????¡¤???¨??¥?§¨©??¨????¡???????¥¨§¨©?? (1) defined for §???? , £???? ,
19250	7208	the noise injection and blending phase falls in line with the original IBFV. The process is purely image based for both ISA and IBFVS. Namely, an image, ? , is related to a previous image, ? , by  ????? ? © ? ¡ ? ??? ????? ???????¡ ? ??????? © (14) ¥ ¡ ? ¥ ? ©?? ? ? ? where represents a pathline ? and defines a blending ?¦? coefficient. Thus ??? a point, , of ??? an image , is the result of a
19250	7208	4 (top, right) shows a sample (? blended image ) and Figure 4 (bottom, left) shows a sample (? noise image ). More details about the noise injection process can be found in previous work , . H. Image Overlay Application For both ISA and IBFVS the rendering of the advected image and the noise blending may be followed by an image overlay. An overlay enhances the resulting texture-based
19250	7209	to 3D flow. Two new algorithms, namely, those introduced by Laramee et al.  (named here as ISA, Image Space Advection) and IBFVS (Image Based Flow Visualization for Curved Surfaces) by Van Wijk  have recently been introduced and overcome the computation time hurdle by generating a dense representation of flow on surfaces at fast frame rates, even for unsteady flow. ISA and IBFVS generate
19250	7209	field. For simplicity we assume that this velocity is confined to the surface (possibly by projection), i.e., . The more general case is discussed by Laramee ??? ??? ?s? et al.  and Van Wijk . Given a model, viewing, and perspective transformation, the mesh can be projected on the screen. We denote this by £???? , where £ ? is a 2D point on the screen and where ? ¡?£?© ? denotes the
19250	7209	the junction of three blood vessels. An abnormal cavity has developed that hinders the optimal distribution of blood. ISA and IBFVS have also been applied to the visualization of surface topology  and isosurfaces . Fig. 13. A close-up, top view of the surface of the intake port mesh shown in Figure 12. Here we illustrate user-supported zooming with automatic, on the fly recalculation of
19250	7209	settings for colors and bounds for the parameterization can be defined that give good results for a variety of geometries. Further examples of surface visualization are illustrated by Van Wijk . C. Performance We have implemented both ISA and IBFVS in the same software application in order to facilitate comparison of the two. Our implementation is based on OpenGL 1.1. Performance for both
19250	7211	time and perceptual issues. Future work also includes the application of more specialized graphics hardware features like programmable per-pixel operations in the manner of Weiskopf et al. . ACKNOWLEDGMENTS Portions of this work have been done via a cooperation between two research projects of the VRVis Research Center (www.VRVis.at) which is funded by the Austrian research program
8921495	20454	find a maximum in the tropospheric O3 column over the Middle East in September-October. Wang et al.  and Mickley et al.  show a broad Eurasian maximum in summer. Jonson et al.  show high values (>90 ppbv) at 500 hPa over the Middle East in July, but even higher values at northern midlatitudes. Horowitz et al.  show a 500 hPa O3 maximum over the Middle East in July.
8921495	20454	of O3 in the model uses a chemical mechanism with 120 species to describe tropospheric O3-NOxhydrocarbon chemistry. A detailed description and evaluation of the model is given by Bey et al. . Anthropogenic emissions are specified using a base emission inventory for 1985 scaled to 1993-1995 following Bey et al. . The base inventory includes NOx emissions from Benkovitz et al.
8921495	19287	low. Removal of O3 from the region is mostly by outflow to the Indian Ocean and to the Sahara at about 700 hPa. Sources Contributing to the Ozone Maximum We used tagged O3 (odd oxygen) tracers  and sensitivity simulations to further understand the geographical source regions and precursor emissions contributing to the Middle East maximum. The tagged tracer simulation transports as
8921495	19287	low. Removal of O3 from the region is mostly by outflow to the Indian Ocean and to the Sahara at about 700 hPa. Sources Contributing to the Ozone Maximum We used tagged O3 (odd oxygen) tracers  and sensitivity simulations to further understand the geographical source regions and precursor emissions contributing to the Middle East maximum. The tagged tracer simulation transports as
4851177	19321	increases. When the queue size is larger than 50 packets the rate of each VALM session converges to a stable value. Another observation, as expected, is that the rate gets stable (no oscillation ) when the queue size is larger than ? · Number connections; in our case the number of connections is 10, then a queue size larger than 40 packets is needed to get a stable rate for VALM. 5
4851177	19322	other hand, it may penalize the convergence of the algorithm for long sessions. In the present paper we present a simple scheme based on the queuing delay estimation, and inspired from TCP Vegas . For optimality reasons, our algorithm needs to estimate the round trip time only once over the session, and thus introduces a negligible overhead. However, it can work well without any round trip
4851177	19322	somewhere (both can send at the maximum band? K, width B), then the shares would satisfy Bi Bj where K is a constant. TCP Vegas is an example of algorithms that satisfy the bounded fairness . The algorithm we propose below ensures a bounded fairness between the receivers of the same session as well as between different sessions. 3 Rate adaptation using RTT variations The scheme we
4851177	19323	it is up to the receivers to choose the appropriate number of layers they can receive with a reasonable loss rate. Several algorithms have been proposed for the rate adaptation at the receivers  to cite just few. In all these algorithms the persisting problem of one layer oscillation is not completely solved. Indeed, when the estimated loss rate at a given receiver is small enough, the
4851177	2243	may be worse if several sessions compete for an available bandwidth that is not sufficient for supporting any session’s new layer . In , a back-off mechanism, similar to TCP’s timeout , is introduced in order to reduce these unnecessary joins. When a receiver joins a new layer and the join fails within a certain period of time ”D”, then the join timer for that layer is backed-off
4851177	2243	window in TCP represents the number of packets (amount of data) sent but not yet acknowledged (packets in transit). Thus, by varying the size of this window, TCP controls its sending rate . The congestion window is increased if no losses are observed otherwise it is decreased. In Vegas version of TCP, the mean round trip time RT T is estimated every time an acknowledgment is
4851177	5723	that try to mimic the behavior of an ideal TCP connection. TCP-Friendly algorithms are more fair, in the sense that the bandwidth a connection will get is close to that of an ideal TCP connection . An ideal TCP connection would use a bandwidth given by the following formula : C · MT U B = RT T · ? , (1) P where, C is a constant (ranging from 1.22 to 1.3 ), MTU is the mean
4851177	5723	rate B(li) at which RTP packets arrive when it has subscribed to i layers as well as the loss ratio P (li) by observing the sequence number of the received packets. It joins an upper layer only if  P (li) < Lmini ? = 1.613 · ?(li) RT T 2 · B(li) 2 and it leaves the upper layer only if P (li) > Lmaxi ? = 1.613 · ?(li) RT T 2 · B(li) 2 (2) (3) 1 In the rest of the paper we will express B in
4851177	5363	it is up to the receivers to choose the appropriate number of layers they can receive with a reasonable loss rate. Several algorithms have been proposed for the rate adaptation at the receivers  to cite just few. In all these algorithms the persisting problem of one layer oscillation is not completely solved. Indeed, when the estimated loss rate at a given receiver is small enough, the
4851177	5363	different sessions (inter-session fairness). The problem may be worse if several sessions compete for an available bandwidth that is not sufficient for supporting any session’s new layer . In , a back-off mechanism, similar to TCP’s timeout , is introduced in order to reduce these unnecessary joins. When a receiver joins a new layer and the join fails within a certain period of time
4851177	5363	to space limitations, we’ll focuss more on the description of our algorithm rather than its evaluation; therefore many simulations where we evaluate our algorithm andscomparing it to others (mainly ), in terms of covergence, stability and intra as well as intersession fairness, will be subject of a future communication. 2 Background 2.1 Feedback information Internet real-time multimedia
4851177	5363	in order to avoid successive joins and drops, thus drops or joins are separated at least by the timeout D (see figure 1). This timeout (D) is estimated as in RLM (Receiver-driven Layered Multicast) . We first estimate the detection time (TD) each time a layer is dropped. We used an estimator for each layer; when layer i is dropped, T D is updated to the time T elapsed between the last join,
4851177	5363	The measurement interval for the loss estimation is set to 0.5 seconds, and the duration of all the simulations is 2000 seconds. The stream sent by the multicast session is composed by 10 CBR-like  equivalent layers of 50 kbps each. 4.1 VALM vs RLM As seen in many simulations, RLM  is indeed fair when the connections start almost at the same time. However, when a connection (a TCP
4851177	5395	subject of a future communication. 2 Background 2.1 Feedback information Internet real-time multimedia applications generally use the Real-Time Transport Protocol (RTP) to transport their data. RTP  (data) packets contain header information that helps these applications to perform a number of typically needed tasks, such as payload type identification, packet sequence reordering, media
4851177	5395	of the clocks, just assuming no drift. B , resp. ? ? = RT T ? T > ? 3.2 Round Trip time and queuing delay estimation To estimate the round trip time, one solution is the use of RTCP reports , so that the source can estimate its round trip time to a given receiver as follows: each Receiver Report (RR) contains, for each RTP source, the timestamp (tlsr) carried in the last RTCP Sender
4851177	19327	it is up to the receivers to choose the appropriate number of layers they can receive with a reasonable loss rate. Several algorithms have been proposed for the rate adaptation at the receivers  to cite just few. In all these algorithms the persisting problem of one layer oscillation is not completely solved. Indeed, when the estimated loss rate at a given receiver is small enough, the
19333	19334	or “mice”), and a significant number of connections have lifetimes of hours and days (referred to as “tortoises” or “elephant”) and carry a high proportion (50% or 60%) of the total carried bytes . Lifetime-based classification schemes have been advocated  to protect short-lived TCP flows from the negative impact of longlived TCP flows. Namely, it has been observed that short-lived
19333	19334	varies from 50% to 70% of the total available demands. Although the fraction varies with the definition of long-lived flows, we argue that 60% is a reasonable approximation for the total demand  with our definition of long-lived flows. However, we also study cases where the allocation between the two classes is dynamic and based on estimates of the demands based on recent measurements
19333	2679	of hours and days (referred to as “tortoises” or “elephant”) and carry a high proportion (50% or 60%) of the total carried bytes . Lifetime-based classification schemes have been advocated  to protect short-lived TCP flows from the negative impact of longlived TCP flows. Namely, it has been observed that short-lived flows are at a disadvantage when competing against long-lived flows.
19333	19335	**1.74) 0 0 20 40 60 80 100 120 140 160 180 200 (c) Threshold=200 Figure 3: Pareto distribution fit on Trace 1 for different long-lived flow separation thresholds. Statistical study of LBL traces  showed that the upper-tail burst size is well-modeled by Pareto distribution 4 . Our analysis was conducted on traces collected by the University of Auckland  on an OC3 link carrying WAN
19333	19336	of hours and days (referred to as “tortoises” or “elephant”) and carry a high proportion (50% or 60%) of the total carried bytes . Lifetime-based classification schemes have been advocated  to protect short-lived TCP flows from the negative impact of longlived TCP flows. Namely, it has been observed that short-lived flows are at a disadvantage when competing against long-lived flows.
19333	19336	The bandwidth of TCP flows under DiffServ architectures was studied extensively in . In particular, the effect of isolation of UDP, short- and long-lived TCP flows was investigated in . However, the traffic under study in  uses TCP flows with the same RTT. The interaction between TCP flows of different RTTs is not present in the traffic. Moreover, it appears that the input
19333	19336	model in  where the full extent of the TCP congestion control algorithm was not simulated but only parts of it. With respect to UDP traffic, we do not add anything more than already present in , and therefore restrict our discussion to handling of TCP control only. In particular, we study several classification schemes that are combinations of lifetime classification and/or RTT
19333	19337	of hours and days (referred to as “tortoises” or “elephant”) and carry a high proportion (50% or 60%) of the total carried bytes . Lifetime-based classification schemes have been advocated  to protect short-lived TCP flows from the negative impact of longlived TCP flows. Namely, it has been observed that short-lived flows are at a disadvantage when competing against long-lived flows.
19333	2242	unfair. By attributing the unfairness to the router drop policy, proposals have indicated how it should be changed to alleviate the problem. One such proposal is Random Early Detection (RED) . Its essence is that, although not all of the TCP flows will receive congestion signals in times of congestion, the TCP flows with more packets stored in the buffer are more likely to receive such
19333	2242	is far from sufficient to ensuring fairness among long–lived TCP flows and protecting short-lived flows from the negative impact of long-lived flows. The fact of unfairness is already known since  but it helps to know what magnitude of unfairness we are up against. Figure 1 demonstrates the ratio of the throughput received by two flows, as a function of the ratio of their corresponding
19333	19338	related control schemes can be substitute, or complement, to RED. Because RED requires dedicated parameter tuning which appears to be highly sensitive to the particular network environment scenario , the proposed classification schemes splits the incoming traffic in enough classes that each one of them is effectively manageable using RED or DropTail. In other words, we trade the complexity of
19333	19339	related control schemes can be substitute, or complement, to RED. Because RED requires dedicated parameter tuning which appears to be highly sensitive to the particular network environment scenario , the proposed classification schemes splits the incoming traffic in enough classes that each one of them is effectively manageable using RED or DropTail. In other words, we trade the complexity of
19333	20523	Statistical study of LBL traces  showed that the upper-tail burst size is well-modeled by Pareto distribution 4 . Our analysis was conducted on traces collected by the University of Auckland  on an OC3 link carrying WAN traffic, and are summarized in Table 1. In our experiments, we use the SYN and FIN flags of the TCP handshake as indications of the start and end of a particular 4 the
19333	19341	related control schemes can be substitute, or complement, to RED. Because RED requires dedicated parameter tuning which appears to be highly sensitive to the particular network environment scenario , the proposed classification schemes splits the incoming traffic in enough classes that each one of them is effectively manageable using RED or DropTail. In other words, we trade the complexity of
19333	20525	of different classes are not completely separated, since lack of backlog in one class allows the other classes to be serviced more frequently, consistent with the WFQ scheduler operation. The ns  simulator was used as the simulation platform and the particular topology is the typical dumbell topology with one bottleneck link where connections with different propagation delays compete. The
19333	19343	quality of service received by short-lived flows is better characterized as response delay, which is defined as the period from delivering the first packet to delivering the last packet of a flow . We also inspect and report the aggregate goodput of short-lived flows as a whole. As we vary the intensity of traffic of short-lived flows, we are interested in the overall bandwidth obtained by
19333	19344	between long-lived TCP connections and short-lived connections, fitness of short-lived TCP connections has only limited impact on our schemes and it is hereby ignored. We note that previous studies  did not show what constitutes a threshold between short and long–lived flows or if it exists in all cases. As a trade off between the processing cost and the connection length distribution
19333	19345	consider the separation and separate control of TCP traffic classes along the two dimensions (lifetime and RTT). The bandwidth of TCP flows under DiffServ architectures was studied extensively in . In particular, the effect of isolation of UDP, short- and long-lived TCP flows was investigated in . However, the traffic under study in  uses TCP flows with the same RTT. The interaction
19333	19346	consider the separation and separate control of TCP traffic classes along the two dimensions (lifetime and RTT). The bandwidth of TCP flows under DiffServ architectures was studied extensively in . In particular, the effect of isolation of UDP, short- and long-lived TCP flows was investigated in . However, the traffic under study in  uses TCP flows with the same RTT. The interaction
19333	19347	consider the separation and separate control of TCP traffic classes along the two dimensions (lifetime and RTT). The bandwidth of TCP flows under DiffServ architectures was studied extensively in . In particular, the effect of isolation of UDP, short- and long-lived TCP flows was investigated in . However, the traffic under study in  uses TCP flows with the same RTT. The interaction
19351	19356	while not significantly changing the perceptual characteristic of the speech signal. A final illustration, Figure 7 presents the results of applying the LAR distance speech quality measure  to the distorted and filtered signals. This measure is based on a finding a set of Linear Predictive Coefficients (LPC) for each frame of the distorted/filtered speech signals and the original
11095323	19361	communities from link information As the WWW grows in size and complexity, inferring high-level structure on the Web becomes increasingly important. There has been a growing amount of work  directed at the integration of textual content and link information to infer structures of communities on the Web. Pitkow and Pirolli  found that co-citation analysis  can be helpful for
11095323	19362	communities from link information As the WWW grows in size and complexity, inferring high-level structure on the Web becomes increasingly important. There has been a growing amount of work  directed at the integration of textual content and link information to infer structures of communities on the Web. Pitkow and Pirolli  found that co-citation analysis  can be helpful for
11095323	19366	coupling quantity is the number of documents that cite both p and q. The larger these quantities are, the more likely p and q are about research in the same field. Gibson and Kleinberg  try to find interesting communities of pages on the Web through an analysis of link topology. The communities can be viewed as containing a core of central, &quot;authoritative pages&quot; linked together by
11095323	19366	hierarchy even if the node-to-node distance of pages is not taken into account. Moreover, we differentiate homepages from objects on them, instead of mixing them all in a hypertext structure as in  and , so that we can take into account virtual objects, whether they be HTML documents or not. Finally, the relatedness function can work well independently of whether a page or a site is chosen
257634	19374	even if some other part of S is given to an adversary. 3 In the past few years, composable security has attracted a lot of interest and lead to important new denitions and proofs (see, e.g.,  or ). Recently, the notion of universal composability has been generalized to the quantum case . Universally composable security denitions are usually based on the idea of characterizing
257634	19378	e.g., only the mutual information between the key S and the outcome of an arbitrary measurement of the adversary's quantum state is required to be small (for a formal denition, see, e.g.,  or ). 6 Let ussrst state some technical lemmas to be used for the proof of Theorem 4.1. Lemma 4.3. Let Z be a random variable with range Z, let  be a random state, and let F be a random function with
8718103	19390	As can be seen, the four packages use different search heuristics, including evolution strategies (Bäck 1996, Bäck and Schwefel 1993), neural networks (Bishop 1995, Haykin 1999), scatter search (Glover 1999), simulated annealing (Aarts and Korst 1989), and tabu search (Glover and Laguna 1997). Note that genetic algorithms (Michalewicz 1996) is another well-known heuristic that is used for
11489891	19400	Our approach can integrate with DiffServ and IntServ to aware them of multimedia packets inherit properties. This is done by replacing the queuing system with our PAQ (see section 3). Some works  try to achieve multimedia quality by ignoring the network and supposing it to be like ATM that inherently has Qos mechanisems like similar to IntServ, and then working in the end host. However,
11489891	19405	intensive and inherently slow, but considering the relatively lower traffic of the edge routers, and using fast algorithms such as common multiple-field packet classification algorithms ,  it can be done reasonably well. In DMDP, the standard router queue has been replaced by a new Priority Aged Queue (PAQ). PAQ classifies incoming packets according to their required class of service
19406	19407	oilrig operations in the North Sea. We have used a variety of knowledge acquisition (KA) techniques developed in the expert system community to understand how humans perform weather forecasting . 1 Department of Computing Science, University of Aberdeen, Aberdeen, Scotland, U.K., email: {ssripada,ereiter}@csd.abdn.ac.uk. 2 Weathernews (UK) Ltd, Aberdeen, Scotland, U.K., email:
19406	19408	the lessons we learnt from our experience of building, installing and maintaining SUMTIME-MOUSAM. 2 SUMTIME-MOUSAM SUMTIME-MOUSAM follows the simple pipeline architecture for text generation  as shown in Figure 1. Input to SUMTIMEMOUSAM is obtained by sampling forecaster edited data from the NWP model prediction at the required grid point.sInput Data Document Planning Figure 1.
19406	8753	Content determination involves selecting ‘important’ or ‘significant’ data points from the underlying weather data to be included in the forecast text using the bottom-up segmentation algorithm . Marfors Data Editor Initial Data Marfors Data Editor Edited Data Output Text Realization NWP Data SUMTIMEMOUSAM SUMTIMEMOUSAM Micro-planning: This stage is responsible for lexical selection,
19406	19409	Content determination involves selecting ‘important’ or ‘significant’ data points from the underlying weather data to be included in the forecast text using the bottom-up segmentation algorithm . Marfors Data Editor Initial Data Marfors Data Editor Edited Data Output Text Realization NWP Data SUMTIMEMOUSAM SUMTIMEMOUSAM Micro-planning: This stage is responsible for lexical selection,
19406	19413	aligned to ‘gradually increasing SSW34-39’. For this example pair, we count ‘then’ deletion, ‘SSW’ addition and ‘midnight’ deletion. More details of our evaluation work have been described in . The results of our evaluation have been shown in the following table by classifying the mismatches:sS. No. Mismatch Type % 1. Ellipses (word additions and deletions) 65 2. Data Related
19406	19414	first version of SUMTIME-MOUSAM was developed based on a method suggested by one of the experts from Weathernews. This method used what can be called a template-based approach to text generation . Essentially a template-based approach is based on manipulation of character strings to produce text output. There is no explicit linguistic knowledge in these systems and also there is no
19419	20608	of networks. The nature of the wireless channel, the lack of synchronization, and also the lack of any predetermined topology creates many challenging research topics in the area of ad hoc networks , . Traditionally, research has been concentrated on random access –, transmission scheduling , and routing , . Networks with energy constraints are also being studied ,
19419	20609	The nature of the wireless channel, the lack of synchronization, and also the lack of any predetermined topology creates many challenging research topics in the area of ad hoc networks , . Traditionally, research has been concentrated on random access –, transmission scheduling , and routing , . Networks with energy constraints are also being studied , .
19419	19425	creates many challenging research topics in the area of ad hoc networks , . Traditionally, research has been concentrated on random access –, transmission scheduling , and routing , . Networks with energy constraints are also being studied , . Lately, there has also been work on determining the capacity of ad hoc networks. In a recent landmark paper , the
19419	9284	many challenging research topics in the area of ad hoc networks , . Traditionally, research has been concentrated on random access –, transmission scheduling , and routing , . Networks with energy constraints are also being studied , . Lately, there has also been work on determining the capacity of ad hoc networks. In a recent landmark paper , the authors
19419	3551	routing , . Networks with energy constraints are also being studied , . Lately, there has also been work on determining the capacity of ad hoc networks. In a recent landmark paper , the authors derived lower and upper bounds on the performance of a class of networks in the limit of a large number of nodes, in terms of a single figure of merit, the maximum uniformly achievable
19419	3551	as we add more nodes, which implies that networks can take advantage of spatial separation even when more nodes are placed in the same area. This finding is reminiscent of the results appearing in . We have also established that the relative performance shown in the previous figures does not change under a wide range of the transmitter powers, thermal noise powers, and the exponential decay
19419	19431	for any duration , which implies that the achievable data rate is independent of the number of bits transmitted. Our formulation is not consistent with previous information theoretical approaches , ; however, it is well justified for any practical modulation and coding strategy, as long as the number of bits transmitted is much greater than the bits per symbol (for uncoded modulation),
8921527	19433	In these approaches, texts are the main source for knowledge acquisition . Concepts and relations result only from a corpus analysis, without external knowledge. Aussenac-Gilles and al.  follow this point of view but affirm that there can be other knowledge sources than corpus. Such an approach includes two steps. The first one consists of the construction by a corpus
8921527	19433	to terms (conceptual primitives ) and the extraction of lexical relations. The result of this stage is a base of primitive concepts and first concept relations (terminological knowledge base  & ). In this stage, the designer has to select terms and relations that will be modeled, those that are relevant and in the case of several meanings for one term or relation, which one must be
8921527	19437	set of concepts and relations is also structured into a concept semantic network. An expert of the corpus domain must validate this network to express which relations are relevant (normalization ). The result of the entire process is a hierarchical structure of a set of terms of the domain . This model is also an ontology, which can be formalized by a formal or semi-formal
8921527	19439	each one a unique ancestor whose last heir is a basic concept. These hierarchies are normalized: each father is divided into sons by a unique criterion. They also respect the Guarino-Rigid-Property . 3.3.3 Models Unification The two precedent processes give ontology linked to the current problem and a hierarchical structure linked to texts. We proceed to the unification of these two models.
8921527	19440	relations between concepts (there is a relation between two concepts when one of them is an attribute of the other). These relations are in accordance with the Attribute Consistency Postulate : in each predicative relation, any value of an attribute is also an instance of the concept corresponding to that attribute. 3.2 Terminology definition The terminology is built from the union of
8921527	19441	terms (conceptual primitives ) and the extraction of lexical relations. The result of this stage is a base of primitive concepts and first concept relations (terminological knowledge base  & ). In this stage, the designer has to select terms and relations that will be modeled, those that are relevant and in the case of several meanings for one term or relation, which one must be kept.
8921527	19444	corpus domain must validate this network to express which relations are relevant (normalization ). The result of the entire process is a hierarchical structure of a set of terms of the domain . This model is also an ontology, which can be formalized by a formal or semi-formal representation. Using such a method is powerful for syntactically correct texts but not for poor quality corpora.
19445	539	in participating media. Pharr and Hanrahan  also provide an extensive list of existing methods. One simple approximation is to only consider single scattering, as done by Ebert and Parent , Sakas , Max , Nakamae et al. , and Nishita et al. . However, they cannot easily reproduce important qualitative effects due to multiple scattering like glows around
19445	19459	and highly directional effects. In the context of subsurface scattering, fast integration techniques, bearing some similarity to our approach have been developed. For example, Jensen and Buhler  extended the diffusion approximation to be computationally more efficient by precomputing and storing illumination in a hierarchical grid. Lensch et al.  implemented this method in
19445	17361	approach is to use the diffusion approximation, first introduced in graphics by Stam , following conceptually similar work by Kajiya and von Herzen . More recently, Jensen et al.  (also Koenderink and van Doorn ) applied it to subsurface scattering, deriving a simple analytic formula. This provides a practical approach to a problem that had previously required an
19445	17361	media, where one can assume the angular distribution of radiance is nearly uniform. It is also technically valid only for infinite plane-parallel media. Further, the approach of Jensen et al.  is specialized to subsurface effects on objects, and cannot be easily extended to volumes. Our method can be seen as analogous to the diffusion point-spread function for general media, where we
19445	19461	sources, or subsurface scattering. Another approach is to use the diffusion approximation, first introduced in graphics by Stam , following conceptually similar work by Kajiya and von Herzen . More recently, Jensen et al.  (also Koenderink and van Doorn ) applied it to subsurface scattering, deriving a simple analytic formula. This provides a practical approach to a
19445	19477	in computer graphics. The interested reader is referred to a survey by Perez et al.  for detailed classification of global illumination algorithms in participating media. Pharr and Hanrahan  also provide an extensive list of existing methods. One simple approximation is to only consider single scattering, as done by Ebert and Parent , Sakas , Max , Nakamae et al.
19445	19482	effects due to multiple scattering like glows around light sources, or subsurface scattering. Another approach is to use the diffusion approximation, first introduced in graphics by Stam , following conceptually similar work by Kajiya and von Herzen . More recently, Jensen et al.  (also Koenderink and van Doorn ) applied it to subsurface scattering, deriving a
10779	499	is evaluated by XOR’ing consecutive time instants. The method relies on the creation of a symbolic network which can become quite large. To perform exact switching activity estimation, BDDs  have to be created for each output of the symbolic network, which can be very time-consuming. To handle transition probabilities at primary inputs the method requires constraints on the BDD
10779	499	For example, in the simple circuit of Figure 1, with unit gate delays we will have, for the various signals, the following polynomial waveforms, a : Pa b : Pb c : Pc g : Pg, Pg f : Pf , Pf , Pf  representing the different time instants that each input/gate can make transitions. We need a polynomial simulation algorithm that can simulate a gate-level network with
10779	499	the following observations apply equally well to the general-delay model. At node d we have polynomials corresponding to instants 1 and 2, both a function of Pa and Pb, respectively Pd(Pa, Pb) and Pd(Pa, Pb). If the variable Pa is replaced by its numerical value, thus obtaining P ? d(Pb) and P ? d(Pb), the temporal correlation between P ? d and P
10779	19492	temporal correlation are ignored. The transition density work of Najm  introduces temporal correlation, but still ignores correlation between internal signals. Improvements to the basic strategy  model some internal correlation, but do not serve as a basis for an exact method. In , signal probability evaluation and power estimation is based on pairwise correlations between signals. This
10779	12168	under a zero-delay model the method is still exact (this may not be true for a general delay model, which we analyze in the next section). It is useful to introduce the concept of graph dominator . Definition 2: A vertex v dominates another vertex w ?= v in a directed graph G if every path from the root vertex to w contains v. Thus, if we determine that a given gate g is the dominator of
10779	19493	correlation, but still ignores correlation between internal signals. Improvements to the basic strategy  model some internal correlation, but do not serve as a basis for an exact method. In , signal probability evaluation and power estimation is based on pairwise correlations between signals. This results in efficient estimation schemes, however, correlation between triplets of signals
10779	19494	between these instants. Still, correlation between internal signals is ignored. Tsui  extends Najm’s method by including some correlation coefficients in the probability waveforms. In , Boolean functions representing all possible logical values at each time point for each gate are computed, and the probability of switching activity is evaluated by XOR’ing consecutive time
10779	19494	on limited depth reconvergent path analysis described in the previous sections. We present results for different values of l and compare them with the exact value obtained with symbolic simulation . The first part of the approximation algorithm involves computing TABLE III POWER ESTIMATION RESULTS. for each node in the circuit the set of active nodes, i.e., the variables the polynomials at
10779	19494	of 20 MHz was assumed. A probability of 0.25 was used for all primary input events. The two columns under “Symbolic” show the power (in µW ) computed using the symbolic simulation method of  and the CPU time (in seconds) taken by this computation. For some of the circuits, this method run out of memory and this is indicated with a “N/A” in the table. In the columns under “l = 1” are
10779	19495	proposed. The use of probabilities to estimate power was first proposed by Cirit . In this work, both signal spatial and temporal correlation are ignored. The transition density work of Najm  introduces temporal correlation, but still ignores correlation between internal signals. Improvements to the basic strategy  model some internal correlation, but do not serve as a basis for an
10779	19496	(glitching) at the output of a gate. Due to different input path delays, gates may switch more than once during a clock cycle. In order to model general-delay transport delays, Najm proposes in  propagating probability waveforms through the circuit. These represent the time instants where nodes can toggle, together with information about static signal probability between these instants.
8631290	19504	simulators, NS2 is the most widely used simulator for evaluating the behavior of TCP, the TCP variants that have been proposed by researchers, and the behavior of routed UDP in large networks (Breslau, et al. 2000). NS2 is able to simulate the behavior of these protocols under various forms of stress, such as might be caused by competition for network resources when multiple applications share a network and
8631290	19510	The flexibility agents provide can be used to more efficiently link simulation components together, such as by using filters to reduce inter-simulation traffic. An example of this can be found in (Wilson, et al. 2000). The second class of simulations use agents to model entities within a simulated world. (Lee, et al. 2001) employs a mix of continuous and discrete-time simulators in an air-traffic control
8921533	19520	For a thorough, complete and even historical analysis of this matter the reader can be referred to Feferman , although, there are many other papers continuously re-examining this point . Self-referencing languages surely imply a fundamental dilemma within philosophical logic. However, this property also demands particular interest from the Artificial Intelligence community.
8921533	19520	of limited space, individual instances of the above categories are not discussed here. The former is followed by Tarski and Kerber , while the latter is preferred by Feferman  and Perlis . 3 An examination of formal liar sentences As was shown previously, naive truth theory is considered to be demolished by paradoxes of self-reference, inspiring to create alternative theories
8921533	19520	of the liar sentence challenging naive truth theory. 3.1 Liar sentence translated by biconditional Many authors discussing the liar paradox represent sentence (2) as was shown in section 2.1, e.g. . Obviously, l ? ?t(?l?) (4) is not a literal translation of (2), thus many authors disagree with this formula. Another reason against the use of the biconditional here is that according to
8921533	20715	model of the phenomenon. We present an alternative and novel manner of representing the matter relying on intensional conformal text representation language (iCTRL) initiated by one of the authors . It is a knowledge representation tool closer to natural languages, preserving not only truth as traditional logical language does, but it also models natural grammatical relations. Accordingly, it
8921533	20715	at the same time it allows verification of soundness of the preceding issues. There is only limited space here to give a detailed formal introduction to iCTRL, so the reader is referred to . Although, a rather reduced sublanguage of iCTRL is sufficient for the whole description, the reader should be made familiar with some new notation. We focus immediately on truth definition. The
8921533	20716	model of the phenomenon. We present an alternative and novel manner of representing the matter relying on intensional conformal text representation language (iCTRL) initiated by one of the authors . It is a knowledge representation tool closer to natural languages, preserving not only truth as traditional logical language does, but it also models natural grammatical relations. Accordingly, it
8921533	20716	at the same time it allows verification of soundness of the preceding issues. There is only limited space here to give a detailed formal introduction to iCTRL, so the reader is referred to . Although, a rather reduced sublanguage of iCTRL is sufficient for the whole description, the reader should be made familiar with some new notation. We focus immediately on truth definition. The
8921536	19536	traditional biomedical scientists that computer models are not appropriate candidates as targets for experimentation. At the heart of this problem is a logical error called “inscription error??? , wherein it is believed that a computer model will only demonstrate behavior that its human author designed in. While this is, of course, true, it presumes that the limits of computation are short
8921541	19541	the only one that cannot cause the symptom. This fact is just lost in the original answer. As our sentence planner, we chose EXEMPLARS (White & Caldwell 1998) over better known systems such as FUF (Elhadad 1993) and Penman (Bateman 1994) because of the complexity and learning curve of the latter two. Efficiency and rapid prototyping are among the reasons we chose EXEMPLARS. EXEMPLARS is an
8921541	19542	the metrics we will discuss later (such as task performance measures) may be appropriate as well. To our knowledge, the only ITSs with an NL interface which has been formally evaluated is CIRCSIM (Evens et al. 1993; Kim, Glass, & Evens 2000), but the results of the evaluation are not available yet. We will first discuss DIAG, the ITS authoring shell we are using. We will then discuss the work we have
8921541	19543	a fullfledged dialogue interface, e.g. see the work at the CIRCLE 1 center (http://www.pitt.edu/˜circle/), or (Hume et al. 1996; Moore, Lemaire, & Rosenbloom 1996; Rosé, Di Eugenio, & Moore 1999; Freedman 1999; Graesser et al. 2000). On the contrary, our approach to adding NLG capabilities to an Intelligent Tutoring System falls on the weak side of the divide: we are concentrating on simple sentence
8921541	11596	of the message to be conveyed. Further, in order to improve our surface realization capabilities, we are planning to use a different version of EXEMPLARS which is integrated with RealPro (Lavoie & Rambow 1997), a surface realizer developed at CogenTex like EXEMPLARS. First observations of human consulting The aggregation rules we implemented in EXEMPLARS appear to be plausible, but they have no
8921541	19552	they fail. (B) But the ignitor assembly never does. We have started some work in this direction, in which we have coupled EXEMPLARS to a knowledge base built via the SNePS representation system (Shapiro & Rapaport 1992). SNePS is a semantic network formalism where each node represents a proposition. In general, it is very difficult to access the knowledge about the physical structure of the system and causal
8921541	11604	Generation (NLG) capabilities to ITSs. Our choice has been to apply simple NLG techniques to improve the feedback provided by an existing ITS, specifically, one built within the DIAG framework (Towne 1997a). We evaluated the original version of the system and the enhanced one with a between subjects experiment. On the whole, the enhanced system is better than the original one, other than in helping
8921541	11604	feedback by exploiting more sophisticated NLG techniques, and the data collection we have started, to study how tutors verbalize the information that the ITS wants to communicate. DIAG DIAG (Towne 1997a; 1997b) is a shell to build ITSs that teach students to troubleshoot complex artifacts and systems, such home heating and circuitry. DIAG in turn builds on the VIVIDS authoring environment (Munro
8921541	11604	a series of troubleshooting problems of increasing difficulty. DIAG’s tutoring strategy steers the student towards performing the tests that have the greatest potential for reducing uncertainty (Towne 1997a). Most of the times, a test consists of the visual observation of an indicator. DIAG keeps track of the tests the student performs, and the inferences that could be made from the symptoms shown.
7664111	17771	domain, is what we refer to as domain modeling. The absence of explicit and well managed domain models plays an important part in the failure of a significant number of system development projects . Achieving clarity and consensus ? This paper results from the ArchiMate project (http://archimate.telin.nl), a research consortium that aims to provide concepts and techniques to support
7664111	17771	Modeling This section provides a brief discussion of the activity of domain modeling. A more detailed discussion on domain modeling, and its role in the system development process, can be found in . In general, the goals for doing (business) domain modeling are : 1. articulate clear and concise meanings of business domain concepts and 2. achieve a shared understanding of the concepts among
7664111	17771	specific goal in mind, it is referred to as an explicit conceptualization process. The above mentioned stream of activities called concept specification is an explicit conceptualization process. In  a reference model for explicit conceptualization processes is provided. This reference model distinguishes five streams of activities or phases (each being a sub-stream of the concept specification
7664111	19574	capturing the definitions of the terms. However, we believe that RUP’s notion of a glossary, in particular when viewed as a “list of terms”, is too light a mechanism for domain modelling. In  the importance of proper domain modeling is argued as well. The book spents an entire chapter on the issues involved in domain modeling. It, however, does not provide explicit guidance to modelers,
7664111	16029	using in practice) ORM as a domain modeling approach. ORM , and its many variations such as NIAM, PSM and NORM, have a rich theoretical foundation dating back to at least the 1980s and 1990s . Even though the UML  is used intensively during the development of software systems, ORM still has an important role to play in the early stages of system development. An active community of
7664111	19578	using in practice) ORM as a domain modeling approach. ORM , and its many variations such as NIAM, PSM and NORM, have a rich theoretical foundation dating back to at least the 1980s and 1990s . Even though the UML  is used intensively during the development of software systems, ORM still has an important role to play in the early stages of system development. An active community of
7664111	19583	example, ternary and objectified relationships may be flattened/reduced to binary relationships. These transformations are essentially design optimizations “at the conceptual level”, as reported in . Another example is the use of exclusion constraints and subtypes. In ORM a subtype hierarchy can in some situations also be expressed in terms of entities and roles. Exclusion constraints can be
19586	16757	have found a positive link between public capital (especially infrastructure) and productivity growth and have reported rates of return to public capital far exceeding those to private capital (Aschauer 1989; Munnell 1992). Critics have pointed out that there are a number of econometric problems with this approach because variables measured in level form have common trends and measurement errors.
19586	20811	unit. The model is: 2 Tatom (1993) has conducted a causality test for productivity and infrastructure capital using time-series data and finds a two-way impact. Holtz-Eakin and Schwartz (1995) found a one-way impact of infrastructure on productivity growth.sy it = a 0 + m ?aeyit? e + ? e= 1 k = 1 n it? k 4 ? x + ? + u , (1) k i it where i=1, …, N; t=m+2, …, T; the á’s and â’s are
19586	20813	m j columns. However, it is found that estimations on first-differenced equations produce unsatisfactory results in both simulation and empirical studies (Mairesse and Hall 1996). Blundell and Bond (1998) proposed a system GMM method by combining the firstdifferenced and level equations and show it has a relatively better performance. In addition to the above instruments for the first-differenced
19586	20813	effects á’s and the error terms, these regressors can be used as instruments together with the above Zi. Using these instruments and following the estimation strategy outlined by Blundel and Bond (1998) 5 , the coefficients for the lagged dependent variables and predetermined variables can be estimated for the purpose of causality tests. The estimation was based on a pooled time-series (1971 to
19586	20813	until the lag length is more than two. It seems the impact of infrastructure development on TFP might not be instantaneously exerted. The result confirms the findings by Canning and Pedroni (1998) that the casual impact of road development on productivity exists as a long run rather than a short run relationship. In the equations for TFP, similar results can be observed. When lag length is
19586	20817	3 we estimate the impact 1 The literature on the puzzle of productivity and public capital has primarily focused on developed countries so far. Among those studies on developing countries, Canning (1999) has evaluated the returns to public capital in both lower-income and higher-income countries using cross-country data, while Binswanger et al. (1993) and Fan, Hazell, and Thorat (2000) have
19586	20818	3 we estimate the impact 1 The literature on the puzzle of productivity and public capital has primarily focused on developed countries so far. Among those studies on developing countries, Canning (1999) has evaluated the returns to public capital in both lower-income and higher-income countries using cross-country data, while Binswanger et al. (1993) and Fan, Hazell, and Thorat (2000) have
19586	20822	Canning (1999) has evaluated the returns to public capital in both lower-income and higher-income countries using cross-country data, while Binswanger et al. (1993) and Fan, Hazell, and Thorat (2000) have estimated the returns for a particular developing country.s3 of increases in the capital stock on productivity growth in India. Our conclusions comprise Section 4. 2. CAUSALITY TEST IN A PANEL
19586	20822	problem. For recent development in this area, please see papers collected in a special issue of Oxford Bulletin of Economics and Statistics (Vol. 61, November 1999) and a review by Baltagi and Kao (2000).s?y it = m ? e= 1 a ?y e it?e + m ? e= 1 it?e 5 ? ?x + ?u , (2) e it For the first-difference equation (2), suitably lagged endogenous variables can be used as instruments. For example, if uit are
19629	20859	as equally important. A more sophisticated citation-based impact factor than normalized in-degree count as proposed by Pinski and Narin is discussed in Egghe and Rousseau  and Kleinberg . We follow the more intuitive description in Kleinberg . The impact of a journal j is measured by its influence weight w j . Modeling the collection of journals as a graph, where the nodes
19629	20859	font and capitalization information stored in hitlists (the IR score) with the PageRank measure. User feedback is used to evaluate search results and adjust the ranking functions. Cho et al.  describe the use of PageRank for ordering pages during a crawl so that the more important pages are visited first. It has also been used for evaluating the quality of search engine indexes using
19629	20859	Reinforcement Approach. A method that treats hyperlinks as conferrals of authority on pages for locating relevant, authoritative WWW pages for a broad topic query is introduced by Kleinberg in . He suggested that Web-page importance should depend on the search query being performed. This model is based on a mutually reinforcing ACM Computing Surveys, Vol. 34, No. 4, December 2002.sA
19629	20859	to p . The same algorithm has also been used for finding densely linked communities of hubs and authorities . One of the limitations of Kleinberg’s  mutual reinforcement principle is that it is susceptible to the Tightly Knit Communities (TKC) effect. The TKC effect occurs when a community achieves high scores in link-analysis algorithms even
19629	20859	for relating Web documents based on user accesses. Web sites that automatically improve their organization and presentation by learning from access patterns are addressed by Perkowitz and Etzioni . Sites may be adaptive through customization (modifying pages in real time to suit individual users, e.g., goal recognition) or optimization (altering the site itself to make navigation easier for
19669	19673	the Web service transaction system proposals  and  support the idea of incorporating arbitrary WS-ATMs. However, even though formal meta-models for general ATMs exist and were published in  and ,  and  simply describe a small number of specific WS-ATMs in an informal style. In this paper we discuss a solution for arbitrary ATMs in the Web service world (WS-ATMs) that can be
19669	19673	(flat transactions, nested transactions, transactions with savepoints, and multi-level transactions) in XML. In the next step we will analyse another advanced transaction meta-model called ACTA . If it is reasonable, we will create an XML representation of the ACTA model and make a comparison between the two XML ATM meta-models. To prove the capabilities of the model(s) we will create a
19669	19674	arrives. All ports are deactivated now. The same is true in the case of a commit signal. It is written down in rule 4. Nested transactions (for a detailed discussion of nested transactions see ) can be described with the following rules: SB(Tkn): ? +(SA(TK)|SA(Tkn)), ,BEGIN WORK (rule 1) SA(Tkn): ? ,,ROLLBACK WORK (rule 2) SC(Tkn): C(Tk) ? ,,COMMIT WORK (rule 3) Rule 1 introduces a new AA
19684	5153	can be ensured with multicast push, which employs point-to-multipoint communication (multicast) and sends documents from the server to clients in the absenceof explicit client requests (push) . Multicast push is scalable in that the addition of new clients does not change the server workload and the client-perceived response time. Multicast push can be combined with traditional unicast
19684	5153	and unicast pull to deliver less popular (cool) data . The resulting hybrid scheme strives for the scalability of multicast push and avoids clogging a multicast channel with cool data items . However, the hybrid scheme introduces three inter-related data management problems at the server, and the primary contribution of this paper is to propose an integrated solution for these
19684	5153	. In addition to directly related work, some other work has been done addressing the issue of hot and cold documents and of bandwidth division, though not in the context we are describing. In  the issue of mixing pull and push documents together on a single broadcast channel is examined. The idea is that popular documents are similarly considered hot, and are continuously broadcast while
19684	19685	can be ensured with multicast push, which employs point-to-multipoint communication (multicast) and sends documents from the server to clients in the absenceof explicit client requests (push) . Multicast push is scalable in that the addition of new clients does not change the server workload and the client-perceived response time. Multicast push can be combined with traditional unicast
19684	19685	. In addition to directly related work, some other work has been done addressing the issue of hot and cold documents and of bandwidth division, though not in the context we are describing. In  the issue of mixing pull and push documents together on a single broadcast channel is examined. The idea is that popular documents are similarly considered hot, and are continuously broadcast while
19684	19688	content-delivery networks . Moreover, back-end methods are deployed between a web server and a back-end database server and include web server cache plug-in mechanisms and asynchronous caches . These approaches follow the traditional unicast pull paradigm, whereby data is delivered from the server to each client individually on demand. In turn, the unicast pull approach severely limits
19684	19689	a constant less than 1. Our solution to document classification and bandwidth division is to use an integrated algorithm that minimizes average latency. The starting point is a method suggested by  that minimizes the bandwidth B to achieve a target latency L. The known method is not directly applicable to document classification and bandwidth division because our goal, on the contrary, is to
19684	19689	request rate is . The popularity profile pi and the rate can be determined by the push popularity algorithm described in Section 2.2. Algorithm 1 uses a target average latency L. Its goal is, as in , to partition the documents between unicast pull and multicast push and to split the system bandwidth so as to minimize the system bandwidth B required to achieve latency L. If document i is
19684	19689	If document i is assigned to the push channel, it will use bandwidth Si=L, which is also the rate at which the document must be broadcast to give worst-case response time L. It was then stated  that a document should be pushed if piSi > Si : (1) L Algorithm 1 follows this approach. However, as we are interested in the average latency of a pushed document instead of the worst-case latency,
19684	19690	of new clients does not change the server workload and the client-perceived response time. Multicast push can be combined with traditional unicast pull in a hybrid data dissemination scheme . In the hybrid method, the document set is partitioned into two groups: the multicast push documents Supported in part by NSF grants CCR-0098752, ANI-0123705, and ANI-0325353. y Supported in part
19684	19693	rate at the server during all broadcast cycles. 4. RELATED WORK Scalable data delivery has often been approached through data caching and replication such as, for example, in client or proxy caches , server-side caches , and content-delivery networks . Moreover, back-end methods are deployed between a web server and a back-end database server and include web server cache plug-in
19684	19694	can be ensured with multicast push, which employs point-to-multipoint communication (multicast) and sends documents from the server to clients in the absenceof explicit client requests (push) . Multicast push is scalable in that the addition of new clients does not change the server workload and the client-perceived response time. Multicast push can be combined with traditional unicast
19684	19696	. In addition to directly related work, some other work has been done addressing the issue of hot and cold documents and of bandwidth division, though not in the context we are describing. In  the issue of mixing pull and push documents together on a single broadcast channel is examined. The idea is that popular documents are similarly considered hot, and are continuously broadcast while
19684	11180	rate at the server during all broadcast cycles. 4. RELATED WORK Scalable data delivery has often been approached through data caching and replication such as, for example, in client or proxy caches , server-side caches , and content-delivery networks . Moreover, back-end methods are deployed between a web server and a back-end database server and include web server cache plug-in
19684	19697	content-delivery networks . Moreover, back-end methods are deployed between a web server and a back-end database server and include web server cache plug-in mechanisms and asynchronous caches . These approaches follow the traditional unicast pull paradigm, whereby data is delivered from the server to each client individually on demand. In turn, the unicast pull approach severely limits
19684	19699	content-delivery networks . Moreover, back-end methods are deployed between a web server and a back-end database server and include web server cache plug-in mechanisms and asynchronous caches . These approaches follow the traditional unicast pull paradigm, whereby data is delivered from the server to each client individually on demand. In turn, the unicast pull approach severely limits
19684	19700	from the push channel, thus forcing clients to send explicit requests. Such requests can then be counted and the document popularity estimated . A related problem is multicast group estimation , which can be specialized as follows in our context: remove a document from the multicast push channel and re-insert it as soon as the first request for that document is received. The document
19684	19702	to near-optimality the bandwidth division and document classification problems. Our algorithm is evaluated through emulations on a comprehensive middleware platform for scalable data dissemination . The algorithm exhibited lower average latency than previous schemes. The underlying reason is that if document selection is addressed separately from bandwidth division, a certain bandwidth split
19684	19702	middleware supports the hybrid dissemination scheme utilizing multicast push and unicast pull. It acts as a reverseproxy to a Web server for the delivery of documents that are materialized views . A simulated client uses the middleware and generates Poisson requests for documents with a Zipf probability distribution. In this paper, we report on the case in which the size of the documents is
19684	19702	and server on the same machine so that network effects would not be visible. (Although the emulation runs on a single machine, the middleware is capable of running on a distributed environment .) Aggregate requests from multiple clients was simulated by a background request filler. The filler simulates a specified number of clients, and sends requests to the server. The requests by the
19684	19703	average statistics from these runs. The computer used in these simulations was a 2.0Ghzsdual processor machine with 1.2GB of RAM and running Linux Redhat Version 8.0. JRMS was used for multicasting . Parameter Value Default Document Size 0.5K bytes 0.5K bytes Zipf parameter 1.1 - 2.0 1.5 System Bandwidth 100000 bytes/sec 100000 bytes/sec Request rate 250 / sec 250/sec Total items n 100 - 10000
19684	19704	can be ensured with multicast push, which employs point-to-multipoint communication (multicast) and sends documents from the server to clients in the absenceof explicit client requests (push) . Multicast push is scalable in that the addition of new clients does not change the server workload and the client-perceived response time. Multicast push can be combined with traditional unicast
19684	19704	handles multicasting. Hybrid data dissemination can be evaluated along various performance metrics, and much work has focused on the average serverside delay before a document is transmitted (e.g., ). Client-side latency can be minimized by assigning multicast push to deliver the most popular (hot) data and unicast pull to deliver less popular (cool) data . The resulting hybrid scheme
19684	19704	solution for these problems. In the hybrid scheme, the server must dynamically assign each documents either to the unicast pull channel or to the multicast push channel (document classification) . Furthermore, the server must also partition dynamically its bandwidth between unicast pull and multicast push (bandwidth division). Document classification and bandwidth division are inter-related
19684	19704	placed on the push channel. The function G(k) is a weighted average of the average latency for pushed documents and the average latency for pulled documents. A graph showing an idealized G(k) from  is shown in Figure 3. The function G(k) has a unique local minimum, which can be be found by local search . Figure 3 shows that the minimum of G(k) is to the right of the intersection of the
19684	19704	order to determine the usefulness of our proposed push popularity scheme, we compare it to a solution found in a comparable work to our own. The solution for the push popularity problem proposed in  was to occasionally drop each pushed document i off of the push channel so that clients would have to make explicit requests to i. However, there is a danger that these explicit requests for i
19684	19705	. In addition to directly related work, some other work has been done addressing the issue of hot and cold documents and of bandwidth division, though not in the context we are describing. In  the issue of mixing pull and push documents together on a single broadcast channel is examined. The idea is that popular documents are similarly considered hot, and are continuously broadcast while
19684	19706	can be ensured with multicast push, which employs point-to-multipoint communication (multicast) and sends documents from the server to clients in the absenceof explicit client requests (push) . Multicast push is scalable in that the addition of new clients does not change the server workload and the client-perceived response time. Multicast push can be combined with traditional unicast
338756	3193	favor specific requirements: in Gnutella, the emphasis is on accommodating highly volatile peers and on fast file retrieval, with no guarantees that files will always be located. In Freenet , the emphasis is on ensuring anonymity. In contrast, distributed hash tables such as CAN , Chord , Pastry , and Tapestry  guarantee that files will always be located, but do not support
338756	939	peers and on fast file retrieval, with no guarantees that files will always be located. In Freenet , the emphasis is on ensuring anonymity. In contrast, distributed hash tables such as CAN , Chord , Pastry , and Tapestry  guarantee that files will always be located, but do not support wildcard searches. One way to optimize these tradeoffs is to understand user behavior. In
338756	5902	file retrieval, with no guarantees that files will always be located. In Freenet , the emphasis is on ensuring anonymity. In contrast, distributed hash tables such as CAN , Chord , Pastry , and Tapestry  guarantee that files will always be located, but do not support wildcard searches. One way to optimize these tradeoffs is to understand user behavior. In this paper we analyze
338756	5903	no guarantees that files will always be located. In Freenet , the emphasis is on ensuring anonymity. In contrast, distributed hash tables such as CAN , Chord , Pastry , and Tapestry  guarantee that files will always be located, but do not support wildcard searches. One way to optimize these tradeoffs is to understand user behavior. In this paper we analyze user behavior in
338756	19709	We discuss the causes of these emergent small-world patterns in Section VI. The significance of these newly uncovered patterns is twofold (Section VII): First, it explains previous results  and confirms (with formal support) the intuition behind them. Second, it suggests ways to design mechanisms that exploit these naturally emerging patterns. II. INTUITION It is not news that
338756	19709	This social network is then used to identify experts and to guide searches around them. Sripanidkulchai et. al came close to the intuition of the data-sharing graph in their Infocom 2003 article : they improve Gnutella’s flooding-based mechanism by inserting and exploiting interest-based shortcuts between peers. Interest-based shortcuts connect a peer to peers who provided data in the past.
338756	19709	independence from any underlying infrastructure (in this case, the distribution of data on peers and the location mechanism) and gives a theoretical explanation of the performance improvements in . The data-sharing graph can be exploited for a variety of decentralized file management mechanisms in resource-sharing systems (such as peer-to-peer or Grids). • In a writable file-sharing system,
338756	19710	guide efficient solution design. A well known example is the relationship between file popularity in the Web and cache size. The popularity of web pages has been shown to follow a Zipf distribution , : few pages are highly popular and many pages are requested few times. As a result, the efficiency of increasing cache size is not linear: caching is useful for the popular items, but there is
338756	175	efficient solution design. A well known example is the relationship between file popularity in the Web and cache size. The popularity of web pages has been shown to follow a Zipf distribution , : few pages are highly popular and many pages are requested few times. As a result, the efficiency of increasing cache size is not linear: caching is useful for the popular items, but there is
338756	18605	in 1735. The field has since extended from theoretical results to the analysis of patterns in real networks. Social sciences have apparently the longest history in the study of real networks , with significant quantitative results dating from the 1920s . The development of the Internet added significant momentum to the study of networks: by both facilitating access to collections of
338756	18605	Internet at the router and the AS level  and the email graph . The study of large real networks led to fascinating results: recurring patterns emerge in real networks (see , , ,  for good surveys). For example, a frequent pattern is the power-law distribution of node degree, that is, a small number of nodes act as hubs (having a large degree), while most nodes have a small
338756	7174	access to collections of data and by introducing new networks to study, such as the Web graph, whose nodes are web pages and edges are hyperlinks , the Internet at the router and the AS level  and the email graph . The study of large real networks led to fascinating results: recurring patterns emerge in real networks (see , , ,  for good surveys). For example, a
338756	7174	at its time was the proof that the random graph-based models of the Internet (with their Poisson degree distribution) were inaccurate: the Internet topology had a power-law degree distribution . Other results followed: the web graph ,  and the Gnutella overlay (as of year 2000)  are also power-law networks. Another class of networks are the “small worlds”. Two characteristics
338756	15918	the Internet at the router and the AS level  and the email graph . The study of large real networks led to fascinating results: recurring patterns emerge in real networks (see , , ,  for good surveys). For example, a frequent pattern is the power-law distribution of node degree, that is, a small number of nodes act as hubs (having a large degree), while most nodes have a
338756	19716	a large degree), while most nodes have a small degree. Examples of power-law networks are numerous and from many domains: the phone-call network (long distance phone calls made during a single day) , , the citation network , and the linguistics network  (pairs of words in English texts that appear at most one word apart). In computer science, perhaps the first and most surprising
338756	18587	degree), while most nodes have a small degree. Examples of power-law networks are numerous and from many domains: the phone-call network (long distance phone calls made during a single day) , , the citation network , and the linguistics network  (pairs of words in English texts that appear at most one word apart). In computer science, perhaps the first and most surprising result
338756	19718	power-law networks are numerous and from many domains: the phone-call network (long distance phone calls made during a single day) , , the citation network , and the linguistics network  (pairs of words in English texts that appear at most one word apart). In computer science, perhaps the first and most surprising result at its time was the proof that the random graph-based models
338756	2285	degree distribution) were inaccurate: the Internet topology had a power-law degree distribution . Other results followed: the web graph ,  and the Gnutella overlay (as of year 2000)  are also power-law networks. Another class of networks are the “small worlds”. Two characteristics distinguish small-world networks: first, a small average path length, typical of random graphs
338756	2285	interval and temporal user activity, meaning that users are not uniformly active during a period, but follow some patterns (for example, downloading more music files during weekends or holidays ). Thus, we ask: Q4 Are the patterns we identified in the data-sharing graph, especially the large clustering coefficient, an inherent consequence of these well-known behaviors? To answer this
338756	19721	Questions Newman shows that scientific collaboration networks in different domains (physics, biomedical research, neuroscience, and computer science) have the characteristics of small worlds , , . Collaboration networks connect scientists who have written articles together. Moreover, Girvan and Newman  show that welldefined groups (such as a research group in a specific
338756	19725	download and not about files searched for (therefore, typos are naturally filtered). Details on how Kazaa traces were recorded as well as a thorough analysis of the Kazaa traffic are presented in . # Requests per pecond 8 7 6 5 4 3 2 1 0 0 50 100 150 200 250 300 350 400 450 500 Time (’000 seconds) # Requests 10000 1000 100 10 1 0 3000 6000 9000 12000 User rank Fig. 5. Left: Activity level
338756	19726	Another is the degree distribution of the datasharing graphs which, as in many other real networks, is far from the Poisson distribution of a random graph (Figures 8, 9, and 10). Newman et al. ,  propose a model for random graphs with given degree distributions. These graphs, therefore, will not be random in the Erd?s-Rényi sense, but will be random members of a class of graphs with a
338756	19729	that these are untested but promising ideas for future work. A. Relevance of the Data-Sharing Graph Structure Some recommender systems have a similar flavor to the data-sharing graph. ReferralWeb  attempts to uncover existing social networks to create a referral chain of named individuals. It does this by inferring social relationships from web pages, such as co-authorship, research groups
338756	19731	decentralized, self-organizing fashion (a similar idea is explored in ). • In large-scale, unreliable, dynamic peer-to-peer systems file replication may be used to insure data availability  and transfer performance. The datasharing graph may suggest where to place replicas closer to the nodes that access them. Similarly, it may be useful for dynamic distributed storage: if files
338756	19732	input. This can be used for scheduling, migrating or replicating data-intensive jobs. B. Relevance of Small-World Characteristics The idea underlying the data-sharing graph was first presented in  as a challenge to design a file-location mechanism that exploits the small-world characteristics of a file-sharing community. Meanwhile we completed the design and evaluation of a mechanism that
9910	19735	P. Crucitti et al. / Physica A 320 (2003) 622 – 642 623 The study of the structural properties of the underlying network can be very important to understand the functions of a complex system . For instance the architecture of a computer network is the rst critical issue to take into account when we want to design an e cient communication system. Similarly, the e ciency of the
9910	19735	in the active state. The KE model generates scale-free networks with degree distribution P(k)=2m2k?3 (for k ¿ m) and average connectivity ?k? =2K=N =2m . Furthermore, by varying in the interval  the model makes possible to study the cross-over between a case with high L and C (the model = 0 has been studied previously in Ref. ), and a case with small L and C ( = 1 corresponds exactly
9910	19741	large networks (the World Wide Web, Internet, metabolic networks and protein networks) are scale-free, i.e., have a power-law degree distribution P(k) ? k ? . Neither random graph theory , nor the WS model to construct networks with the small-world properties  can reproduce this feature: in fact both give P(k) peaked around the average value of k. In Ref.  Barabasi and Albert
9910	19741	systems of interest for the spreading of sexually transmitted diseases , and the connectivity network of atomic clusters’ systems  display a similar behavior. Neither random graphs , nor small-world networks constructed according to the WS model, have a power-law degree distribution P(k) like the one observed in real-large networks. In fact for a random graph P(k) is described
9300924	7799	1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time,
9300924	7799	of hierarchical model over the flat model. 3 Generating Hierarchical Structure 3.1 Document Representation To generate hierarchies, we need to address the question of how to represent texts(Cutting et al., 1992), (Lawrie and Croft, 2000). The total number of words we focus on is too large and it is computationally very expensive. We use two statistical techniques to reduce the number of inputs. The first
9300924	4762	However, the increasing number of documents and categories often hamper the development of practical classification systems, mainly by statistical, computational, and representational problems(Dietterich, 2000). One strategy for solving these problems is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies
9300924	1233	evaluation. 2 Related Work Automatically generating hierarchies is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in
9300924	1233	should be able to obtain further advantages in efficiency in the hierarchical approach by reducing the number of features which are not useful discriminators within the lower-level of hierarchies(Koller and Sahami, 1997). Table 6: Accuracy by hierarchical level(Manual) Level Clusters Categories miR miP miF Top 25 25 0.753 0.724 0.744 Second 16 63 0.524 0.553 0.543 Third 37 37 0.431 0.600 0.500 Fourth 43 43 0.276
9300924	19754	to improve parameter estimates by taking advanYoshimi Suzuki Interdisciplinary Graduate School of Medicine and Engineering Univ. of Yamanashi ysuzuki@ccn.yamanashi.ac.jp tage of the hierarchy(McCallum, 1999). They tested their method using three different real-world datasets: 20,000 articles from the UseNet, 6,440 web pages from the Industry Sector, and 14,831 pages from the Yahoo, and showed improved
9300924	19754	NB classifiers. Recent studies on a Naive Bayes classifier which is proposed by McCallum et al. reported high performance over some other commonly used versions of NB on several data collections(McCallum, 1999). We use the model of NB by McCallum et al. which is shown in formula (2). È ? ? ??? ? ? ? È ? ? ? ? ¥ ???? ?? È Û??? ? ?? ? ? È??? Ö? È Ö ? ? ? ¥ ???? ?? È Û??? ? Ö? ? ? Û??Ö? ??Ø? ? È ÛØ ? ?? ? ?
9300924	9962	to obtain further advantages in efficiency in dealing with a large collection of data, (iii) comparing the method with other techniques such as hierarchical agglomerative clustering and ‘X-means’(Pelleg and Moore, 2000), and (iv) developing evaluation method between manual and automatic construction of hierarchies to learn more about the strengths and weaknesses of the two methods of classifying documents.
9300924	19756	Ü ? Ý ?? ? È?? Ý ? Ü (6) ?? in formula (6) denotes a set of seed points(categories) of ??. We note that the true output distribution È Ý ? Ü in formula (6) is unknown for each sample Ü. Roy et al.(Roy and McCallum, 2001) proposed a method of ? Ø?Ú? Ð??ÖÒ?Ò? that directly optimizes expected future error by log-loss, using the entropy of the posterior class distribution on a sample of the unlabeled examples. We
9300924	19757	is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document
9300924	19758	102 categories which have at least one document in each data. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-of-speech tagger(Schmid, 1995), and stop word removal. The number of categories per document is 3.21 on average. For both of the hierarchical and non-hierarchical cases, we select the 1,000 features with the largest MI for each
19760	19762	approach consists in learning a set of first-order rules in a first step, then using them as new attributes in a classical attribute-value naive Bayesian classifier. The1BC system we developed  does not learn first-order rules, it generates instead a set of first-order conditions, that are used then as attributes in a classical attribute-value naive Bayesian classifier. It can thus be
19760	19762	kind are used: hasBlue(S):-set2tuple(S,T),hasColour(T,blue). hasCircle(S):-set2tuple(S,T),hasShape(T,circle). has2(S):-set2tuple(S,T),hasNumber(T,2). A first-order Bayesian classifier like 1BC  can generate and use such features. Therefore it can perform a decomposition of probability distribution over structured individuals. 4 Discussion Bayesian classifiers require to estimate the
19760	19763	perspectives will be helpful in our analysis: the term perspective and the database perspective. In the term perspective, our structured objects are viewed as terms in a typed programming language ; thus, we can say that an individual is described by a set of tuples (more precisely: by a term which is an instance of such a type). In the database perspective, a dataset is described by a
19760	19765	individuals. There have been works on Bayesian classification on structured individuals. Pompe and Kononenko describe an application of naive Bayesian classifiers in a first-order context . Their approach consists in learning a set of first-order rules in a first step, then using them as new attributes in a classical attribute-value naive Bayesian classifier. The1BC system we
19784	19785	Moreiro Studies geared to improving software There is a clear prevalence, in terms of volume, of papers geared to expanding and enriching the WordNet structure. One of such endeavours is VerbNet , designed to make up for shortcomings in the associative relations between verbs; another is the Lingua::WordNet interface , which furnishes an editable presentation of WordNet, with meronym
19784	19785	a knowledge base, as well as to reduce polysemy in verbs, classified by their meanings via predicate associations, with a view to optimising information retrieval. Methods for nouns  and verbs  has already been analysed. At the same time, new disambiguation models have been tested in conjunction with WordNet by: generating ontological databases with a systematic classification of multiple
19784	19786	and all are interconnected through an interlinguistic index, for which relations have been added and modified and new levels identified in WordNet. For a multilingual description of EuroWordNet see . This paper poses the possibility of automatically transferring a list of English verbs, classified by their syntactic characteristics, to WordNet synsets. 3.2 Improvements in Natural Language
19784	19786	a knowledge base, as well as to reduce polysemy in verbs, classified by their meanings via predicate associations, with a view to optimising information retrieval. Methods for nouns  and verbs  has already been analysed. At the same time, new disambiguation models have been tested in conjunction with WordNet by: generating ontological databases with a systematic classification of multiple
19784	19790	feature with the development of multiple database access systems and one in which WordNet’s identification and interpretation of semantic equivalents is extraordinarily useful . Mandala  proposed the use of WordNet as a tool for the automatic construction of thesauri, based either on co-occurrence determined by automatic statistical identification of semantic relations, or on the
19784	19791	association, in which the most significant words of an environment (predicate) and those with which they relate are identified to construct the argument. In another vein, Moldovan  opted to use WordNet in the development of a natural language interface to optimise the precision of Internet search engines by expanding queries. Concept identification in natural language This
19784	19791	followed a dual course in such applications:sWordNet Applications 273 1. Disambiguation i.e., precision and relevance in response to a query via resolution of semantic inconsistencies. Moldovan  described schematically the semantic disambiguation as follows: (1) All the noun–verb pairs in the sentence are selected. (2) The most likely meaning of the term is chosen (subprocess that Moldovan
19784	19793	to process local context; for the exploitation of a Bayesian network able to establish lexical relations with WordNet as a source of knowledge, integrating symbolic and statistical information ; for the development of a statistical classifier, implemented with WordNet lexical relations, able to identify the meaning of words, combining the context with local signs ; and as support for
19784	19794	information ; for the development of a statistical classifier, implemented with WordNet lexical relations, able to identify the meaning of words, combining the context with local signs ; and as support for the development of a computational similarity model to add on-line semantic representation to the statistical corpus. WordNet has, therefore, proved its worth as an ideal
19784	19797	the same time, new disambiguation models have been tested in conjunction with WordNet by: generating ontological databases with a systematic classification of multiple meanings derived from WordNet ; or generating broad corpora to signify words on the grounds of WordNet synonymies or definitions in the wording of queries . One result has been the appearance of GINGER II, an extensive
19784	19798	systematic classification of multiple meanings derived from WordNet ; or generating broad corpora to signify words on the grounds of WordNet synonymies or definitions in the wording of queries . One result has been the appearance of GINGER II, an extensive dictionary semantically tagged using 45 WordNet categories and an algorithm for interpretings274 Jorge Morato, Miguel Ángel Marzal,
19784	19804	Computational linguistics is, however, the area that has placed the greatest emphasis on relations and semantic distances between lexemes, the measures of which were classified by A. Budanitsky . This paper highlights the measures that use WordNet as a resource and for implementation of functions, in particular: Hist-St. And Leacock– Chodorow, in which similarity, albeit in the IS-A link
19784	19806	expansion In 1994 Smeaton  proposed an expansion system based on calculating the tf-idf for the query terms and adding to it half the tf-idf for the WordNet synonyms for these terms. Gonzalo  later reported the benefits of applying WordNet to queries, using it as a WSD (Word Sense Disambiguator) able to enhance the search process by including semantically related terms and thus retrieve
19784	19811	such as the WebOntEx (Web Ontology Extraction) prototype developed by Keng Woei Tan  which is designed to create ontologies for the semantic description of data in the web. Judith Klavans  devised an algorithm for automatically determining the genre of a paper on the grounds of the WordNet verb categories used. With their WN-Verber, they determined that some verbal synsets and their
19784	19812	typologies. Audio and video retrieval This is a challenge in need of increasingly urgent attention in view of the burgeoning development of hypermedia and non-text information. The MultiMediaMiner , is a prototype to extract multimedia information and knowledge from the web that uses WordNet to generate conceptual hierarchies for interactive information retrieval and build multi-dimensional
19784	19813	for multi-media data. Finally, WordNet has been used in query expansion to index radio news programme transcriptions effected by a prototype designed to retrieve information from radio broadcasts . Other WordNet applications Parameterisable information systems While anecdotal, the J. Chai  proposal to create an information system (called Meaning Extraction System) that can be configured
19784	19814	transcriptions effected by a prototype designed to retrieve information from radio broadcasts . Other WordNet applications Parameterisable information systems While anecdotal, the J. Chai  proposal to create an information system (called Meaning Extraction System) that can be configured in terms of a specific user profile is appealing. The user chooses from a series of texts
19784	19816	a given language . As a translation aid based on the application of semantic distance algorithms, WordNet has also been used to develop a potential error corrector for the positioning of words . One very intuitive formula consists of using conceptual interlingua representation of texts and queries such as used in the CINDOR system, which accommodates WordNet-supported inter-linguistic
19784	19817	combinations, obviating the need for an expert translation for retrieval. The CINDOR system was presented and tested at TREC-7 and seems to be useful for cross- or combined linguistic retrieval .s276 Jorge Morato, Miguel Ángel Marzal, Juan Lloréns, José Moreiro 4 Trends Trends are difficult to ascertain and evaluate in view of the clearly instrumental and application-based dimension that
19819	19821	in this article are considered in the context of web resources. As such they are not particularly tailored to database transformations (see e.g. our earlier work on transformations in ). The above mentioned forms of heterogeneity may pose problems in a retrieval setting if there is a mismatch between the user’s wishes on the one hand and the form and/or format of resources on the
19819	19825	in Section 3. 2.1 Overview Our model of information supply is based on the distinction between data and information. The entities found on the Web, which can be identified by means of a URI , are data resources. These data resources are “information”, if and only if they are relevant with regard to a given information need as it is harbored by some user. Data resources may, at least
19831	19837	large-scale semantic knowledge bases contain a substantial amount of story knowledge. Mueller compared several systems (Cyc (Lenat, 1995), FrameNet (Johnson, 1998), Gordon’s Expectation Packages (Gordon, 1999), ThoughtTreasure (Mueller, 1998), and WordNet 1.6 (Fellbaum, 1998) and found that these systems consisted largely of facts and rules, and not cases and stories against which case-based reasoning
19831	19842	our machines in a way that makes commonsense reasoning possible has been a challenge. Davis reviews some of the known knowledge representation techniques in (Davis, 1990), and the Cyc project (Lenat, 1995) has built a vast ontology of logical terms that can be used to describe a variety of commonsense situations, but there is still a long way to go to build effective commonsense knowledge
19831	19842	began in earnest four years ago with the launch of the Open Mind Common Sense web site. At the time there was only one large-scale commonsense knowledge base, the well-known Cyc knowledge base (Lenat 1995). Marvin Minsky was a great supporter of the Cyc project, but at the same time had been encouraging us and the rest of the world to start alternative projects to Cyc. He has long argued that giving
19831	19842	Mueller is a notable exception (Mueller, (in press)). No present large-scale semantic knowledge bases contain a substantial amount of story knowledge. Mueller compared several systems (Cyc (Lenat, 1995), FrameNet (Johnson, 1998), Gordon’s Expectation Packages (Gordon, 1999), ThoughtTreasure (Mueller, 1998), and WordNet 1.6 (Fellbaum, 1998) and found that these systems consisted largely of facts
19831	19848	bases contain a substantial amount of story knowledge. Mueller compared several systems (Cyc (Lenat, 1995), FrameNet (Johnson, 1998), Gordon’s Expectation Packages (Gordon, 1999), ThoughtTreasure (Mueller, 1998), and WordNet 1.6 (Fellbaum, 1998) and found that these systems consisted largely of facts and rules, and not cases and stories against which case-based reasoning could be performed (Mueller,
19831	19849	(Mueller, 1998), and WordNet 1.6 (Fellbaum, 1998) and found that these systems consisted largely of facts and rules, and not cases and stories against which case-based reasoning could be performed (Mueller, 1999). A large-scale story knowledge base would be a fundamentally new kind of resource. In addition, from the perspective of turning to the general public to build knowledge bases, we suspect that the
19831	19851	the notion that useful stories could perhaps be generated automatically from the temporally-ordered and causally-connected knowledge in the OMCS corpus. We built a system called MAKEBELIEVE (Liu & Singh, 2002) that generated simple first-person storylines collaboratively with a person by alternating turns. The stories generated by MAKEBELIEVE were often quirky and silly because while the system had a
19831	19852	the open mind site (at http://openmind.media.mit.edu) to browse the knowledge people have contributed. What has surprised us most of all is the high quality of this knowledge. An early analysis (Singh et al. 2002) showed that 90% of the contributed statements were rated 3 or higher (on a 5 point scale) along the dimensions of truth and objectivity, and about 85% of the statements were rated as things anyone
19831	19853	to try to build a web site that explicitly collected knowledge in the form of stories. Open Mind Experiences (OMEX) was a first attempt to gather structured story knowledge from the general public (Singh and Barry, 2003). In many ways, OMEX improved upon and fixed several problems with the original OMCS web site OMEX focused exclusively on knowledge entry through template activities, so all the collected knowledge
19860	21103	simulation. parallel speedup 7 6 5 4 3 2 1 0 4 6 8 10 12 # processors = # AS Figure 8: SSFNet Parallel Simulation Speedups The parallel simulation performance of SSFNet was also reported in , where experiments were done on Sun enterprise server with up to 12 processors. A similar ring of campus network model was used while the size of an individual campus network was smaller. We
19860	19865	the comprehensive, distributed memory approach needs to be developed. We have described the details of our novel approach to scalability and efficiency of parallel network simulation in Genesis in . Genesis combines simulation and modeling in a single execution. It decomposes a network into parts, called domains, and simulation time into intervals, and simulates each network partition
19860	19865	of parallel simulators. We have shown that it achieved significant speed-up for TCP or UDP traffic simulations. The total execution time could be decreased by an order of magnitude or more . Our primary application was the use of the on-line simulation for network management . For distributed simulation of BGP protocol, Genesis embeds an event-level synchronization mechanism into
19860	19866	the comprehensive, distributed memory approach needs to be developed. We have described the details of our novel approach to scalability and efficiency of parallel network simulation in Genesis in . Genesis combines simulation and modeling in a single execution. It decomposes a network into parts, called domains, and simulation time into intervals, and simulates each network partition
19860	19866	An execution scheme is shown in Figure 1 that illustrates also synchronization between the repeated iterations over the same time interval and use of checkpoints for the domain simulators . Figure 1: Genesis Execution Scheme In this paper, we focus on the design of distributed BGP network simulation in Genesis. In particular, we describe the system extension that allows the user to
19860	19866	because initially, during the synchronization, Genesis rolled back the simulation for the iteration in which the burst happened, to adjust each distributed simulator for this network traffic change . After the Genesis simulation converged on this change in the network, it proceeded without this rollback. In this stage, because of the distributed UDP flooding traffic flowing from one
19860	19867	one domain choosing a server randomly from the Subnetwork 1 in the domain that is a successor to the current one in the ring. This individual campus network model was the same as the one we used in . However, in the experiments in this paper, we set the traffic send-rate at the higher rate of 0.02 second per packet. In addition, we reduced the total simulation time from 400 seconds to 100
19860	19868	event-level synchronization mechanism into its coarse granularity synchronization framework. This work was done using our previous development of Genesis based on SSFNet , which was reported in . In Genesis, to simulate a network running BGP protocol for inter-AS (Autonomous System) routing, with background TCP or UDP traffic, the network is decomposed along the boundaries of AS domains.
19860	19869	or UDP traffic simulations. The total execution time could be decreased by an order of magnitude or more . Our primary application was the use of the on-line simulation for network management . For distributed simulation of BGP protocol, Genesis embeds an event-level synchronization mechanism into its coarse granularity synchronization framework. This work was done using our previous
8921587	2315	wireless arena expands in various settings? Currently, most of the simulation studies on wireless networks and protocols consider simplistic association and mobility patterns for the wireless users , . There are a few studies on the mobility and association patterns in cellular networks , , . However, the rapid deployment of the IEEE802.11 infrastructures in various environments
8921587	19884	networks and protocols consider simplistic association and mobility patterns for the wireless users , . There are a few studies on the mobility and association patterns in cellular networks , , . However, the rapid deployment of the IEEE802.11 infrastructures in various environments triggers new applications and services, that in turn, generate a richer set of traces for
8921587	19885	and protocols consider simplistic association and mobility patterns for the wireless users , . There are a few studies on the mobility and association patterns in cellular networks , , . However, the rapid deployment of the IEEE802.11 infrastructures in various environments triggers new applications and services, that in turn, generate a richer set of traces for analysis.
8921587	19886	and protocols consider simplistic association and mobility patterns for the wireless users , . There are a few studies on the mobility and association patterns in cellular networks , , . However, the rapid deployment of the IEEE802.11 infrastructures in various environments triggers new applications and services, that in turn, generate a richer set of traces for analysis. There is
8921587	19887	duration and next association to preparesthe handoff, share clients or traffic load with each other, and ensure a better quality of service characteristics. This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual
8921587	19887	nicely tie with our earlier results that model the trajectory of the sequence of APs as a markov-chain and predicting with high probability (86%) the AP of the next association of a client . Section II describes briefly the wireless infrastructure and our techniques for acquiring the data. Section III focuses on the session generation and its features for characterizing the mobility
8921587	19887	among 75 buildings. A client is a device that communicates with the campus wireless infrastructure and is identified by a unique id based on its anonymized MAC address. In our earlier work , we describe in detail about how clients communicate with APs, the events that allow us to log the clients’ activities, and the measures taken to ensure privacy. The campus primarily uses Cisco
8921587	19887	with the AP. The majority of APs on campus were configured to send this data via syslog messages to a syslog server in our department. The messages sent by the APs are detailed in , . A session consists of a sequence of visits (one or more) without any disconnection between these consecutive visits. Each client may have one or more sessions. We will present the session
8921587	19887	the BiPareto fit because the tail is more linear. Currently, we are trying to model the uncensored off duration by a BiPareto distribution. V. RELATED WORK This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual
8921587	19889	clients or traffic load with each other, and ensure a better quality of service characteristics. This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual clients rather than on the entire population of mobile clients
8921587	19889	380ft). There are 585 such sessions generated by 274 clients. Tbl. III summarizes the percentiles of several variables of the movement pattern. A. Review of duration modeling Balachandran et al. considered a conference setting with four APs and modeled user session durations undersCCDF Fraction of clients 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0-10% 10-20% 20-30% 30-40% 40-50% 50-60% All
8921587	19889	we are trying to model the uncensored off duration by a BiPareto distribution. V. RELATED WORK This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual clients rather than on the entire population of mobile clients
8921587	19890	with each other, and ensure a better quality of service characteristics. This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual clients rather than on the entire population of mobile clients and in a finer time
8921587	19890	the uncensored off duration by a BiPareto distribution. V. RELATED WORK This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual clients rather than on the entire population of mobile clients and in a finer time
8921587	9686	a better quality of service characteristics. This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual clients rather than on the entire population of mobile clients and in a finer time granularity. We monitor the
8921587	9686	by a BiPareto distribution. V. RELATED WORK This research extends our earlier study , the studies by Kotz and Essien , Balachandran et al. , Tang and Baker , and Balazinska and Castro  by focusing more closely on the association and mobility patterns of individual clients rather than on the entire population of mobile clients and in a finer time granularity. We monitor the
8921587	19893	has been used before in wired network to model the number of TCP connections generated by a user session and also the average connection interarrival times within a user session. Saniee et al.  provide more details on the BiPareto distribution and its estimation method. For the model of the stationary session duration, we subjectively estimated the parameters to be (0.05, 1.1, ???
8921588	19895	University Park, PA. Email: wlee@cse.psu.edu. Jianliang Xu, Xueyan Tang, Wang-Chien Lee requests for the same object by a single transmission, leading to more efficient use of shared bandwidth . In general, there are two broadcast approaches: pushbased broadcast computes the broadcast program based on historical statistics; on-demand broadcast schedules broadcast items based on
8921588	19896	algorithm used to select and broadcast requested items from outstanding requests. While there has been significant work on the development of on-demand broadcast scheduling algorithms (e.g., , , ), none of them considered the time constraints associated with requests. On the other hand, 1salthough time-critical scheduling algorithms have been investigated for unicast-based
8921588	19896	Finally, Section VII concludes the paper. II. RELATED WORK There has been significant work on the development of on-demand broadcast scheduling algorithms . Recently, Acharya and Muthukrishnan  studied some scheduling algorithms for variable-size data items and introduced a new performance metric called stretch. Aksoy and Franklin  proposed a low overhead and scalable scheduling
8921588	19896	have equal size (and hence equal service time). Note that the factor of item size (or service time) can be easily incorporated in broadcast scheduling algorithms to handle variable-size data items . IV. PROPOSED SIN-? ALGORITHM In this section, we propose a new scheduling algorithm with low-complexity, called SIN-?. We start by illustrating the factors affecting the performance of
8921588	19685	algorithm used to select and broadcast requested items from outstanding requests. While there has been significant work on the development of on-demand broadcast scheduling algorithms (e.g., , , ), none of them considered the time constraints associated with requests. On the other hand, 1salthough time-critical scheduling algorithms have been investigated for unicast-based real-time
8921588	19685	algorithms . Recently, Acharya and Muthukrishnan  studied some scheduling algorithms for variable-size data items and introduced a new performance metric called stretch. Aksoy and Franklin  proposed a low overhead and scalable scheduling algorithm called R×W. Datta et al.  took into consideration the energy saving issue in the design of on-demand broadcast systems. Liberatore
8921588	19685	time in the literature. III. SYSTEM MODEL As shown in Figure 1, we consider a satellite-based broadcast architecture that captures all essential components of a typical on-demand broadcast system . In this architecture, a large group of clients retrieve data items (e.g., traffic information) maintained by a data server. The clients send requests to the server through an uplink channel. Each
8921588	19685	has a scheduling complexity of at least O(m), where m is the number of items with pending requests. In the following, we present a more efficient implementation of SIN-?, which was inspired by . We group the pending requests in the service queue by the requested items. Two data structures, an S-list and an N-list, are used to index the requested items in the service queue. Each item has
8921588	19685	larger than 16. C. Performance Comparison: Impact of Request Arrival Rate In this and subsequent subsections, we compare SIN-1 with the existing algorithms EDF, MRF, and the recently proposed R×W . With R×W, the server broadcasts the page that has either a large number of pending requests or a long waiting time. The objective of R×W is to reduce the response time of requests. To simulate a
8921588	19898	associated with requests. On the other hand, 1salthough time-critical scheduling algorithms have been investigated for unicast-based real-time systems and push-based broadcast systems (e.g., , , ), they are not applicable or not effective to on-demand broadcast systems. In this paper, we are interested in developing new scheduling algorithms for time-critical on-demand
8921588	19898	Other related work concerning on-demand broadcast includes energy-efficient retrieval , , client cache management , combination with pushed broadcast , and fault-tolerant broadcast . These studies complement to our work in different aspects. In summary, existing time-critical scheduling algorithms are confined to push-based broadcast and unicast systems only. Meanwhile,
8921588	6322	algorithms. However, none of these algorithms considered the time constraints of requests in making scheduling decisions. There exist a few studies on broadcast scheduling with time constraints , . However, these studies considered periodic push-based broadcast only, which is fundamentally different from on-demand broadcast in system architecture. Xuan et al.  evaluated several
8921588	175	pages sorted in descending order are shown in Figure 4. It can be seen that the access pattern follows a Zipf-like distribution, which is consistent with the observation made in the literature . To simulate different levels of workloads, we changed the time scale of the trace by introducing a request 2 Interested readers are referred to  for more details of the WorldCup98 web server
8921588	19899	broadcast. Other closely related areas include task scheduling in real-time systems and transaction processing in realtime databases. Many algorithms and theoretical results have been developed , . One of the most classical scheduling algorithms is the EDF algorithm . It offers the optimal performance in light-load conditions in terms of deadline missing rate. In overloaded
8921588	19899	in real-time systems. In this approach, each task is characterized by an importance value and the scheduling algorithm aims to maximize the cumulative value of the tasks that meet their deadlines . Our problem resembles value-deadline scheduling in that the number of requests for a data item can be treated as the value of the item. However, in an ondemand broadcast system, the number of
8921588	19899	§ § § ¢¡¢¡¢s¡ ¡s¥¡¥ ¦¡¦ ¦¡¦ § § § ¥¡¥ Server ¥¡¥ ¦¡¦ § pull requests Fig. 1. A Satellite-based Broadcast Architecture Data Scheduler Service Queue Data Base algorithm (i.e, value-density scheduling , which is analogous to the most requests first (MRF) algorithm for fixed-size items) shows poor performance for on-demand broadcast scheduling. Other related work concerning on-demand broadcast
8921588	19901	for variable-size data items and introduced a new performance metric called stretch. Aksoy and Franklin  proposed a low overhead and scalable scheduling algorithm called R×W. Datta et al.  took into consideration the energy saving issue in the design of on-demand broadcast systems. Liberatore  studied the scheduling algorithms for requests asking for a list of dependent items. Hu
8921588	19904	These constraints can be imposed either by the users or the applications. Consider a driver who queries a traffic information server to select one of several alternative routes at some point ahead . Clearly, it is necessary for the server to provide the driver with the traffic information (e.g., which route is less congested) before he reaches that point; otherwise, the information is of no
8921588	19904	system designs for time-constrained data accesses and showed that on-demand broadcast with the earliest deadline first (EDF) scheduling algorithm performs well. Fernandez and Ramamritham  studied an adaptive hybrid broadcast system in which the EDF algorithm is employed for on-demand broadcast. These two studies concentrated on the system design aspect of on-demand broadcast, which
8921588	19905	consideration the energy saving issue in the design of on-demand broadcast systems. Liberatore  studied the scheduling algorithms for requests asking for a list of dependent items. Hu and Chen  investigated dynamic traffic-aware scheduling algorithms. However, none of these algorithms considered the time constraints of requests in making scheduling decisions. There exist a few studies on
8921588	19906	for on-demand broadcast scheduling. Other related work concerning on-demand broadcast includes energy-efficient retrieval , , client cache management , combination with pushed broadcast , and fault-tolerant broadcast . These studies complement to our work in different aspects. In summary, existing time-critical scheduling algorithms are confined to push-based broadcast and
8921588	19907	with requests. On the other hand, 1salthough time-critical scheduling algorithms have been investigated for unicast-based real-time systems and push-based broadcast systems (e.g., , , ), they are not applicable or not effective to on-demand broadcast systems. In this paper, we are interested in developing new scheduling algorithms for time-critical on-demand broadcast. The main
8921588	19907	However, none of these algorithms considered the time constraints of requests in making scheduling decisions. There exist a few studies on broadcast scheduling with time constraints , . However, these studies considered periodic push-based broadcast only, which is fundamentally different from on-demand broadcast in system architecture. Xuan et al.  evaluated several
8921588	19908	HTTP requests are normally associated with timeout values to prevent users from unlimited waiting when the web servers are heavily loaded. Furthermore, in mobile location-based information services , the query response (e.g., the nearest bus station) is useful to a mobile user only if it arrives soon after the query issuance; otherwise, the user may have moved away from the original location
8921588	19909	proposed a low overhead and scalable scheduling algorithm called R×W. Datta et al.  took into consideration the energy saving issue in the design of on-demand broadcast systems. Liberatore  studied the scheduling algorithms for requests asking for a list of dependent items. Hu and Chen  investigated dynamic traffic-aware scheduling algorithms. However, none of these algorithms
8921588	1496	systems and transaction processing in realtime databases. Many algorithms and theoretical results have been developed , . One of the most classical scheduling algorithms is the EDF algorithm . It offers the optimal performance in light-load conditions in terms of deadline missing rate. In overloaded conditions, not all tasks can be completed by their deadlines. When task service times
8921588	19911	fixed-size items) shows poor performance for on-demand broadcast scheduling. Other related work concerning on-demand broadcast includes energy-efficient retrieval , , client cache management , combination with pushed broadcast , and fault-tolerant broadcast . These studies complement to our work in different aspects. In summary, existing time-critical scheduling algorithms are
8921588	19914	used to select and broadcast requested items from outstanding requests. While there has been significant work on the development of on-demand broadcast scheduling algorithms (e.g., , , ), none of them considered the time constraints associated with requests. On the other hand, 1salthough time-critical scheduling algorithms have been investigated for unicast-based real-time systems
8921588	19914	algorithms and the analytical optimum. Finally, Section VII concludes the paper. II. RELATED WORK There has been significant work on the development of on-demand broadcast scheduling algorithms . Recently, Acharya and Muthukrishnan  studied some scheduling algorithms for variable-size data items and introduced a new performance metric called stretch. Aksoy and Franklin  proposed a
8921588	19916	with time constraints , . However, these studies considered periodic push-based broadcast only, which is fundamentally different from on-demand broadcast in system architecture. Xuan et al.  evaluated several alternative system designs for time-constrained data accesses and showed that on-demand broadcast with the earliest deadline first (EDF) scheduling algorithm performs well.
8884949	19950	domain of its intended applicability. Instead, tests and evaluations are conducted until sufficient confidence is obtained that a model can be considered valid for its intended application (Sargent 1982, 1984 and Shannon 1975). The relationshipssCost Value Cost 0% Model Confidence 100% Figure 1: Model Confidence Value of Model to User of cost (a similar relationship holds for the amount of time)
8884949	19950	reviewed work using both of these ways and concluded that the simple way more clearly illuminates model verification and validation. This author recommends the use of a simple way (see, e.g., Sargent 1982), which is presented next. Consider the simplified version of the modeling process in Figure 2. The problem entity is the system (real or proposed), idea, situation, policy, or phenomena to be
8884949	19950	remaining data are used to determine (test) whether the model behaves as the system does. (This testing is conducted by driving the simulation model with either distributions or traces (Balci and Sargent 1982a, 1982b, 1984b).) Historical Methods: The three historical methods of validation are rationalism, empiricism, and positive economics. Rationalism assumes that everyone knows whether the underlying
10091228	19987	to recognize the narrow scope and provisionality of their representations. Given that no hard enduring boundaries exist in reality (a more detailed presentation of this argument can be found in Richardson, 2001), the use of the term “system” can be misleading as it suggests the existence of completely autonomous entities. Maybe we should rename complexity science as the ‘science of partial complex
10091228	19991	realisable. If we assume that it is realisable, the critical idea underlying the quest will be perverted into its opposite, i.e. into a false pretension to superior knowledge and understanding” (Ulrich, 1993). In many ways complexity science provides insights concerning analysis that might be seen as nothing more than common sense. The need for an awareness of the provisionality of all understanding
19993	14062	¦?¨???????? an elastic active net : . Active nets ? or deformable grids are defined in . The net deforms according to the minimization of an energy. It behaves as a set of active contours  that evolve together with mutual interaction. Our energy consists of three terms. 1. A regularization term as in , ??? : ??? ¡£¢ ??? ¦?? ¨ ? ? ¨ ? ? ? ¨ ? ¦?? ? ? ? ? ?? ¨ ? ? ? ? ¨ ? ? ? ?
19993	19995	same organ. In consequence, we will con§ ? ? ? ? ? ? ? ? ? sider the set of curves as ¦©¨???¨?¦?¨???? ? ??¨£? ¦?¨???????? an elastic active net : . Active nets ? or deformable grids are defined in . The net deforms according to the minimization of an energy. It behaves as a set of active contours  that evolve together with mutual interaction. Our energy consists of three terms. 1. A
19993	18332	through time in an ultrasound sequence, the third constraint must be very soft, and ? ??? ? ¨ ¦?¨§? , ? and ? are posi2 should be small compared to the other coefficients. We ? proceed as in  and use a steepest gradient descent, discretization was done by finite differences. The energy can have many local minima. Therefore we must use an initialization close to the solution. We choose
20002	20004	more reusable than code specific tests in the case of software evolution. While in the literature, it is admitted that use-cases and scenarios offer a good input to generate test cases and oracles , many obstacles remain, that have been identified by Binder  in terms of three questions : How do I choose test cases? ; In what order should I apply my tests? ; How do I know when I’m done? The
20002	20004	1 are consistent with sequential constraints, and the last one concerns the definition of a non-ambiguous test adequacy criterion. To our knowledge, the only work in that direction is Briand’s one  with a complete system-based testing methodology. In this paper, we go along the same lines and focus on requirement-based testing and not on system testing in the sense we want our approach to be
20002	20004	or just left implicit. Here, we propose to make it explicit in a declarative way. Indeed, we claim that a declarative specification is easier to express than an activity diagram as proposed in  (activity diagrams are a graphical language used in  to express sequential constraints between the usescases). As shown in Section 5.3, activity diagrams rapidly become too complex while
20002	20004	than to draw an activity diagram, the ADs may lead to generate a significant amount of invalid test cases. 6 Related work The main contribution for system testing from use cases can be found in . The authors propose to express the Proceedings of the 14th International Symposium on Software Reliability Engineering (ISSRE’03) 1071-9458/03 $ 17.00 © 2003 IEEE sequential constraints of the use
20002	20004	more reusable than code specific tests in the case of software evolution. While in the literature, it is admitted that use-cases and scenarios offer a good input to generate test cases and oracles , many obstacles remain, that have been identified by Binder  in terms of three questions : How do I choose test cases? ; In what order should I apply my tests? ; How do I know when I’m done? The
20002	20004	1 are consistent with sequential constraints, and the last one concerns the definition of a non-ambiguous test adequacy criterion. To our knowledge, the only work in that direction is Briand’s one  with a complete system-based testing methodology. In this paper, we go along the same lines and focus on requirement-based testing and not on system testing in the sense we want our approach to be
20002	20004	or just left implicit. Here, we propose to make it explicit in a declarative way. Indeed, we claim that a declarative specification is easier to express than an activity diagram as proposed in  (activity diagrams are a graphical language used in  to express sequential constraints between the usescases). As shown in Section 5.3, activity diagrams rapidly become too complex while
20002	20004	than to draw an activity diagram, the ADs may lead to generate a significant amount of invalid test cases. 6 Related work The main contribution for system testing from use cases can be found in . The authors propose to express the Proceedings of the 14th International Symposium on Software Reliability Engineering (ISSRE’03) 1071-9458/03 $ 17.00 © 2003 IEEE sequential constraints of the use
20002	673	size), since a system can hardly be entirely described with a unique state machine (and if it is possible, its size would be too large to use it). Generating tests from contracts has been done in  for unit testing. The principle was to use contracts written in JML to generate a test oracle. Nevertheless only the oracle is generated, and not the entry data in this method. 7 Conclusion and
20002	20007	are taken into account. Other approaches rely on use cases and scenarios to generate system tests. In  a method is detailed in order to systematically derive test cases for system testing. In , a complete approach is proposed to generate system-level test cases from an accurate description of the use cases of a system. The use cases description includes pre and post conditions in natural
20002	2050	in the sense we want our approach to be independent from analysis and design steps. We propose a systematic requirement-by-contract approach, close to the design-bycontract approach proposed in , except that logical expression are very simple to fit with requirement level of preciseness. This approach answers Binder’s problematic by identifying in the contracts the variables involved in
20002	2050	clear in the UML 1.x semantics. We then propose to treat extensions and inclusions the same way. generalization link: the semantics we propose for generalization is the same as the one proposed in  for contracts on methods. If a use case UC1 inherits of a use case UC2, thenpre(UC1) is transformed into the disjunction pre(UC1)orpre(UC2),andpost(UC1) is transformed into the conjunction
20002	20009	The authors suggest then to focus on methods guiding the testers into a systematic test approach. The test generation from UML artifacts is now the object of a large number of works. In particular,  proposes an approach to generate tests from UML state machines, w.r.t. efficient test coverage. This technique is well suited for unitstesting, but not for system testing (or for system of small
20002	20010	propose a semantical criterion, which is not directly based on the UCTS but rather on the contracts system. This criterion is similar (but not identical) to the full predicate coverage proposed in  in the context of test criteria for state-based functional specification. The philosophy of the criterion All Precondition Terms is to guarantee that all the possible ways to apply a use case are
20002	20011	Then we obtain the test cases using a template associated to each use case giving the syntactic requirements of the implementation. Other existing test cases generation can be used such as TGV or AGATHA. 5.2 Study of the generated test efficiency System structure. For the experimental validation, we used a Java implementation of the virtual meeting. This implementation is made of
20002	20012	more reusable than code specific tests in the case of software evolution. While in the literature, it is admitted that use-cases and scenarios offer a good input to generate test cases and oracles , many obstacles remain, that have been identified by Binder  in terms of three questions : How do I choose test cases? ; In what order should I apply my tests? ; How do I know when I’m done? The
20002	20014	more reusable than code specific tests in the case of software evolution. While in the literature, it is admitted that use-cases and scenarios offer a good input to generate test cases and oracles , many obstacles remain, that have been identified by Binder  in terms of three questions : How do I choose test cases? ; In what order should I apply my tests? ; How do I know when I’m done? The
20002	20014	of specifying infeasible use-case sequences). Moreover, not all the interactions between actors are taken into account. Other approaches rely on use cases and scenarios to generate system tests. In  a method is detailed in order to systematically derive test cases for system testing. In , a complete approach is proposed to generate system-level test cases from an accurate description of the
20002	20015	functional and robustness test cases from the requirements enhanced with contracts. We study the efficiency of the generated test cases on a case study. 1 Introduction The conclusion of a survey  of industrial software projects insists on the industrial need to base system tests on use cases and scenarios. However, most projects lack a systematic approach for defining test cases based on
8921606	20054	The generator permits the creation of a great number of prototype variants 17 V ? 5 ?10 . In comparison with that, using the 7 Microsoft AppWizard V ? 6 ?10 prototype variants can be created. The frame borders are labelled in the comments, in the source of the generated prototypes. The frames can be updated in the generator after the editing of source. This makes Roundtrip-Engineering
14155662	8921610	quantization condition of integer multiples, ix i.e. they are present in certain N number of atoms, as experimentally established in the superfluid phase of 4 He, ? vs. dl = 2? . n? / m4 = n. ? o (2) where m4 is the helium particle mass, and ?o is the quantum of circulation (Nozieres & Pines 1990, Thouless et al. 2001). Furthermore, quantized vortices is a topological excited state, which takes
14155662	14155664	equation. As we know, for the wave function to be well defined and singlevalued, the momenta must satisfy Bohr-Sommerfeld’s quantization conditions (Van Holten 2001): ? ? p . dx = 2? . n? (3) for any closed classical orbit ?. For the free particle of unit mass on the unit sphere the left-hand side is © 2004 C. Roy Keys Inc. — http://redshift.vif.comsApeiron, Vol. 11, No. 1, January 2004
14155662	20067	to the implications of equation (6) in regards to the basic vortex model. If T is the orbit period of the above planet around the Sun, then by Kepler’s third law, Or r ? 3 2 2 ? T ? ( 2 r / v) (7) 2 2 v r ? 4? = kspring where r, T, v, kspring represents semimajor axes, orbit period, orbit velocity, and ‘spring constant’ of the dynamics system, respectively. xvi For gravity case, one obtains
14155662	4442	at various scales using equation (6). Therefore, by using a fractional derivative method as described by Kolwankar (1998, eq. 2.9), then q q q q q d f ( ? x) / = ? .{ d f ( ?x) / } (9) where it is assumed that for dx ? 0, d( ? x) ? dx . Hence this author obtained (Christianto 2002b) a linear scaling factor for equation (6): This equation implies : a ? 2 2 0 . . / o v GM n = (10)
14155662	8921615	} (9) where it is assumed that for dx ? 0, d( ? x) ? dx . Hence this author obtained (Christianto 2002b) a linear scaling factor for equation (6): This equation implies : a ? 2 2 0 . . / o v GM n = (10) ( / ) 2 2 v = v ? (11) 1 o o In other words, for different scaling reference frames, specific velocity v1 may vary and may be influenced by a scale effect ?. To this author’s present knowledge,
14155662	4444	o Godfrey et al.’s (2001) model of superfluid vortices. First, we note here that Gibson (1999) has shown that his NavierStokes-Newton model yields the following solution: 2 vr = m'. Gt / r (12) where r, t, G, m’, vr represents semimajor axes, time elapsed, Newton gravitation constant, mass of the nucleus of orbit, and specific velocity, respectively. It is clear therefore that equation
14155662	4444	quadratic quantum number n 2 . A plausible reason for this missing quantum number is that Gibson (1999) assumed a normal fluid in his model instead of quantum liquid. He also argued that equation (12) only governs the formation stage (such as spiral nebulae formation); while equation (13) is also applicable for present time provided we assert a quantum liquid for the system. Therefore we also
14155662	4447	(2001) argued that the fluid at the edge of the disk moves a distance 4?cR in a time T (with angular velocity ? = 2?/T), thus having a critical dimensional linear velocity of vdisk c = 2? . ? R / ? (17) In this equation, ?c represents critical amplitude where damping of the oscillations reduce to a value, which was interpreted as the damping due only to viscosity of the normal fluid component. In
14155662	4447	observe such critical amplitude for the motion of celestial objects. Of course for spherical orbit systems the equation of critical dimensional linear velocity is somewhat different from equation (17) above (Godfrey et al. 2001). To this author’s present knowledge such theoretical linkage between critical amplitude of superfluid vortices and astronomical orbital motions has also never been made
14155662	4448	deviation (s) between these observed and predicted values, we can solve equation (6) for the reduced mass ? to get the most probable distribution for outer planet orbits: ? = m . m ) /( m + m ) (18) ( 1 2 1 2 © 2004 C. Roy Keys Inc. — http://redshift.vif.comsApeiron, Vol. 11, No. 1, January 2004 136 It is worth noting here, that a somewhat similar approach using reduced mass ? to derive
14155662	4448	as follows: ? g 2 2 ?2 2 2 ( ? ? / ?r + ?? / r?r + r . ? ? / ? ) + V? = E? / 2? . ? (18a) though Neto et al. (2002) did not come to the same conclusion as presented here. Result of this method (18) is presented in Table 1 below. Table 1. Predicted orbit values of inner and outer planets in Solar system From Table 1 above we obtain ? = 26.604.m1, for the minimum standard deviation s = 0.76AU.
20129	20130	and Electronic-Optical (E/O) conversion may be indispensable for the control of optical networks. Our research focuses on the efficient utilization of regeneration resources in optical networks . This study * This work was supported in part by NSF grants (ANI0074121 and EPS-0091900). will introduce dynamic routing schemes into our network architecture and merge it with the trend of using
20129	20135	with the modified Dijkstra’s or Bellman-Ford algorithm to achieve an optimal solution. Recent research efforts have extended these algorithms to route wavelengths in an optical networks . Subject to bandwidth or quality constraints on links, the routing objective can be either a constrained or a minimum metric . Many proposed algorithms use static or offline routing, which
20129	3260	these algorithms to route wavelengths in an optical networks . Subject to bandwidth or quality constraints on links, the routing objective can be either a constrained or a minimum metric . Many proposed algorithms use static or offline routing, which calculates routes prior to the existence of connection requests. Instead, dynamic or online routing satisfies the requests one-by-one
20129	20136	utilization. Over the last few years, there has been a trend to develop a unified control plane covering both IP and optical networks under the umbrella of Multiprotocol Label Switching (MPLS) . Using MPLS signaling (Resource Reservation Protocol (RSVP) and Label Distribution Protocol (LDP)), OXCs can inter-operate with IP Label-Switched Routers (LSRs) to establish a lightpath as a Label
8832677	14625	as constructing control policies that achieve maximum throughput under different stochastic assumptions on the input and service processes; for example, drift analysis (Hajek 1982), fluid models (Dai and Meyn 1994; Dai 1995; Meyn 1996), sample path analysis (Baccelli and Bremaud 1994; Walrand 1988). On the other hand, it is a much harder task George Michailidis Derek R. Bingham Department of Statistics The
8832677	14623	policies that achieve maximum throughput under different stochastic assumptions on the input and service processes; for example, drift analysis (Hajek 1982), fluid models (Dai and Meyn 1994; Dai 1995; Meyn 1996), sample path analysis (Baccelli and Bremaud 1994; Walrand 1988). On the other hand, it is a much harder task George Michailidis Derek R. Bingham Department of Statistics The University
8832677	20155	relationship between the system response and the inputs requires a flexible model to adequately approximate the network dynamics. One could use a non-parametric regression approach (e.g. MARS (Friedman 1991)) to help model the response. However, non-parametric procedures do not provide an easy to interpret model relating the input factors to the response. Moreover, the theory of which input points to
8832677	20155	Derived from the Naive Approach (Left Panel) and the Plot of (Predicted-True) Delays in Log-Scale (Right Panel) third competing approach that fits multivariate adaptive regression splines (MARS) (Friedman 1991) to the responses at the 27 locations used in the treed models under these two criteria are given in Table 1. It can be seen that our flexible approach has the smallest EISE (0.218 2 )andMAE
8832677	20158	that achieve maximum throughput under different stochastic assumptions on the input and service processes; for example, drift analysis (Hajek 1982), fluid models (Dai and Meyn 1994; Dai 1995; Meyn 1996), sample path analysis (Baccelli and Bremaud 1994; Walrand 1988). On the other hand, it is a much harder task George Michailidis Derek R. Bingham Department of Statistics The University of Michigan
8921675	4337	and delay effects, which may intermittently cause logic-error and failure of the chip. In recent years, there have been some research in the signal integrity area to model and test noise and delay    . Regardless of the methods to detect integrity loss, we need a mechanism to manage the test session within or independent of other test sessions for a SoC. The signals carrying noise
8921675	4337	. ¯Test methodologies: Several self-test methodologies have been developed to test interconnects for signal integrity in high-speed SoCs. At-speed test of crosstalk in chip interconnects , testing interconnect crosstalk defects using on-chip processor , a BIST to test long interconnects for signal integrity  and using boundary scan and IDDT for testing bus  are some of the
8921675	6838	delay effects, which may intermittently cause logic-error and failure of the chip. In recent years, there have been some research in the signal integrity area to model and test noise and delay    . Regardless of the methods to detect integrity loss, we need a mechanism to manage the test session within or independent of other test sessions for a SoC. The signals carrying noise and
8921675	6838	in high-speed SoCs. At-speed test of crosstalk in chip interconnects , testing interconnect crosstalk defects using on-chip processor , a BIST to test long interconnects for signal integrity  and using boundary scan and IDDT for testing bus  are some of the proposed methods. The experiments show that short interconnects as well as long interconnects are susceptible to the integrity
8921675	6838	to measure jitter and skew in the range of few picoseconds. The authors in  presented a sample and hold circuit that probes the voltage directly within the interconnects. The work presented in  proposed two cells, called noise detector (ND) and skew detector (SD) cells, based on a modified cross-coupled PMOS differential sense amplifier. These cells sit physically near the end of an
8921675	4338	effects, which may intermittently cause logic-error and failure of the chip. In recent years, there have been some research in the signal integrity area to model and test noise and delay    . Regardless of the methods to detect integrity loss, we need a mechanism to manage the test session within or independent of other test sessions for a SoC. The signals carrying noise and delay
8921675	4338	been developed to test interconnects for signal integrity in high-speed SoCs. At-speed test of crosstalk in chip interconnects , testing interconnect crosstalk defects using on-chip processor , a BIST to test long interconnects for signal integrity  and using boundary scan and IDDT for testing bus  are some of the proposed methods. The experiments show that short interconnects as
8921675	4341	21st International Conference on Computer Design (ICCD’03) 1063-6404/03 $ 17.00 © 2003 IEEE fault model  is one of the fault models proposed for crosstalk. Analysis of crosstalk is described in  . Analysis of interconnect defects coverage of test sets is explained in . They address the problem of evaluating the effectiveness of test sets to detect crosstalk defects in
8921675	4341	mechanism. However, throughout this paper, we call it multiple transition fault model for two reasons. First, we wanted a terminology similar to maximum aggressor fault model widely used  . Second, because MT pattern set covers the high-speed interconnect coupling faults comprehensively. Figure 2 shows the simulation results of two MT-patterns (i) 0110110 ? 1001001 and (ii) 0110101 ?
8921675	4348	to detect crosstalk defects in interconnections of deep submicron circuits. Several researchers have worked on test pattern generation for crosstalk noise and delay and signal integrity   . There is a long list of possible design and fabrication solutions to reduce signal integrity problems on the interconnect. None guarantees to resolve the issue perfectly . ¯Test methodologies:
8921675	4348	play, some researchers showed that MA may not reflect the worst case and presented other ways (pseudorandom or deterministic) to generate test patterns to create maximal integrity loss   . As reported in , a chip failed when the nearest aggressor line changes in one direction and the other aggressors are in the opposite direction. This cannot be covered by MA fault model and
8921675	4355	focused on the development of deterministic test for interconnect faults at board level. BIST test pattern generators for board level interconnect testing and delay testing are proposed in  and , respectively. A modified boundaryscan cell using an additional level sensitive latch (called Early Capture Latch or ECL) was proposed in  for delay fault testing. IEEE 1149.4 mixed-signal test
8921675	4335	amplifier. These cells sit physically near the end of an interconnect and samples the actual signal plus noise. To detect delay violation, an integrity loss sensor (called ILS) has been designed in  which is flexible and tunable. The acceptable delay region is defined as the time period from the triggering clock edge during which all output transitions must occur. ¯Boundary Scan Application:
8921675	4335	A test methodology targeting bus interconnects defects using IDDT and boundary scan has been presented in . An extending JTAGsis proposed to test SoC interconnects for signal integrity in  . The maximum aggressor (MA) fault model was used in . MA test patterns are generated and applied to the interconnects by modified boundary scan cells placed at the output of a core. The other
8921675	4335	cells cited at the input of a core (at the end of interconnects) collect the integrity loss information. The drawback is that MA fault model is not included inductance. We completed our work in  in which we assumed that test patterns are already generated based on a fault model including inductance. The test patterns are scanned by an external tester into the boundary scan cells and
8921675	4335	generated with other models can be applied by the external tester through the new boundary scan cells in the conventional mode of the boundary scan architecture. B. Observation BSC (OBSC) In , we proposed a new BSC at the receiving side of the interconnects which employs the ILS cell. Figure 12 shows the new BSC named observation BSC (OBSC). As shown, ILS is added to the receiving side
8921675	21444	i.e. 1110111 ? 0001000 applied to a seven interconnect system while the middle one is victim and the others are aggressors. Extraction and simulation are done by OEA tool (BUSAN)  and TISPICE , respectively. The interconnect model is distributed RLC and coupling capacitance and mutual inductances are considered between the lines using OEA tool for 0?18µm technology. As shown, MT-patterns
20197	20198	channel capacity. If the channel fade level is known at the transmitter, then Shannon capacity is achieved by adapting the transmit power, data rate, and coding scheme relative to this fade level . Based on these results, an adaptive variablerate variable-power transmission scheme using uncoded -ary quadrature amplitude modulation (MQAM) was proposed in . This adaptive technique is 17 dB
20197	12296	scheme relative to this fade level . Based on these results, an adaptive variablerate variable-power transmission scheme using uncoded -ary quadrature amplitude modulation (MQAM) was proposed in . This adaptive technique is 17 dB more power efficient than nonadaptive modulation in fading. Adaptive modulation Paper approved by J. Huber, the Editor for Coding and Coded Modulation of the IEEE
20197	12296	this paper we assume perfect channel estimates at the receiver with zero delay in the feedback path. The effects of estimation error and feedback path delay on adaptive modulation were analyzed in , where it was found that an estimation error less than 1 dB and a feedback path delay less than 0.001/ results in minimal performance degradation, for the Doppler frequency of the fading channel.
20197	12296	to obtain a good estimate. In addition, hardware and pulse shaping considerations generally dictate that the constellation size must remain constant over tens to hundreds of symbols. We showed in  that this requirement translates mathematically to the requirement that , where is the symbol time and is the average time that our adaptive modulation scheme continuously uses the constellation .
20197	12296	the fading stays within the region . The value of is inversely proportional to the channel Doppler and also depends on the number and characteristics of the different fade regions. It was shown in  that in Rayleigh fading with an average SNR of 20 dB and a channel Doppler of 100 Hz, ranges between 0.7–3.9 ms, and thus for a symbol rate of 100 ksymbols/s, the signal constellation remains
20197	12296	a coset code with a general class of adaptive modulation techniques is outlined in Section II. Section III describes the superposition of trellis codes onto the adaptive modulation proposed in  and obtains the corresponding spectral efficiency. Analytical and simulation results for the BER and spectral efficiency of this trellis-coded adaptive MQAM are presented in Section IV. We compare
1056702	20209	do not discuss these factors in detail, but alternatives may be based on the conceptual relevance of the object types occuring in the alternatives (, ), or the user’s past behaviour (). Let us presume the user is interested in presidents who are married and the number of children that resulted from these marriages. In the starting node, the user may select ‘the president’ as the
1056702	16062	introduced as they form the backbone of the query by navigation system. We base ourselves on the formalisation of the path expressions as provided in , and the formalisation of ORM given in . 3.1 ORM Schemas A conceptual schema is presumed to consist of a set of types TP. Within this set of types two subsets can be distinguished: the relationship types RL, and the object 4stypes OB.
1056702	16062	relation is defined by the following four derivation rules: 1. x ? TP ? x ? x 2. x IdfBy y ? x ? y 3. x ? y ? y ? x 4. x ? y ? z ? x ? z Note that when using ORM with advanced concepts (, ) such as sequence types, set types, etc., the definition of ? needs to be refined. 3.2 Linear Path Expressions The central aspect of query by navigation are the (linear) path expressions ().
1056702	10702	the (linear) path expressions need to be introduced as they form the backbone of the query by navigation system. We base ourselves on the formalisation of the path expressions as provided in , and the formalisation of ORM given in . 3.1 ORM Schemas A conceptual schema is presumed to consist of a set of types TP. Within this set of types two subsets can be distinguished: the
1056702	10702	This relation is defined by the following four derivation rules: 1. x ? TP ? x ? x 2. x IdfBy y ? x ? y 3. x ? y ? y ? x 4. x ? y ? z ? x ? z Note that when using ORM with advanced concepts (, ) such as sequence types, set types, etc., the definition of ? needs to be refined. 3.2 Linear Path Expressions The central aspect of query by navigation are the (linear) path expressions
1056702	10702	the reversed binary relation associated to role r. Note that in a forthcoming Asymetrix research report the path expressions will be discussed in more detail. For the moment, however, refer to  for an elaborate formal definition, and to  for a more detailed informal discussion. When displaying linear path expressions in the nodes of the query by navigation mechanism, the linear
1056702	20212	r. Note that in a forthcoming Asymetrix research report the path expressions will be discussed in more detail. For the moment, however, refer to  for an elaborate formal definition, and to  for a more detailed informal discussion. When displaying linear path expressions in the nodes of the query by navigation mechanism, the linear path expressions need to be verbalised. These
1056702	20213	with an adaption of the existing idea of query by navigation to make it fit within the InfoAssistant product. Query by navigation has been discussed before in , , , and . In this report we base ourselves mainly on the latest version of the query by navigation mechanism as discussed in . We have, however, stripped the existing definition from its information
1056702	20213	the result so far the user can select the GO! button. This should result in a window showing the result of the path formulated in the query by navigation session. In , , , and , the query by navigation system also included the population. In such a system, a user can either navigate through the population of the schema, or through the conceptual schema (the type level) as
1056702	20213	a more complete implementation of the query by navigation mechanism may be considered. For instance the navigation through the population and the support of multiple abstraction layers (). Furthermore, the verbalisation of (linear) path expressions is an area in which more improvements are possible. Finally, extensive testing of the user interface of the query by navigation tool is
1056702	20215	discussed in . We have, however, stripped the existing definition from its information retrieval context to tailor it better to the InfoAssistant context. In the previous Asymetrix reports  and , we have already provided an elaborate motivation for the query formulation tools we envision for InfoAssistant. Therefore, we do not provide any further motivation for query by
1056702	20215	on multiple factors. In this paper we do not discuss these factors in detail, but alternatives may be based on the conceptual relevance of the object types occuring in the alternatives (, ), or the user’s past behaviour (). Let us presume the user is interested in presidents who are married and the number of children that resulted from these marriages. In the starting node,
1056702	20216	. We have, however, stripped the existing definition from its information retrieval context to tailor it better to the InfoAssistant context. In the previous Asymetrix reports  and , we have already provided an elaborate motivation for the query formulation tools we envision for InfoAssistant. Therefore, we do not provide any further motivation for query by navigation in this
1056702	20218	This report is concerned with an adaption of the existing idea of query by navigation to make it fit within the InfoAssistant product. Query by navigation has been discussed before in , , , and . In this report we base ourselves mainly on the latest version of the query by navigation mechanism as discussed in . We have, however, stripped the existing
1056702	20218	an impression of query the result so far the user can select the GO! button. This should result in a window showing the result of the path formulated in the query by navigation session. In , , , and , the query by navigation system also included the population. In such a system, a user can either navigate through the population of the schema, or through the conceptual
189978	20221	the server log data for two media servers in use at major public universities in the United States. Both servers contain higher quality videos than servers or client workloads previously analyzed . The eTeach system  was introduced in September 2000 to deliver the lectures and laboratory demos for a computer science course with an enrollment of 280 students. A key feature of this course
189978	20221	of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a mix of education and entertainment videos in the mMod Table 1: Summary Statistics system , and education content on an internal server of a large international corporation  have provided data on particular aspects of those media server workloads (e.g., example daily variation in
189978	20221	or session durations, relative frequency of various types of interactive requests), and have pointed out that access patterns for educational media may be different than for entertainment media . A parallel study by Chesire et al.  analyzes the RTSP sessions originating from a large university to numerous media servers on the Internet, and characterizes session duration, object and
189978	20221	of the requests and more than 95% of the minutes delivered are for videos encoded at 350500 kb/s. These rates are considerably higher than the rates observed in previous streaming workload studies . The load per day for stored content on each server (Figure 2) has a similar profile as in . The BIBS server typically has 500-800 client sessions per weekday, with occasional peaks of up to
189978	20221	characterizations of media server file access frequencies are the log-log plots of number of accesses versus file rank for an unstated period of time on the mMOD education and entertainment server  and for a university one-week trace of RTSP media requests to multiple servers from a given large population . The latter access frequencies are accurately modeled by a single Zipf-like
189978	7041	whereas the time between interactive requests within eTeach sessions is more accurately modeled by the heavy-tailed Pareto distribution, as has been found in more traditional Web server workloads . ??? For each server, there are very few periods of stationary relative file access frequency. • Over periods of reasonably stable relative file access frequency, or over one-day periods, the file
189978	7041	eTeach and 7am for BIBS). Requests for files that are not media files or were not found on the servers were removed from each log. 2.2 Related Work There have been several studies of Web workloads  but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a
189978	7041	exponential, gamma, Weibull, and Pareto distributions to determine the distribution of time between requests. Examples of the fitted curves are shown in Figure 5. Like other types of Web workloads , client session arrivals (overall and per file) in BIBS were found to be approximately Poisson, whereas the time between requests in eTeach has a heavy-tailed Pareto distribution in every case
189978	20223	eTeach and 7am for BIBS). Requests for files that are not media files or were not found on the servers were removed from each log. 2.2 Related Work There have been several studies of Web workloads  but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a
189978	175	on either server can be characterized by the concatenation of two Zipf-like distributions, rather than the single Zipf-like distribution observed in previous Web and streaming media workloads . • On each server, a significant fraction of the new files accessed each hour are not requested again for more than eight hours, which motivates the need to reevaluate the traditional
189978	175	eTeach and 7am for BIBS). Requests for files that are not media files or were not found on the servers were removed from each log. 2.2 Related Work There have been several studies of Web workloads  but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a
189978	175	(a) BIBS 3/23 6pm-1am, File Size: 50-55min 0.15 0.05 (b) eTeach 10/11 0am-11pm, File Size: 0-5min Figure 9: Example Distributions of File Access Frequencies frequencies for Web HTML and image files , videos at a video rental store , and the one-week multi-server streaming workload from a given client population , which are accurately modeled by a single Zipf-like distribution. The
189978	8655	eTeach and 7am for BIBS). Requests for files that are not media files or were not found on the servers were removed from each log. 2.2 Related Work There have been several studies of Web workloads  but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a
189978	20224	eTeach and 7am for BIBS). Requests for files that are not media files or were not found on the servers were removed from each log. 2.2 Related Work There have been several studies of Web workloads  but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a
189978	20226	0.15 0.05 (b) eTeach 10/11 0am-11pm, File Size: 0-5min Figure 9: Example Distributions of File Access Frequencies frequencies for Web HTML and image files , videos at a video rental store , and the one-week multi-server streaming workload from a given client population , which are accurately modeled by a single Zipf-like distribution. The number of files, the total access
189978	6327	strategies, and quantifying how much server bandwidth can be saved in an interactive educational environment by using recently developed multicast streaming methods for stored content (e.g., ). Analyses presented in this paper that have not, to our knowledge, previously been reported for media workloads include the ? This work was partially supported by the National Science Foundation
189978	6327	indicating a high degree of interactivity. To quantify how much server bandwidth can be saved by multicast delivery in eTeach, we simulated the Closest Target multicast technique described in  for client traces during periods of stationary total request rate, varying from 10 to 70 requests per hour. For the three files with highest request count in each period, Table 7 compares the
189978	20227	the server log data for two media servers in use at major public universities in the United States. Both servers contain higher quality videos than servers or client workloads previously analyzed . The eTeach system  was introduced in September 2000 to deliver the lectures and laboratory demos for a computer science course with an enrollment of 280 students. A key feature of this course
189978	20227	system , a mix of education and entertainment videos in the mMod Table 1: Summary Statistics system , and education content on an internal server of a large international corporation  have provided data on particular aspects of those media server workloads (e.g., example daily variation in server load, distribution of media stream or session durations, relative frequency of
189978	20227	of the requests and more than 95% of the minutes delivered are for videos encoded at 350500 kb/s. These rates are considerably higher than the rates observed in previous streaming workload studies . The load per day for stored content on each server (Figure 2) has a similar profile as in . The BIBS server typically has 500-800 client sessions per weekday, with occasional peaks of up to
189978	20227	further in Section 5. 3.3 Media Delivered per Session For both servers, we observed a significant fraction of requests of short duration (less than 3 minutes of video), as in other recent workloads . In this section, we further characterize the media delivered per session on the BIBS server, which to our knowledge has not been done in previous media workload studies. As shown in Figure 4, the
189978	20227	the frequency of each interactive request type (e.g., pause, jump forward, etc.), and the distribution of ON and OFF times in the session. Previous characterizations of interactive media sessions  have computed some of these statistics only for the aggregate of all sessions in the client workload that was analyzed, whereas we provide a summary of the variations in the statistics obtained for
189978	20228	the server log data for two media servers in use at major public universities in the United States. Both servers contain higher quality videos than servers or client workloads previously analyzed . The eTeach system  was introduced in September 2000 to deliver the lectures and laboratory demos for a computer science course with an enrollment of 280 students. A key feature of this course
189978	20228	but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a mix of education and entertainment videos in the mMod Table 1: Summary Statistics system , and education content on an internal server of a large international corporation  have provided
189978	20228	BIBS logs end during the final exam week for the semester. Figure 1(a) shows that eTeach has higher variability in stored and requested file sizes than education media servers previously studied , with 25-30% of requests to short videos (i.e., under 5 minutes) and other requests approximately evenly distributed to videos of duration 5-10, 10-15, 15-20, 20-25 and 30-35 minutes. Figure 1(b)
189978	20228	a majority (i.e., 70%) of BIBS requests for stored content are for 50-55 minute videos, similar to previously studied education server workloads that contain stored content from classroom lectures . Nearly all videos requested on the eTeach server are encoded for 300 kilobits per second. For BIBS, 80% of the requests and more than 95% of the minutes delivered are for videos encoded at 350500
189978	20228	are considerably higher than the rates observed in previous streaming workload studies . The load per day for stored content on each server (Figure 2) has a similar profile as in . The BIBS server typically has 500-800 client sessions per weekday, with occasional peaks of up to 2000 sessions, and a typical average of 15–25 min of media per session. The eTeach server
189978	2679	whereas the time between interactive requests within eTeach sessions is more accurately modeled by the heavy-tailed Pareto distribution, as has been found in more traditional Web server workloads . ??? For each server, there are very few periods of stationary relative file access frequency. • Over periods of reasonably stable relative file access frequency, or over one-day periods, the file
189978	2679	exponential, gamma, Weibull, and Pareto distributions to determine the distribution of time between requests. Examples of the fitted curves are shown in Figure 5. Like other types of Web workloads , client session arrivals (overall and per file) in BIBS were found to be approximately Poisson, whereas the time between requests in eTeach has a heavy-tailed Pareto distribution in every case
189978	20229	the server log data for two media servers in use at major public universities in the United States. Both servers contain higher quality videos than servers or client workloads previously analyzed . The eTeach system  was introduced in September 2000 to deliver the lectures and laboratory demos for a computer science course with an enrollment of 280 students. A key feature of this course
189978	20229	have been several studies of Web workloads  but those workloads did not include significant access to media content. Recent studies of client access to MANIC system audio content , low-bitrate videos in the Classroom2000 system , a mix of education and entertainment videos in the mMod Table 1: Summary Statistics system , and education content on an internal server of
189978	20229	BIBS logs end during the final exam week for the semester. Figure 1(a) shows that eTeach has higher variability in stored and requested file sizes than education media servers previously studied , with 25-30% of requests to short videos (i.e., under 5 minutes) and other requests approximately evenly distributed to videos of duration 5-10, 10-15, 15-20, 20-25 and 30-35 minutes. Figure 1(b)
189978	20229	a majority (i.e., 70%) of BIBS requests for stored content are for 50-55 minute videos, similar to previously studied education server workloads that contain stored content from classroom lectures . Nearly all videos requested on the eTeach server are encoded for 300 kilobits per second. For BIBS, 80% of the requests and more than 95% of the minutes delivered are for videos encoded at 350500
189978	20229	the frequency of each interactive request type (e.g., pause, jump forward, etc.), and the distribution of ON and OFF times in the session. Previous characterizations of interactive media sessions  have computed some of these statistics only for the aggregate of all sessions in the client workload that was analyzed, whereas we provide a summary of the variations in the statistics obtained for
189978	8700	whereas the time between interactive requests within eTeach sessions is more accurately modeled by the heavy-tailed Pareto distribution, as has been found in more traditional Web server workloads . ??? For each server, there are very few periods of stationary relative file access frequency. • Over periods of reasonably stable relative file access frequency, or over one-day periods, the file
189978	8700	exponential, gamma, Weibull, and Pareto distributions to determine the distribution of time between requests. Examples of the fitted curves are shown in Figure 5. Like other types of Web workloads , client session arrivals (overall and per file) in BIBS were found to be approximately Poisson, whereas the time between requests in eTeach has a heavy-tailed Pareto distribution in every case
20250	941	computing; thus multicast communication is likely to increase in importance as multicast routing protocols for ad hoc networks become more mature. To deal with such attacks, recently researchers  have proposed security extensions to existing routing protocols that include mechanisms for authenticating the routing control packets in the network. However, none of the proposed secure routing
20250	941	the multilevel key chain scheme proposed by Liu and Ning  to generate long key chains. 6. Related Work Our work is related to previous work on secure routing in ad hoc networks. Dahill et al  identify several security vulnerabilities in AODV and DSR, and proposed to use asymmetric cryptography for securing ad hoc routing protocols. Although their approach could provide strong security,
20250	933	computing; thus multicast communication is likely to increase in importance as multicast routing protocols for ad hoc networks become more mature. To deal with such attacks, recently researchers  have proposed security extensions to existing routing protocols that include mechanisms for authenticating the routing control packets in the network. However, none of the proposed secure routing
20250	933	a routing discovery protocol that assumes a security association (SA) between a source and a destination, whereas the intermediate nodes are not authenticated. Hu, Perrig and Johnson designed SEAD  which uses one-way hash chains for securing DSDV, and Ariadne  which uses TESLA and HMAC for securing DSR. In LHAP, we also utilize these techniques due to their efficiency. The main difference
20250	932	computing; thus multicast communication is likely to increase in importance as multicast routing protocols for ad hoc networks become more mature. To deal with such attacks, recently researchers  have proposed security extensions to existing routing protocols that include mechanisms for authenticating the routing control packets in the network. However, none of the proposed secure routing
20250	932	delivery ratio is mainly affected by the TESLA interval, traffic rate and node mobility model. Our detailed simulation using the Network Simulator (ns2)  and similar parameter settings as in , shows that in most cases the traffic delivery ratio in LHAP is greater than 99.9%. Moreover, we note that one or several nodes occasionally dropping broadcast packets has very little effect in the
20250	932	(SA) between a source and a destination, whereas the intermediate nodes are not authenticated. Hu, Perrig and Johnson designed SEAD  which uses one-way hash chains for securing DSDV, and Ariadne  which uses TESLA and HMAC for securing DSR. In LHAP, we also utilize these techniques due to their efficiency. The main difference between LHAP and their protocols is in the design goals. Their
20250	20251	using any TESLA keys that have not been released yet by node A due to the one-wayness of hash functions. Collaborative Outsider Attack A collaborative outsider attack (also called a wormhole attack ) is launched by multiple colluding outside attackers. In Fig. 2, the attackers P1 and P2 have a private channel that allows them to communicate directly. P1 forwards every message it eavesdropped
20250	935	another node E, a node A might detect the attack if they both have met a third node P at about the same time but at quite different locations (if GPS is equipped). Zhang and Lee , Marti et al.  have studied the intrusion and misbehavior detection issue in mobile networks. Finally, after detecting the attacks and identify the compromised nodes, all the remaining nodes add the compromised
20250	20253	such as digital signatures are relatively expensive to compute. Finally, we assume loose time synchronization in the ad hoc network since LHAP utilizes the TESLA broadcast authentication protocol . (In the discussion below, we assume that reader is familiar with TESLA .) 2.2. Notation We use the following notation to describe security protocols and cryptography operations in this paper: •
20250	20254	and computation. Perrig et al.  use symmetric primitives for securing routes between nodes and a trusted base station in a resource extremely constrained sensor network. Papadimitratos and Hass  propose a routing discovery protocol that assumes a security association (SA) between a source and a destination, whereas the intermediate nodes are not authenticated. Hu, Perrig and Johnson
20250	20255	their approach could provide strong security, performing a digital signature on every routing control packet could lead to performance bottleneck on both bandwidth and computation. Perrig et al.  use symmetric primitives for securing routes between nodes and a trusted base station in a resource extremely constrained sensor network. Papadimitratos and Hass  propose a routing discovery
20293	20293	zip codes) at a total cost of 12,000 seconds, or over three-and-a-half hours. Using our simple scheme, we need only 191 queries at a total cost of 229 seconds. This is over 52 times faster (see  for more complete experimental results). 3. ACQUIRING WEB DATA A critical requirement to reconstruct database images is to minimize the number of queries required to retrieve the data. In general,
20293	20293	queries is directly related to how data is accessed and represented, finding alternative means to access the data, or different representations, may simplify the problem. We refer the reader to  for more details on these issues. It is worth pointing out that the problem of reconstructing database images through restricted interfaces is very broad, and in this paper we propose a solution to
20293	20294	accessing a particular record within a big file can be rather cumbersome. The alternative of giving direct access to the databases through expressive query languages such as SQL  or XML-QL  is not practical, as these languages are too complex for casual Web users. Form interfaces are thus a good choice as they provide a very simple way to query (and filter) data. Simplicity however,
20293	19649	sense that it is placed behind form interfaces, and published on demand in response to users’ requests. It is estimated that 80% of all the data in the Web can only be accessed via form interfaces . There are many reasons for providing such interfaces on the Web. Transferring a big file with all the information in a database can unnecessarily overload Web servers (especially if users are
20293	20295	query interfaces readily available on the Web. The problem of querying Web sites through restrictive query interfaces has been studied in the context of information mediation systems (see e.g., ). However, to the best of our knowledge, the problem of generating efficient query covers that retrieve complete images of the databases through restricted query interfaces has not been considered
20313	20317	regions with specific properties of interest. One class of algorithms seeks to adaptively model the background “clutter,” and detect targets as deviations from the model predictions (e.g., , , , , , ). Another class of algorithms uses prior knowledge to construct a target signature, and compares the image under test to the signature (e.g., , , , , , ).
20313	20324	(e.g., , , , , , ). Another class of algorithms uses prior knowledge to construct a target signature, and compares the image under test to the signature (e.g., , , , , , ). The latter comparison may be accomplished in a variety of ways, ranging from optimized neural networks (e.g., , ), to statistically optimal likelihood functions (e.g.,
20349	20353	the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank
20360	1977	practices were strong determinants of child nutritional status in this population, particularly among children from poorer families and children whose mothers had less than secondary schooling (Ruel et al. 1999). A crucial question then, addressed here, is what are the main constraints to optimal child-cares3 practices in this population. Answers to this question will aid the design of interventions so
20360	1977	constant, maternal education is strongly and positively associated with better child-care practices. Good care practices, in turn, have a large positive effect on children’s nutritional status (Ruel et al. 1999), particularly among children from poorer families and among children whose mothers have less than secondary schooling. Thus, in this population, maternal education does act largely through
20360	1977	practices. The fact that good care practices did not make any difference among children of mothers with secondary schooling or higher, however, indicates that other mechanisms are also involved (Ruel et al. 1999). Household income did not appear to be a major constraint to good child-care practices among the age group studied (0-36 months), which is not surprising,s18 considering that at least up to 6
20360	1977	refuse to eat. These non-optimal feeding practices, when combined into a care index, were found to be strong determinants of poor nutritional status in this sample of children 4-36 months of age (Ruel et al. 1999). The prevalence of stunting among children whose mothers were in the lowest care practices tercile was more than three times higher than among children whose mothers were in the highest care
20360	20380	was particularly good, such as using fortified cereals for complementary feeding, a score of 1 was given. Practices were considered good or bad based on current child feeding recommendations (WHO 1995) and on available scientific evidence about their benefits or risks. For example, breast-feeding between 4 and 9 months of age was given a score of 1 because it is a recommended practice and it is
194608	19741	lattice (p = 0) into a random graph (p = 1), without altering the average number of neighbours equal to k =2K/N. The behavior of L and C in the two limiting cases can be estimated analytically . For the regular lattice (p =0),we expect L ? N/2k and a relatively high clustering coefficient C =/. For the random graph (p = 1), we expect L ? ln N/ ln(k ? 1) and C ? k/N. It
194608	21665	=55nodes,K = 413 and K = 564 connections. Global efficiency, local efficiency and cost are reported in the first two lines of the table. The results are compared to the efficiency of random graphs . The nervous system of C. elegans is better described by a weighted network: the network consists of N = 282 nodes and K = 2462 edges which can be of two different kind, either synaptic connections
194608	21665	loc Cost Macaque 0.52 0.57 0.70 0.35 0.18 Cat 0.69 0.69 0.83 0.67 0.38 C. elegans 0.46 0.48 0.47 0.12 0.06 Weighted: Eglob Eloc Cost C. elegans 0.35 0.34 0.18 while Eloc is larger than Erandom loc . These results indicate that in neural cortex each region is intermingled with the others and has grown following a perfect balance between cost, local necessities (fault tolerance) and widescope
194608	21665	stations). And now, to decide if the MBTA is a small world we have to compare the obtained L to the respective values for a random graph with the same N and K. But, when we consider random graphs  we get disconnected graphs and L = ?. So, we are unable to draw any conclusion. On the other side, the same unweighted network can be perfectly studied by using the efficiency formalism of Section
4041	4044	is not necessarily calibrated, e.g., the Naive Bayes classifier.Procedures for calibrating classifiers have been proposed in different contexts: In weather prediction tasks , in game theory , and more recently in the context of pattern classification . Zadrozny and Elkan were also the first to notice the need of calibrating classifiers when used as decision making aids. Our own
4041	4045	for calibrating classifiers have been proposed in different contexts: In weather prediction tasks , in game theory , and more recently in the context of pattern classification . Zadrozny and Elkan were also the first to notice the need of calibrating classifiers when used as decision making aids. Our own incentive to study calibration came from applying probabilistic
4041	4045	used for scheduling purposes, we also need to accompany each forecast withsan accurate estimate of the probability of the forecast. We applied a variant of the calibration procedure suggested in  and noticed that in addition to producing more accurate estimates, the classification accuracy of our induced classifiers increased. While these empirical results agree with those of Zadrozny and
4041	4045	process of calibrating may be seen as the process of bringing ˆp(C|X) closer to the real density. Calibrating a classifier is a mapping from ˆp(c|x) to p(c|t). In fact the procedures proposed in  essentially implement this mapping. Thus, under certain conditions we outline below the optimal threshold ? ? of the original classifier is one where in the calibration mapping p(c|? ? ) = 0.5. To
4041	4045	on t for which p(C = 1|t) = c10 c10+c01 .s5 Calibration with finite data With finite data sets, we want to estimate p(C = 1|ˆp(C = 1|x)) reliably. A procedure for this estimation was provided in , where ˆp(C = 1|x) is binned on the interval  and the calibration map is estimated by counting the number of samples that fall into each bin. The procedure was originally suggested as a
4041	4046	for calibrating classifiers have been proposed in different contexts: In weather prediction tasks , in game theory , and more recently in the context of pattern classification . Zadrozny and Elkan were also the first to notice the need of calibrating classifiers when used as decision making aids. Our own incentive to study calibration came from applying probabilistic
4041	4046	noticed that in addition to producing more accurate estimates, the classification accuracy of our induced classifiers increased. While these empirical results agree with those of Zadrozny and Elkan , a theoretical guarantee that calibration cannot degrade classification performance was still missing. Our investigation of the calibration produced the following results which we prove in Sections
4041	4046	process of calibrating may be seen as the process of bringing ˆp(C|X) closer to the real density. Calibrating a classifier is a mapping from ˆp(c|x) to p(c|t). In fact the procedures proposed in  essentially implement this mapping. Thus, under certain conditions we outline below the optimal threshold ? ? of the original classifier is one where in the calibration mapping p(c|? ? ) = 0.5. To
4041	4046	on t for which p(C = 1|t) = c10 c10+c01 .s5 Calibration with finite data With finite data sets, we want to estimate p(C = 1|ˆp(C = 1|x)) reliably. A procedure for this estimation was provided in , where ˆp(C = 1|x) is binned on the interval  and the calibration map is estimated by counting the number of samples that fall into each bin. The procedure was originally suggested as a
4041	4046	with small training sets. Reducing the possibility of overfitting can be done by smoothing of the calibration map or estimating a smooth function (such as the sigmoid) as the calibration map . As a note, the number of thresholds on the decision function would depend on the smoothing function used, e.g., with a sigmoid, only one 10 5sthreshold can be found, which might not be always
4041	4047	at which the forecaster predicted C = 1 with probability t on a set of N samples, with N ? ?. As such, given the probability density of the features, p(x), ?(t) can be expressed as: ?(t) = ? p(x) . (6) x?Rt Let p(C = 1|t) be the probability that C = 1 given that the forecaster predicts C = 1 with probability t. The Brier score can be rewritten as (see  for derivation): BS = ? ?(t)(t ? p(c|t))
4041	4048	minimize the classification error. We show that when a single threshold is derived from the calibration procedure, the result is equivalent to finding the point of minimum error in an ROC curve . However, when calibration produces multiple thresholds on the decision rule, the error achieved with those is lower than that of any single threshold derived from the ROC based methods. Thus, in
4041	4048	= 1) thus by varying the threshold ?, we can generate the entire ROC curve using the calibration map. At this point it is clear that methods that find the threshold of minimum error from ROC curves  produce the exact same result as the calibration procedure, when the calibration map does not cross 1/2 more than once. However, the calibration procedure generalizes more than what can be achieved
4041	4048	especially with small sample sizes. Extending the method beyond binary classification problems is another research direction; similar to methods extending ROC curves beyond binary classification . We are also exploring the use of calibration in semi-supervised learning, helping eliminate the possibility of performance degradation when using unlabeled data to learning classifiers, a
4041	4051	calibrating Naive Bayes classifiers, but is applicable to any classifier that outputs probabilities, or a distance measure that can be converted to probabilities (e.g., Tree-augmented Naive Bayes , Logistic regression, mixture models, and SVMs). The empirical success of calibration on various (typically large sized data sets) has been shown in previous works – in this section we aim at
4041	4052	the possibility of performance degradation when using unlabeled data to learning classifiers, a phenomenon that occurs with biased models that output uncalibrated a-posteriori probabilities . Acknowledgments We thank Terence Kelly both for his help and suggestions and his work on the I/O response time prediction. We also thank Kim Keeton for providing the I/O data, Tom Fawcett for his
7208081	20396	corpus” (Leech 1992) for the development of the translation lexicon and the RBMT system. To some extent, some of this linguistic information can be extracted semi-automatically (see for example Brent 1993; Smadja 1993; Melamed 2000; Véronis 2000). 3.3 Dialogue model An important element in Dialogue MT (Boitet 1993, 1999) is a model of the dialogue. The dialogue model can be used for the translation
7208081	20397	get relevant information without even making an appointment with the GP. There has been a considerable amount of relevant work in this area, notably on Tailored Patient Information (TPI) systems (Buchanan et al. 1995; Cawsey et al. 1995; Reiter and Osman 1997). Navigation of the help facility can be system-led or patient-led. In the latter case it would work in much the same way as the help facility in, say, a
7208081	20401	a less polished translation will normally fall on the doctor, who will gain experience of the system with use, and – on the evidence of early users of less sophisticated MT systems (cf. Church and Hovy 1993) – will quickly get used to its quirky style. The notion of “restricted input” relates to the widely accepted notion of “sublanguage”-based approaches to MT (Kittredge and Lehrberger 1982),
7208081	21679	whose only “handicap” is lack of a shared language. 4 Langer and Hickey (1999) report on growing contacts between the AAC and NLP research communities, as evidenced by dedicated workshops (e.g. Copestake et al. 1997) and journal editions (Langer 1998). The work of Grisedale et al. (1997) is also of interest: they developed a GUI for healthcare workers in rural India, like us facing the problems of
7208081	20402	input” relates to the widely accepted notion of “sublanguage”-based approaches to MT (Kittredge and Lehrberger 1982), especially inasmuch as a corpus can help to define the sublanguage (cf. Deville and Herbigniaux 1995; McEnery and Wilson 1996:147ff; Sekine 1997). The experience of the DIPLOMAT project (Frederking et al. 1997) is especially relevant to this project, since their system was developed specifically
7208081	20404	effective is an architecture which tries various strategies in parallel and then tries to reconcile the results. This is the “multi-engine” approach seen in the PANGLOSS and DIPLOMAT systems (Frederking et al. 1994, 1997). The engines that our system will use will be an EBMT/TM system (Somers 1999), a rule-based transfer system (Trujillo 1999:121ff), and a simple lexical look-up system; it is to be expected
7208081	20405	1982), especially inasmuch as a corpus can help to define the sublanguage (cf. Deville and Herbigniaux 1995; McEnery and Wilson 1996:147ff; Sekine 1997). The experience of the DIPLOMAT project (Frederking et al. 1997) is especially relevant to this project, since their system was developed specifically with rapid development of new language pairs for use in a dialogue situation between an experienced user and a
7208081	20407	script- or frame-based interfaces have been reported, for example the UNICORN system (Dye et al. 1997; Iwabuchi et al. 2000), which is specifically aimed at multilingual communication, DRAFTER (Hartley and Paris 1997), for multilingual document preparation, Floorgrabber (Alm and Arnott 1998) and Frametalker (Higginbotham et al. 2000) for users with communication difficulties. The “intelligence” derives from
7208081	20425	in such situations. For most of our purposes, what is important is not so much the verbatim transcripts, but the model of the discourse and the examples of the kinds of things that are said (cf. Passonneau and Litman 1997; Berthelin et al. 1999). This being the case, the utterances in the corpus can legitimately be “cleaned up”. The corpus will be marked up, especially for dialogue function in a TEI-conformant
7208081	20428	an appointment with the GP. There has been a considerable amount of relevant work in this area, notably on Tailored Patient Information (TPI) systems (Buchanan et al. 1995; Cawsey et al. 1995; Reiter and Osman 1997). Navigation of the help facility can be system-led or patient-led. In the latter case it would work in much the same way as the help facility in, say, a word-processor offers “Type in your query
7208081	20429	approaches to MT (Kittredge and Lehrberger 1982), especially inasmuch as a corpus can help to define the sublanguage (cf. Deville and Herbigniaux 1995; McEnery and Wilson 1996:147ff; Sekine 1997). The experience of the DIPLOMAT project (Frederking et al. 1997) is especially relevant to this project, since their system was developed specifically with rapid development of new language pairs
7208081	9651598	and to inform target-word selection in RBMT and word-by-word translation (the correct translation may depend on the context, i.e. the discourse function of the utterance, cf. Somers et al. 1990; Somers and Jones 1992). The model also plays a role in the interface, simplifying and determining the options offered in the menu-driven mode for both doctor and patient (cf. Alm et al. 1989). 3.4 The doctor’s interface
7208081	20433	Somers et al. 1994), and to inform target-word selection in RBMT and word-by-word translation (the correct translation may depend on the context, i.e. the discourse function of the utterance, cf. Somers et al. 1990; Somers and Jones 1992). The model also plays a role in the interface, simplifying and determining the options offered in the menu-driven mode for both doctor and patient (cf. Alm et al. 1989). 3.4
8921732	20452	new wireless LANs and mobile networks. In addition to supporting traditional web applications, these networks are emerging as important Internet video access systems. Meanwhile, both the Internet  and wireless networks are evolving to higher bitrate platforms with even larger amount of possible variations in bandwidth and other Quality-of-Services (QoS) parameters. For example, IEEE
8921732	20453	wireless LANs and mobile networks. In addition to supporting traditional web applications, these networks are emerging as important Internet video access systems. Meanwhile, both the Internet  and wireless networks are evolving to higher bitrate platforms with even larger amount of possible variations in bandwidth and other Quality-of-Services (QoS) parameters. For example, IEEE
8921732	20454	LANs and mobile networks. In addition to supporting traditional web applications, these networks are emerging as important Internet video access systems. Meanwhile, both the Internet  and wireless networks are evolving to higher bitrate platforms with even larger amount of possible variations in bandwidth and other Quality-of-Services (QoS) parameters. For example, IEEE 802.11a
8921732	5351	scalable video compression methods have been proposed and used extensively in addressing the bandwidth variation and heterogeneity aspects of the Internet and wireless networks (e.g., -, ). Examples of these include receiver-driven multicast multilayer coding, MPEG-4 Fine-Granularity-Scalability (FGS) compression, and H.263 based scalable methods. These and other similar approaches
8921732	20457	optimum rate-distortion (RD) approach for the transcaling of a wide range of video sequences is under way. This RD-based analysis, which is based on recent RD models for compressed scalable video , will provide robust estimation for the level of quality improvements that TS can provide for a given video sequence. Consequently, an RD-based analysis will provide an in-depth (or at least an
8921732	20458	for combining TS with other scalable video coding schemes such as 3D motion-compensated wavelets. Furthermore, TS in the context of cross-layer design of wireless networks is being evaluated . ? Optimum networked TS that tradeoffs complexity and quality in a distributed manner over a network of proxy video servers. Some aspects of this analysis include distortion-complexity models
8921732	20459	for combining TS with other scalable video coding schemes such as 3D motion-compensated wavelets. Furthermore, TS in the context of cross-layer design of wireless networks is being evaluated . ? Optimum networked TS that tradeoffs complexity and quality in a distributed manner over a network of proxy video servers. Some aspects of this analysis include distortion-complexity models for
8921732	21742	aspects of a networked TS framework will be investigated in the context of new and emerging paradigms such as overlay networks and video communications using path diversity (see for example ). 24sAcknowledgments Scalable Video TranScaling for the Wireless Internet Camera Ready Version of MIPWN.115 to JASP, July 10, 2003 The authors would like to thank three anonymous
9406394	20462	to measure that thoroughness. There are several metrics that can be used, such as path coverage, branch coverage, statement coverage , pie analysis , impact analysis  or code coverage  to name a few. In the case of embedded systems its validation is hard because of their heterogeneity. Software and hardware should be simulated simultaneously, and furthermore hardware and software
9406394	20462	that allow the program to execute the path. The process of specifying the execution path is done manually. In the future we plan to do this automatically by specifying the desired coverage level . 3.4 Obtaining the input test vectors With the path specification and all the lp constraints for the input program, we get the lp constraints for that specific path. Those lp conditions that we
9406394	20464	where the test cases are sought by formulating an optimization problem. Our work is motivated by recent work on functional vector generation for an hardware description language (hdl) . In this work an algorithm is presented that is an integration of linear programming (lp) techniques and 3-satisfiability (3SAT) checking . Given a path in the hdl model, an input stimuli that
9406394	20464	module in that path. In the case of software path sensitization it corresponds to sensitize every single statement in that path. The algorithm to circuit path sensitization proposed by Fallah et al  is an hybrid algorithm for satisfiability checking that seamlessly integrates lp feasibility and 3-satisfiability checking. This integration is necessary due to the correlation between word-level
9406394	20465	on the type of metric to measure that thoroughness. There are several metrics that can be used, such as path coverage, branch coverage, statement coverage , pie analysis , impact analysis  or code coverage  to name a few. In the case of embedded systems its validation is hard because of their heterogeneity. Software and hardware should be simulated simultaneously, and furthermore
9406394	20466	generation for an hardware description language (hdl) . In this work an algorithm is presented that is an integration of linear programming (lp) techniques and 3-satisfiability (3SAT) checking . Given a path in the hdl model, an input stimuli that exercises that path is obtained. In our work, we develop a method for obtaining the inputs that allow an embedded software program written in a
9406394	20467	hardware and software simulations must be kept synchronized, so that they behave as close as possible to the physical implementation. Several methods have been proposed for co-simulation .sy=a yes INPUT a,b x=a-b x>0 OUTPUT y no y=b Figure 1: Example flowgraph for the max function. Integrating our method with a vector generator for hardware will give us a complete vector generation
9406394	20471	hardware and software simulations must be kept synchronized, so that they behave as close as possible to the physical implementation. Several methods have been proposed for co-simulation .sy=a yes INPUT a,b x=a-b x>0 OUTPUT y no y=b Figure 1: Example flowgraph for the max function. Integrating our method with a vector generator for hardware will give us a complete vector generation
9406394	17817	by the designer and on the type of metric to measure that thoroughness. There are several metrics that can be used, such as path coverage, branch coverage, statement coverage , pie analysis , impact analysis  or code coverage  to name a few. In the case of embedded systems its validation is hard because of their heterogeneity. Software and hardware should be simulated
20476	20477	can be selected by the user at run-time. Once the iso-surface has been selected, we can use point-based surface rendering instead of full volume splatting. We used the method described in  for creating a point-based surface model from a volumetric iso-surface. Different from most of the previous work, our emphasis is to generate images that provide strong motion hints for the viewer
20476	20482	applied to any geometric model. Post-processing techniques only address the 2D problem (extended by  for 2.5D problems) and are capable of creating high quality motion blurred images.  requires a per-pixel correlation and requires significant computation time.  proposes an approach of producing motion blur images by time convolution of the normal image with the motion
20476	20483	over time. Following , we can categorize existing motion blur techniques on polygonal models based on the different terms in Equation (1). Monte-Carlo based methods   use statistical super-sampling to approximate the entire integral of Equation (1).  used hardware to accelerate this method with multi-pass z-buffer scan conversions. Super-sampling is
20476	20485	models based on the different terms in Equation (1). Monte-Carlo based methods   use statistical super-sampling to approximate the entire integral of Equation (1).  used hardware to accelerate this method with multi-pass z-buffer scan conversions. Super-sampling is inevitably slow since it needs to process more samples, and also, the number of samples required
20476	20489	volume rendering. For volume rendering, the geometric visibility problem persists when semitransparent or opaque compositing is used. However, for X-ray-like or emission volume models, Max  and Crawfis et al.  have shown that the compositing order is immaterial. Mueller et al.’s method simplified the problem and did not address the visibility problem, but only the integrated
20476	20495	I.3.3 : Display algorithms, I.3.7 : Three-Dimensional Graphics and Realism: Color, shading, shadowing, and texture. 1. Introduction Point-based rendering  has gained much popularity in recent years. One reason for this is the simplicity of rendering point-based primitives. Another reason is that, given the high level of detail of many
20476	20495	representing the point samples as extended kernel functions,sXin Guan & Klaus Mueller / Point-based Surface Rendering with Motion Blur such as Gaussians, tents, or squares (see, for example, . Our paper extends anti-aliasing by ways of extending kernels into the temporal domain. This gives rise to a variety of motion-blur effects, which are able to give suggestive hints of the
20476	20495	the latter technique does not offer the same image quality as the former one, as shown in Figure 2, and we choose to use the normal-oriented splats. Finally, in contrast to the 2-pass approaches of  which use a z-buffer method in conjunction with blending to determine occlusion, we first bucket-sort the points with respect to the viewpoint and then render them in front-to-back
20476	20495	motion displayed as streak lines. Second row: Left: with rotational motion. Right: motion blur trail only. Third row: Left: rotational motion with color-coded trails. Right: translational motion.  could provide help in this effect. Here, the intensity of the motion splat, and even its width, could be determined by the z-buffer pixel ID’s. Hidden splats would not produce a motion splat at
20476	20497	CCS): I.3.3 : Display algorithms, I.3.7 : Three-Dimensional Graphics and Realism: Color, shading, shadowing, and texture. 1. Introduction Point-based rendering  has gained much popularity in recent years. One reason for this is the simplicity of rendering point-based primitives. Another reason is that, given the high level of detail of
20476	20497	by representing the point samples as extended kernel functions,sXin Guan & Klaus Mueller / Point-based Surface Rendering with Motion Blur such as Gaussians, tents, or squares (see, for example, . Our paper extends anti-aliasing by ways of extending kernels into the temporal domain. This gives rise to a variety of motion-blur effects, which are able to give suggestive hints
20476	20500	splats at both ends. This idea is similar to  by creating a motion volume for each geometric elements. 2.3. Post-processing Techniques Post-processing techniques    operate on the rendered images, and can be applied to any geometric model. Post-processing techniques only address the 2D problem (extended by  for 2.5D problems) and are capable of
20476	20501	systems,  renders the particle points as line segments along their motion trail to model fuzzy objects.  solves the visible surface problem at each screen pixel independently, while  creates a transparent motion volume to approximate the motion blur of constantcolored opaque objects.  addresses the shading problem by proposing an object space method that interpolates
20476	20501	the time domain. The elongated Gaussian splat is constructed as a rectangle that spans the motion vector of a voxel, with two half-spherical Gaussian splats at both ends. This idea is similar to  by creating a motion volume for each geometric elements. 2.3. Post-processing Techniques Post-processing techniques    operate on the rendered images, and can be
20476	20502	Graphics]: Display algorithms, I.3.7 : Three-Dimensional Graphics and Realism: Color, shading, shadowing, and texture. 1. Introduction Point-based rendering  has gained much popularity in recent years. One reason for this is the simplicity of rendering point-based primitives. Another reason is that, given the high level of detail of many geometric
20476	20502	the point samples as extended kernel functions,sXin Guan & Klaus Mueller / Point-based Surface Rendering with Motion Blur such as Gaussians, tents, or squares (see, for example, . Our paper extends anti-aliasing by ways of extending kernels into the temporal domain. This gives rise to a variety of motion-blur effects, which are able to give suggestive hints of the object in
20476	20502	Photographer’s techniques to generate motion blur images while maintaining the clear shape of objects and the hint of motion direction. choose to use a simplified version of EWA surface splatting . Instead of going through the expensive computation of the elliptical Gaussian texture mapping, we use one 2-dimensional round Gaussian splat that is perpendicular to the normal of this point. We
20476	20502	technique does not offer the same image quality as the former one, as shown in Figure 2, and we choose to use the normal-oriented splats. Finally, in contrast to the 2-pass approaches of  which use a z-buffer method in conjunction with blending to determine occlusion, we first bucket-sort the points with respect to the viewpoint and then render them in front-to-back order.
20476	20502	ellipsoid I3 into the motion ellipsoid, a 3 × 3 variancecovariance (VC) matrix: Mellipsoid = T × I3 × T ? . The screen-space ellipse can be obtained by dropping the ellipsoid’s last row and column . The eigenvalues and eigenvectors of this 2D matrix determine the orientation and stretch of the polygon onto which the Gaussian footprint is mapped for projection as follows. Given a general
20503	2050	In a componentbased approach using a design-by-contract methodology, the following considerations make mutation analysis useful for several analysis levels: - in a design-by-contract approach , components integrate “contracts” that are systematically derived from the specification. Contracts behave as executable assertions that automatically check the components consistency
20503	17817	The approach presented in this paper aims at providing a consistent framework for building trust into components. By measuring the quality of test cases (the revealing power of the test cases ) we seek to build trust in a component passing those test cases. The analysis also shows that a design-by-contract approach associated to the notion of embedded selftest significantly improves the
29595978	2733	(seconds) Fig. 3. Average MAC layer drops 100-nodes, 30-flow V. SIMULATIONS We present simulation results of uni-path SRP done in Case IV: Line 10. By the precondition of this case that GloMoSim . We compare the performance to AODV, DSR, trivially satisfies Eqs. 3 – 4. LDR, and OLSR . Like other ad hoc routing protocols, SRP uses several heuristics to improve performance in simulated
29595978	20512	and J.J. Garcia-Luna-Aceves Computer Engineering Dept. Baskin School of Engineering University of California Santa Cruz, CA 95064 ?mmosko, jj?@soe.ucsc.edu a digraph. These protocols include GB , LMR  , and TORA . GB and LMR operate by reversing the direction of certain links at each iteration of the algorithm. This is realized by associating an ordered pair «??? to each node ? and
29595978	20520	Computer Engineering Dept. Baskin School of Engineering University of California Santa Cruz, CA 95064 ?mmosko, jj?@soe.ucsc.edu a digraph. These protocols include GB , LMR  , and TORA . GB and LMR operate by reversing the direction of certain links at each iteration of the algorithm. This is realized by associating an ordered pair «??? to each node ? and defining a lexicographic
29595978	2734	true ordering is Ô?Õ, it sets the RREQ ordering to Ôs? Õs.IfÔ? , the node sets the RREQ ordering to Ô£?s? Õ£?s, where we used ? ? in simulation. Our simulation parameters generally follow those in . We use an 802.11 MAC layer on a 2200m x 600m terrain with 100-nodes and 30 CBR traffic flows. This is the highest traffic rate modelled in . Each CBR packet is 512 bytes and the flows send
29595978	20522	to the requesting node. Thus, the requesting node has the complete path. A third class of on-demand loop-free protocols operate by maintaining node labels in a topological order. AODV , ROAM , and LDR  use such a technique. AODV maintains a sequence number and hop-count per destination at each node. AODV’s use of sequence numbers is such that when a node looses its successor to a
20523	20524	does not use the H.263 variable-length code method for compressing motion vectors. Instead, the difference-coded vectors are packed and compressed with BZIP2, based on the Burrows-Wheeler transform . The residual undergoes 8 × 8 DCT. Only the first frame is intra-coded, and is transformed and coded exactly like residuals. GTV diverges from H.263 mostly in that it replaces the quantization and
20523	21810	on spatial and lumachroma neighbors, as well as sub-band level, with a cross-frame adaptivity parameter ? =0.5. H.263 results were generated with the public-domain Telenor H.263 TMN5 implementation . It is evident that GTV does not do as well as H.263 at low bit-rates, i.e. 100 and 200kbps. However, GTV becomes competitive around 300kbps, and at higher bit-rates it outperforms H.263 on most
20523	20525	Proceedings of the Data Compression Conference (DCC’04) 1068-0314/04 $ 20.00 © 2004 IEEE 1sOne BPC algorithm, for encoding wavelet-transformed images, was presented by Hong and Ladner in , and later adapted to handle block-transforms, including DCT . This algorithm, GT-DCT, is similar to JPEG, but replaces the scalar quantization and entropy coding stages, with bit-plane coding
20523	20525	Golomb code , and almost achieves the entropy bound for i.i.d. sources. The key parameter for the algorithm is the group size (number of bits) tested at each iteration. Hong and Ladner  applied context-based group testing to the significance pass of the bit-plane coding paradigm. Insignificant coefficients are divided into classes, or contexts, and each class is coded separately
20523	20530	published since the late 1990’s. Most of those are based on zerotree methods, applied to either motioncompensated residuals (e.g. ) or to 3D (time/space) wavelet transform coefficients (e.g. ). Context-based coding is applied to 2D and 3D wavelet coefficients by several authors, e.g. , following the ideas introduced in EBCOT. 3 The Basic GTV Algorithm GTV is a hybrid video
20523	20531	by Taubman in EBCOT , and later served as the basis for the JPEG-2000 image compression standard. Some work has been applied to block-transform compression as well, e.g.  for DCT, and  for lapped transforms. Both are based on zerotrees, although the hierarchical structure is defined somewhat arbitrarily. BPC for video: Several BPC-based video coders were published since the late
20523	20532	by Taubman in EBCOT , and later served as the basis for the JPEG-2000 image compression standard. Some work has been applied to block-transform compression as well, e.g.  for DCT, and  for lapped transforms. Both are based on zerotrees, although the hierarchical structure is defined somewhat arbitrarily. BPC for video: Several BPC-based video coders were published since the late
20523	20533	and broadband streaming video (with appropriate error resilience additions). A discussion of how the rate scalability of GTV can be used to produce near-constant quality video can be found in . The rest of this paper is organized as follows: section 2 surveys related work on bit-plane coding and video compression; section 3 describes the baseline GTV coder in detail; section 4 describes
20523	20533	the same bit-rate to all inter-frames, and allows the user to specify how many bits to assign to the first, intra-coded frame. More elaborate bit-allocation algorithms are developed and tested in . However, those affect the variance of the PSNR rather than its average, so the results in section 5 remain mostly unchanged. 3.1 Bit-plane coding Scalar quantization methods code coefficients one
20523	20534	based methods to bit-plane coding ones (e.g. JPEG-2000). Introduced in a seminal paper by Shapiro in 1993 , bit-plane coding (BPC) has shown significant gains over scalar quantization methods . BPC coders are embedded, allowing precise bit-rate control and excellent rate scalability properties. One of the arguments against using bit-plane coders for video has been its relative slowness
20523	20534	and entropy coding stages, with bit-plane coding using context-based group testing. GT-DCT consistently and significantly outperforms JPEG, and is competitive with other BPC methods, such as SPIHT . Since video compression standards use a very JPEG-like algorithm for coding residual images, they might be improved by replacing this with an appropriate GT-DCT-like method. In this work, we begin
20523	20534	structure of wavelet coefficients facilitates efficient coding of coefficient trees, using a method called zerotree coding. EZW consistently outperformed JPEG. Said and Pearlman developed SPIHT , which is similar to EZW in concept, but takes advantage of further statistical structure in the wavelet transform. Since then, a number of algorithms in the same tack have been published. Another
20523	20535	arbitrarily. BPC for video: Several BPC-based video coders were published since the late 1990’s. Most of those are based on zerotree methods, applied to either motioncompensated residuals (e.g. ) or to 3D (time/space) wavelet transform coefficients (e.g. ). Context-based coding is applied to 2D and 3D wavelet coefficients by several authors, e.g. , following the ideas
20523	20536	in the wavelet transform. Since then, a number of algorithms in the same tack have been published. Another approach, using context-based arithmetic coding, was introduced by Taubman in EBCOT , and later served as the basis for the JPEG-2000 image compression standard. Some work has been applied to block-transform compression as well, e.g.  for DCT, and  for lapped
20523	20538	coding, was introduced by Taubman in EBCOT , and later served as the basis for the JPEG-2000 image compression standard. Some work has been applied to block-transform compression as well, e.g.  for DCT, and  for lapped transforms. Both are based on zerotrees, although the hierarchical structure is defined somewhat arbitrarily. BPC for video: Several BPC-based video coders were
20540	20541	all tasks are guaranteed to complete their execution before the given deadlines. The programming language of Kelb is highly influenced by the graphical input language used in the analysis tool Times developed at Uppsala University. To perform analysis of a Kelb program, the Kelb compiler derives (in addition to the binary code to be executed on AperiOS) a model of timed automata that can be
8921742	20543	the offset texture coordinates for Texture0, the previous frame of the video sequence (line 11). !!ARBvp1.0 .... 2 OUTPUT oTexture0 = result.texcoord; 3 OUTPUT oTexture1 = result.texcoord; 4 ATTRIB iPosition = vertex.position; 6 PARAM vector = program.local; .... 11 ADD oTexture0, iPosition, vector; 12 MOV oTexture1, iPosition; END The following section of code shows the fragment
8921742	20543	time for FBM at different pixel accuracies. !!ARBfp1.0 1 OUTPUT output = result.color; 2 TEMP frame1,frame2; 3 TEX frame1, fragment.texcoord, texture, RECT; 4 TEX frame2, fragment.texcoord, texture, RECT; 5 SUB frame1, frame1, frame2; 6 ABS output, frame1; END The results shown here were generated using a 1.6 GHz Pentium 4 machine with a GeForce FX 5600 GPU. In the FBM scheme used
8921742	20544	motion vectors there are even more, depending on the accuracy required. The matching criteria used here is the sum of absolute differences (SAD) SAD(b, d) = N?1 ? i=0 M?1 ? j=0 |DFD(b(i, j), d)|. (2) The function b maps the current block pixel position (i, j) to a pixel in the full image. Rearranging Equation 1 gives us the definition of the DFD at position x and displacement d DFD(x, d) =
20553	20561	in this case) units of flow, any of the standard flow routing algorithms can be used. For example, path augmentation based maximum flow algorithms from Ford & Fulkerson  or, Edmonds & Karp  can be used to route the flow. As our requirement is to route only F units of flow, these algorithms can be stopped after the sufficient flow is routed. In any given network, there may be various
20553	20562	we summarize the complexity of the algorithms. The assignment step of link capacities to equivalent SONET rate takes order of O(E). Minimum cost flow problems can be efficiently solved using .  has a running time of O((ElogV )(E + VlogV)) and is known to be among the fastest available algorithms. However, for our application a simpler algorithm such as successive shortest path
20553	20553	Also, FLCAS is designed such that a network element running it can interoperate with an element running standard LCAS with no additional overhead. Further details on the protocol is available in . FLCAS when used in conjunction with PESO delivers excellent protection speed of well under 50ms. Infact, it achieves this bound for most of the practically likely VCG sizes and failure scenarios.
20613	20614	propagation in a strictly prescribed manner. Interestingly, this is directly related to the notion of focusing proofs used to justify logic programming languages and efficient theorem provers . Bi-directional checking is robust in that it applies in many situations such as those involving polymorphism and subtyping, fully dependent types, dependent refinements, and datasort refinements.
20613	20618	static checking as a type system, however, we hope to attain increased portability across implementations and predictability to the programmer. Our goals are similar to those of soft typing , but our technical approach is rather different. As with soft typing, we insist on a sound logic, and we require that the programmer state assertions that are to be verified. In contrast to soft
20613	20619	the theory of various framework refinements in order to allow more immediate and natural encodings for a larger class of object languages and theories. These include subtyping , linearity , and definitional equality . A cornerstone for further techniques is a recent robust proof for properties of logical frameworks . Meta-Logical Frameworks. Most recently, we have been
20613	20623	intuitionistic, classical, and linear logics, the latter leading to the discovery of new algorithms . In another study we investigated properties of terms in continuation-passing style . In this instance we were able to prove an open conjecture with the help of Elf. Framework Refinements and Extensions. We designed and implemented a calculus for the modular presentation of
20613	20624	property associated with it. • The class of properties is rich enough to include both refinement types and dependent refinements. Initial work in this direction was carried out by Ewen Denney  and Susumu Hayashi  for purely functional languages. We propose to extend this framework to languages with computational effects in two, distinct senses. First, we propose to account for
20613	20625	Although in most cases the verification is sound, in the sense of affirming only correct operational behavior, in some instances soundness is sacrificed in favor of a more efficient checker . In our view this is an unfortunate compromise since it forces the programmer to consider the validity of error messages, and encourages a natural tendency to ignore a warning in the belief that
20613	20627	in two respects. First, in order to guarantee soundness in the presence of effects we eliminated the law of distributivity for subtyping and introduced a form of the so-called value restriction . Second, in order to work well at module boundaries we have required more programmer annotations in a system of bi-directional type checking, simultaneously making the system more efficient and
20613	20633	theories. These include subtyping , linearity , and definitional equality . A cornerstone for further techniques is a recent robust proof for properties of logical frameworks . Meta-Logical Frameworks. Most recently, we have been working toward the automation of meta-logical proofs, generalizing the previous paradigm of proof implementation and checking. This work is
20613	20636	local reasoning about the world. We propose to investigate how to integrate these methods into the framework of type refinements. The motivation here is similar to that of types-and-effects systems , but the technical development we propose is rather different. In particular, by maintaining a clear separation between properties and types we avoid the problem of always having to simultaneously
20613	20637	as kg m and kg m/s 2 . Flexible treatment of exponents requires a constraint domain similar to that required for dependent types. We propose to investigate the combination of dimensional analysis  with integral constraint domains to track dimensionality of expressions in a program. Resources. Sub-structural logics support local reasoning about ephemeral properties of the “world” [WCM00,
20613	20638	were biased towards inference. In contrast, our emphasis is on checking implementations against type specifications. Types and Extended Static Checking. Recently, extended static checking  has been proposed as a method for program property verification that is integrated into the development process. The goal of extended static checking is to use machine assistance to verify
20613	20639	previous extreme, we have exceedingly concise programs, but we pay for this with a very limited set of program properties that can be verified. And type inference, though theoretically intractable , is practically feasible in this case. In part this can be attributed to the fact that its complexity is pseudo-linear in the size of the largest required type . Similar results are likely
20613	20640	theoretically intractable , is practically feasible in this case. In part this can be attributed to the fact that its complexity is pseudo-linear in the size of the largest required type . Similar results are likely for some type refinements, although there also some negative results . Several recurring themes of type checking algorithms provide some theoretically motivated
20613	20641	of values. We expect to build on the work of Davies and Pfenning on modal type systems  as a foundation for this work, which in turn is a refined analysis of Moggi’s monadic metalanguage . By an ephemeral property we mean one that may hold at a given moment of execution, but may fail to hold at some later stage. For example, a given assignable variable may hold an even integer at a
20613	16025	to combine two decidable and practical refinement systems (datasort and dependent refinements), one can quickly slide into undecidable problems. General techniques for combining decision procedures  might help in some cases, but often they do not apply because the underlying theories interact too strongly. One possible solution is to erect boundaries in the very definition of the type
20613	16164	by insisting that the programmer write out a formal proof that the program satisfies its specification. With technology such as the LF logical framework  or oracle-based checking , such proofs can be checked efficiently in practice. However, the correctness proofs would outweigh the size of the program and would be much more difficult to construct, even if we make the
20613	20645	unwieldy. 9sWhat we seek, instead, are methods for specifying only the “local” behavior of a function, its effect on those aspects of the world that it actually uses. Recent work by Reynolds , O’Hearn  and Walker , among others, has stressed the importance of sub-structural logics to support such local reasoning about the world. We propose to investigate how to
20613	7106	For even in the presence such effects, it is important to be able to state and verify persistent properties of values. We expect to build on the work of Davies and Pfenning on modal type systems  as a foundation for this work, which in turn is a refined analysis of Moggi’s monadic metalanguage . By an ephemeral property we mean one that may hold at a given moment of execution, but
20613	20646	simultaneously making the system more efficient and modular, at a small cost in additional type annotations. Datasort refinements have been applied to functional languages and logical frameworks . As a first example, we consider two singleton refinements of the type color. The two constructors have the obvious properties. refine red <: color refine black <: color prop red : red prop black :
20613	20646	designed and analyzed the theory of various framework refinements in order to allow more immediate and natural encodings for a larger class of object languages and theories. These include subtyping , linearity , and definitional equality . A cornerstone for further techniques is a recent robust proof for properties of logical frameworks . Meta-Logical Frameworks. Most
20613	20619	this and related work on the LF logical framework and its implementation in Elf are accessible at http://www.cs.cmu.edu/~twelf/. A structured surveys of work on logical frameworks can be found in  and . This research is directly relevant to the proposed work, because the technology of logical frameworks relies heavily on dependent types, which also constitute a cornerstone of type
20613	20619	work on the LF logical framework and its implementation in Elf are accessible at http://www.cs.cmu.edu/~twelf/. A structured surveys of work on logical frameworks can be found in  and . This research is directly relevant to the proposed work, because the technology of logical frameworks relies heavily on dependent types, which also constitute a cornerstone of type refinements. A
20613	20649	and partially checked, including various properties of compiler correctness, type preservation, and cut elimination. We have also used a version of the system for experiments in proof-carrying code . Twelf is also in active use at Stanford and Princeton for experiments in proof-carrying code  and certifying decision procedures . 2.3 Staged Computation, F. Pfenning and P. Lee
20613	13101	the automation of meta-logical proofs, generalizing the previous paradigm of proof implementation and checking. This work is described in . The current implementation of the Twelf system  (the successor to Elf) can automatically prove a number of interesting theorems previously only implemented and partially checked, including various properties of compiler correctness, type
20613	13101	would also seem to apply and interact well with the class hierarchy. Evaluation. Evaluation proceeds in two ways. One is to take existing, large code bases such as the Twelf implementation , or the TIL compiler  that were developed here at Carnegie Mellon University and instrument them with refinement specifications that the current ML compiler will view as comments and our
20613	20650	of refinement properties in order to avoid excessively verbose annotations that might arise in more expressive refinement languages. This is related to a similar approach by Pierce and Turner  for polymorphism and subtyping. Even in datasort refinements such local inference might be exploited when there are additional internal properties of functions that are not exported. For example,
20613	6781	to the fact that its complexity is pseudo-linear in the size of the largest required type . Similar results are likely for some type refinements, although there also some negative results . Several recurring themes of type checking algorithms provide some theoretically motivated intermediate points between these extremes with many practical merits. One of the major items of proposed
20613	4395	system for experiments in proof-carrying code . Twelf is also in active use at Stanford and Princeton for experiments in proof-carrying code  and certifying decision procedures . 2.3 Staged Computation, F. Pfenning and P. Lee Frank Pfenning and Peter Lee were co-principal investigators on CCR-9619832, supported under the Software Engineering and Languages program at NSF,
20613	20651	and reduces complexity of checking (when compared to full inference). Constraint-Based Inference. A second recurring theme in type-checking is constraintbased inference (see, for example, ). If we can arrange that type refinement checking reduces to constraint satisfaction over some domain, we can interleave type-checking proper and incremental constraint simplification to obtain a
20613	20652	Frameworks. Most recently, we have been working toward the automation of meta-logical proofs, generalizing the previous paradigm of proof implementation and checking. This work is described in . The current implementation of the Twelf system  (the successor to Elf) can automatically prove a number of interesting theorems previously only implemented and partially checked, including
20613	20654	refinements in order to allow more immediate and natural encodings for a larger class of object languages and theories. These include subtyping , linearity , and definitional equality . A cornerstone for further techniques is a recent robust proof for properties of logical frameworks . Meta-Logical Frameworks. Most recently, we have been working toward the automation of
20613	20658	type system is robust in the sense that it applies to several languages. Practical dependent type systems for functional , imperative , object-oriented  and even assembly language  have by now been developed. To illustrate this, we can refine red/black trees further by their black height, that is, the number of black interior nodes on each path from a leaf to the root. refine
20613	7117	* ? bt -> ? bad & red * ? rt * ? entry * ? bt -> ? bad & red * ? bt * ? entry * ? rt -> ? bad The second major approach has been the design of dependent type systems over decidable index domains . Instead of finite lattices and algorithms on tree automata, type-checking here requires decision procedures for the index domain, such as Presburger arithmetic. The major applications have been
20613	7117	generated by type-checking. Fortunately, there are many useful domains where this is the case, such as the integers with equality and inequalites. This has been the basis for DML (Dependent ML) , one of the prior experiments with type refinement. Combining Decision Procedures. The interactions of bi-directional checking and constraint-based inference are non-trivial and have to be
20613	20659	checking  and similar verification tasks. Again the type system is robust in the sense that it applies to several languages. Practical dependent type systems for functional , imperative , object-oriented  and even assembly language  have by now been developed. To illustrate this, we can refine red/black trees further by their black height, that is, the number of black
20613	7118	algorithms on tree automata, type-checking here requires decision procedures for the index domain, such as Presburger arithmetic. The major applications have been for static array bounds checking  and similar verification tasks. Again the type system is robust in the sense that it applies to several languages. Practical dependent type systems for functional , imperative ,
20613	7117	array bounds checking  and similar verification tasks. Again the type system is robust in the sense that it applies to several languages. Practical dependent type systems for functional , imperative , object-oriented  and even assembly language  have by now been developed. To illustrate this, we can refine red/black trees further by their black height, that is,
20613	20660	verification tasks. Again the type system is robust in the sense that it applies to several languages. Practical dependent type systems for functional , imperative , object-oriented  and even assembly language  have by now been developed. To illustrate this, we can refine red/black trees further by their black height, that is, the number of black interior nodes on each
20613	20660	have reasonably large code bases for evaluation. We may also consider Java as a second testbed for versions of type refinements. Already, Xi has designed a version of dependent refinements for Java , and datasort refinements would also seem to apply and interact well with the class hierarchy. Evaluation. Evaluation proceeds in two ways. One is to take existing, large code bases such as the
20661	20662	router/link. Previous studies on TCP over hybrid wired/wireless links have primarily focused on improving the performance by hiding or reducing packet losses due to random transmission errors , . Among the main ideas along these lines, we would quote • the addition of mechanisms that allow TCP to identify and to ignore packet losses due to random transmission errors , , ;
20661	9830	router/link. Previous studies on TCP over hybrid wired/wireless links have primarily focused on improving the performance by hiding or reducing packet losses due to random transmission errors , . Among the main ideas along these lines, we would quote • the addition of mechanisms that allow TCP to identify and to ignore packet losses due to random transmission errors , , ; This
20661	20663	errors , . Among the main ideas along these lines, we would quote • the addition of mechanisms that allow TCP to identify and to ignore packet losses due to random transmission errors , , ; This work was funded in part by the Alcatel-INRIA OSC ’End-to-End Performance Evaluation of Packet Networks’. • the reduction of such packet losses obtained either by FEC (Forward Error
20661	20664	errors , . Among the main ideas along these lines, we would quote • the addition of mechanisms that allow TCP to identify and to ignore packet losses due to random transmission errors , , ; This work was funded in part by the Alcatel-INRIA OSC ’End-to-End Performance Evaluation of Packet Networks’. • the reduction of such packet losses obtained either by FEC (Forward Error
20661	11922	obtained either by FEC (Forward Error Correction), or by breaking the endto-end connection into two parts (one from the mobile to the base station and one from the base station to the destination) , . For all situations where TCP has to cope with some bit/packet error rate, no formula seems to be available for characterizing the interplay between these transmission error losses and losses
20661	20666	either by FEC (Forward Error Correction), or by breaking the endto-end connection into two parts (one from the mobile to the base station and one from the base station to the destination) , . For all situations where TCP has to cope with some bit/packet error rate, no formula seems to be available for characterizing the interplay between these transmission error losses and losses due
20661	2234	flow from the knowledge of the RTT, the capacity of the shared outer/link and random loss rate. This work builds upon the AIMD (Additive Increase, Multiplicative Decrease) model introduced in ,  for describing the interaction of TCP flows over wired links (, ). The main new feature of the present paper is the addition of random transmission errors. This new model, which will be
20661	2234	introduce the large population asymptotic model. B. Large Population Asymptotics The next theorem is the main structural result of the paper. It is in the continuation of results used in  and  and is related to the mean field method of statistical physics (for more on the matter, see the references in ). This theorem is based on conditions using certain subsets ?N of {1,...,N}. The
20661	2234	examples. This work can be extended to further cases such as heterogeneous TCP flows with different classes of random transmission errors and multiple tail-drop/AQM routers (see our previous papers ,  for heterogeneous TCP flows over multiple wired links and ,  for AQM routers.) 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004 NS NSsACKNOWLEDGMENT The authors would like to
20661	5723	error rate and the synchronization rate in the bottleneck router. We also show that the obtained throughput formula is actually a refinement to the classical square root formula for TCP throughput , . In §V, the finite population model and models based on other classes of point processes are also studied. A sufficient condition is obtained for the existence of congestion times in the case
20661	5723	B. Refinement of the Square Root Throughput Formula Let us show how the throughput formula obtained by this approach is actually a refinement to the classical throughput formulas for TCP throughput , . To this end, ? is replaced by 1 R . If there are no congestion epochs, i.e. if ? ? ?? ,wehave x = 2 R2? = ? 2 R ? , ploss (19) where ploss = R2 ? 2 2 . If ?<? ? , the mean number of packets
20661	5723	. (20) ? ? cp , (21) ?? where ? ?? + p f(c, p, R, ?) = 2?? ? cpR2? . We conclude from (19) and (21) that our framework leads to formulas compatible with the classical estimates for TCP throughput (, ). V. THE FINITE POPULATION MODEL AND MODELS BASED ON OTHER CLASSES OF POINT PROCESSES In this section, we come back to the general framework of §II. In particular, here, N is finite and the
20661	6533	rate and the synchronization rate in the bottleneck router. We also show that the obtained throughput formula is actually a refinement to the classical square root formula for TCP throughput , . In §V, the finite population model and models based on other classes of point processes are also studied. A sufficient condition is obtained for the existence of congestion times in the case of
20661	6533	of the Square Root Throughput Formula Let us show how the throughput formula obtained by this approach is actually a refinement to the classical throughput formulas for TCP throughput , . To this end, ? is replaced by 1 R . If there are no congestion epochs, i.e. if ? ? ?? ,wehave x = 2 R2? = ? 2 R ? , ploss (19) where ploss = R2 ? 2 2 . If ?<? ? , the mean number of packets sent
20661	6533	? ? cp , (21) ?? where ? ?? + p f(c, p, R, ?) = 2?? ? cpR2? . We conclude from (19) and (21) that our framework leads to formulas compatible with the classical estimates for TCP throughput (, ). V. THE FINITE POPULATION MODEL AND MODELS BASED ON OTHER CLASSES OF POINT PROCESSES In this section, we come back to the general framework of §II. In particular, here, N is finite and the
20661	20667	The derivation of (5) is given in §VIII-A in the appendix. For a validation of this kind of fluid dynamics and its comparison with NS based simulation, the interested reader should consult  and  for the particular case without transmission errors and §VI for the case considered here. III. MEAN FIELD ASYMPTOTICS OF THE POISSON TRANSMISSION ERROR, RATE INDEPENDENT SYNCHRONIZATION CASE The
20661	20667	This work can be extended to further cases such as heterogeneous TCP flows with different classes of random transmission errors and multiple tail-drop/AQM routers (see our previous papers ,  for heterogeneous TCP flows over multiple wired links and ,  for AQM routers.) 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004 NS NSsACKNOWLEDGMENT The authors would like to thank
20661	20670	TCP flows with different classes of random transmission errors and multiple tail-drop/AQM routers (see our previous papers ,  for heterogeneous TCP flows over multiple wired links and ,  for AQM routers.) 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004 NS NSsACKNOWLEDGMENT The authors would like to thank Dohy Hong for his contributions to this paper and his help during its
20671	13258	In this study, the partial sequences described above are refered as unique exon blocks (UEBs). The following is a formal definition of UEBs. To establish this definition, we extended Delpher’s MUMs  so that they can be defined for more than two sequences. Definition 1 A UEB e is a string that satisfies the conditions below. (A1) e is a substring of an input sequence. (A2) e is longer than u
841790	7144	present the probabilistic framework in which we formalise a quantitative approach to noninterference. In particular, we consider a probabilistic calculus that was introduced in  and used in  to define a probabilistic extension of the nondeterministic approach to noninterference of . Such a calculus derives from a simple nondeterministic process algebra where actions are
841790	7144	solution to the equation system: ? ?? 1 if a = ? ? P ? C ? Q?G ?? Prob(P, ?, Q) · Prob(Q, ? ? , C) if a = ? ? P ?? C ? Q?G Prob(P, ?, Q) · Prob(Q, ? ?a, C) + Prob(P, a, C) if a ?= ? As shown in , this system has a least solution. We are now ready to define the 6sAldini, Di Pierro P a?,q ???? P ? Q a? ???? P + p Q a?,p·q ???? P ? P a,q ???? P ? Q GAct ???? P + p Q a,p·q ???? P ? P a?,q ????
841790	7144	?PB, introduced in Section 2.3, while the choice of the low-level models to be compared depends on the definition of the security property. The property we consider here was introduced in  as a probabilistic extension of the Strong Nondeterministic Noninterference property of . Such a property, which we call Probabilistic Noninterference (PNI ), compares the low-level view of
841790	7147	between the two low-level views considered by PNI . The criterion we adopt to establish the indistinguishability of two probabilistic processes refers to a probabilistic notion of weak bisimulation . An approximated version of noninterference can be formalised through the definition of a corresponding similarity relation. We define such a relation in terms of the observable difference between
841790	7147	present the syntax and the semantics of the probabilistic calculus. The interested reader is referred to  for more details. Then, we describe a notion of process equivalence  based on a probabilistic extension of the weak bisimulation of . 2.1 Syntax The syntax of the probabilistic process calculus is as follows: P ::= 0 | ?.P | P + p P | P ? p S P | P \L | P/p a
841790	7147	for the analysis (e.g. behaviours which are observable from an external or low-level viewpoint). We consider here a probabilistic variant of the weak bisimulation which was introduced in . Such a relation, denoted by ?PB, is a probabilistic extension of the weak bisimulation (?B) of . In essence, ?PB replaces the classical weak transitions of ?B by the probability of reaching
841790	7147	a generative action, therefore computing the probability associated with a mixed trace of generative/reactive actions (like, e.g., ? ? a?) does not actually make sense. Note also that, as shown in , the first equation in Definition 2.3 can be equivalently written as Prob(P, ? ? a? ? , C) = Prob(Q, ? ? a? ? , C). Example 2.4 Processes P ? = a + 1 2 b and Q ? = ?.Q + 1 3 (a + 1 2 b) behave the
841790	7147	number of abstract states (or classes, represented by the columns) as K but without any transitions between the “extra” n ? ? n states. The weak probabilistic bisimulation relation introduced in  can now be formulated in a linear operator setting as follows: Definition 5.1 Let P, Q ? G be two processes and let nP and nQ be the number of states reachable by P and Q respectively. Then P and Q
841790	20678	does not consider probabilistic behaviours; instead it relies on a worst case analysis based on all possible ways to resolve nondeterminism. Another related approach is the one presented in , where the amount of confidential information which may be leaked by programs written in a simple imperative language is analysed by using Shannon’s information theory. Desharnais et al.
841790	20681	More precisely, we can refer to a restricted version of the labelled transition system (G, Act, T ) that considers fully generative processes only. Based on such a restriction, as described in , where fully probabilistic processes are considered, we can associate to the probabilistic relation T the following linear operator: M(T ) = ? Ma(T ), a?GAct where for all a ? GAct and P, Q ? G,
841790	20681	being, to consider only finite-dimensional vector spaces and finite-dimensional linear operators. The results we present can nevertheless be extended to the general infinite case along the lines of . Our aim is to re-formulate the weak probabilistic bisimulation semantics introduced in Section 2.3 in terms of the above defined linear operators M(P ) representing the operational semantics of a
841790	20681	Definition 2.3 (first equation), we now need to define an operator which encodes probabilities of the form Prob(P, ? ? a, C), with C an equivalence class in a given relation R ? G × G. As shown in , equivalence relations are in a one-to-one correspondence with a particular class of linear operators called classification operators: if ? is an equivalence relation on a set X, then there is a
841790	20683	processes and the confidence that the tests outcome is correct. The application of this method for a statistical interpretation of the approximation of confinement properties was first described in ; a detailed description of this statistical interpretation in our process algebraic setting can be found in . In  an approximated notion of noninterference is proposed by
841790	7156	of this statistical interpretation). The following example shows an application of the noninterference general idea to study interferences between honest users and malicious parties (see, e.g.,  for an application of the noninterference approach to the analysis of cryptographic protocols). Example 3.3 Let us assume that Low represents a user that interacts with the system in order to
841790	20688	by comparing the effect on the lowlevel view of the absence/presence of the high-level user. A different approach aiming to the same objective of quantifying information flow has been proposed in , where the “quantity” is defined in terms of the behaviours of the highlevel user that are distinguishable from a low-level user point of view. This approach does not consider probabilistic
20692	20700	It also has been influenced by joint work with P. Crouch described in  and . The mathematical structure underlying sets of trees was worked out in . This exposition derives, in part, from , which is an expository account of symbolic expressions which arise in the study of differential equations. The figures are taken from . ? This research is supported in part by NASA grant
20692	20700	trees associated with the vector fields Ej. 2 The basic idea In this section, we describe the basic idea of how trees can be used to organize computations involving vector fields following  and . Consider a control system ?x(t) = E1(x(t)) + u1(t)E2(x(t)) + u2(t)E3(x(t)), x(0) = x 0 ? R N , (1) where E1, E2 and E3 are vector fields defined in a neighborhood of x0 ? RN and t ?? ui(t) are
20692	20702	This paper is based upon  and . It also has been influenced by joint work with P. Crouch described in  and . The mathematical structure underlying sets of trees was worked out in . This exposition derives, in part, from , which is an expository account of symbolic expressions which arise in the study of differential equations. The figures are taken from . ? This
20692	20702	of the root is a right and left unit for this product. It can also be shown that the maps defining the coalgebra structure are algebra homomorphisms, so that k{T } is a bialgebra. For details, see . The bialgebra k{T } is graded: k{T } n has as basis all trees with n + 1 nodes. Because the bialgebra k{T } is graded connected, it is a Hopf algebra. We summarize the above discussion in the
20692	20702	of the tree (except for the root) is labeled with a symbol from the set {E1, . . ., EM}. We can define the product and coproduct as above, and, once again, the resulting space is a bialgebra. See  for details. Let k{LT } denote this algebra. Let R denote a subring of the commutative ring of smooth functions on 7sR N . We now define an action of the algebra of Cayley trees B = k{LT } on the
20692	20703	operators Ej in terms of the basis ? , ?xµ1 ?2 ?xµ1?xµ2 , . . . , µ1, µ2, . . . = 1, . . . , N, compute the composition of the rightward and downward pointing arrows in the diagram above. In ,  and , we show that the algorithm is much more efficient than naive substitution, which corresponds to computing the diagonal arrow directly. In some common cases, the improvement in
20692	20704	operators Ej in terms of the basis ? , ?xµ1 ?2 ?xµ1?xµ2 , . . . , µ1, µ2, . . . = 1, . . . , N, compute the composition of the rightward and downward pointing arrows in the diagram above. In ,  and , we show that the algorithm is much more efficient than naive substitution, which corresponds to computing the diagonal arrow directly. In some common cases, the improvement in efficiency
20692	20705	which is the universal enveloping algebra of a Lie algebra L, we say that p ? H ? has finite Lie rank if L ? p is finite dimensional. 6s4 The algebra of Cayley trees In this section, we follow  and define a bialgebra structure on spaces of trees. The relation between trees and differential operators goes back at least as far as Cayley  and . Of this literature, the work most closely
20692	20706	operator p ? A. Then p · f = ?(p) · f. Here the action on the left views R as an A-module algebra, while the action on the right views R as B-module algebra. The first assertion is proved in  and the second assertion follows from the first assertion and the definitions. From this theorem, we get: 9 (5)sNo. of terms Form of terms 8N 3 12N coeff. Dµ1 3 coeff. Dµ2Dµ1 4N 3 coeff. Dµ3Dµ2 Dµ1
20692	20706	Ej in terms of the basis ? , ?xµ1 ?2 ?xµ1?xµ2 , . . . , µ1, µ2, . . . = 1, . . . , N, compute the composition of the rightward and downward pointing arrows in the diagram above. In ,  and , we show that the algorithm is much more efficient than naive substitution, which corresponds to computing the diagonal arrow directly. In some common cases, the improvement in efficiency is
20692	20707	to compute effectively the vector field expressions which arise in nonlinear control theory. It also describe the mathematical structure that sets of trees carry. This paper is based upon  and . It also has been influenced by joint work with P. Crouch described in  and . The mathematical structure underlying sets of trees was worked out in . This exposition derives, in part,
20692	20707	produced by R. The following theorem gives a condition which is stated in terms of the action of H on H ? for p ? H ? to be the generating series associated with an input-output map. (See  for a complete exposition.) Theorem 6.1 Let L be a Lie algebra, and let H = U(L). Then p is of finite Lie rank if and only if p is differentially produced by an augmented algebra R with ker ?/(ker
35087	6499	networks that are overlaid on top of an existing network infrastructure, and the attempt to adapt the overlay topology and routing to ensure the best possible performance for its application, e.g.,  and the RON project . The use of such overlays can help overcome sub-optimal routing decisions by the default routing mechanisms that control the network paths on which packets are forwarded. In
35087	919	on top of an existing network infrastructure, and the attempt to adapt the overlay topology and routing to ensure the best possible performance for its application, e.g.,  and the RON project . The use of such overlays can help overcome sub-optimal routing decisions by the default routing mechanisms that control the network paths on which packets are forwarded. In particular, default
35087	20709	into two major categories. The first category involves studying traces gathered over a period of time to extract a better understanding of traffic and loss patterns. For example, works such as , , and  were aimed at identifying mathematical models to characterize loss traces and reveal temporal dependencies that might exist. Other works such as  and  were concerned with
35087	20709	relation between the discrete state transition matrices of the two flows. III. ACCURACY OF THE MODEL As mentioned in Section I, there have been other models used to describe packet losses, e.g., , , most of which focused on directly describing the temporal dependency of losses instead of capturing the variation of path state. Representative samples of such models include the Bernoulli
35087	20709	the average loss rate is R = N? ?ibi(1). (5) i=1 To evaluate the accuracy of the model, we use different loss traces as the input to the HMM estimation algorithm. We use two traces obtained from , one is a 2.5-hour trace from Seattle to UMass with a sampling interval of 20ms (20Dec97), and the other is a 6-hour trace from Atlanta to UMass with a sampling interval of 40ms (26Jun98). We
35087	20709	b0(k), 1 ? b1(k), ..., 1 ? bN(k)] T and I = T . We then compare the estimated loss length distribution (k = 1) to the actual statistics for two traces: 20Dec97 and 21Dec97, both from . The results are shown in Fig. 3. It can be seen that the model yields loss length distribution reasonably close to those of the original data traces, especially in the short loss burst region. To
35087	20711	For example, works such as , , and  were aimed at identifying mathematical models to characterize loss traces and reveal temporal dependencies that might exist. Other works such as  and  were concerned with studying the stationarity of the loss process on Internet paths and analyzing its predictability. Such analyses are clearly useful to understand the general loss
35087	20711	dramatically when it comes to traces with loss rate ? 1%. Periodicity also exists in the loss process. Such periodicity could for example come from the diurnal behavior of network users. In , the author also observed periodicities because of the timer synchronization between routers. Based on such observations, we believe that it is difficult to capture loss variations across a broad
35087	20711	For example, works such as , , and  were aimed at identifying mathematical models to characterize loss traces and reveal temporal dependencies that might exist. Other works such as  and  were concerned with studying the stationarity of the loss process on Internet paths and analyzing its predictability. Such analyses are clearly useful to understand the general loss characteristics
35087	20711	random process, in which stationarity and non-stationarity coexist. The level of stationarity often varies with the observation time scale and the level of congestion on the path. For example, in , the majority of the loss processes exhibit some level of stationarity when the observation is restricted to traces with loss rate less than 1%, while such stationarity decreases dramatically when
35087	11928	this paper. Probes have been used to obtain various network performance measures such as available bandwidth, delay, and loss. For instance, probing a path using packet pairs  or packet trains  can provide estimates of bottleneck link speed as well as the available bandwidth. Similarly, in  He et al. use a probing method to measure end-to-end cross traffic by exploiting the long range
35087	20713	bandwidth, delay, and loss. For instance, probing a path using packet pairs  or packet trains  can provide estimates of bottleneck link speed as well as the available bandwidth. Similarly, in  He et al. use a probing method to measure end-to-end cross traffic by exploiting the long range dependence nature of Internet traffic. Closer to our goals,  introduces a method for measuring
35087	20714	available bandwidth. Similarly, in  He et al. use a probing method to measure end-to-end cross traffic by exploiting the long range dependence nature of Internet traffic. Closer to our goals,  introduces a method for measuring network delay using ICMP timestamp probes. Most relevant to this paper,  and  use various inference techniques and end-to-end multicast/unicast probes to
35087	20715	exploiting the long range dependence nature of Internet traffic. Closer to our goals,  introduces a method for measuring network delay using ICMP timestamp probes. Most relevant to this paper,  and  use various inference techniques and end-to-end multicast/unicast probes to estimate the loss rates on individual links. The main difference with our work is that we are primarily
35087	20716	the long range dependence nature of Internet traffic. Closer to our goals,  introduces a method for measuring network delay using ICMP timestamp probes. Most relevant to this paper,  and  use various inference techniques and end-to-end multicast/unicast probes to estimate the loss rates on individual links. The main difference with our work is that we are primarily interested in
35087	20717	of the end-to-end loss, we use the information gathered by the probes to construct a Hidden Markov Model (HMM) that captures the main characteristics of the loss process. Salamatian et al.  first proposed using HMM to model the loss performance of network paths. But our work extends the basic model along two dimensions. Specifically, we consider two major issues in constructing the
35087	20717	and is, therefore, associated with a different loss probability. Thus, the problem boils down to deriving the model parameters from the results of probing, and we use a Hidden Markov Model (HMM)  for this purpose. We first derive a discrete Markov model from the observation sequence, then convert it to a continuous-time model, which can then be used to predict the performance that a
35087	20717	we have analyzed, randomly selected initial values typically result in the algorithm converging to the same point. In other words, the number of local maxima appears to be limited in our case. In , the authors observed a similar phenomenon. The second concern about HMM is that the convergence process is often slow and the convergence time varies. The major factors that affect the convergence
35087	20717	we believe that a multi-second convergence time will be adequate. Another issue with an HMM approach is the actual number of states needed to precisely describe the state of an Internet path. In , the authors indicate that most loss traces they studied can be represented by a 2 or 3-state model, while very few exhibit 4-state behaviors. In our study, we choose to use a 2-state model in most
35087	20718	is, therefore, associated with a different loss probability. Thus, the problem boils down to deriving the model parameters from the results of probing, and we use a Hidden Markov Model (HMM)  for this purpose. We first derive a discrete Markov model from the observation sequence, then convert it to a continuous-time model, which can then be used to predict the performance that a
35087	20719	therefore, associated with a different loss probability. Thus, the problem boils down to deriving the model parameters from the results of probing, and we use a Hidden Markov Model (HMM)  for this purpose. We first derive a discrete Markov model from the observation sequence, then convert it to a continuous-time model, which can then be used to predict the performance that a given
35087	1069	{O1O2...Ot,qt = Si|?}. Similarly, the backward variable ?t(i) is defined as ?t(i) =P {Ot+1Ot+2...OT |qt = Si,?}. Both the forward and the backward variables can be solved inductively given ? and O . We also define ?t(i, j), the probability of being in state Si at time t, and state Sj at time t +1, given the model and the observation sequence, i.e., ?t(i, j) =P {qt = Si,qt+1 = Sj|O, ?}. From
35087	1069	j) = ? N i=1 ?t(i)aijbj(Ot+1)?t+1(j) ? . (1) N j=1 ?t(i)aijbj(Ot+1)?t+1(j) Using the variables defined above, we can re-estimate the parameters ( ¯ ?) of an HMM following the Baum-Welch procedure , i.e., ¯?i = ?ij = N? ??1(i, j), j=1 ?T ?1 t=1 ?t(i, j) ?T ?1 ?N t=1 ?T ?N t=1 s.t. Ot=vk ?T t=1 j=1 ?t(i, j) , ¯ j=1 bj(k) = ?t(i, j) ?N j=1 ?t(i, . (2) j) It has been proved that starting from an
35087	1069	Thus the goal of the re-estimation procedure becomes adjusting ? to maximize W? P (O|?) = P (O (k) W? |?) = k=1 k=1 Consequently, the re-estimation formulas of aij and bj(l) should be modified as : aij = ?W k=1 1 ?Tk?1 Pk t=1 ?k t (i)aijbj(O (k) ?W k=1 1 ?Tk?1 Pk t=1 ?k t (i)?k t (i) bj(l) = Pk t+1 )?k t+1(j) , (12) ?W k=1 1 ?Tk?1 Pk t=1s.t.Ot=vt ?k t (j)?k t (j) ?W k=1 1 ?Tk?1 Pk t=1 ?k t
35087	20721	between the discrete state transition matrices of the two flows. III. ACCURACY OF THE MODEL As mentioned in Section I, there have been other models used to describe packet losses, e.g., , , most of which focused on directly describing the temporal dependency of losses instead of capturing the variation of path state. Representative samples of such models include the Bernoulli model
35087	20722	of consecutive packet losses. For some applications (e.g., video or audio), given the same loss rate, variations in loss burstiness can result in dramatic differences in application-level quality . Using HMM, we can compute the burst length distribution of losses from the steady state probability distribution (?) of the path state, the loss probability in each state (B), and the state
35087	20722	the number of states did not improve the accuracy of the estimates for trace 21Dec97. It is worth noting that (7) can also be used to compute the distribution of v0 burst length, or loss distance , simply by letting k =0. This metric is sometimes more convenient for evaluating application quality. Since the distribution of loss distance can be derived from the average loss rate and the loss
35087	20454	traffic sources. For example, a packet-based voice flow is typically composed of active and inactive periods, corresponding to the activity of human speech. Moreover, in some video servers , streaming is implemented in bursts of packets (typically with a burst duration of hundreds of milliseconds), in order to lower the overhead on the server. We consider a flow with ON-OFF ratio N1 :
20724	20727	and Li, Kia and Doermann  present algorithms for detecting text in video image sequences. However, none of above papers have addressed the issue of the “size” of the problem. Lawrence and Giles  have estimated the number of web pages on the internet. However this work does not address the issue of images on the web. Antonacopoulos and Karatzas  recently proposed an anthropocentric
20724	20728	connected components. Wu, Manmatha and Riseman  propose a text detection and extraction algorithmthat is based on analyzing the image texture. Lienhart and Stuber  and Li, Kia and Doermann  present algorithms for detecting text in video image sequences. However, none of above papers have addressed the issue of the “size” of the problem. Lawrence and Giles  have estimated the number
20724	20729	the color and then detects connected components. Wu, Manmatha and Riseman  propose a text detection and extraction algorithmthat is based on analyzing the image texture. Lienhart and Stuber  and Li, Kia and Doermann  present algorithms for detecting text in video image sequences. However, none of above papers have addressed the issue of the “size” of the problem. Lawrence and Giles
20724	20731	Zhou  have proposed an approach for extracting text strings from complex images on the web that first quantizes the color and then detects connected components. Wu, Manmatha and Riseman  propose a text detection and extraction algorithmthat is based on analyzing the image texture. Lienhart and Stuber  and Li, Kia and Doermann  present algorithms for detecting text in video
20733	20736	togeneral temporal relations for which there is no stated relationship between valid and transaction time . Such temporal dependencies have been termed intraelement integrity constraints . (2) P 2(c t ?c v ) c v c t . With this predicate, only snapshots that concern a past state of reality, relatively to the time the snapshot was current, are required to satisfy the snapshot
20733	20744	xed maximum delay of two seconds from when a temperature is sampled until it is actually stored in the database. In this example, the update pattern is thus closely tied to the observation pattern . Assuming that an experiment x1 starts at 9:00.00 a.m., its time patterns may be given as follows. O x1 A U x1 A (0) = 9:00.05? Ox1 A (0) 2 [9:00.05? 9:00.07)? :::? U x1 A (1) = 9:00.15? :::?
20733	20744	?c v ) True. This yields the temporal functional dependency as rst de ned, and is relevant togeneral temporal relations for which there is no stated relationship between valid and transaction time . Such temporal dependencies have been termed intraelement integrity constraints . (2) P 2(c t ?c v ) c v c t . With this predicate, only snapshots that concern a past state of reality,
20733	20752	model|relations of tuples timestamped with bitemporal elements. We nowde ne some algebraic operators on these objects that will be used later. A complete algebra for the BCDM is de ned elsewhere . We rst de ne bitemporal analogues of some of the snapshot relational operators, to be denoted with the superscript \ B &quot;. De ne a relation schema R = (A1?:::?AnjT), and let r be an instance of
20766	20767	active queue management (AQM) schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , . Unlike previous analyzes that investigated the queueing behavior of TCP, and usually used the fluid flow modelling approach, here we
20766	13796	bandwidth-delay product does indeed make the queueing system work conserving. Inother words, during congestion, a buffer at the congested router will maintain the so-called “queue never empties”  condition. Our focus of TCP with drop-tail is motivated by: (1) most data traffic nowadays is TCP based, and (2) despite many proposals for sophisticated active queue management (AQM) schemes ,
20766	20769	runs which are based on different propagation delay deviates of the exponential (with parameter 1/100) distribution. There is a strong evidence that Internet topology is governed by Power Laws , , , therefore it is important to also consider cases where distances between nodes and therefore the propagation delays are governed by a heavy Queue length (packets) Queue length
20766	7174	are based on different propagation delay deviates of the exponential (with parameter 1/100) distribution. There is a strong evidence that Internet topology is governed by Power Laws , , , therefore it is important to also consider cases where distances between nodes and therefore the propagation delays are governed by a heavy Queue length (packets) Queue length (packets) Queue
20766	20774	condition. Our focus of TCP with drop-tail is motivated by: (1) most data traffic nowadays is TCP based, and (2) despite many proposals for sophisticated active queue management (AQM) schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , ,
20766	2242	Our focus of TCP with drop-tail is motivated by: (1) most data traffic nowadays is TCP based, and (2) despite many proposals for sophisticated active queue management (AQM) schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , .
20766	20776	of TCP with drop-tail is motivated by: (1) most data traffic nowadays is TCP based, and (2) despite many proposals for sophisticated active queue management (AQM) schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , .
20766	2237	TCP with drop-tail is motivated by: (1) most data traffic nowadays is TCP based, and (2) despite many proposals for sophisticated active queue management (AQM) schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , . Unlike
20766	2243	The latter is especially relevant if the traffic is composed mostly of data flow supported by the transmission control protocol (TCP). The popular TCP Reno  congestion control mechanism  reacts to congestion (packet loss) by halving its congestion window (cwnd). If buffers are too small, such reaction to congestion often empties the queue and creates a situation whereby the system
20766	6529	(AQM) schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , . Unlike previous analyzes that investigated the queueing behavior of TCP, and usually used the fluid flow modelling approach, here we consider a discrete-time model of
20766	6529	B>µ?, we obtain the equilibrium values for Wmin and Qmin as follows: and Wmin = Qmin = B + µ? 2n (9) B ? µ? . (10) 2 These results are consistent with results of other TCP analyzes , , , . This result has an important practical implication. It indicates relationship between the condition of B>µ?(the buffer is larger than the bandwidth-delay product) and the condition Qmin > 0.
20766	2355	schemes , , , , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , . Unlike previous analyzes that investigated the queueing behavior of TCP, and usually used the fluid flow modelling approach, here we consider a discrete-time model of TCP
20766	6533	, , , , , , , , , drop-tail is still the most popular AQM. The TCP protocol has been extensively studied , , , , , , , , , , , . Unlike previous analyzes that investigated the queueing behavior of TCP, and usually used the fluid flow modelling approach, here we consider a discrete-time model of TCP Reno with drop-tail
20766	20781	runs which are based on different propagation delay deviates of the exponential (with parameter 1/100) distribution. There is a strong evidence that Internet topology is governed by Power Laws , , , therefore it is important to also consider cases where distances between nodes and therefore the propagation delays are governed by a heavy Queue length (packets) Queue length (packets)
20787	17447	or activities, and arrows between the nodes that represent sequencing of activities. The resulting diagrams look like Petri nets, and so Petri nets seem a natural technique for modelling workflows . The following arguments are often used to support this: Petri nets are graphical, they have a formal semantics, they can express most of the desirable routing constructs, there is an abundance of
20787	17447	knowledge of Petri nets and highlevel Petri nets (see e.g. ). We have looked at Petri net variants that are traditionally used to model and analyse workflows, namely Workflow Nets , Information Control Nets , INCOME/WF , FunSoft nets , MILANO WFMS . Next, we have looked at Petri net variants that are not specifically tailored towards workflow modelling but
20787	17447	In particularly, we discuss whether and how the statechart step semantics can be modelled in Petri nets. We end with conclusions. 2 Workflow This section is based on literature (amongst others ) and several case studies that we did. A workflow is a set of business activities that are ordered according to a set of procedural rules to deliver a service. A workflow model (also known as
20787	17447	is done by a workflow management system (WFMS), on the basis of workflow models. In general, two important dimensions of workflows are the control-flow dimension and the resource dimension . The control-flow dimension concerns the ordering of activities (or tasks) in time (what has to be done). The resource dimension concerns the organisational structure (who has to do it). Since both
20787	17447	internal clock. 3 UML Activity Diagrams Syntax. We explain the syntax by means of a small example. In Fig. 3 the workflow of “Processing Complaints” is shown (converted from a Petri net model in ; see Fig. 7 below). Ovals represent activity states and rounded rectangles represent wait states. In an activity state, some activity is busy executing whereas in a wait state, an external event is
20787	20788	the clock denote external and temporal trigger events respectively. Unfortunately, although the importance of input events is recognised, hardly ever a semantics is given for them. Van der Aalst  gives an interesting motivation for abstracting from events for analysis purposes, that we will discuss in Section 4.6. But first we study two approaches to model events in ordinary Petri nets and
20787	20788	that are used to model and analyse workflows. Van der Aalst, Van Hee and Houben  use high-level nets to model and analyse Petri net based workflow models that also model resources. Van der Aalst  uses Workflow nets, low-level Petri nets with a single start and a single end place, to verify proper termination of a workflow model. Although Van der Aalst recognises the need for modelling input
20787	20789	cases. In Petri nets that model activities as transitions, the routing is not modelled at all. Therefore, such Petri nets do not model a WFS. As an aside, note that in some variant of Workflow Nets , some transitions can be labelled with a silent action that is not observable for the environment. Van der Aalst  suggests to use the silent step to model routing transitions. Transitions
20787	17449	performed. The WFMC  specifies four possible ordering relationships between activities: sequence, choice, parallelism and iteration. Van der Aalst et al. identified more ordering relationships . And, to facilitate readability and re-use of workflow definitions, an ordered set of activities can be grouped into one compound activity. A compound activity can be used in other workflow
20787	20791	looked at Petri net variants that are traditionally used to model and analyse workflows, namely Workflow Nets , Information Control Nets , INCOME/WF , FunSoft nets , MILANO WFMS . Next, we have looked at Petri net variants that are not specifically tailored towards workflow modelling but nevertheless can be useful: Open Nets , Petri nets with synchronous communication
20787	20791	net variant for workflow modelling. The focus is on the modelling of resources, like documents, not on the modelling of events. The standard Petri net semantics for high-level nets is used. Milano  is a research prototype to investigate the issue of flexible workflow models. The Petri nets that are used do not contain events, loops, data, real-time. Moreover, these nets must be safe. It is
20787	20792	, FunSoft nets , MILANO WFMS . Next, we have looked at Petri net variants that are not specifically tailored towards workflow modelling but nevertheless can be useful: Open Nets , Petri nets with synchronous communication , Signal-Event Nets , Contextual Nets , Zero-Safe nets , and several variants of Object-Oriented Petri Nets . More information
20787	20792	. In these approaches, the environment is not specified, but the suggestion is made that the environment fills the interface places spontaneously, but no formal semantics is presented. Open nets  gives a formal semantics for nets with interface places, which could be used for Trigger Modelling and Workflow Nets. One important difference of the event-as-token approach with our semantics of
20787	17451	it should be done), e.g. how fast a response is. We therefore abstract away from internal implementation details of the WFS. The best way to do this is to adopt the perfect synchrony hypothesis . For a WFS, this hypothesis states that the WFS starts reacting to events immediately when it receives them, and also that the WFS reacts infinitely fast to these events. In a reaction, therefore,
20787	20793	towards workflow modelling but nevertheless can be useful: Open Nets , Petri nets with synchronous communication , Signal-Event Nets , Contextual Nets , Zero-Safe nets , and several variants of Object-Oriented Petri Nets . More information about some of these references can be found in recent overviews and collections about the use of Petri nets for workflow
20787	20793	obeying the firing rules, is allowed. So, it could be possible that under the standard Petri semantics an event lives longer than a step. Recently, a new Petri net variant, called zero safe nets , has been proposed that seems a good starting point for modelling the statechart step semantics. In zero safe nets, some places, called zero places, represent unobservable system states. A marking
20787	20795	A compound activity can be used in other workflow definitions. A noncompound activity is called an atomic activity. Architecture (Fig. 2). The following architecture is based upon amongst others . A workflow system (WFS), which is a WFMS instantiated with one or more workflow models, connects a database system and several applications that are used by actors to do work for the cases. In
20787	20795	semantics the implementation-level semantics. This semantics stays close to the informal UML definition of state machines  (underlying UML statecharts) and the architecture of workflow systems . In the implementation-level semantics, the WFS is considered as a white box, consisting of the components shown in Fig. 2. The Router component is responsible for producing the desired reaction:
20787	20796	Next, we have looked at Petri net variants that are not specifically tailored towards workflow modelling but nevertheless can be useful: Open Nets , Petri nets with synchronous communication , Signal-Event Nets , Contextual Nets , Zero-Safe nets , and several variants of Object-Oriented Petri Nets . More information about some of these references can be found in
20787	20796	In Petri nets, one can simulate an event by labelling a transition with the event name and interpret the firing of the transition as the event occurrence. By specifying synchronisation constraints  between the event transition and the system transitions, it can be specified that an event occurrence triggers a system transition. Note, however, that then the environment is being modelled
20787	20796	net semantics. We discuss their approach in more detail below. Event as transition. There are several Petri net variants that have incorporated synchronisation between transitions in their models. The work of Christensen and Hansen  introduces the concept of synchronous transitions. They focus on symmetric synchronisation. Object-oriented Petri nets  use both symmetric and
20787	12941	for UML activity diagrams that are intended for workflow modelling . The goal is to use these semantics for analysing workflow models in activity diagram notation by means of model checking . The first semantics is a high-level semantics, based upon the Statemate semantics of statecharts , that is easy to analyse (both for a computer and for a person) but somewhat abstract. By
20787	20798	We have looked at Petri net variants that are traditionally used to model and analyse workflows, namely Workflow Nets , Information Control Nets , INCOME/WF , FunSoft nets , MILANO WFMS . Next, we have looked at Petri net variants that are not specifically tailored towards workflow modelling but nevertheless can be useful: Open Nets , Petri nets with
20787	20798	to different behaviour in the abstract model, when compared to the concrete model. Consequently, the verification results obtained for the abstract net might not be reliable anymore. FunSoft nets  are high-level nets for software process modelling, but they can also be used for workflow modelling. Their semantics is defined in terms of Predicate/Transition nets . FunSoft nets focus on
20787	20798	and do not focus on modelling events. Some shorthands are defined to model for example FIFO queues. Several analysis techniques, including verification have been developed for FunSoft nets . INCOME/WF  is a a workflow management system based on high-level Petri nets where the tokens are nested relations. Nested relations are introduced to increase the concurrency of the net: the
20787	20800	the domain of workflow modelling. Even if one does not agree with the choices we made, our discussion gives – we hope – more insight in possible answers to the question what actually is a Petri net . Our most important design choice is that the semantics for activity diagrams must be reactive. The token-game semantics, which is characteristic for Petri nets, does not represent reactivity,
20787	20801	and event-as-transition approaches. In the event-as-token approach, a queue can be modelled straightforwardly by switching to Petri nets with integers (counters) as is done in the FunSoft approach . And a special place can be introduced to store the event that is currently being processed by the Router. But still, since in the event-as-token approach the statechart step semantics we use,
20787	20802	for UML activity diagrams that are intended for workflow modelling . The goal is to use these semantics for analysing workflow models in activity diagram notation by means of model checking . The first semantics is a high-level semantics, based upon the Statemate semantics of statecharts , that is easy to analyse (both for a computer and for a person) but somewhat abstract. By
20787	20802	state machines, but it is more difficult to analyse than the first semantics. We have implemented verification support using model checking for the first semantics in our diagram editing tool TCM . In this paper we discuss the design choices that underlie both our formal execution semantics. Since our purpose is to make analysis of activity diagram workflow models possible, the semantics
20787	20802	chaotically, but it must respect the dependencies between value change input events (see Section 4 for more details). For analysis purposes, we assume that the environment behaves in a fair way . Step semantics. The key part in both of the previous semantics is the execution of a step. A step is a collection of edges that are enabled in a certain state. By taking a step, the system reacts
20787	20804	semantics  is not formal (nor precise), and it is not intended for workflow modelling . We therefore defined two semantics for UML activity diagrams that are intended for workflow modelling . The goal is to use these semantics for analysing workflow models in activity diagram notation by means of model checking . The first semantics is a high-level semantics, based upon the
20787	20804	terminates Send questionnaire terminates {Send ques? tionnaire, WAIT?3} state 3 {WAIT?1, WAIT?3} state 4 Fig. 4. Example run in requirements-level semantics one is a requirements-level semantics  that is based upon the Statemate semantics of statecharts . In the requirements-level semantics, the WFS is considered as a black box. In specifying requirements for the WFS, we are interested
20787	20804	state.sIn both semantics, we have adopted the same step semantics of statecharts. We here give a brief introduction to and motivation for this step semantics. More details can be found elsewhere . We consider two cases. The first one is the basic case, in which action expressions on edges are not considered. Although statecharts are (in)famous for the numerous semantics invented for them,
20787	20804	is similar to the behaviour of the corresponding activity diagram (statechart) under the statechart fixpoint semantics of Pnueli and Shalev , as we explained in Section 3. But in our semantics , only  would be possible. As explained in Section 3, we regard the fixpoint semantics (and thus the signal-event step semantics) as counter intuitive here, since it seems that e is ignored
20787	20804	WFS WAIT?1 environment A A /done WAIT?1 e e WAIT?2 A terminates e done done WAIT?2 B B B terminates Fig. 10. Activity diagram and a similar signal-event net both our semantics  the input set acts as a kind of registry in which the events that are generated during a step are stored. This can only be simulated by treating events as tokens; if events are treated as
20787	20809	are also used quite extensively in business modelling, especially in UML-based approaches, for example . In academia, several WFMS research prototypes use event-based workflow models (e.g. ), often inspired by active databases .sregister i processing required send quest? ionnaire evaluate processing nok process questionnaire no processing process complaint time out archive check
20787	20809	branch a separate cancel event needs to be generated. Second, the broadcast mechanism is used quite extensively in the field of workflow systems. Several non-Petri net based WFMS prototypes ) also use a broadcast semantics in their workflow models. The industry standard for workflow interoperability , defined by OMG and WFMC, uses a so-called publish-subscribe notification
20787	20811	are also used quite extensively in business modelling, especially in UML-based approaches, for example . In academia, several WFMS research prototypes use event-based workflow models (e.g. ), often inspired by active databases .sregister i processing required send quest? ionnaire evaluate processing nok process questionnaire no processing process complaint time out archive check
20787	20811	branch a separate cancel event needs to be generated. Second, the broadcast mechanism is used quite extensively in the field of workflow systems. Several non-Petri net based WFMS prototypes ) also use a broadcast semantics in their workflow models. The industry standard for workflow interoperability , defined by OMG and WFMC, uses a so-called publish-subscribe notification
20787	20813	for analysing workflow models in activity diagram notation by means of model checking . The first semantics is a high-level semantics, based upon the Statemate semantics of statecharts , that is easy to analyse (both for a computer and for a person) but somewhat abstract. By contrast, the second semantics is low-level and resembles both the behaviour of an abstract workflow system
20787	20813	current marking . But in a reactive system a transition can be taken (fired) if all its source nodes (input places) are in the current configuration (marking) and its trigger event occurs . This trigger event is an event in the environment of the system, that the system will react to by taking the transition. Although Petri nets in our view are not reactive, we will study different
20787	20813	be found in recent overviews and collections about the use of Petri nets for workflow modelling . A comparison of our semantics with other formal modelling techniques (in particular Statemate ) can be found elsewhere . Structure. We start by explaining some characteristics of workflows and workflow systems in more detail. In Section 3 we discuss our two activity diagram semantics and
20787	20813	WAIT?3} state 3 {WAIT?1, WAIT?3} state 4 Fig. 4. Example run in requirements-level semantics one is a requirements-level semantics  that is based upon the Statemate semantics of statecharts . In the requirements-level semantics, the WFS is considered as a black box. In specifying requirements for the WFS, we are interested in qualitative requirements (what should be done), but not in
20787	20813	diagram, but can be added without a problem. In the basic case, all the statechart step semantics exhibit the following three properties. This includes the most well-known ones of Harel and Naamad , implemented in the Statemate toolset as well as the different UML statechart step semantics , and the fixpoint semantics by Pnueli and Shalev . First we list two of the three properties
20787	20817	UML there is in addition a priority constraint, stating that edges with higher priority should be added first to a step. The precise definition of when an edge has priority over another one differs . We now explain the third property of basic statechart steps in the basic case. – Steps are maximal. Not imposing this constraint would imply that some edges that are enabled would not have to be
20787	20818	is ignored in node WAIT-2. So there WAIT?5 are circumstances in which the fixpoint semantics computes a counter intuitive step (this was Fig. 6. Event generation first pointed out by Leveson et al.  using a similar example, but they mistakenly attribute the fixpoint semantics to Statemate). That is why in practice the Statemate approach is taken, even in the UML, where events are called
20787	20818	UML. As an aside, note in this interpretation too, there are anomalies. One may for example get infinite loops in which events are generated for ever, because some events cause each other to occur . The state of the practice. We do not know of any commercial WFMS that allows for the specification of workflow models using UML activity diagrams. But few of the current commercial workflow
20787	20820	are not specifically tailored towards workflow modelling but nevertheless can be useful: Open Nets , Petri nets with synchronous communication , Signal-Event Nets , Contextual Nets , Zero-Safe nets , and several variants of Object-Oriented Petri Nets . More information about some of these references can be found in recent overviews and collections about the use of
20787	20820	be overly complex and more involved than the statechart step semantics. A better alternative is to model the control flow between an interface place and a transition that it triggers as a read arc , also known as context relation. A read arc from a place to a transition means that although a token must be present in the place to let the transition fire, this token is not consumed. (A read arc
20787	20820	That is not what we want, because the concurrency of the WFS is then reduced. In addition, the resulting net would look like ravioli if there are many case attributes. To circumvent this, read arcs  can be used for read access. Interestingly, apparently read arcs have been proposed just to solve this problem of simultaneously access to shared data . But unfortunately, read arcs do not
20787	20822	(see e.g. ). We have looked at Petri net variants that are traditionally used to model and analyse workflows, namely Workflow Nets , Information Control Nets , INCOME/WF , FunSoft nets , MILANO WFMS . Next, we have looked at Petri net variants that are not specifically tailored towards workflow modelling but nevertheless can be useful: Open Nets , Petri
20787	20822	not focus on modelling events. Some shorthands are defined to model for example FIFO queues. Several analysis techniques, including verification have been developed for FunSoft nets . INCOME/WF  is a a workflow management system based on high-level Petri nets where the tokens are nested relations. Nested relations are introduced to increase the concurrency of the net: the actual
20787	20827	be found in recent overviews and collections about the use of Petri nets for workflow modelling . A comparison of our semantics with other formal modelling techniques (in particular Statemate ) can be found elsewhere . Structure. We start by explaining some characteristics of workflows and workflow systems in more detail. In Section 3 we discuss our two activity diagram semantics and
20787	20827	are also used quite extensively in business modelling, especially in UML-based approaches, for example . In academia, several WFMS research prototypes use event-based workflow models (e.g. ), often inspired by active databases .sregister i processing required send quest? ionnaire evaluate processing nok process questionnaire no processing process complaint time out archive check
20787	20827	branch a separate cancel event needs to be generated. Second, the broadcast mechanism is used quite extensively in the field of workflow systems. Several non-Petri net based WFMS prototypes ) also use a broadcast semantics in their workflow models. The industry standard for workflow interoperability , defined by OMG and WFMC, uses a so-called publish-subscribe notification
20787	22147	In particularly, we discuss whether and how the statechart step semantics can be modelled in Petri nets. We end with conclusions. 2 Workflow This section is based on literature (amongst others ) and several case studies that we did. A workflow is a set of business activities that are ordered according to a set of procedural rules to deliver a service. A workflow model (also known as
20787	22147	A compound activity can be used in other workflow definitions. A noncompound activity is called an atomic activity. Architecture (Fig. 2). The following architecture is based upon amongst others . A workflow system (WFS), which is a WFMS instantiated with one or more workflow models, connects a database system and several applications that are used by actors to do work for the cases. In
20787	22147	semantics the implementation-level semantics. This semantics stays close to the informal UML definition of state machines  (underlying UML statecharts) and the architecture of workflow systems . In the implementation-level semantics, the WFS is considered as a white box, consisting of the components shown in Fig. 2. The Router component is responsible for producing the desired reaction:
20787	22147	the Petri net semantics is used. We think that this observation holds for other workflow management tools as well, sincesmost adhere to the reference model of the Workflow Management Coalition  that, as we do, views workflow systems as reactive systems (cf. Fig. 2) that have coordination functionality. Acknowledgements. The authors would like to thank Wil van der Aalst and Jörg Desel for
20787	22148	according to roles. A role is a set of characteristics of actors. A role can refer to skills, responsibility, or authority for people, and it can refer to computing capabilities for machines . Roles link actors and activities. The modelling of actors and roles, and the connection with workflow models falls outside the scope of this paper. The effect of an activity can be constrained
20787	22148	be specified fully since execution of the activity falls outside the scope of the WFMS. The pre-condition also functions as guard: as long as it is false, the activity cannot be performed. The WFMC  specifies four possible ordering relationships between activities: sequence, choice, parallelism and iteration. Van der Aalst et al. identified more ordering relationships . And, to facilitate
8921779	20830	address and ZIP code reading, data acquisition in banks, text-voice conversions etc. As a result of intensive research and development efforts, systems are available for English language , , , , Chinese language , Japanese language, and handwritten numerals . There is still a significant performance gap between the human and the machine in recognizing unconstrained
8921779	20837	handwriting recognition have used datasets specifically collected for the particular research. Most of them have used constrained handwriting forcing the writer to write on a ruled paper ,. Due to the varying nature of the datasets used for training, the recognition results can hardly be compared. Therefore, a standard database of images is needed for Proceedings of the Seventh
20838	20839	. – JRip: the RIPPER rule learning algorithm . – PART: the PART rule learning algorithm . – J48: the decision tree learning algorithm C4.5 . – IBk: the k nearest neighbor algorithm . ??? K*: an instance-based learning algorithm with entropic distance measure . – NB: the Naive Bayes algorithm  using the kernel density estimator rather than assume normal distributions for
20838	20840	algorithm . – J48: the decision tree learning algorithm C4.5 . – IBk: the k nearest neighbor algorithm . – K*: an instance-based learning algorithm with entropic distance measure . ??? NB: the Naive Bayes algorithm  using the kernel density estimator rather than assume normal distributions for numeric attributes. – SMO: the sequential minimal optimization algorithm for
20838	3787	10 base-level classification algorithms, which are run with default parameter values unless otherwise stated: – DT: the decision table algorithm of . – JRip: the RIPPER rule learning algorithm . – PART: the PART rule learning algorithm . – J48: the decision tree learning algorithm C4.5 . – IBk: the k nearest neighbor algorithm . – K*: an instance-based learning algorithm with
20838	7278	Discovery aiming at improving the predictive accuracy of a single classification or regression model. Within the Machine Learning community this area is commonly referred to as Ensemble Methods . Models that have been derived from different executions of the same learning algorithm are often called Homogeneous. Such models can be induced by injecting randomness into the learning algorithm
20838	4762	called Homogeneous. Such models can be induced by injecting randomness into the learning algorithm or through the manipulation of the training instances, the input attributes and the model outputs . Homogeneous models are typically combined through weighted or unweighted voting. Models that have been derived from running different learning algorithms on the same data set are often called
20838	20843	and Selection. Each of the classification models is evaluated (typically using 10-fold cross-validation) on the training set and the best one is selected for application on the test set. In , the accuracy of the models is estimated locally on the different examples that surround each test example. Such approaches belong to the family of Dynamic Classifier Selection introduced in ,
20838	4787	20], the accuracy of the models is estimated locally on the different examples that surround each test example. Such approaches belong to the family of Dynamic Classifier Selection introduced in , which was the first work discussing the idea of using a different function for classifier combination in different partitions of the training set. Stacked Generalization , also known as
20838	20844	tree learning algorithm C4.5 . – IBk: the k nearest neighbor algorithm . – K*: an instance-based learning algorithm with entropic distance measure . – NB: the Naive Bayes algorithm  using the kernel density estimator rather than assume normal distributions for numeric attributes. – SMO: the sequential minimal optimization algorithm for training a support vector classifier
20838	20845	with the WEKA implementations of the following 10 base-level classification algorithms, which are run with default parameter values unless otherwise stated: – DT: the decision table algorithm of . – JRip: the RIPPER rule learning algorithm . – PART: the PART rule learning algorithm . – J48: the decision tree learning algorithm C4.5 . – IBk: the k nearest neighbor algorithm . –
20838	20846	a support vector classifier using polynomial kernels . – RBF: an algorithm for training a radial basis function network . – MLR: the multi-response linear regression algorithm, as used in . The meta-level training data for Stacking are produced using 10-fold stratified cross-validation on the training set. The same procedure is used for estimating the accuracy of the above base-level
20838	20847	are run with default parameter values unless otherwise stated: – DT: the decision table algorithm of . – JRip: the RIPPER rule learning algorithm . – PART: the PART rule learning algorithm . – J48: the decision tree learning algorithm C4.5 . – IBk: the k nearest neighbor algorithm . – K*: an instance-based learning algorithm with entropic distance measure . – NB: the Naive
20838	10759	introduced in , which was the first work discussing the idea of using a different function for classifier combination in different partitions of the training set. Stacked Generalization , also known as Stacking in the literature, is a method that combines multiple classifiers by learning a meta-level model that predicts the correct class based on the decisions of the classifiers.
792009	20856	each population with a deterministic sub-goal identification mechanism, it was decided to examine hypotheses 2 and 3 using one mechanism. The Feudal Q-Learning approach to reinforcement learning (Dayan and Hinton, 1993) is a simple approach to hierarchy construction that requires a pre-identified subdivision of the state space into small Q-tables and a preselected hierarchy of Q-tables. A Q-table at level n in
792009	20858	therefore has much in common with the work of Dorigo. The main body of previous investigation into hierarchical forms of LCS was performed by Dorigo and collegues (e.g. Dorigo and Schnepf, 1993; Dorigo and Colombetti, 1994). Using ALECSYS they created fixed control hierarchies. Their work was characterised by the dependency upon direct environmental feedback for the reward of switching decisions made by the upper
792009	20859	behaviour, and this work therefore has much in common with the work of Dorigo. The main body of previous investigation into hierarchical forms of LCS was performed by Dorigo and collegues (e.g. Dorigo and Schnepf, 1993; Dorigo and Colombetti, 1994). Using ALECSYS they created fixed control hierarchies. Their work was characterised by the dependency upon direct environmental feedback for the reward of switching
792009	9187	internal reward is paid out when the Animat controlled by XCS enters this state. For the situation where there is only ever a single sub-goal per environment subdivision the Optimality Hypothesis (Kovacs, 1996) implies that XCS will learn the optimal state × action × payoff mapping for each environmental subdivision. The problem of learning a route from the start state to the reward state is thus
792009	11889	allows prediction learning within exploitation so that the pathway to the reward state will rapidly become established through exploitation once discovered. Lanzi’s ‘teletransportation’ mechanism (Lanzi, 1997) is an example of the use of this approach. An alternative approach that does not require a change in the environment definition would be to dynamically modify the division between exploration and
792009	20862	THE POPULATION In order to investigate Hypothesis 1 a simple structuring of the population space within XCS was devised. The approach taken is based on the methods used within HQ learning (Wiering and Schmidhuber, 1996), although greatly simplified. The developed XCS formulation is therefore known as a Simple H-XCS (SH-XCS),. The standard XCS implementation was modified so that an array of populations is
20874	20877	reported here we choose u to be the tricube kernel  which gives decreasing weight to distant neighbours. Holmes & Adams implement an efficient reversible jump Markov chain Monte Carlo (RJMCMC)  scheme to draw samples ? (t) = {k (t) , ? (t) } from the posterior distribution of the parameters p(? | D). Uncertainty in k and ? when classifying x can then be taken into account by averaging
20874	20879	0.1 0.05 0 0 ? 1 0.5 1 0 0.2 0.4 0.6 ? 2 0.8 1sflexibility in the variable metric method clearly permits better classification, achieving rates at least has high as those reported elsewhere, e.g., . 4 Conclusion We have presented a straightforward scheme for learning a metric for the probabilistic k-nn classifier. Results on synthetic data and real data show that the variable metric method
20881	20886	knowledge, while keeping in sight the main goal of data mining: the discovery of new information. This new approach consists on the use of a constraint relaxation (expressed as a formal language  ) to guide the mining process, allowing the user to choose the strength of the restriction he wants to impose in the pattern mining process. This relaxation is just an approximation
20881	20886	the discovered patterns. The use of constraints also reduces the search space, which contributes significantly to achieve better performance and scalability levels (,  and ). 2.2.1 Algorithms with Constraints SPIRIT  is a family of apriori-based algorithms that uses a regular language to constrain the mining process. The core of the algorithm is
20881	20886	existing knowledge and the discovery of unknown information? We argue that the answer to this challenge is the use of constraint relaxations. The notion of constraint relaxation was introduced in , to improve the performance of SPIRIT algorithm, and consists of a weaker constraint. If those relaxations are used to mine new patterns, instead of simply being used for filtering the patterns
20881	338	sequence that is frequent. 2.2 Algorithms There are two main approaches to the sequential pattern-mining problem: apriori-based and pattern-growth methods, with GSP  and PrefixSpan  their best-known implementations, respectively. Despite there are several implementations of apriori-based methods, most of them assume some specific situations (for example that the entire
20881	338	deal with sequential pattern mining problems. The key idea is to avoid the candidate generation step altogether, and to focus the search on a restricted portion of the initial database. PrefixSpan  is the most promising of the pattern-growth methods and is based on recursively constructing the patterns, and simultaneously, restricting the search to projected databases. An ?-projected
8921790	22218	and lexicons should not need to include recursion of entries.” To accomodate entries like pantalun, with its subentry manca-pantalun, the Document Type Definition given by Bell and Bird  does define Lexeme recursively. However it allows subentries only at the end of an entry. 16 It does not permit a subentry under a particular sense, so does not accomodate many of the subentries in
8832677	20901	will add them to the simulation when they are selected. 6 PRELIMINARY USES Portions of the USAR simulation have already been used in a series of teleoperation experiments involving camera control (Hughes et al. 2003) and gravity referenced attitude displays (Lewis et al. 2003). The full USAR simulation was publicly demonstrated at the First Robocup American Wang, Lewis, and Gennari Open held at Carnegie Mellon
8832677	20905	USES Portions of the USAR simulation have already been used in a series of teleoperation experiments involving camera control (Hughes et al. 2003) and gravity referenced attitude displays (Lewis et al. 2003). The full USAR simulation was publicly demonstrated at the First Robocup American Wang, Lewis, and Gennari Open held at Carnegie Mellon University April 30- May 4, 2003. In conjunction with
20910	20917	explicit quantum nature of systems © 2001 C. Roy Keys Inc.sApeiron, Vol. 8, No. 4, October 2001 55 via the use of EPR nonlocality, since this nonlocality cannot be duplicated by a classical system . The researchers had noted that the nonscientific literature was replete with instances in which illness or trauma in one of a pair of identical twins affects the other, even when they are far
20910	20927	factors and empathy. Analysis of both the Duane-Behrendt and Grinberg-Zylberbaum research, appears to reveal that controllable biological quantum nonlocality may have been achieved at that time . 4. University of Washington NIH Study A research project commenced last year at Bastyr University and the University of Washington, under a 2 year grant from the NIH, to replicate the
20910	20940	by the nonstimulated subject, while both of them are in Faraday chambers. c. The ‘binding’ problem, or how it is that the brain can fuse together the many disparate features of a complex perception . I.e., what mechanism transforms the firing of neurons in numerous areas of the brain into a unified experience? In this instance we could be dealing with a large number of neurons which are
20910	20941	problem are inextricably linked, since any solely mental stimulus would have to be nonlocal. I have also attempted to deal with this matter of quantum coherence at the individual neuronal level . d. The problem concerning ‘transference of conscious subjective experience’, and whether this can be transferred from the photostimulated subject having this experience, to a nonstimulated subject
20910	20943	subject being awake in one experiment and under general anesthesia in another. The same rationale applies to the ‘hard’ problem, or that of understanding the nature of the conscious experience . 7. Transcranial Magnetic Stimulation (TMS) in lieu of patterned photostimulation In addition to the use of patterned photostimulation, I have also suggested that TMS might be used . This is a
20951	20952	algorithms are used in case of heterogeneous links. It has been shown that maximum throughput is achieved by striping data over each channel in proportion to the channel’s bandwidth-delay product . More recent research has explored adaptive inverse multiplexing for CDPD wireless networks . In this scheme the packets are split into fragments of size proportional to the observed throughput
20951	6519	reordering might not reduce multimedia application performance noticeably, it can complicate TCP RTT computation and decrease TCP throughput. Packet reordering is not uncommon in today’s Internet , and in the event that reordering becomes significant, there are approaches that can mitigate performance degradation . Another key issue in our overall system design is the identification of
20951	20955	TCP throughput. Packet reordering is not uncommon in today’s Internet , and in the event that reordering becomes significant, there are approaches that can mitigate performance degradation . Another key issue in our overall system design is the identification of the preferred protocol layer for the multiplexing function. Since IP performs routing and multiplexing, it is natural to
20951	20957	across multiple, parallel communication channels is a conventional communications technique used to improve system performance or reliability in relatively staticallyconfigured disk storage systems  and fixed, wired LAN– WAN interconnection systems . In stark contrast, due to end-device heterogeneity, mobility, and time-varying link transmission characteristics, the system we
20951	20958	multiple links into a high capacity bundle. Our goal of cooperation and resource aggregation among collaborating devices is similar to the vision of the mobile grouped devices (MOPED) architecture . The goal of MOPED project is to enable group mobility such that a user’s set of personal devices appear as a single mobile entity connected to the Internet. The MOPED routing architecture builds a
20951	19133	across multiple, parallel communication channels is a conventional communications technique used to improve system performance or reliability in relatively staticallyconfigured disk storage systems  and fixed, wired LAN– WAN interconnection systems . In stark contrast, due to end-device heterogeneity, mobility, and time-varying link transmission characteristics, the system we
20951	20963	case the endpoints of the WAN connections forming the virtual link are the same. The bandwidth of mobile users with multiple interfaces is aggregated at the transport layer in pTCP (parallel TCP) . pTCP is a wrapper that interacts with a modified TCP called TCP-virtual (TCP-v). A TCP-v connection is established for each interface, and pTCP manages send buffers across the TCP-v pipes. The
20951	20965	multiple links into a high capacity bundle. Our goal of cooperation and resource aggregation among collaborating devices is similar to the vision of the mobile grouped devices (MOPED) architecture . The goal of MOPED project is to enable group mobility such that a user’s set of personal devices appear as a single mobile entity connected to the Internet. The MOPED routing architecture builds a
20951	20966	or against other application flows (intercharacterization). Examples of the former include Multiple Description video Coding (MDC)  and the imprecise computation model  that is widely used in the real-time computing community. That is, an application flow has multiple representations or versions expressing different degrees of satisfaction (being
20951	20966	chk’s bandwidths assigned to sf i 1 and sf i 2. Also, if there is not enough bandwidth available, the least important subflows are not transported at all, realizing a form of imprecise computation . The actual number of channels to be allocated for each subflow are determined by the subflow’s requirements of delivery delay or bandwidth. For example, one can compute the bandwidth and delay
20951	20971	reflect the bandwidth-delay product. Coordinating communications from multiple mobile computing devices has become a new focus of interest. Network connection sharing has been proposed in . This architecture permits use of a single, idle WAN connection among collaborating mobile devices but it does not address aggregation of multiple links into a high capacity bundle. Our goal of
20951	20972	and, instead, uses an approximation of bit-WRR, there is frequent packet misordering if the link bandwidths vary greatly. The effect of link bandwidth disparity in TCP throughput is explored in . Several techniques as weighted packet fragmentation  and multiple parallel TCP connections  can be adopted to address this problem. B. CBR Media Traffic over UDP Many media applications
20951	20972	algorithms are used in case of heterogeneous links. It has been shown that maximum throughput is achieved by striping data over each channel in proportion to the channel’s bandwidth-delay product . More recent research has explored adaptive inverse multiplexing for CDPD wireless networks . In this scheme the packets are split into fragments of size proportional to the observed throughput
20951	5395	CBR traffic carried over UDP. We studied the loss and jitter observed for a 920(=8×115)kb/s CBR stream from a video source to a piconet destination. The RTP delay jitter as described in RFC 1889  was measured at the receiver. The topology used for this set of experiments was the same as the one for the TCP throughput experiments. 7sTABLE II CBR LOSS RATE (%) AS A FUNCTION OF PICONET SIZE. #
20951	5395	media file to the receiver at one of various bit rates (64kb/s, 128kb/s, 175kb/s, and 256kb/s). Chariot generates a traffic pattern intended to resemble the video transmission of Cisco’s IPTV. RTP  is used as the stream transport protocol. Each experiment was repeated 25 times, measuring the loss rate and RTP delay jitter observed by the receiver. Without channel aggregation the receiver can
20951	20974	is frequent packet misordering if the link bandwidths vary greatly. The effect of link bandwidth disparity in TCP throughput is explored in . Several techniques as weighted packet fragmentation  and multiple parallel TCP connections  can be adopted to address this problem. B. CBR Media Traffic over UDP Many media applications generate CBR traffic carried over UDP. We studied the loss
20951	20974	computed value for RTT, directly decreasing throughput. With unequal link latencies, round-robin is not the optimal scheduling algorithm. Weighted fair queuing techniques such as that proposed in  can reduce packet misordering and hence improve TCP throughput. We now measure the TCP throughput in a highly dynamic piconet where channels are added and removed from the resource pool. The
20951	20974	by striping data over each channel in proportion to the channel’s bandwidth-delay product . More recent research has explored adaptive inverse multiplexing for CDPD wireless networks . In this scheme the packets are split into fragments of size proportional to the observed throughput of component links. Here the goal is to create variable fragments sizes such that each fragment
20951	20976	flow can be described by itself (intracharacterization) or against other application flows (intercharacterization). Examples of the former include Multiple Description video Coding (MDC)  and the imprecise computation model  that is widely used in the real-time computing community. That is, an application flow has multiple representations or versions expressing different degrees
20951	20976	the subflow priority. The I-frame subflow (sfI) is sent over the most reliable channels, and so on. • Independent-Path Striping (IPS): This algorithm is well suited to multiple state video coding , where a stream is encoded into multiple independently decodeable subflows. Moreover, information from one subflow can be used to correct the errors in another subflow. Hence, it is important for a
20980	10690	retrieval include indexing textual documents without performing OCR, just by basing the indexing on similarities or signatures of the connected components , query on the layout structure , or various information spotting techniques . In our case, we are interested in graphics-rich documents, typically technical documentation containing text, but also a lot of graphics. In
20980	20988	vectorization . There is also a large number of methods dedicated to the recognition of a given class of symbols , in areas such as architecture , mathematics , diagrams and schematics , maps , etc. However, there are a growing number of applications where the need is not for conversion, but for efficient integration of heterogeneous
20980	1199	of the challenges just described. Signatures are indeed often used for indexing and retrieval purposes; however, most work has concentrated on text-based signatures  or image-based signatures . We think that there is also room for graphics-specific signatures, to achieve an efficient localization and recognition of symbols. We currently work in two directions: Quick and robust symbol
20980	1199	what are the most appropriate means of finding information in a reliable and computationally acceptable manner? The main efforts during the last decade or so have been directed toward “indexing”  (of which the mentioned signature methods form a direct spinoff). The word indexing itself has to be used with care, since it bearssquite different meanings, according to the research community.
20980	20993	been to retrieve the original characters, words, sentences or structures of the document. In graphics-rich documents, this has led to work on raster-to-vector conversion, also called vectorization . There is also a large number of methods dedicated to the recognition of a given class of symbols , in areas such as architecture , mathematics , diagrams and schematics ,
20980	20995	framework of technical documents . The approach builds on all the spatial relations which can be found between segments, following an idea initially used in computer vision by Etemadi et al. . The main relations used are parallelism, connectivity and overlapping. Ultimately, our idea is that signatures can be used for symbol spotting and identification of broad hypotheses. This will
20980	20996	application domain, has also become a hot research topic. Müller and Rigoll propose a statistical framework for graphical retrieval in a database of engineering drawings . Fränti et al.  use the Hough transform to retrieve graphical parts from images. We believe that there are plenty of specific research subjects in this general area, when we want to use graphics recognition
20980	20998	use signatures computed directly on these vector data. Such signatures can be computed from various features, in the same way as Huang uses various graphical primitives to index a set of images . One of Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE the few works we are aware of for vectorial data is
20980	21004	signature and metric are usually thoroughly studied and documented, the context is most often embedded in the selection of the test images, the application domain and the used algorithms themselves . When dealing with the problem of retrieving semantics from a set of random documents, the previous relationship proves to be non reversible: Given a semantic concept, one may list a number of
20980	21004	connection between indexes and graphic components, connectivity of graphic components, etc.) that obviously make this initial work a hinder for full generalization. As we already mentioned in , the main drawback is not so much the quality of the image, or the appropriateness of the used algorithms, but the lack of formalization of the context in which the approach remains valid! 4.2.
20980	21004	the issue of encoding context comes from the observation that document analysis is conducted as a stacked pipeline taking syntax as an input, and producing semantics as an output on each layer . Moreover, we introduced the concept of Component Algebra, where syntax and semantics are simply associated to data, and operators are associated to elementary components transforming data of one
21005	2387	is verbal, we turn now to a brief discussion of the natural language processing in the interface. A more detailed description of how we process the natural language input is discussed elsewhere (Perzanowski et al. 1998). In our interface, users can speak into a wireless microphone. The auditory signal is then processed by a domain-specific grammar and dictionary that we have developed using IBM's ViaVoice speech
21005	2387	With the sensor, the robot is capable of tracking the user's hands and interpreting their motion as vectors or measured distances. The method of obtaining this type of gesture is found in (Perzanowski et al. 1998). However, suffice it to say here, once a gesture is perceived, it is identified as either a continuous or a stationary gesture. Gestures are queued and the most recent gesture from the queue is
21005	2388	therefore, opt for as much detailed information from all our input sources as is possible. Furthermore, we have recently begun to track goal information and whether or not goals have been attained (Perzanowski et al. 1999). A full syntactic and semantic parse of each utterance provides us with a high level of confidence for tracking goals and whether or not they have been obtained. In our system, linguistic
70055	21010	being studied (e.g. all genes within an organism) and an index associating the articles to appropriate genes. Such reference lists are often available from sequence databases such as SWISS-PROT (Bairoch and Apweiler, 1999); genomic databases such as SGD (Cherry et al., 1998), MGD (Blake et al., 2002), FlyBase (Gelbart et al., 1997), and WormBase (Stein et al., 2001); or can be compiled automatically by scanning
70055	21012	of NDPG to bring the content of literature to bear on a bioinformatics problem might include using literature in sequence-based homology modeling or motif finding (MacCallum et al., 2000; Chang et al., 2001). These results suggest the potential for NDPG in the same sorts of analyses across different species. ACKNOWLEDGEMENTS RBA is supported by NIH LM06244, GM61374, NSF DBI-9600637, and a grant from
70055	21016	codes remains an active area with many groups experimenting with different strategies involving manual and computational analysis of literature, sequence, and experimental data (Hill et al., 2001; Hvidsten et al., 2001; Dwight et al., 2002; Raychaudhuri et al., 2002a; Schug et al., 2002; Xie et al., 2002). The online resources for the different organisms rely more heavily on different strategies of annotation.
70055	21022	to successfully identify in that GO branch and organism. Neighbor divergence per gene (NDPG) method The NDPG method used here will be described in detail elsewhere, but can be summarized here (Raychaudhuri, Schütze, and Altman, 2003). Data types: document corpus and reference index. NDPG calculation of a gene group requires a corpus of documents relevant to all genes in the organism, and a reference index indicating the
70055	4834	form—often as full text, and almost always in abstract form (http://www. ncbi.nlm.nih.gov/PubMed/). Our method uses statistical natural language processing (NLP) methods (Manning and Schütze, 1999; Rosenfeld, 2000) to mine the literature and assign a functional coherence score to the group of genes. Our method can be used to do a literature-based evaluation of gene groups produced by analytical algorithms or
8921799	355	in a certain topology. The switching technique is one of the primary aspects that characterizes the architecture of a switch. Wormhole switching, frequently also referred to as wormhole routing , and its variations are widely used in a variety of parallel systems and more recently in system area networks . In wormhole switching, a packet is usually divided into a number of flits (flow
8921799	2082	block access to an output link. The network throughout and latency, however, can be improved by organizing the buffers associated with each physical channel to implement multiple virtual lanes 1 , with each flit carrying a small field indicating the virtual channel to which it belongs. Flits from different packets, as long as they belong to different virtual lanes, may now be interleaved
8921799	2082	in the transmission of IP packets over ATM networks in Internet backbones. The concept of virtual channels in ATM networks is not the same as the concept of virtual channels introduced in . Therefore, in order to avoid confusion over terminology, we use the term virtual lanes in our current context instead of virtual channels. 1s2 competing issues of achieving fairness among the
8921799	2082	period of time. Strict fairness, however, is desirable in a variety of contexts in which virtual lanes are employed in switches: • In systems that use virtual lanes for deadlock avoidance , it is important that data traffic flows from different virtual lanes are treated fairly by the network. Strict fairness, besides being intuitively appealing, can actually improve performance by
8921799	21030	a good performance at the same time. FBRR exhibits sub-optimal delay characteristics, while PBRR has among the worst throughput characteristics of all scheduling strategies for wormhole networks . FCFS achieves low average delays but is fundamentally unfair as given by the widely used measure of relative fairness first proposed in . In this paper, we present a novel scheduling strategy,
8921799	21030	packets that have begun transmission are not available in the virtual lane buffer. PBRR, therefore, has among the worst throughput characteristics of all scheduling strategies for wormhole networks . Low latencies and in addition, throughput characteristics similar to FBRR, may be obtained using Anchored RoundRobin (ARR) , proposed for use in wormhole networks with virtual lanes. ARR
8921799	4082	of all scheduling strategies for wormhole networks . FCFS achieves low average delays but is fundamentally unfair as given by the widely used measure of relative fairness first proposed in . In this paper, we present a novel scheduling strategy, called Anchored Opportunity Queueing (AOQ), that achieves provable fairness as well as a very low latency. 1.1. Motivation The motivation for
8921799	4082	of these have been adopted for use in Internet routers to ensure fairness in the management of best-effort traffic. These include Deficit Round Robin (DRR) , Self-Clocked Fair Queueing (SCFQ) , Worstcase Fair Weighted Fair Queueing (WF 2 Q)  and Smoothed Round Robin (SRR) . These scheduling disciplines differ in the level of fairness guarantees they provide and the ease of
8921799	4082	among all the active virtual lanes. Note that, these rules for assigning the OpportunityCount and the VirtualCount are similar to those used in the SFQ and SCFQ scheduling disciplines . The AOQ scheduler also maintains a linked list, called the ActiveList, of all the active virtual lanes excluding the AnchorVL. This linked list is sorted in the increasing order of the
8921799	4082	OpportunityCount of each virtual lane is now normalized with respect to its weight. 4. Fairness Analysis We use a popular measure of fairness called the Relative Fairness Bound (RFB), first used in  and later in numerous research articles on fair scheduling algorithms. In our evaluation of the AOQ scheduler, we define the RFB in terms of the number of opportunities offered to each virtual
8921799	6277	case the max-min fair share policy is modified such that the shared resource allocated to an entity is normalized by its corresponding weight. 3s4 The Generalized Processor Sharing (GPS) algorithm  satisfies the above classical notion of fairness. Among all requesting entities with equal rights to the resource, GPS serves an infinitesimal amount of the shared resource to each of the
8921799	21036	of best-effort traffic. These include Deficit Round Robin (DRR) , Self-Clocked Fair Queueing (SCFQ) , Worstcase Fair Weighted Fair Queueing (WF 2 Q)  and Smoothed Round Robin (SRR) . These scheduling disciplines differ in the level of fairness guarantees they provide and the ease of implementation. The above fair scheduling strategies designed for use in Internet routers do
8921799	21037	beginning the transmission of another packet. Transmitting multiple flits from a packet also increases the throughput by eliminating the per-flit acknowledgments traveling on the reverse path . The use of packet-based roundrobin (PBRR), however, wastes bandwidth since the physical channel may be idle when all the flits of a packets that have begun transmission are not available in the
8921799	21039	characteristics of all scheduling strategies for wormhole networks . Low latencies and in addition, throughput characteristics similar to FBRR, may be obtained using Anchored RoundRobin (ARR) , proposed for use in wormhole networks with virtual lanes. ARR attempts to achieve the low latency properties of a packet-by-packet transmission strategy by attempting to transmit all the flits of
8921799	21043	by a non-active virtual lane which becomes active after a long time. In order to avoid the complexity of emulating GPS as is done in some fair scheduling algorithms proposed for the Internet , we do not define the VirtualCount function with respect to the GPS system. The VirtualCount function is defined to be equal to the lowest value of the OpportunityCount among all the active virtual
8921803	21057	all have different Quality of Service (QoS) demands, i.e. different demands on delay, packet loss ratio, throughput, etc. The literature on variable data rate for ad hoc networks is limited. In  a rate adaptive MAC protocol called the Receiver-Based AutoRate (RBAR) protocol is presented. The protocol is based on the RTS/CTS mechanism. In  the routing layer uses the channel conditions
8921803	21058	data rate for ad hoc networks is limited. In  a rate adaptive MAC protocol called the Receiver-Based AutoRate (RBAR) protocol is presented. The protocol is based on the RTS/CTS mechanism. In  the routing layer uses the channel conditions estimated at the receiver for optimal route selection. The modifications in this study are made on the IEEE 802.11 and the dynamic source routing (DSR)
8921803	21060	SNR and data rates used in our model correspond to an information block size, Ps, of 256 bits at a packet error probability of 10 ?4 , and bandwidth of 10 MHz, see Table I. This information is from . Since information about the lower data rates are missing, we had to do extrapolation to find these values. B. Data Link Layer CSMA is one of the most frequently used MAC protocols in ad hoc
21068	11171	as the data appear in the stream and use the summary data structure to directly answer queries quickly in an approximate manner. The 4sconstraints that are imposed by streams (see Babu and Widom ) make techniques for stream computations suitable for sensors. These summary data–structures are typically computed as decomposable aggregates, i.e. they can be expressed as an aggregation function
21068	21069	for sensor networks. Here the idea is to systematically gather the sensed data from each sensor, so that it is eventually transmitted to the base–station for further processing. Chang and Tassiulas  give a flow– based approach to gather data in an optimal manner. However this is very expensive in terms of the energy consumed by the sensor network because of the size of the raw data that must
21068	21069	its value to the base station at the end of each round. The values recorded by each sensor are simply forwarded without any data aggregation. We use the flow–based algorithm by Chang and Tassiulas  for routing. We assume that the data is 2 bytes and the header is 8 bytes. Therefore the packet size is 10 bytes. A complete histogram is computed at the base–station from the raw data collected
21068	21069	are 101, 354 and 387 respectively, therefore we use ValQFixed(10), ValQFixed(35) and ValQFixed(39) for the three datasets respectively. For the NoA method we use the approach by Chang and Tassiulas  to compute the lifetimes. This approach gives exact results, therefore, the MSE, REL and REN are always zero for this approach. For the NA and NoA approaches, the errors are always 0. For the
21068	21070	for sensor networks. Here the idea is to systematically gather the sensed data from each sensor, so that it is eventually transmitted to the base–station for further processing. Chang and Tassiulas  give a flow– based approach to gather data in an optimal manner. However this is very expensive in terms of the energy consumed by the sensor network because of the size of the raw data that must
21068	21070	are 101, 354 and 387 respectively, therefore we use ValQFixed(10), ValQFixed(35) and ValQFixed(39) for the three datasets respectively. For the NoA method we use the approach by Chang and Tassiulas  to compute the lifetimes. This approach gives exact results, therefore, the MSE, REL and REN are always zero for this approach. For the NA and NoA approaches, the errors are always 0. For the
21068	21071	range queries. Guha et al.  compute wavelets over streams for constructing histograms. In the sensors domain, special purpose aggregates have been proposed. For example, Cosidine et. al.  present approximate sketches for COUNT, SUM and AVG that are robust with respect to node failures,etc. However, these aggregates are over all the sensors. Hellerstein et al.  argue that
21068	21072	querying of particular sensors. To overcome these drawbacks, we propose an alternate approach that is inspired by OnLine Analytical Processing (OLAP) and is the approach taken in the AQUA project  for query processing over data streams. The idea is to maintain and update a small space summary data structure as the data appear in the stream and use the summary data structure to directly
21068	21073	that the synopses data structure to be computed depends on the application at hand and in our case, the kind of queries we wish to answer. Several data structures (see for e.g. Gibbons and Matias ) have been proposed in literature. For example, small space samples and histograms are popularly used for answering selectivity and aggregate range queries and to compute the size of joins and
21068	10128	has emerged into a core service supported in sensor networks (for e.g. TAG by Madden et al. ). Several studies have been carried out toward performing in–network aggregation in sensor networks . The main focus here is to come up with good data aggregation trees along which in–network aggregation can be carried out. In Pegasis , sensors form chains so that each sensor transmits and
21068	1530702	Cosidine et. al.  present approximate sketches for COUNT, SUM and AVG that are robust with respect to node failures,etc. However, these aggregates are over all the sensors. Hellerstein et al.  argue that monitoring applications demand more sophisticated aggregate query processing over sensor networks. They compute wavelets over the sensor data that can be used to answer approximate
21068	1530702	know before hand which coefficients to maintain. Therefore one has to estimate the top–k Haar coefficients using in–network aggregation (for e.g. by using approaches suggested by Hellerstein et al.  or Guha et al. ). In the experimental results we present only results using the true top–k DWT coefficients. (The results with the estimated top–k DWT coefficients are much worse and are 12snot
21068	4875	has emerged into a core service supported in sensor networks (for e.g. TAG by Madden et al. ). Several studies have been carried out toward performing in–network aggregation in sensor networks . The main focus here is to come up with good data aggregation trees along which in–network aggregation can be carried out. In Pegasis , sensors form chains so that each sensor transmits and
21068	21075	Next we describe the data–aggregation based approaches we use for estimating queries. For evaluating the lifetimes using these approaches we use data–aggregation techniques by Kalpakis et al . For experiments with small number of sensors (Field 1) we use the MLDA  approach that gives near optimal lifetimes for data aggregation. For larger sensor networks (Field 2), we use the A–MLDA
21068	21078	of the sensors to save on the communication costs. In–network aggregation of data has been shown to dramatically increase the lifetimes of sensor networks (see for e.g.Krishnamachari et al. ). For this reason, data fusion or aggregation has emerged into a core service supported in sensor networks (for e.g. TAG by Madden et al. ). Several studies have been carried out toward
21068	21080	has emerged into a core service supported in sensor networks (for e.g. TAG by Madden et al. ). Several studies have been carried out toward performing in–network aggregation in sensor networks . The main focus here is to come up with good data aggregation trees along which in–network aggregation can be carried out. In Pegasis , sensors form chains so that each sensor transmits and
21068	21080	and eventually reaches the base station. Nodes take turns to transmit to reduce the average energy spent by any single sensor, which leads to different chains in different rounds. Lindsey et al.  propose a hierarchical scheme based on Pegasis for data–aggregation. Kalpakis et al.  propose the Maximum 3sLifetime Data Aggregation  algorithm for data aggregation that gives a near
21068	21080	lifetime by intelligently selecting aggregation trees from a given candidate set of trees. For the candidate set of trees they use the randomly mutated trees produced from the LRS trees from . 1.1 Query Processing in Sensor Networks From databases perspective, in–network aggregation is a mode for evaluating aggregate queries over the sensor network. Sensor network applications rely
21068	21081	it is often not the individual raw data from the sensors that matters to most applications, but certain information in the form of summaries or aggregations of the data that is of more relevance . Data fusion or aggregation allows for in–network aggregation of data, i.e. combining of packets from different sensors while enroute to the base–station. Therefore rather than the traditional
21068	7748	Queries are also considered a natural way for users to interact with the sensor network . Therefore, several studies have been carried toward (aggregate) query processing in sensor networks . The common approach to query processing in sensor networks has two phases. Users specify queries at the base–station using an SQL–like query language. During the dissemination phase the query
21068	5749	of sensor networks (see for e.g.Krishnamachari et al. ). For this reason, data fusion or aggregation has emerged into a core service supported in sensor networks (for e.g. TAG by Madden et al. ). Several studies have been carried out toward performing in–network aggregation in sensor networks . The main focus here is to come up with good data aggregation trees
21068	5749	Queries are also considered a natural way for users to interact with the sensor network . Therefore, several studies have been carried toward (aggregate) query processing in sensor networks . The common approach to query processing in sensor networks has two phases. Users specify queries at the base–station using an SQL–like query language. During the dissemination phase the query
21068	21083	aggregate queries over the sensor network. Sensor network applications rely heavily on aggregate queries. Queries are also considered a natural way for users to interact with the sensor network . Therefore, several studies have been carried toward (aggregate) query processing in sensor networks . The common approach to query processing in sensor networks has two phases. Users
21100	8177	Other semistructured databases. The UnQL query language  is based on a graph-structured data model similar to OEM. For query optimization, a translation from UnQL to UnCAL is defined , which provides a formal basis for deriving optimization rewrite rules such as pushing selections down. However, UnQL does not have a cost-based optimizer as far as we know. The Strudel Web-site
21100	8177	to the current specification of XML. XQL  is a simpler query language based on single path expressions and is strictly less powerful than XML-QL, Lorel , StruQL , or UnQL . To the best of our knowledge no full cost-based query optimizer has been developed for XMLQL or XQL, and the optimization principles presented in this paper should be directly applicable when that
21100	21103	in Lore and preliminary performance results are reported. 1 Introduction The World-Wide Web community is rapidly embracing XML as a new standard for data representation and exchange on the Web . At its most basic level, XML is a document markup language permitting tagged text (elements), element nesting, and element references. However, XML also can be viewed as a data modeling language,
21100	21103	has been defined as data that may be irregular or incomplete, and whose structure may change rapidly or unpredictably. Although data encoded in XML may conform to a Document Type Definition, orDTD , DTD’s are not required by XML. Due to the nature of information on the Web and the inherent flexibility of XML—with or without DTD’s—we expect that much of the data encoded in XML will exhibit the
21100	21103	252 Figure 1: A tiny OEM database 317 Room &5 Title Project Project &6 &16 Title &quot;Tsimmis&quot; incoming edges) must be specified in XML with explicit references, i.e., via ID and IDREF(S) attributes . The following XML fragment corresponds to the rightmost Member in Figure 1, where Project is an attribute of type IDREFS. <Member Project=&quot;&5 &6&quot;> <Name>Jones</Name> <Age>46</Age> <Office>
21100	21107	discussed above, has considered the problem of optimizing the evaluation of generalized path expressions, which describe traversals through data and may contain regular expression operators. In  an algebraic framework for the optimization of generalized path expressions in an OODBMS is proposed, including an approach that avoids exponential blow-up in the query optimizer while still
21100	21108	extensions specific to the current specification of XML. XQL  is a simpler query language based on single path expressions and is strictly less powerful than XML-QL, Lorel , StruQL , or UnQL . To the best of our knowledge no full cost-based query optimizer has been developed for XMLQL or XQL, and the optimization principles presented in this paper should be directly
21100	21110	path expressions in an OODBMS is proposed, including an approach that avoids exponential blow-up in the query optimizer while still offering flexibility in the ordering of operations. In  two optimization techniques for generalized path expressions are presented, query pruning and query rewriting using state extents. Lore’s techniques for handling generalized path expressions are
21100	21113	databases. Many of the points discussed in the previous paragraph apply to object-oriented databases as well. There has been some work on optimizing path expressions in an OODBMS context . They propose a set of algorithms to search for objects satisfying path expressions containing predicates, and analyze their relative performance. Our work differs in that we consider many
21100	21114	for declarative navigation and updates of semistructured databases. Recently we migrated Lore to a fully XML-based data model, and extended the Lorel query language accordingly. For details see . The results presented in this paper apply directly to the new XML version of Lore. This paper describes Lore’s query processor, with a focus on its cost-based query optimizer. While our general
21100	21114	<Building>Gates</Building> <Room>252</Room> </Office> </Member> As mentioned earlier, we have migrated Lore to a fully XML-based data model, and extended the Lorel query language accordingly . The primary changes to the model were the introduction of ordered subobjects, attribute-value lists, and reference edges in addition to normal subobject edges. Corresponding changes were made to
21100	7740	the query execution engine and perform the low-level steps required to execute the query and construct the result. We use a recursive iterator approach in query processing, as described in, e.g., , and we assume the reader is familiar with the basic concepts associated with iterators. 3.4 Lore Indexes As in a conventional DBMS, indexes in Lore enable fast and efficient access to the data. In
21100	21115	an edge with a given label. The edge index, whichwe term the Bindex, supports finding all parent-child object pairs connected via a specified label. In addition to these indexes, Lore’s DataGuide  provides the functionality of a path index, or Pindex. Details on Lore indexes, including coercion issues addressed by the Vindex, can be found in . 4 Motivation As in any declarative
21100	21115	operator is implemented in Lore by the link index (Section 3.4). 3. Pindex(PathExpression, x): Lore maintains a dynamic “structural summary” of the current database called a DataGuide . The DataGuide also can be used as a path index, enabling quick retrieval of oid’s for all objects reachable via a given path expression. The Pindex operator places into x all objects reachable via
21100	21115	cost of a physical query plan. Initially we stored statistics in the DataGuide, but quickly were limited by the fact that we could only store statistics about paths beginning from a named object . The optimizer may choose to begin evaluating a path expression anywhere within the path (via the Bindex or Vindex operator), so we needed more flexible statistics. Our new approach is to store
21100	21121	techniques for generalized path expressions are presented, query pruning and query rewriting using state extents. Lore’s techniques for handling generalized path expressions are described in , but the work of  coulds&2 &7 &quot;Clark&quot; Name Member Name Age &3 Office Office Member Name &4 DBGroup Member &8 &9 &10 &11 &12 &13 &14 &15 &1 Age Office Member Project Project
21100	21122	quite different from the case where y was bound by following subobjects from x. Statistics are discussed further in Section 5.3. A list and description of most of our logical operators is given in . Here we will focus on the Discover and Chainlogical operators used for path expressions. Each simple path expression in the query is represented as a Discover node, which indicates that in some
21100	21122	limitations preclude a full description of logical query plans or examples of more complex queries, but the general flavor and flexibility of our approach should be evident. For details please see . 5.2 Physical Query Plans The number of physical query plan operators is large; a list and description of the more common operators appears in . Here we focus on some of the more interesting
21100	21122	operation over a subplan, ensure the existential and universal quantification of a variable, perform set and arithmetic operations between two subplans, and others. For details please see . The physical operators can be combined in numerous ways, producing a vast search space for even relatively simple queries. Oursplan enumeration and pruning heuristics will be discussed in Section
21100	21122	our cost-based optimizer as shown by the performance results in Section 6. Due to space constraints, our formulas for estimated I/O and CPU cost and number of evaluations are omitted but appear in . As an example, consider the I/O formula for the Vindex(Op, Value, l, x) operator: BLevel l;type1+ Selectivity1(l, Op, Value) +BLevel l;type2+ Selectivity2(l, Op, Value). Here BLevel gives the
21100	21122	joining two simple path expressions together unless they share a common variable. This restriction substantially reduces the number of ways to order the evaluation of simple path expressions. (See  for a detailed discussion.) The Pindex operator is considered only when a path expression begins with a name, and no variable except the last is used elsewhere within the query. The latter
21100	21126	optimization principles presented in this paper should be directly applicable when that task is tackled. 3 Preliminaries 3.1 Data Model Lore’s original data model, OEM (for Object Exchange Model) , was designed for semistructured data. An example OEM database containing (fictitious) information about the Stanford Database Group appears in Figure 1. Data in OEM is schema-less and
8921810	22518	work around the political conditions facing them. A more systematic attempt to assess the level and nature of group activity in NRM, under a range of political conditions, is needed. Poffenberger's (1993) study describing the political conditions which have led to spontaneous activity by forest user groups in the Indian highlands provides a good example of this type of assessment. Selecting key
6949054	21172	In this simplified scenario, segmentation into blocks is not necessary as the user is expected to be able to differentiate between them. Segmentation into text lines was presented previously (see ) and will be mentioned only briefly in this paper. We will concentrate on the separation of the different components of the date provided that the user has specified a start and end point for the
6949054	21172	kind of boundary g, a probability distribution curve is generated. The position of a training boundary is normalised with respect to the width of the date and it ranges therefore in the interval . The probability distribution curve which results from the samples is smoothed by a gaussian function. The variance decreases with an increasing number of samples (see Equation (3)). An appropriate
6949054	21173	assumed in  did not hold in our case. For word boundaries, we have a similar situation. Word boundary detection schemes that rely on the predictions from the recognised characters itself  are not applicable as this information does not exist. Approaches which evaluate gap sizes for differentiating between word boundaries and character boundaries such as  are more
6949054	21177	characters itself  are not applicable as this information does not exist. Approaches which evaluate gap sizes for differentiating between word boundaries and character boundaries such as  are more appropriate as they do not require text recognition. However, in old church registers, adjacent words may by connected. Thus, we have to expect word gaps within text objects as well as
6949054	21177	Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE The search of boundaries between words will be made by analysing local features of potential boundaries (as e. g. ). Additionally, we assume that some strokes within a script object may constitute a word boundary between two objects being connected. As this decreases the performance of segmentation, we use
6949054	21177	word boundary combinations being included in the best four hypotheses (see Fig. 4). This is in the range of results of techniques which require gaps for detecting word boundaries (e. g. 87.6% in , 95.6% in ). In case of using the local features only, we obtain 88%, while the processing of the distribution curve without the local features results in 69%.sA fast automatic evaluation of the
6949054	4818	characters itself  are not applicable as this information does not exist. Approaches which evaluate gap sizes for differentiating between word boundaries and character boundaries such as  are more appropriate as they do not require text recognition. However, in old church registers, adjacent words may by connected. Thus, we have to expect word gaps within text objects as well as
6949054	4818	Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE The search of boundaries between words will be made by analysing local features of potential boundaries (as e. g. ). Additionally, we assume that some strokes within a script object may constitute a word boundary between two objects being connected. As this decreases the performance of segmentation, we use
6949054	4818	combinations being included in the best four hypotheses (see Fig. 4). This is in the range of results of techniques which require gaps for detecting word boundaries (e. g. 87.6% in , 95.6% in ). In case of using the local features only, we obtain 88%, while the processing of the distribution curve without the local features results in 69%.sA fast automatic evaluation of the tests was
6949054	21178	characters itself  are not applicable as this information does not exist. Approaches which evaluate gap sizes for differentiating between word boundaries and character boundaries such as  are more appropriate as they do not require text recognition. However, in old church registers, adjacent words may by connected. Thus, we have to expect word gaps within text objects as well as
6949054	21178	above, the shape of the script was changed. An additional source of error was punctuation which was not removed due to its size. A more elaborated removal technique such as the one applied in  might solve this problem. correct hypotheses correct hypotheses 180 180 160 140 120 100 80 60 40 20 0 0 51.7% (154) 28.9% (86) 14.4% (43) 2.3% (7) 0.0% (0) 0.7% (2) 0.3% (1) 0.0% (0) 0.0% (0) 0.0%
4213209	21191	the row template. 2.2. Creating Finite State Automata . 1 The token includes the separator right after it except for ”<”, which is included into the token behind it 6sA grammar induction algorithm  is applied to find the repeated data that may correspond to rows. The entire sequence of tokens in the web page is viewed as a string in a language generated by a regular grammar, and the goal is
4213209	21193	with a smaller number of pages, even a single page, thus overcoming the potential problem that the number of similar pages available on a particular site at any given time is often quite limited . ??? Unlike , our approach does not require the effort of manually identifying similar web pages as training examples. ? The efficiency is improved by ignoring most of the unrelated web page data
4213209	21196	several similar web pages to generate the page template in the first step; the general algorithms used, such as DataPro  for learning data pattern and AutoClass for unsupervised classification , are computationally intensive, a serious problem in Web applications. Our work aims to improve the performance of information extraction from lists and tables in the web page. Because
4213209	21198	replace, most web clients rely on existing information extraction techniques, typically Web Wrappers . A wrapper is a program that enables a Web source to be queried as if it were a database . Extraction rules used by the wrapper to identify the beginning and end of the data field to be extracted, form an important part of the wrapper. Quick and efficient generation of extraction rules,
4213209	21199	the wrapper. Quick and efficient generation of extraction rules, so called Wrapper Induction, has been an active area of research in recent years . The first wrapper induction system, WIEN  is a supervised learning agent, i.e. it requires manually labelled examples with output information, to learn patterns. A recent wrapper induction algorithm, STALKER, generates high accuracy
4213209	21200	field to be extracted, form an important part of the wrapper. Quick and efficient generation of extraction rules, so called Wrapper Induction, has been an active area of research in recent years . The first wrapper induction system, WIEN  is a supervised learning agent, i.e. it requires manually labelled examples with output information, to learn patterns. A recent wrapper induction
4213209	21201	difficult to extract information by HTML parsing. Until more structured representations replace, most web clients rely on existing information extraction techniques, typically Web Wrappers . A wrapper is a program that enables a Web source to be queried as if it were a database . Extraction rules used by the wrapper to identify the beginning and end of the data field to be
4213209	21201	of sources  than previous systems, but it still requires manually labelled examples. To overcome the shortcomings of supervised learning, attention is shifting towards unsupervised learning , which needs no manually labelled input. That work proposes a suite of unsupervised learning algorithms, which induce the structure of lists by exploiting the regularities both in the 1sFigure 1.
4213209	21201	to build the finite state automaton of the web page. States that represent the same data contents are then merged, and cycles are generated. The longest cycles that correspond to rows are selected  That system was tested on 14 typical examples with about 70% accuracy. It is undoubtedly a significant effort, but the accuracy still needs further improvement. Moreover, it requires several
4213209	21201	adopted by the authors is that a dynamic web page including lists and tables is generated by a template, which describes the format of each data field and the visual layout of the whole page. The server-side program fills the template with results of a database query submitted by a Web client (browser). Thus, template extraction is a necessary step in this process. To extract the
4213209	21201	from the same source. Several similar web pages that are usually generated by the same server-side program are required to extract common information among them, in the form of a Page Template . DEFINITION 2.1 (Page template). A set of strings that the web server uses to automatically generate pages and fill them with the results of database query. Since the objective of this work is to
4213209	21202	1 are shown in Table I. They are separated into 2 groups corresponding to the value of N, i.e., 12 and 13. Then we calculate L(N) by summing up the length len of cycle sets in each group. Thus, L(13) = 2+5+3+9+4+6+3+3 and L(12) = 9+8+7+...+16+15+14. One or more groups are expected to stand out. An empirical observation related to the number of repeated tokens is that, for a table with n rows,
4213209	21205	to learn patterns. A recent wrapper induction algorithm, STALKER, generates high accuracy extraction rules that accept all positive and reject all negative user-labelled training examples , and extracts data that complies with these rules with about 80% accuracy on test examples. STALKER uses wildcards and disjunctive rules and therefore has the ability to wrap a larger variety of
4213209	21205	comparison of the published performance of the systems WIEN, STALKER and Lerman’s  with our system is shown in Table VII. The accuracies of WIEN and STALKER are given as reported in . However, our system has only been tested on the same web sites as Lerman’s  since web sites, on which WIEN and STALKER were tested, were not specified in . 22sFigure 15. An example from
4213209	21205	the former. We draw the following conclusion about information extraction from our study: ? User labeling of the training data represents the major bottleneck in using wrapper induction techniques . We demonstrate that user labelling of training examples can be avoided while achieving high accuracy. ? Although some user interaction is required in our system, it is minimal compared with fully
21210	21222	! 1 p(n) where R n denotes the set of all (2 2 `(n) ) Boolean functions over the domain f0; 1g `(n) . 6 Pseudorandom functions exists if and only if there exist one-way functions (cf.,  and ). Pseudorandom functions which can be evaluated by NC circuits (one circuit per each function) exist 7 , assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is
21210	21227	E(x) as well as the entire bsc 1 2 \Gamma c sec 8 (E(x)). 12 We note that Maurer has shown that this version of the problem can be reduced to the original one by using bidirectional communiaction . Cr'epeau (private comm., April 1997) has informed us that, using the techniques in , one may obtain an alternative efficient solution to the original Wire-Tap Channel Problem again
21210	21228	pseudorandom functions . Assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is secure, one can construct pseudorandom functions in NC (cf., ), and so all of our &quot;gap theorems&quot; will follow with concept classes having NC circuits. We next consider classification noise at rate j ! 1 2 . That is, the label of each example is flipped with
21210	21228	functions which can be evaluated by NC circuits (one circuit per each function) exist 7 , assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is secure (cf., ). 2.2 A Probabilistic Coding Scheme We present an efficient probabilistic encoding scheme having constant rate (information/codeword ratio), constant (efficient) error-correction capability for
21210	21229	learning by requiring more data, but do not completely preclude learning. For example, it is NP-hard to find a k-term-DNF formula 2 consistent with a set of data labeled by a k-term-DNF formula . The computationally efficient algorithm most commonly used for learning k-term-DNF works by finding a consistent hypothesis from an hypothesis class (kCNF) which strictly contains the target
8921832	21241	using DIMACS benchmark data . BBDPTHREAD combines a DP (dynamic programming) algorithm and a branch-and-bound procedure, where the DP algorithm is developed based on a former work by the authors . We performed computational experiments on CLIQUETHREAD and BBDPTHREAD in order to evaluate practical computation time and usefulness for improving the accuracy of profile threading. We used a PC
8921832	21008	with profiles is also known as a powerful method for protein structure prediction. In particular, PSI-BLAST is widely used both for sequence similarity search and protein structure prediction . Therefore, we have recently developed two algorithms for computing optimal threadings with profiles and constraints . In this short abstract, we show performances of these two algorithms. 2
21245	800	are grids in which certain additional services are run, including services for parallel TCP striping (GridFTP), and data replication services (Globus Replica Catalog and Globus Replica Management) .sData webs are web based infrastructures for data employing web services to provide access to remote and distributed data . Data webs for working with large data sets also employ specialized
21245	17752	the most common is to provide web front ends to remote file systems, databases, archival systems, and hierarchical storage systems. For a general survey of work in this and related areas, see . Data grids and data webs are two emerging technologies which are complementary to this approach. Data grids are infrastructures which provide authentication, authorization, and access controls
9685	9607	is the development of scalable resource discovery techniques that allow client applications to locate services and devices, in increasingly large-scale environments, using detailed intentional  descriptions. Resource discovery systems should achieve three main goals: (i) handle sophisticated resource descriptions and query patterns; (ii) handle dynamism in the operating environment,
9685	9607	Twine evenly distributes resource information and queries among participating resolvers. Finally, our system efficiently handles both resource and resolver dynamism. Twine is integrated with INS , the Intentional Naming System from MIT and now forms the core of its architecture. Therefore, we refer to INS/Twine nodes as Intentional Name Resolvers (INRs). INS/Twine leverages recent work in
9685	9607	information, and resolving queries. 2.1 Resource Descriptions Resources in INS/Twine are described with hierarchies of attribute-value pairs in a convenient language (e.g., XML, INS name-specifiers , etc.). Our approach is to convert any such description into a canonical form: an attribute-value tree (AVTree). Figure 1 shows an example of a very simple resource description and its AVTree. All
9685	9607	and values being sought. The ultimate results of query matching depend on the local query processing engine attached to each resolver. Examples of this include INS’s subtree-matching algorithm , UnQL  or the XSet query engine for XML . Since query routing relies on exact matches of both attributes and values, it is possible to allow more flexible queries by separating string values
9685	9607	In that case, a single answer is returned. It is the resource that matches the given description and that has the lowest application-level metric. This feature comes from the original INS design . Splitting resource descriptions into strands is critical to INS/Twine’s ability to scale well. It enables resolvers to specialize in holding information and answering queries about a subset of
9685	9717	infrequent (e.t., DNS , LDAP ). They do not work well when the number of resources grows, and updates are common. Static resource partitioning  and hierarchical organization of resolvers  solve scalable and dynamic resource discovery. Static partitioning relies on some application-defined attribute to divide resource information among resolvers. However, static partitioning does not
9685	15115	differ in the details of how they name resources and how these names resolve to the appropriate network location. However, they all essentially rely on semistructured resource descriptions , attribute-based naming schemes with orthogonal attribute-value bindings, in which some attributes are hierarchically dependent on others.sMany resource discovery schemes have been designed
9685	15115	being sought. The ultimate results of query matching depend on the local query processing engine attached to each resolver. Examples of this include INS’s subtree-matching algorithm , UnQL  or the XSet query engine for XML . Since query routing relies on exact matches of both attributes and values, it is possible to allow more flexible queries by separating string values into
9685	9128	dependent on others.sMany resource discovery schemes have been designed primarily for small networks , or for networks where dynamic updates are relatively uncommon or infrequent (e.t., DNS , LDAP ). They do not work well when the number of resources grows, and updates are common. Static resource partitioning  and hierarchical organization of resolvers  solve scalable
9685	21250	dynamic updates are relatively uncommon or infrequent (e.t., DNS , LDAP ). They do not work well when the number of resources grows, and updates are common. Static resource partitioning  and hierarchical organization of resolvers  solve scalable and dynamic resource discovery. Static partitioning relies on some application-defined attribute to divide resource information
9685	939	resolvers should be contacted to resolve queries or to update resource information. To achieve these goals, INS/Twine relies on an efficient distributed hash table process (such as Chord , CAN  or Pastry ), which it uses as a building block. Twine transforms each resource description into a set of numeric keys. It does so in a way that preserves the expressiveness of semistructured
9685	939	table, where each node on the network keeps key-value bindings within a dynamically determined key range. Several efficient peer-to-peer algorithms have recently been proposed for this purpose: CAN , Chord , or Pastry . Given a key, these systems find the node on the network that should store the corresponding value. Chord and Pastry are based on some variant of consistent hashing
9685	943	subset of resolvers should be contacted to resolve queries or to update resource information. To achieve these goals, INS/Twine relies on an efficient distributed hash table process (such as Chord , CAN  or Pastry ), which it uses as a building block. Twine transforms each resource description into a set of numeric keys. It does so in a way that preserves the expressiveness of
9685	943	each node on the network keeps key-value bindings within a dynamically determined key range. Several efficient peer-to-peer algorithms have recently been proposed for this purpose: CAN , Chord , or Pastry . Given a key, these systems find the node on the network that should store the corresponding value. Chord and Pastry are based on some variant of consistent hashing , where a
9685	943	of fault-tolerance by periodically sending each description not to one, but to k > 1 nodes per strand. This scheme relies on the capability of the underlying distributed hash table process (Chord  in our case) to rebuild the overlay network of interconnections as nodes join and leave. It also relies on the fact thatsR ? INR ? ? INR ? INR INR INR ? remove remove INR INR ? ? INR R Fig. 4.
9685	943	resource present in the network with the same performance as the underlying KeyRouter layer (Chord in our implementation). For all queries, O(log N) resolvers are contacted at the KeyRouter layer  to find the set of nodes associated with a given key. When replication is used, k resolvers then resolve the query in parallel. To evaluate the distribution of queries among resolvers, we used 800
9685	940	be contacted to resolve queries or to update resource information. To achieve these goals, INS/Twine relies on an efficient distributed hash table process (such as Chord , CAN  or Pastry ), which it uses as a building block. Twine transforms each resource description into a set of numeric keys. It does so in a way that preserves the expressiveness of semistructured data, facilitates
9685	940	network keeps key-value bindings within a dynamically determined key range. Several efficient peer-to-peer algorithms have recently been proposed for this purpose: CAN , Chord , or Pastry . Given a key, these systems find the node on the network that should store the corresponding value. Chord and Pastry are based on some variant of consistent hashing , where a nodesis
9685	22625	the Internet and letting everyone announce resources of global interest to users around the world. Such resources may be file servers,scameras showing the weather in different cities, Web services , and so on. Similarly we could imagine deploying INS/Twine within a city and letting users access resources such as air quality sensors, water temperature/quality indicators at public beaches,
9685	19245	, Chord , or Pastry . Given a key, these systems find the node on the network that should store the corresponding value. Chord and Pastry are based on some variant of consistent hashing , where a nodesis responsible for all keys whose identifier falls between the node identifier and the closest preceding node identifier currently on the network. Hence only local disruptions occur
21258	4318	set We consider four broad categories of security functions: (1) public-key algorithms, which are required for key exchanges; (2) signature algorithms, which are required for user authentication; (3) symmetric-key algorithms, which are required to encrypt and decrypt messages for confidentiality; (4) hash functions, which are used to verify the integrity of messages. Table 1 shows which
21258	4318	of this paper are: (1) a selection of cryptography algorithms suitable for constrained environments, (2) a description of the operations used by Elliptic Curve Cryptography algorithms, (3) a characterization of the instructions executed by these algorithms, and (4) demonstration that a simple processor is sufficient. We show the operations and instructions needed by elliptic-curve
21258	21260	= u(x)b(x) for all polynomials u(x) of degree at most 3. 2. c(x) = 0. 3. For k from 15 downto 0 do 3.1. For j from 0 to 3 do Let u = (u3,u2,u1,u0) where ui is bit (4k+i) of a. Add Tu to (c,c,???,c). 3.2. If k != 0 then c(x) = c(x)x 4 . 4. Return c(x). 1 Nomenclature for algorithm descriptions: Polynomials are represented using lower-case letters: a(x), b(x), c(x) etc. When addressing
21258	21260	? b × C Figure 3 Elliptic-curve ElGamal 7. EC-ElGamal, EC-DSA, AES and SHA Elliptic-curve ElGamal (EC-ElGamal) (Figure 3) is the elliptic-curve analog of the integer ElGamal algorithm described in . It is used to securely transmit the coordinates of the point Pm from Alice to Bob (assume that the original plaintext m is embedded in Pm). We assume that Alice and Bob have previously agreed on a
21258	21261	the ability to perform security functions with limited computing resources has become increasingly important. In mobile devices such as personal digital assistants (PDAs) and multimedia cell phones , the processing resources, memory and power are all very limited, but the need for secure transmission of information may increase due to the vulnerability to attackers of the publicly accessible
21258	21261	Tu = u(x)b(x) for all polynomials u(x) of degree at most 3. 2. c(x) = 0. 3. For k from 15 downto 0 do 3.1. For j from 0 to 3 do Let u = (u3,u2,u1,u0) where ui is bit (4k+i) of a. Add Tu to (c,c,…,c). 3.2. If k != 0 then c(x) = c(x)x 4 . 4. Return c(x). 1 Nomenclature for algorithm descriptions: Polynomials are represented using lower-case letters: a(x), b(x), c(x) etc. When
21258	21261	3 + 1. 1. For i from 5 downto 3 do 1.1. t = c. 1.2. c = c ? (t << 29) ? (t << 32) ? (t << 35) ? (t << 36). 1.3. c = c ? (t >> 28) ? (t >> 29) ? (t >> 32) ? (t >> 35). 2. t = c & 0xFFFFFFF800000000. 3. c = c ? (t >> 28) ? (t >> 29) ? (t >> 32) ? (t >> 35). 4. c = c & 0x00000007FFFFFFFF. 5. Return (c,c,c). Algorithm 5 Table lookup method for polynomial
21258	21262	generation and verification, in addition to smaller key storage needs. Even though much literature exists about these algorithms that focus on algorithmic optimizations and hardware implementations , workload characterization studies are very rare. In this paper, we provide a comprehensive workload characterization of security algorithms suitable for constrained environments. We consider
21258	21262	P × u and + AT × u 2 = ( xo , y v = x mod n 1 o 0 4. Accept signature if v = r Figure 4 10 Elliptic-curve Digital Signature Algorithm Detailed descriptions of the AES algorithm can be found in  including some optimizations. We report our findings in workload characterization for two AES implementations. The first is the reference implementation, the second is a table-lookup based
21258	21262	instructions. The shift instructions are used in the table lookups for effective address computations; the logical instructions (primarily xor) are used to combine the results of table lookups . Table 10, derived from Table 8, shows the increase in the ratio of dynamic instruction counts when the field size is increased from 163 bits to 233 bits. For example, the first value in this table
21258	21264	generation and verification, in addition to smaller key storage needs. Even though much literature exists about these algorithms that focus on algorithmic optimizations and hardware implementations , workload characterization studies are very rare. In this paper, we provide a comprehensive workload characterization of security algorithms suitable for constrained environments. We consider
21258	21264	by x); second, check the 163 rd bit of the shifted result, and add p(x) to it if this bit is one (which corresponds to modular reduction). Algorithm 4 3 Polynomial reduction (adapted from  for 64-bit datapath). INPUT: Binary polynomial c(x) of degree at most 324. OUTPUT: c(x) mod p(x), where p(x) = x 163 + x 7 + x 6 + x 3 + 1. 1. For i from 5 downto 3 do 1.1. t = c. 1.2. c =
21258	21264	a (x) , such that ?1 ?1 a ( x) a( x) mod p( x) = a( x) a ( x) mod p( x) = 1 . The operator denotes a swap of the two values on either side. ?sAlgorithm 7 Modified Almost Inverse Algorithm (MAIA) 5  for polynomial inversion INPUT: Binary polynomial a(x), a ( x) ? 0 k OUTPUT: b( x) ? GF( 2 ) and t ? such that t b( x) a( x) ? x mod p( x) 1. b(x) = 1, c(x) = 0, u(x) = a(x), v(x) =
21258	21264	for polynomial multiplication is the shift-and-add algorithm (Algorithm 2). It is presented for illustrative purposes because of its simplicity. The second algorithm, Algorithm 3, is described in  and is significantly faster than the shiftand-add algorithm but requires more storage for the table lookups involved. Algorithm 2 differs from Algorithm 3 in that it does not perform reduction
21258	21264	zeros between the consecutive bits in the binary representation of a(x). This is facilitated by using a 512byte table that is precomputed to hold the 16-bit squares of each 8-bit polynomial . For polynomial inversion, we present two algorithms: Extended Euclidean Algorithm (EEA)  and the Modified Almost Inverse Algorithm (MAIA)  5 . These are summarized as Algorithm 6 and
21258	14640	used, and these are listed in Table 5. Since these instructions are a subset of the PLX processor, we use the PLX architectural testbed and tools for the simulations and workload characterization . Table 5 Instructions in the basic RISC ISA grouped by instruction classes Arithmetic add addi sub subi loadi Unconditional branch call ret Logical and andi or xor not Conditional branch beqz bnez
21258	21270	generation and verification, in addition to smaller key storage needs. Even though much literature exists about these algorithms that focus on algorithmic optimizations and hardware implementations , workload characterization studies are very rare. In this paper, we provide a comprehensive workload characterization of security algorithms suitable for constrained environments. We consider
21258	21270	architecture community, we describe important ECC characteristics in the following sections. Table 1 Algorithm set Class Algorithm(s) Elliptic-curve Diffie-Hellman key Public-key exchange (EC-DHKE) , ellipticcurve ElGamal (EC-ElGamal)  Signature Elliptic-curve Digital Signature Algorithm (EC-DSA)  Symmetric-key Advanced Encryption Standard (AES)  Hash Secure Hash Algorithm (SHA)
21258	21270	a (x) , such that ?1 ?1 a ( x) a( x) mod p( x) = a( x) a ( x) mod p( x) = 1 . The operator denotes a swap of the two values on either side. ?sAlgorithm 7 Modified Almost Inverse Algorithm (MAIA) 5  for polynomial inversion INPUT: Binary polynomial a(x), a ( x) ? 0 k OUTPUT: b( x) ? GF( 2 ) and t ? such that t b( x) a( x) ? x mod p( x) 1. b(x) = 1, c(x) = 0, u(x) = a(x), v(x) =
21258	21270	hold the 16-bit squares of each 8-bit polynomial . For polynomial inversion, we present two algorithms: Extended Euclidean Algorithm (EEA)  and the Modified Almost Inverse Algorithm (MAIA)  5 . These are summarized as Algorithm 6 and Algorithm 7 respectively. EEA is a basic but slow algorithm that we provide for illustration, whereas the MAIA (and similar variants of the Almost
21258	21270	EEA is a direct extension of the basic Euclidean algorithm used for integers, the reader is referred to  and  for the details of MAIA. 5 Almost Inverse Algorithm was originally described in . In this study, we use a modified version of that algorithm called Modified Almost Inverse Algorithm, which is described in . 5. Diffie-Hellman key exchange This section focuses on the
8921838	21272	Linear Time Temporal Logic (LTL) . It has been well studied since and enjoys various decision procedures for both its model checking and its satisfiability problem. They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on
8921838	21273	both its model checking and its satisfiability problem. They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on sets of formulas but equip the universal player (refuter) with a simple tool that allows him to neatly
8921838	21274	has been well studied since and enjoys various decision procedures for both its model checking and its satisfiability problem. They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on sets of formulas but equip the
8921838	21275	(LTL) . It has been well studied since and enjoys various decision procedures for both its model checking and its satisfiability problem. They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on sets of formulas
8921838	21276	They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on sets of formulas but equip the universal player (refuter) with a simple tool that allows him to neatly show the regeneration of least fixpoint constructs --
8921838	21276	:= ttU# and G# := ffR#. Finally, we write X k # to denote X . . . X | {z } k times #. 3 Focus Games for Satisfiability of LTL Formulas Focus game for satisfiability of LTL formulas as presented in  work as follows. Two players called # and # play on sets of subformulas of a given formula # in order to determine whether or not # is satisfiable. Player # believes that it is whereas player #
8921838	21276	between 5 C. Dax and M. Lange C i and C n player # has not used rule (FC), (v) there is an iss.t. C i = C n =  #  , # for some #, #, and between C i and C n player # has used rule (FC). Lemma 3.1  Every play of the game #(#) has length at most O(|#| 2 |#| ). This is mainly because there are only |#|s2 |#| many di#erent configurations in the game #(#). Theorem 3.2  Player # has a winning
8921838	21276	employ results about alternating complexity classes from . This is because an alternating algorithm only follows a single play. Here, a single play can be played using polynomial space only, see  for details. This would naturally lead to an EXPTIME procedure for deciding the winner because EXPTIME equals alternating PSPACE. However, player #'s moves can be determinised using the above
8921838	21276	Buchi automata and both have similar gametheoretic characterisations in terms of focus games. However, they are dual to each other: satisfiability focus games naturally lead to a nondeterministic , model checking games to a universal procedure . From a complexitytheoretic point of view, this is of course the same . From a game-theoretic point of view, the roles of the players are
8921838	21277	depends on which path player # chooses. However, he would only do this after player # has committed to a particular disjunct. Thus, the games would not be sound anymore. For a detailed example cf. . Given a transition system T = (S, -#, #), a distinguished state s 0 and an LTL formula # 0 , the LTL model checking foci game G T (s 0 , # 0 ) is played 12 C. Dax and M. Lange s #  # 0 # # 1  k
8921838	21278	Linear Time Temporal Logic (LTL) . It has been well studied since and enjoys various decision procedures for both its model checking and its satisfiability problem. They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on
8921838	21284	various decision procedures for both its model checking and its satisfiability problem. They include tableau methods , resolution , reductions , automata-theoretic procedures  as well as symbolic methods . Satisfiability games for LTL have been defined in  in terms of focus games. They work on sets of formulas but equip the universal player (refuter) with a simple
751986	21294	that goes beyond leaf-labelled phylogenetic trees. Indeed, for each of the algorithms described in this paper, we are currently developing MinCutSupertree type algorithms (Page et al., 2002; Semple and Steel, 2000) that overcomes this limitation. Lastly, if a supertree is returned by one of the algorithms described in this paper, two natural questions arise: (i) how many such supertrees are there and (ii)
9302926	21297	methods cannot be established simply by formal means, they are often tested for their effectiveness in empirical studies . 2 Multiple Views, Linking and Brushing The use of multiple views  is one of the central paradigms in InfoVis. Displaying the data in several views makes it possible to communicate more information without overloading a single view with too much information. The
9302926	21299	are also often done for processes that change over time, adding time as an additional dimenion (with 10 to 100 time steps for a data set). We have developed a visualization system called SimVis  which is capable of different visualization techniques (scatterplots, histograms, parallel coordinates, spatial 3D views, etc.) for CFD data. All these views are linked, and have been enhanced to
9302926	21300	technique, orsFigure 1: Linking and brushing, a sample visualization of a high-dimensional simulation dataset: in a scatter-plot (shown on the left side, two data dimensions), smooth brushing  was used to mark datapoints of low pressure and low velocity; a linked 3D view (on the top right, spatial view) shows the same data with the brushed data-points high-lighted; thirdly, the parallel
9302926	21300	analysis of high-dimensional data is possible through the means of interaction. To improve the quality of interactive work, we proposed 2 advanced brushing techniques, e.g., smooth brushing  and angular brushing . 3 Interactive Focus+Context Visualization One problem with the visualization of large datasets is that either an overview of data without details is conveyed, or the
9302926	21301	are also often done for processes that change over time, adding time as an additional dimenion (with 10 to 100 time steps for a data set). We have developed a visualization system called SimVis  which is capable of different visualization techniques (scatterplots, histograms, parallel coordinates, spatial 3D views, etc.) for CFD data. All these views are linked, and have been enhanced to
9302926	21302	techniques are fisheye views  and the document lens . In recent work, we have demonstrated that focus+context visualization can be generalized to other visualization dimensions, as well . Through the uneven use of graphics resources such as space, color, opacity, etc., a differentiated view can besFigure 2: Histograms for time-dependent data. Left: TimeHistograms in 3D. One axis
9302926	21303	data is possible through the means of interaction. To improve the quality of interactive work, we proposed 2 advanced brushing techniques, e.g., smooth brushing  and angular brushing . 3 Interactive Focus+Context Visualization One problem with the visualization of large datasets is that either an overview of data without details is conveyed, or the visualization has zoomed in
9302926	21305	using the rest of the available space to show the rest of the data as context (in reduced form). The most prominent examples for distortion-oriented F+C visualization techniques are fisheye views  and the document lens . In recent work, we have demonstrated that focus+context visualization can be generalized to other visualization dimensions, as well . Through the uneven use of
9302926	21306	are scatterplots and histograms. In addition to these (rather historic) approaches, other new techniques have been proposed such as parallel coordinates , icon- or pixel-oriented techniques , as well as many others – Kosara et al. give a useful overview about visualization techniques . Visualization exploits the powerful human visual system to effectively transport information from
9302926	21307	InfoVis point of view, the combination of visualization techniques with solutions from statistics and data mining seems very promising. The potential of this combination (called visual data mining ) arises from the fact that InfoVis and statistics pursue different approaches to reach the same goal: provide the user with insight into complex datasets. Mixing visualization with traditional data
9302926	21309	have been proposed such as parallel coordinates , icon- or pixel-oriented techniques , as well as many others – Kosara et al. give a useful overview about visualization techniques . Visualization exploits the powerful human visual system to effectively transport information from the outside world to the human apparatus of perception, recognition, cognition, and reasoning
9302926	21309	of the data without providing sufficient information about the conetxt of the depicted data. To overcome this problem, various techniques for focus-plus-context visualization (F+C visualization ) have been developed, with the goal of integrating both options of visualization: overview and details. Usually, spatial distortions are used to open up more space for the depiction of details in a
9302926	21314	with each other visually. An often used solution to visually link separate views of one dataset is to choose the same color for visualization components which represent the same data items . This visual linking between views becomes especially useful, when interactive brushing is supported in at least one of the views . Brushing means that the user can interactively select
9302926	21316	the results of mathematical data mining and as a tool for data analysis itself. Visualization can also be supported with the results of statistical analysis to improve the display or interaction . The combination of visualization with statistical analysis provides more and faster insight into data, as well as easier communication of results. Acknowledgements Parts of this work have been
3993787	21331	Thompson noticed that his tactical chess analyzer suffered from the GHI problem. He cured it for interior nodes by using a DCG (directed cyclic graph) representation and considering the history. (Baum & Smith 1995) suggest a solution to the GHI problem for their best-first search algorithm. Their algorithm stores the whole DCG in memory and recognizes the case when a node reached through different paths must
3993787	21334	always used. A New General Solution to the GHI Problem Our solution utilizes two techniques: We encode path information using methods from (Zobrist 1970) and use Kawano’s simulation technique (Kawano 1996) to search efficiently. The outline of our solution to the GHI problem is as follows: When a proven or disproven position stored in the transposition table is reached via a new path, instead of
3993787	21334	while MaxDepth increases by a factor of 2 or 3. Invoking Simulation for Correctness Tree simulation was invented by Kawano to effectively deal with useless interposing piece drops in tsume-shogi (Kawano 1996). Tanase applied this idea extensively in shogi, to reduce the overhead of calling the tsume-shogi solver within the normal ?? search (Tanase 2000). In AND/OR trees, which are the common concept on
3993787	21335	heuristic value can be obtained by performing a sequence of null window searches as in MTD(f) (Plaat et al. 1996). Game-Specific Implementation Details Go Domain-specific enhancements from (Kishimoto & Müller 2003) are incorporated in our df-pn and ?? implementations. ?? performs iterative deepening, extends the search for forced moves, and searches the best move from a previous iteration first. Checkers
3993787	21336	for df-pn, and incorrect heuristic values for ?? search. However, Theorem 1 guarantees that (dis)proofs returned by our approach are always correct. This theorem is proven for the case of df-pn in (Kishimoto & Müller 2004), and can be analogously proven for ?? with some modifications (see the next section). Algorithm-Specific Implementation Details Df-pn We made the following modifications to the original df-pn
198457	21345	a limitation of ? in dealing with certain inductive proofs involving higher-order abstract syntax. Section 5.5 presents an encoding of the lazy ?-calculus and the notion of applicative bisimulation . Section 5.6 concludes this chapter and discusses some related work. 5.1 Natural numbers We introduce a type nt to encode natural numbers. The type nt has the following constructors: z : nt s : nt
198457	21345	the fact that our current formulation of induction and co-induction do not interact much with ?. 5.5 The lazy ?-calculus We consider an untyped version of the pure ?-calculus with lazy evaluation , following the usual HOAS style, i.e., object-level ?-operator and application are encoded as constants lam : (tm ? tm) ? tm and @ : tm ? tm ? tm, where tm is the syntactic category of object-level
198457	21346	a large subset of the logic may still admit some uniformity in proof search. A fragment of Linc without ? but with (co-)induction has also been separately implemented in the Hybrid system . Another interesting direction for future work is to investigate the connection between explicit co-induction rule with circular proofs, which is particularly attractive from the proof search
198457	20614	these rules asynchronous rules, and the other rules are called synchronous. The more general notion of synchrony and asynchrony in proof search has been already studied in the literature (see e.g., ), and these notions are usually tied to connectives rather than rules. Our attempt to classify rules as synchronous and asynchronous for logical connectives is of more limited scope. We do not
198457	21348	are equal (in the above sense) if and only if every finite prefix of one list is (syntactically) equal to the prefix of the 84ssame length of the other list. This is also known as the take lemma . We show the proof of the take lemma in the following example. Example 5.4. The take lemma. Let us define the tk predicate as follows. tk z L nil ? = ?. tk N nil nil ? = ?. tk (s N) (X :: L) (X ::
198457	21349	is given by eigenvariable, which provides a fresh, scoped constant in proof search. Eigenvariable has been used in the encoding of restrictions in the ?-calculus , nonces in security protocols , reference locations in imperative programming , and constructors hidden within abstract data-types . The ?-encoding is adequate as long as we are concerned only with encoding
198457	21349	to encoding fresh names generation in logic is via the use of eigenvariable (or universal quantifier). This approach is taken in the encoding of ?-calculus in , nonces in security protocols , references to locations in imperative programming languages  and constructors hidden within abstract data types . In these works, the main interest is in encoding computation, while we
198457	21350	of ?-calculus without the restriction operator. Formalization of ?-calculus has also been done in the Calculus of Construction in , in the concurrent logical framework by Watkins, et.al., , and in FM-logic . In the latter, the restriction operator is interpreted by the new quantifier  in the logic. In these encodings bisimulation is not considered. The encoding of
198457	21351	scoped constant in proof search. Eigenvariable has been used in the encoding of restrictions in the ?-calculus , nonces in security protocols , reference locations in imperative programming , and constructors hidden within abstract data-types . The ?-encoding is adequate as long as we are concerned only with encoding computations but not with proving properties about the
198457	21351	eigenvariable (or universal quantifier). This approach is taken in the encoding of ?-calculus in , nonces in security protocols , references to locations in imperative programming languages  and constructors hidden within abstract data types . In these works, the main interest is in encoding computation, while we are also interested in doing reasoning about computation. This
21403	22784	of eight genes were retrieved using LocusLink. Because the 5?-ends of RefSeqs often do not extend to TSS, sequences 3000 bp upstream of the start of each RefSeq were retrieved. Diehn et al. (2002) hypothesized that the nuclear factor of activated T-cells (NFAT) plays a key role in mediating the CD28 signal based on the enrichment of independently confirmed NFAT targets among the CD28
5328	21406	semantic languages such as RDF or OWL; these can be either dispersed in the WWW space, which calls for co-operation with keyword-based search engines , or collected in specialised repositories . Another ‘retrieval’ stream deals with extraction of information (potential annotations) from raw web data, andfocuses on the level of individual documents or sites . Although Permission to make
5328	21407	4 HMM instead of the commonly used bigram, seeking to capture farther-reaching dependencies between slots. The smoothing method applied on lexical probabilities was absolute discounting similar to , while for transition probabilities, linear interpolation was used. The three architectures were as follows: 1. In the naive approach inspired by , we represented each semantic slot with a
5328	21408	while BikeProduct itself has a ’canonical’ name, specified e.g. by its manufacturer. Finally, our way of representing metadata for extracted information is based on reification and inspired by . The metadata should cover information on which analysis module the statement was obtained from, or its certainty factor. Metadata are grouped under an abstract class called Meta. 3.2 RDF
5328	21409	knowledge-based integration, which would take full advantage of the flexibility of RDF. We would also like to compare the performance of statistical IE methods with rule-based ones (such as LP 2 ) on the product catalogue domain. Another problem is that of portability to another (retail-sale) domain: apart of re-training the extraction model, the upper level of IE has to be modified as
5328	5319	specialised repositories . Another ‘retrieval’ stream deals with extraction of information (potential annotations) from raw web data, andfocuses on the level of individual documents or sites . Although Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or
5328	5319	are connected with missing slots, multiple different references toasingle slot, and with transposed tables; for some of these, partial solutions have recently been suggested by IE research (e.g. ) and could be reused. 3. STORING AND QUERYING THE RDF 3.1 RDF Schema for Bicycle Sale Domain The HMM-based extractors discussed above are currently (in the best case) able to yield instances of
5328	5319	IE tools for semantic web are S-CREAM  and MnM . They pay significant attention to efficient coupling of training data mark-up and subsequent automated extraction of new data. Armadillo  is probably the most advanced information extraction tool explicitly addressing the semantic web standards such as RDF (using the AKT triple store ). Its strong point is bootstrapping, which
5328	5319	is worth investigating. Finally, we plan to provide support for on-the-fly application construction from available web services and its user-controlled execution (similar to that of Armadillo ), taking as starting point the conceptual framework for web analysis introduced in . 6. ACKNOWLEDGEMENTS The authors thank Jeen Broekstra and Martin Kavalec for assistance in setting up the
5328	5321	are connected with missing slots, multiple different references toasingle slot, and with transposed tables; for some of these, partial solutions have recently been suggested by IE research (e.g. ) and could be reused. 3. STORING AND QUERYING THE RDF 3.1 RDF Schema for Bicycle Sale Domain The HMM-based extractors discussed above are currently (in the best case) able to yield instances of
20216	21412	the Information Retrieval field, and has proven to be useful. It shall come as no surprise that these mechanisms also apply to the query formulation problem for information systems. In , , ,  such applications of the query by navigation and relevance feedback mechanisms have been described before. When combining the query by navigation and manipulation mechanisms
20216	21414	has some clear parallels to the disclosure problems encountered in document retrieval systems. To draw this parallel in more detail, we quote the information retrieval paradigm as introduced in . The paradigm starts with an individual or company having an information need they wish to fulfil. This need is typically a vague notion and needs to be made more concrete in terms of an
20216	21414	the information request using the information stored in the system. This is illustrated in the information disclosure, or information retrieval paradigm, presented in figure 1 which is taken from . We now briefly discuss why the information retrieval paradigm for document retrieval systems is also applicable for information systems. For a more elaborate discussion on the relation between
20216	21414	need can be aptly described by (): I don’t know what I’m looking for, but I’ll know when I find it. In document retrieval systems this problem is attacked by using query by navigation (, ) and relevance feedback mechanisms (). The query by navigation interaction mechanism between a searcher and the system is well-known from the Information Retrieval field, and has
20216	16062	to a graph. This translation is exactly the same as provided in , but for reasons of completeness we provide it again. We start out from a formalisation of ORM based on the one used in (). However, since only a very limited part of the formalisation is needed, we do not cover the formalisation in full detail. A conceptual schema is presumed to consist of a set of types TP. Within
20216	10702	of the information disclosure of information systems, is the introduction of query languages on a conceptual level. Examples of such conceptual query languages are RIDL (), LISA-D (, ), and FORML (). By letting users formulate queries on a conceptual level, users are safeguarded from having to know the exact mapping to internal representations (e.g. a set of
20216	10702	and the  construct is the path confluence operation. It allows us to combine a variety of path expressions. For more details about the path expression operators, refer to  and the forthcomming Asymetrix report on path expressions. One single edge from the spider query graph is converted to a path expression as follows: where PathSeg(y, l, x) ? NodeExpr(y)
20216	20212	of the information disclosure of information systems, is the introduction of query languages on a conceptual level. Examples of such conceptual query languages are RIDL (), LISA-D (, ), and FORML (). By letting users formulate queries on a conceptual level, users are safeguarded from having to know the exact mapping to internal representations (e.g. a set of tables which
20216	21417	Retrieval field, and has proven to be useful. It shall come as no surprise that these mechanisms also apply to the query formulation problem for information systems. In , , ,  such applications of the query by navigation and relevance feedback mechanisms have been described before. When combining the query by navigation and manipulation mechanisms with the
20216	20215	before, the related notions of query by navigation and query by construction have already been discussed in , , . The point to point query mechanism was already discussed in . The idea behind spider queries is to start out from one object type, and to associate all information that is relevant to this object type. The essential part of a spider query is selecting the
20216	20215	this is subject of further research. In figure 2 a possible screen is depicted for building queries using a point to point query mechanism. No special window is needed for a spider query (see also ). We start out from an existing query in a query by construction window. Note that this could also be single object type, e.g. politician. The spider query mechanism adds one important aspect to
20216	20215	the head and tail object types. Furthermore, the paths can also be used as a starting point of a query by navigation session. This latter posibility is illustrated in figure 5. As stated before in , the query by construction window is basically a syntax directed editor. In the left part of the window all possible constructs from the query language are listed. In our examples we have used the
20216	20215	as a Graph For the purpose of finding a path between object types in a conceptual schema, the schema first needs to be translated to a graph. This translation is exactly the same as provided in , but for reasons of completeness we provide it again. We start out from a formalisation of ORM based on the one used in (). However, since only a very limited part of the formalisation is
20216	20215	new set of nodes that will be considered for further extensions in the next step of ? are determined by 6. In this definition, the conceptual weight function CWeight is the same function as used in , and should provide the conceptual importance of each object type. This importance could for instance be based on the abstraction level at which the object type occurs (). The rationale
20216	20218	information stored in information systems, the spider query mechanism. As stated before, the related notions of query by navigation and query by construction have already been discussed in , , . The point to point query mechanism was already discussed in . The idea behind spider queries is to start out from one object type, and to associate all information that is
20216	21420	know what I’m looking for, but I’ll know when I find it. In document retrieval systems this problem is attacked by using query by navigation (, ) and relevance feedback mechanisms (). The query by navigation interaction mechanism between a searcher and the system is well-known from the Information Retrieval field, and has proven to be useful. It shall come as no surprise that
21424	21425	caustic on the floor. Third, the “MetalRing” contains several sharp caustics scattered all over the scene. Finally, as a practical example we have chosen to also include the “HeadLight??? scene , in which the illumination from a (real) car headlight is being simulated. The curved reflector and front glass in this scene result in very detailed, high-quality caustic patterns that have to be
21424	21427	applications, i.e. the visualization of highly detailed, sharp caustic patterns. Note that our method should also work well for the local pass acceleration technique proposed by Christensen et al. . 3.4. Efficient Construction Once the cost function is defined, the optimized kd-tree can be built in multiple ways. However, care has to be taken to implement this operation efficiently. For
21424	21444	goal in computer graphics. Over the last twenty years, a large variety of algorithms have been developed, such as (bidirectional) path tracing , Metropolis Light Transport , many different types of radiosc? The Eurographics Association and Blackwell Publishing 2004. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main Street,
21424	21445	in each splitting step, which is not the case for the balanced code. Note however that the measurements for left-balancing the kd-tree have been performed using a highly optimized implementation  that is roughly 2–4 times as fast as the original code published in . Even compared to this highly optimized implementation, our overhead for realistic numbers of photons is less than a
21424	21448	is less of an issue: The query time itself is totally independent of the kind and number of geometric primitives , and the ray tracing time also depends but weakly on the scene complexity . 4.2. Construction Time Even though we are mainly interested in photon-mapped walkthroughs with a precomputed photon map (for which the construction time is less of an issue), the applicability of
21424	21448	of 40–66 percent, even including the cost for computing shadows, reflections and refraction, which cannot be accelerated by faster kNN queries. Furthermore we currently do not use the fast SSE code  for shooting these rays, so the ray shooting cost already starts to dominate the rendering time, especially after the kNN queries have been accelerated by factors of 1.7 and 3.4, respectively. Once
21424	9675	interesting to investigate the use of SIMD instructions  for accelerating the query itself. Except for a data-parallel approach of performing four independent queries in parallel (as in ), an alternative approach is to reorganize the kd-tree such that it always stores four neighboring photons in each node. This might eventually include to entirely switch from a kd-tree to a
21450	10497	the targeted power dissipation budget is of paramount importance. Research in CMOS power modeling and optimization is thriving for more accurate predictions and power efficiency. Recent research ,  has focused on the CMOS power consumption estimation. There is a need for precise estimation to guide the designers towards minimizing and meeting strict power budgets. This paper focuses on
21450	21452	accounts for a large fraction of the total power consumption. Recent research work have dealt with the encoding of the signals on the address buses in order to lower their switching activity , , , , . A significant reduction in the switched capacitance is reported, with power savings between 33% and 75% using different coding schemes. One of the most promising encodings that is
21450	21452	data symbols, or in other words, to control the spectrum of the transmitted signal . Low-power techniques for global communication in CMOS VLSI using data encoding methods are overviewed in , where it is shown that such techniques can decrease the power consumed for transmitting information over heavy load communication paths (buses) by reducing the switching activity. One technique
21450	1942	for a large fraction of the total power consumption. Recent research work have dealt with the encoding of the signals on the address buses in order to lower their switching activity , , , , . A significant reduction in the switched capacitance is reported, with power savings between 33% and 75% using different coding schemes. One of the most promising encodings that is used
21450	1942	of this technique is related to the wire quantity required, proportional to 2 n . The Limited-Weight Codes is another technique proposed in order to obtain switching activity reduction on buses . This technique requires transition signaling in order to reduce the switching activity, since with transition signaling only 1’s generate transitions. According to , transition signaling is
21450	1942	for a large fraction of the total power consumption. Recent research work have dealt with the encoding of the signals on the address buses in order to lower their switching activity , , , , . A significant reduction in the switched capacitance is reported, with power savings between 33% and 75% using different coding schemes. One of the most promising encodings that is used to
21450	1942	over the bus. The Bus-Invert method as a means of encoding words for reducing I/O power, in which a word may be inverted and then transmitted if doing so reduces the number of transitions . In this method an extra bus line, called invert is used. The method looks at two consecutive data words on the bus. If the Hamming distance between the next word and the current transmitted word
21450	1941	a large fraction of the total power consumption. Recent research work have dealt with the encoding of the signals on the address buses in order to lower their switching activity , , , , . A significant reduction in the switched capacitance is reported, with power savings between 33% and 75% using different coding schemes. One of the most promising encodings that is used to reduce
21450	1941	as it is, with invert set to 0. Otherwise, each bit of the next word is complemented and invert is set to 1, indicating that the word has been complemented. The Bus-Invert method is explored in  in order to sequence words under the Bus-Invert scheme for the minimum transitions, i.e., 1swords can be complemented, reordered and then transmitted. A Gray code sequence is a set of numbers in
21450	21454	the operands. The Gray code adders and multipliers proposed were generated by describing them in Programmable Logic Array format and synthesizing them usingscript.algebraic in the SIS environment . Thus, it is possible to obtain their power and area estimates and compare to the Binary code operators. Table I shows area estimates for 2 and 4 bits Binary and Gray adders and multipliers in
21450	21455	can be observed from Table II, power consumption decreases when input vectors and real trace are applied to input operators. This fact occurs because of the temporal and spatial correlation effect . As expected, the power decreases more in Gray code than Binary code operators. However, since the Gray code has a higher area as shown in Table I, the power conTABLE III HYBRID CODE REPRESENTATION
21483	22870	intelligence in order to test new concepts and methods for creating fused ground situation pictures. At EricssonsMicrowave Systems, (EMW), a simulation framework, Ground Target SIMulator, GTSIM, , has been developed to be used as a test bench, when developing new methods and concepts for sensor data- and information fusion. Fig. 4. Principle layout of GTSIM test bench. The simulator
12177	21498	imposes a startup delay, which may not be justified for short-lived applications and infrequently executed methods. Java Virtual Machine implementations, such as Sun’s HotSpot Performance Engine , typically use mixed-mode interpretation and compilation to combine interpretation’s shorter startup times with compiled code’s better throughput. Perhaps as a consequence of several problematic
12177	21503	within a basic block. Thepfe instruction is needed because standard SSA Form ?-function semantics require that ?-functions be “executed” at the beginning of the basic block in which they reside . An often overlooked consequence of this rule manifests itself when one or more ?-function (in a loop) reference the result values of ?-functions within the same basic block. In this case, they
12177	21506	can reduce the number of dispatches required, and instruction replication can increase the effectiveness of hardware branch predictors . Our rewritten interpreter will utilizes some type of threaded dispatch and may also make use of superinstructions and replication. Portable interpreter implementations tend to implement operand
12177	21513	difficult for hardware to predict Ertl and Gregg . Threaded execution dispatch techniques  can reduce this overhead. In addition, superinstructions  can reduce the number of dispatches required, and instruction replication can increase the effectiveness of hardware branch predictors . Our rewritten interpreter will
12177	21514	is particularly difficult for hardware to predict Ertl and Gregg . Threaded execution dispatch techniques  can reduce this overhead. In addition, superinstructions  can reduce the number of dispatches required, and instruction replication can increase the effectiveness of hardware branch predictors . Our
32221	21520	These relations typically connect a concept in the Semantic Web with the pages that most pertain to it. It is also possible that some of the pages in the current Web contain semantic markup ( ). However, Semantic Search as described in this paper does not use such markups. We assume that robots will gather such markups so that they are available on the Semantic Web. Distributed
32221	21521	this query interface and then discuss some of the facilities provided by TAP for publishing and consuming data. 3.1 The GetData Query Interface A number query languages have been developed for RDF(, , ), DAML () and more generally for semi-structured data (, ). Why do we need yet another query language? These query languages all provide very expressive mechanisms that are
32221	21118	for publishing and consuming data. 3.1 The GetData Query Interface A number query languages have been developed for RDF(, , ), DAML () and more generally for semi-structured data (, ). Why do we need yet another query language? These query languages all provide very expressive mechanisms that are are aimed at making it easy to express complex queries. Unfortunately, with
32221	21523	interface and then discuss some of the facilities provided by TAP for publishing and consuming data. 3.1 The GetData Query Interface A number query languages have been developed for RDF(, , ), DAML () and more generally for semi-structured data (, ). Why do we need yet another query language? These query languages all provide very expressive mechanisms that are are aimed at
32221	21524	of the text search itself. Intuitively, the text search should be able to exploit an understanding more about what the user is trying to find information about. Work in Latent Semantic Indexing  and related areas has explored the use of semantics for information retrieval. However, much of that work has focussed on generating the semantic structures from text. With the Semantic Web, we are
8921875	21529	such a perceptual evaluation could tell if the generated movements can be fused with audio to enhance speech perception. Several evaluation procedures have been proposed, including Turing tests , general communicational judgements such as quality, appeal ratings  or naturalness , or more specific properties of audiovisual speech such as better performance in identification tasks (of,
8921875	21529	model that renders the face geometry on the screen and the appearance model that is responsible for the final rendering of each pixel of the face. Part of the controversial results obtained in , where poor intelligibility scores seem to contradict the excellent acceptability of the animation could be explained by the fact that evaluation scores are a complex by-product of the deficient
8921875	21535	and a fine-grained appearance model may compensate for inappropriate control movements in a global quality judgment such as rating naturalness or adequacy. We have already proposed elsewhere  using the point-light (PL) technique  in order to concentrate on the quality of driving signals while avoiding the problem of choice of a specific appearance model. PL have already proved to be
8921875	21541	stimuli seem to activate common brain areas as actual speech . In this paper, we are interested in evaluating perceptually the speech movements estimated with our video-based tracking system . This system is outlined in the next section. Then, we describe two intelligibility tests that sketch out a framework for benchmarking motion capture systems against ground truth data. The first
8921875	21541	how the speech movements used in the perception experiments have been obtained. The tracking system is based on speaker-specific models of the face: a 3D shape model and several appearance models . 2.1. The 3D shape model A speaker-specific articulatory model of a female French speaker was constructed. This model emerges from statistical analyses of hundreds facial fleshpoints 3D positions
8921875	21541	are very satisfying because they are similar to those with the ground truth data. 3 In the same constrained video conditions, objective evaluations of these appearance models performed in  has drawn the same conclusion. In less controlled situations however, the objective performance of the several appearance models varied. Perceptual tests based on movements tracked on more
8921876	21549	, 2 : 1) contains ; 2) for any two distinct sequences . ,wehave It was shown in  that a set of quaternary QOSs may be constructed based on an appropriate permutation of Family sequences  and their cyclic shifts. The resulting set of quaternary QOSs of length has a family size of and can be partitioned into nonoverlapping equal size subsets , . Here, is the defining masking sequence
21550	21551	objects. Geographic objects are intrinsically tied to space and inherit from space many of its structural properties such as topological, mereological (part–whole relations) and geometric ones . The goal of this paper is to investigate some of the properties of geographic objects using ecosystems as a representative example of a geographic object. To achieve this goal we will develop a
21550	21553	a theory of instantiation. Formal ontology as a tool for studying complex domains has already demonstrated notable successes in such areas of information technology as medical information systems  and biological classifications . A similar ontological methodology was applied to the concepts of surface hydrology in .s2 A. Sorokine, T. Bittner, C. Renschler There are several purposes for
21550	21555	such areas of information technology as medical information systems  and biological classifications . A similar ontological methodology was applied to the concepts of surface hydrology in .s2 A. Sorokine, T. Bittner, C. Renschler There are several purposes for this study. The study itself will allow to achieve better understanding of the existing ecosystem hierarchies and to provide
21550	21562	be abstract (like the number two) or concrete (like Gertrude). Concrete particulars are physically existing objects that have specific spatial and temporal locations. For details see for example . Between individuals and classes the relation of instantiation holds. For example, Gertrude instantiates the class Felis catus, i.e., Gertrude is an instance of the class Felis catus. However,
21550	21568	We call ATM5 an instance of the no-partial-overlap principle (NPO). In more general case it is possible to encounter classifications that involve partial overlap of classes. As it is suggested in  in such cases it is oftensOntology of Ecosystem Hierarchies 11 desirable to isolate subclassification which form proper trees. Example of such ecosystem classification was discussed in .
21550	21569	in  in such cases it is oftensOntology of Ecosystem Hierarchies 11 desirable to isolate subclassification which form proper trees. Example of such ecosystem classification was discussed in . Thirdly, we add an axiom to the effect that if u is a proper subclass of v then there exists a class w such that w is a proper subclass of c and w and u do not overlap (ATM6). This rules out cases
21550	21569	and classification systems. The theory presented in this paper was successfully tested against other kinds of geographic classifications such as regional ecosystem hierarchies and soil taxonomy .s18 A. Sorokine, T. Bittner, C. Renschler Acknowledgements Support for the second author from the Wolfgang Paul Program of the Alexander von Humboldt Foundation and from the National Science
21580	21587	a query 2 The term “Model” is taken from the original RDF Model & Syntax Recommendation , meaning data model.slanguage ; further I/O modules for N3  and N-triple  and RDF/XML output . Using the API the user can choose to store RDF graphs in memory or in persistent stores. Jena1 provides an additional API for manipulating DAML+OIL . Jena2 provides additional functionality
21580	21587	rdfSyntax+”#parseTypeCollectionPropertyElt”); The URL references used link directly into the W3C rec., tying the software behaviour with the formal specification. Since, as described in , the code closely follows the formal grammar in the recommendation, it is straightforward to provide this behaviour which can be understood by the users in terms of that grammar. 6. INFERENCE
21580	1725	OWL Full, roughly corresponding to the union of OWL Lite and RDFS); whilst the architecture permits plug-in connections to engines being developed by the wider community, such as Racer , FaCT  and the Java Theorem Prover . It is planned that using such plug-ins complete OWL Lite and improved OWL DL reasoning will be supported. The design center for Jena2’s inference API is to enable
21580	22984	semantics allowing for automated inference. RDFS and OWL also provide some useful vocabulary, particularly for building schema and ontologies. Jena is a leading Semantic Web programmers’ toolkit . The heart of the Semantic Web recommendations is the RDF Graph , as a universal data structure. An RDF graph is simply a set of triples. Jena similarly has the Graph as its core interface
21580	22984	Web. 1.2 What is Jena? Jena1 was first released in 2000 and has had over 5,000 downloads. Jena2 was released in August 2003; and has had over 3,500 downloads. The main contribution of Jena1  is the rich Model 2 API for manipulating RDF graphs. Around this API, Jena1 provides various tools, for example: an RDF/XML parser ; a query 2 The term “Model” is taken from the original RDF
21580	22984	efficiency reasons, e.g. for initialising a database. 4. ADDING APIS Within the Graph layer, it is easy to provide triples. However, it is not easy to work with them at the application level (see ). Thus Jena includes an API to act as a presentation layer above the raw Graph, this is the Model API. This provides programming abstractions for the RDF graph which align with the abstractions
21580	22984	additional views of a URIref node within a particular graph, e.g. as an OntClass, as a CardinalityRestriction etc. The functionality of the Model layer has previously been published, for example ; the Ontology layer is described below (section 9). 4.1 The Enhanced Graph Layer Jena2 does not attempt to present one consolidated presentation API onto an RDF graph. A consolidated API might be
21580	21523	or virtual, for example resulting from inference processes applied to other triple sources. This provides a general view mechanism using view definitions in Java. A Semantic Web query language RDQL  is supported, and can be used either on top of materialized graphs, or on the virtual results of RDFS or OWL reasoning. A third presentation interface, the RDF WebAPI , provides query-based
21580	21523	API, Jena1 provides various tools, for example: an RDF/XML parser ; a query 2 The term “Model” is taken from the original RDF Model & Syntax Recommendation , meaning data model.slanguage ; further I/O modules for N3  and N-triple  and RDF/XML output . Using the API the user can choose to store RDF graphs in memory or in persistent stores. Jena1 provides an additional API
21580	21523	be submitted to the database query engine. The query handling operates over all the triples expressed by the Graph, however they are generated - as base assertions or as inferred consequences. RDQL  uses this interface to do the non-constraint parts of its query handling. 3.3 Reification RDF Model & Syntax  suggests making statements about statements by means of reification: i.e.
21580	21523	RDQL – RDF QUERY RDQL (RDF Data Query Language) was pioneered in Jena1. The Jena implementation is the de facto reference implementation. A full description is found in , the original paper is . An RDQL consists of a graph pattern, expressed as a list of triple patterns. Each triple pattern is comprised of named variables and RDF values (URIs and literals). An RDQL query can additionally
21580	21523	yield the same matching results as the original query would on the entire knowledge base. The minimal complete subgraph is the smallest such graph. In a conjunctive query language such as RDQL , this is equivalent to calculating the result triples by substituting each of the query solutions into the graph pattern and merging into a single graph. The graph returned does not have to be
21597	21599	towards understanding the consequences of particular conceptions, we concentrate on indices and algorithms that focus on the relation between the number of intra-cluster and inter-cluster edges. In  some indices measuring the quality of a graph clustering are discussed. Conductance, an index concentrating on the intra-cluster edges is introduced and a clustering algorithm that repeatedly
21597	21599	of those indices and conduct an experimental evaluation of graph clustering approaches. The already known algorithms under comparison are the iterative conductance cut algorithm presented in  and the Markov clustering approach from . By combining proven techniques from graph partitioning and geometric clustering, we also introduce a new approach that compares favorably with respect
21597	21599	Muv ? Muw w?V H ? graph induced by non-zero entries of M C ? clustering induced by connected components of H 3.2 Iterative Conductance Cutting (ICC) The basis of Iterative Conductance Cutting (ICC)  is to iteratively split clusters using minimum conductance cuts. Finding a cut with minimum conductance is NP–hard, therefore the following poly-logarithmic approximation algorithm is used.
21597	21599	? . Actually, ICC computes clusterings with intra-cluster conductance ? close to ? ? . For ? ? = 0.475, ICC continues the splitting quite long and computes a clustering with many small clusters. In  it is argued that coverage should be considered together with intra-cluster conductance. However, ICC compares unfavorable with respect to coverage. For both choices of ? ? , the variation of the
21597	21601	of a graph ? This work was partially supported by the DFG under grant BR 2158/1-1 and WA 654/13-1 and EU under grant IST-2001-33555 COSIN.sclustering. The idea of random walks is also used in  but only for clustering geometric data. Obviously, there is a close connection between graph clustering and the classical graph problem minimum cut. A purely graph-theoretic approach using this
21597	21602	graph clustering and the classical graph problem minimum cut. A purely graph-theoretic approach using this connection more or less directly is the recursive minimum cut approach presented in . Other more advanced partition techniques involve spectral information as in . It is not precisely known how well indices formalizing the relation between the number of intra-cluster and
21597	21607	be computed in polynomial time, constructing a clustering with a fixed number k, k ? 3 of clusters is NP-hard , as well as finding a mincut satisfying certain size constraints on the clusters . 2.2 Performance The performance(C) of a clustering C counts the number of “correctly interpreted pairs of nodes” in a graph. More precisely, it is the fraction of intra-cluster edges together with
21597	21609	main algorithm is dominated by the MST computation. GMC combines two proven concepts from geometric clustering and graph partitioning. The idea of using a MST that way has been considered before . However, to our knowledge the MST decomposition was only used for geometric data before, not for graphs. In our case, general graphs without additional geometric information are considered.
8921885	1029	interrelationships. In order to facilitate a semantic modeling of multimedia content in content sharing and collaborative applications, we have developed Enhanced Multimedia Meta Objects (EMMOs)  in the context of the EU-funded CULTOS project 1 . EMMOs establish tradeable knowledge-enriched units of multimedia 1 CULTOS was carried out from 2001 to 2003 by partners from 11 EU countries and
8921885	1028	Semantic Web, several standards have emerged that can be used to model the semantic relationships between the basic media of multimedia content addressing the content’s semantic aspect, such as RDF , Topic Maps , and MPEG-7 (especially MPEG-7’s Graph tools for the description of content semantics ). For these standards, a variety of proposals for query languages and algebras have been
11509053	21625	implementation of the Davis-Putnam method called SATO . In this paper we propose a method to compute safe beliefs for arbitrary theories based on the Davis Putnam method. Recent research , has shown that when the stable semantics corresponds to the supported semantics, a satisfiability solver like SATO can be used to compute stable models. Interestingly, some examples are presented
11509053	21626	problem recursively adding new atoms to the partially computed safe beliefs in MW and MN . 6 Preferred Safe Beliefs A useful topic of interest in answer set programming is to allow preferences, see . Thus, we extend our safe beliefs paradigm to express a simple but still useful form of preferences. Moreover, our algorithm presented in previous section can be easily adjusted to compute such
11509053	21628	some reductions that can be applied to theories in order to simplify their structure. The notion of reductions and/or transformations has several applications in Logic Programming; see for instance . Some properties of these reductions will be studied. Definition 4.1. For a formula F , we define its reduction with respect to the ? and ? symbols, denoted Reduce?(F ), by replacing each
11509053	21629	problem recursively adding new atoms to the partially computed safe beliefs in MW and MN . 6 Preferred Safe Beliefs A useful topic of interest in answer set programming is to allow preferences, see . Thus, we extend our safe beliefs paradigm to express a simple but still useful form of preferences. Moreover, our algorithm presented in previous section can be easily adjusted to compute such
11509053	21631	some ideas on how to extend this paradigm to incorporate preferences. Keywords: Answer Sets, Davis Putnam, Safe Beliefs, Preferences, Algorithms. 1 Introduction A-Prolog (Stable Logic Programming  or Answer Set Programming ) is the realization of much theoretical work on Nonmonotonic Reasoning and AI applications of Logic Programming (LP) in the last 15 years. This is an important logic
11509053	21631	constitutes the formal basement of the A-Prolog programming paradigm and proved to be particularly useful for several real life and research applications. In the original definition of answer sets , however, the relation of the semantics with a particular logic is not entirely clear and seems to be hidden under an ad hoc setting of reductions and minimal models. Safe Beliefs were introduced
11509053	21632	new atoms in order to perform reductions “thus greatly widening the space of assignments in which the DP procedure has to search in order to find the solutions”, as E. Giunchiglia and R. Sebastiani  have already pointed out. In this section we present a method to compute safe beliefs for arbitrary theories based on the Davis-Putnam method. We will avoid the introduction of new atoms by using
11509053	21632	early reject a model thus avoiding unnecessary branching. Otherwise, the function is called recursively guessing the value of a new atom returning a set containing all the safe beliefs found. As in , we apply the splitting part of the algorithm only for the atoms that occur in the original program. function GetSafeBeliefs(P, MW, MN ) P ?? Reduce(P, MW, MN ). if P = {?} or LP ? MW then return
11509053	21633	this paradigm to incorporate preferences. Keywords: Answer Sets, Davis Putnam, Safe Beliefs, Preferences, Algorithms. 1 Introduction A-Prolog (Stable Logic Programming  or Answer Set Programming ) is the realization of much theoretical work on Nonmonotonic Reasoning and AI applications of Logic Programming (LP) in the last 15 years. This is an important logic programming paradigm that has
11509053	21633	constitutes the formal basement of the A-Prolog programming paradigm and proved to be particularly useful for several real life and research applications. In the original definition of answer sets , however, the relation of the semantics with a particular logic is not entirely clear and seems to be hidden under an ad hoc setting of reductions and minimal models. Safe Beliefs were introduced
11509053	21633	agent knows F if the formula F is an intuitionistic consequence of the theory T . 4 The negation symbol we use (¬) will play the role of the negation as failure in logic programs. The authors in  use, however, the symbol not instead. 4sThe use of intuitionistic logic, a logic of knowledge, as the underlying inference system seems a natural choice for this approach. But we also want our
11509053	21637	describe problems in a more convenient and natural way. We even suspect that this unrestricted syntax can be helpful to model concepts like aggregation in logic programs, as the ones described in . Some other potential applications, suggested by M. Gelfond (e-mail communication), could be found in the field of action languages. Another important property of safe beliefs is that its
21645	23073	the most important events in the history of Russia, not to provide an in-depth explanation of it. Reader interested in a detailed presentation of Russian history should refer to the work Stöckl (1997). The overview over Russian history provided in this section is a hihgly compressed and simplified translation of Orlov et al. (1997). The focus is made on political and economic events in the
21645	23073	of the southern Rus’. Most important cities of this principality were Galich (?????), Peremyshl’ (?????????) and Vladimir3 The by-name &quot;big nest&quot; refers to the numerous children of Vsevolod (Torke (1997)).sCHAPTER 1. GENERAL INFORMATION ABOUT RUSSIA 9 Volynskii (????????-?????????). In this principality, local boiars played an important role, and the entire history of this principality is marked by
8613338	21665	(ICDAR 2003) 0-7695-1960-1/03 $17.00 © 2003 IEEE 3 The above ideas were tested experimentally on an isolated handwritten character recognition task using the NIST 3 database  (see also  for a state-of-the-art character recognition system and comparisons of a large number of classifiers). Similar experiments have been used in other works on variable and adaptive metric methods
8613338	21666	a small number of training examples of those characters derived from contextual information or user corrections. To improve the performance of nearest neighbor methods, a number of authors (e.g.,  ) have proposed usThomas M. Breuel PARC, Inc. 3333 Coyote Hill Rd. Palo Alto, CA 94304, USA ing similarity functions other than the Euclidean distance in nearest neighbor classification, and given
8613338	21666	a large number of prototypes. This remains to be explored in future work. Another question that might be raised is how adaptive nearest neighbor methods like those described in the literature  perform relative to the methods described in this paper. In some sense, the question can be answered trivially: this paper has presented a means by which we can transform any procedure for
8613338	21667	of times as the basis for generalizing across learning tasks in the literature and been demonstrated for knowledge transfer between related learning tasks in optical character recognition task in . The Gaussian model used in the latter paper is closely related to the Gaussian model described in Section 4. Furthermore, the method described in  can be interpreted as a hierarchical Bayesian
21668	21671	Thisisequivalenttosayingthat?d is a regular triangulation of ?(n, 2). These triangulation of ?(n, 2) and the resulting metric fan MFn were studied by De Loera, Sturmfels and Thomas , who had been unaware of an earlier appearance of the same objects in phylogenetic combinatorics . In , Dress considered the polyhedron dual to the triangulation ?d, Pd = ? x ? R n ?0 : xi
21668	21671	of nine quadrangles and two pentagons (hence B = 0920 ), and the Gröbner basis is quadratic (hence C =00).Figure 3 shows one of these tight spans. 4 The 14 Non-regular Triangulations Theorem 4.2 in  states that the hypersimplex ?(n, 2) has non-regular triangulations for n ? 9. In this section we strengthen this result as follows. Theorem 3 The second hypersimplex ?(n, 2) admits non-regular
21668	21672	Cn = ? d ? R ( n 2) : dij ? 0and dij + djk ? dik for all 1 ? i, j, k ? n ? . This is a closed convex pointed polyhedral cone. Its extreme rays have been studied in combinatorial optimization . Among the extreme rays are the splits. The splits are the metrics ? ? i?A j??A eij ? R (n2) as A ranges over nonempty subsets of {1, 2,...,n}. There is an extensive body of knowledge (see )
21668	21675	in POLYMAKE  with the help of the electronic journal of combinatorics 11 (2004), #R44 2sMichael Joswig and Julian Pfeifle. We also explain how its output differs from the output of SPLITSTREE . A complete list of all six-point metrics has been made available at bio.math.berkeley.edu/SixPointMetrics For each of the 339+14 types in Theorem 1, the regular triangulation, Stanley-Reisner
21668	21675	the edge lengths seen here do not represent the actual edge lengths in the tight span. The pictures produced by our software are different from the output produced by the software SPLITSTREE . SPLITSTREE is a program that can, among other things, compute and visualize the split-decompositions of metrics (see ). It decomposes an input metric into a sum of splits plus a split-prime
21668	21677	the corresponding minimal subdivisions of ?(6, 2). In Section 6 we present a software tool for visualizing the tight span Td of any finite metric d. This tool was written written in POLYMAKE  with the help of the electronic journal of combinatorics 11 (2004), #R44 2sMichael Joswig and Julian Pfeifle. We also explain how its output differs from the output of SPLITSTREE . A complete
21668	21677	of the correctness and completeness of the results in . Namely, we computed the cone in the metric fan corresponding to each of the 339 metrics. For each cone we computed (using POLYMAKE ) the facets and the extreme rays of the cone. And it turned out that the extreme rays we found are precisely the 14 types listed above. All the facets and extreme rays of the 339 types of maximal
21668	21677	? ?(6, 2) ? has precisely 14 facets. 6 Visualization of Tight Spans in POLYMAKE POLYMAKE is a software package developed by Ewgenij Gawrilow and Michael Joswig for studying polytopes and polyhedra . It allows us to define a polyhedron by a set of either inequalities or vertices and computes numerous properties of the polyhedron. We implemented a client program to POLYMAKE for visualizing the
21668	21679	quadrangles should form a flat pentagon). The aim of this article is to present the analogous classification for n = 6. The following result was obtained with the help of Rambau’s software TOPCOM  for enumerating triangulations of arbitrary convex polytopes. Theorem 1 There are 194, 160 generic metrics on six points. These correspond to the maximal cones in MF6 and to the regular
8921894	21687	the benefit of using systemlevel specification. The initial SDL specification is 10 times smaller than the produced C/VHDL model. The difference is mainly due to the communication refinement . The simulation time of the SDL model is 15 times faster than the VHDL model produced for solution 5 and 120 times faster than the cosimulation of the C/VHDL model produced for the first solution.
8921894	21693	the benefit of using systemlevel specification. The initial SDL specification is 10 times smaller than the produced C/VHDL model. The difference is mainly due to the communication refinement . The simulation time of the SDL model is 15 times faster than the VHDL model produced for solution 5 and 120 times faster than the cosimulation of the C/VHDL model produced for the first solution.
8921894	21694	The ATM design may require faster implementation. The main restriction of Cosmos is the non-optimization of memory management. The use of transformation similar to those provided by Automium  should induce a drastic increase in performances.sIEEE International Symposium on Signals, Systems, and Electronics ????????Ã ????????Ã ????????? ????????? Figure 6. Refinement Steps in Cosmos
8921894	21698	where the queues are explicit and connected through physical buses. In the case of mixed C/VHDL models the simulation is even slower. In this case, we use a C/VHDL co-simulation based on UNIX/IPC . Model Behavior Lines ommunication Simulation SDL 794 103 1 min VHDL 7.210 5.382 15 min (3) C/VHDL (1) (1) 30 min (2) Table 1. SDL vs. C/VHDL co-design and simulation. (1) same order of magnitude
8921894	11982	design of mixed hardware/software systems starting from system-level specification. These are called co-design or embedded system design tools; they provide a drastic increase in the productivity . This gain in productivity may be used to explore several architectural solutions to improve the quality and to reduce the cost of the final design. This paper discusses the co-design of an ATM
8921895	21715	a generic Web-based learning system that dynamically generates interactive math courses with the content represented in XML-based format and presented to the learner via a standard Web browser (Melis et al., 2001). The architecture proposed by Retalis and Avgeriou (2002) is important for our work in that it is based upon standards and practices from international standardization bodies, as well as on the
8921895	23145	math courses with the content represented in XML-based format and presented to the learner via a standard Web browser (Melis et al., 2001). The architecture proposed by Retalis and Avgeriou (2002) is important for our work in that it is based upon standards and practices from international standardization bodies, as well as on the practices of well-established software and hypermedia
8921895	23145	problems (López, Millán, Pérez-de-la-Cruz, & Triguero, 1998). More recently, we also found other ideas that have something in common with Code Tutor— Prentzas, Hatzilygeroudis, and Garofalakis (2002) use hybrid rules for knowledge representation in Web-based tutoring, Rebai and de la Passardiere (2002) try to capture educational metadata for Web-based learning environments, and Ahmad and Lajoie
21722	21742	teacher decision making as more like juggling, taking account of multiple, competing demands, assessing possibilities, to make the best decision possible within the prevailing circumstances (Jackson, 1968; Doyle & Ponder, 1977). These portrayals have, in turn, raised questions about how teachers manage to teach within such a bewildering array of demands (Lampert, 1985). Systematic inquiry is
21760	21761	the first. So it is enough to show that there is an isometry ? of QU such that ??? is transitive on QU. The analogous statement for the universal homogeneous integral metric space was proved in , and we require this in the proof. If a metric space has a cyclic automorphism, we can identify its points with the integers so that the automorphism is the shift. Then the metric is completely
21760	21761	if h is f-admissible, then there is some prolongation f ? of f such that h is f ? -admissible and (f, h) is an initial segment of a Toeplitz function. This is proved for integral metric spaces in . For the rational case, multiply everything by the least common multiple of the denominators, apply the integral result, and divide by d. ? The proof shows that, in the sense of Baire category,
11731523	21769	the P-tree format. P-trees were initially designed for spatial data that show homogeneity due to the spatial continuity of the data . Multimedia data also show homogeneity in the time dimension . Homogeneities in data can occur for other reasons. Join operations in databases lead to replication of some table entries. Depending on the join algorithm, some these replicated entries appear in
11731523	18400	on the join algorithm, some these replicated entries appear in sequence and can be compressed in a bit-column-wise storage. Sparseness of 1 values furthermore leads to long sequences of 0 values . For data that do not show any homogeneity, a sorting scheme that improves compression of P-trees significantly is introduced. We look at the creation of P-trees as a two-step process in which we
8921906	21772	a suitable representation of a partitioning, and related operators. A number of GA clustering representations have been tried and compared in the literature, with seemingly no clear overall winner . In the end, we have chosen to use a straightforward representation in which each gene represents a data item, and its allele value represents the label of the cluster to which it is assigned. This
8921906	21773	VIENNA (for Voronoi Initialised Evolutionary NearestNeighbour Algorithm).s2.1 PESA-II DRAFT: under submission to PPSN 2004 3 We based VIENNA on the elitist MOEA, PESA-II, described in detail in  and . Briefly, PESA-II updates, at each generation, a current set of nondominated solutions stored in an external population (of non-fixed but limited size), and uses this to build an internal
8921906	21774	(for Voronoi Initialised Evolutionary NearestNeighbour Algorithm).s2.1 PESA-II DRAFT: under submission to PPSN 2004 3 We based VIENNA on the elitist MOEA, PESA-II, described in detail in  and . Briefly, PESA-II updates, at each generation, a current set of nondominated solutions stored in an external population (of non-fixed but limited size), and uses this to build an internal
8921906	21775	solutions. Readers familiar with clustering research may notice similarities between our proposed approach and other recent methods. Certainly, several EAs for clustering have been proposed (), though none to our knowledge have used a Pareto multiobjective EA. Other recent work has also used the term ‘multiobjective’ with regard to clustering , but the approach was based on using an
8921906	7794	for many restrictions on the distance functions used and the nature of the objective function, even when k = 2 . Both classic and a vast array of new algorithms for k-clustering exist . Common to almost all of them is the fact that they optimize either implicitly or explicitly just one measure on the partitioning of the data. For example, kmeans  attempts to minimize the
8921906	21779	have been proposed (), though none to our knowledge have used a Pareto multiobjective EA. Other recent work has also used the term ‘multiobjective’ with regard to clustering , but the approach was based on using an ensemble of clustering algorithms  and then obtaining a consensus clustering from these, similarly to the EA proposed in . Our proposed approach, on
8921906	7812	of methods for unsupervised classification of multidimensional data, namely the clustering of data into homogeneous groups: by now a classic AI problem with algorithms dating back to the 60s . In a broad definition, clustering of data might include the recognition and removal of outliers, the recognition and focusing on key dimensions of the data (i.e. feature selection) and the
8921906	7812	for k-clustering exist . Common to almost all of them is the fact that they optimize either implicitly or explicitly just one measure on the partitioning of the data. For example, kmeans  attempts to minimize the summed variance of points within each cluster from their centroid. Although such a method is very effective on certain sets of data, it is also clear that it will fail to
8921906	7812	clustering and k-means We use a standard implementation of average-link agglomerative clustering, , which is deterministic for a given data order. The implementation of the k-means  algorithm is based on the batch version, that is, cluster centers are only recomputed after the reassignment of all data items. As k-means can sometimes generate empty clusters, empty clusters are
8921906	21781	solutions. Readers familiar with clustering research may notice similarities between our proposed approach and other recent methods. Certainly, several EAs for clustering have been proposed (), though none to our knowledge have used a Pareto multiobjective EA. Other recent work has also used the term ‘multiobjective’ with regard to clustering , but the approach was based on using an
8921906	21781	random Voronoi cells In preliminary work not reported here, we investigated an alternative representation for our EA to use, based on optimizing Voronoi cells. This representation was inspired by , where an EA was used to optimize the location of k cluster ‘centers’, to minimize overall variance when the data points were assigned to the nearest center. This GA achieves results similar to
8921906	21782	the label of the cluster to which it is assigned. This means that for any partition, multiple genotypes code for it, i.e. it is a non-injective encoding — normally thought to be undesirable . This drawback is not serious, however, provided there is not a significant bias or over-representation of certain solutions, and/or we can design operators that work effectively and quickly with
8921906	21783	used a Pareto multiobjective EA. Other recent work has also used the term ‘multiobjective’ with regard to clustering , but the approach was based on using an ensemble of clustering algorithms  and then obtaining a consensus clustering from these, similarly to the EA proposed in . Our proposed approach, on the other hand, optimizes different objectives explicitly in one clustering
8921906	21783	mutation operators in preliminary investigations not reported here, including Grouping GA-based methods , as well as multi-parent recombinations based on expectation maximization of an ensemble . Overall, we have found it very difficult to design operators that enable a GA to explore broadly enough to escape the very strong local attractors found in some problems when optimizing certain
909737	21827	to the reserved words, because they are the only entities that express actions with meaning. 3.1 Entropy Metrics The entropy metrics measure the regularity of the distribution of symbol frequencies . We considered two different sorts of entropies, the Shannon and the Rény. For any string s=si + , with each si?S having the probability pi, the Shannon entropy is defined as And the Rény entropy
122367	21846	. Spread spectrum for both ISI mitigation and multiple access is described in . The ALOHA protocol is reviewed in . An informative survey of routing is . Performance of TCP over wireless links is discussed in . Early work in packet radio is reviewed in . For routing in ad-hoc networks see  A special issue of the IEEE
21850	8498	Modified Hess’s scheme , which is modified by Chen. ? Paterson’s scheme , where also includes one non-pre-computable pairing and two pre-computable pairings. ? Cha's and Cheon's scheme , where one Weil pairing is used for signing and another is used for verification. ? Soldera’s scheme , where two Tate pairings are used for signature verification. 2002. Smart  proposed
21850	21852	message. So the construction of the group of TAs needs to be changed from case to case. There have been a number of papers talking about how to make the shared secret reusable (e.g.,  and ), but none of these schemes is efficient. 2. One authority may be involved in many different groups providing different services. For each group construction, he needs to make an individual
21850	21854	Soldera’s scheme , where two Tate pairings are used for signature verification. 2002. Smart  proposed an identity-based authenticated key agreement protocol. After that, Chen and Kudla  modified Smart's scheme to make it more efficient and, more interesting, to make it with the new property of TA forward security, which they define to mean that the compromise of the TA’s private
21850	8499	using Weil pairing, where 3 Weil pairings are used for signature verification and one of them may be precomputed. 2001. Two identity based encryption algorithms were proposed, one by Cocks  based on the quadratic residuosity assumption and another by Boneh and Franklin  based on the Weil pairing. Some later researches described that using a variant based on the Tate pairing is
21850	8499	of This Work In this subsection, we would like to explain the reason why MTAIBC needs more research. As it has been mentioned above, an existing ordinary IBC scheme (for examples, , , , etc.) allows a single TA to issue an identifier-based private key for a user. Therefore, this TA can masquerade any of his users if he wants to. Even if he does not actually abuse any
21850	8499	escrow in IBC, a number of researchers have proposed some solutions of splitting the authority into two or more co -operating parties. For example, the author of the QR-based IBE method, Cocks in  proposed a secret sharing protocol, in which more than one TA can establish a shared public parameter, which is the product of two large primes as private parameters. The protocol ensures that
21850	21857	residuosity assumption and another by Boneh and Franklin  based on the Weil pairing. Some later researches described that using a variant based on the Tate pairing is more efficient . 2001-2002. A number of identity-based signature schemes were published, including ? Galbraith’s scheme , where four Weil pairings are needed for signature verification. ? Hess's scheme
21850	21861	pair. A TA on the immediately above level issues the private part of this key pair. There have recently been a number of different solutions of hierarchy IBC, for examples, , , and . In this document, we focus on only one level TAs. But the solution described in this document can be extended to multiple levels. 3.4 Calculus of TAssThere are a number of different ways of a
21850	8502	2001-2002. A number of identity-based signature schemes were published, including ? Galbraith’s scheme , where four Weil pairings are needed for signature verification. ? Hess's scheme , where one Tate pairing, that may be precomputed, is used for signature generation and two Tate pairings, where one of them may be precomputed, are used for signature verification. ? Modified
21850	8502	of This Work In this subsection, we would like to explain the reason why MTAIBC needs more research. As it has been mentioned above, an existing ordinary IBC scheme (for examples, , , , etc.) allows a single TA to issue an identifier-based private key for a user. Therefore, this TA can masquerade any of his users if he wants to. Even if he does not actually abuse any user’s key,
21850	8502	Q) r ),s3.2 An IBS scheme 10 V ? H2(p(U, S)) = V ? H2(p(rP, sQ)) = V ? H2(p(R, Q) r ) = m. For the purposes of this document, we sketch the modified Hess scheme. This scheme was invented by Hess  and modified by Chen. There are three players involved in the scheme: Alice – a signer, Bob - a verifier and TA – an off* line trusted authority. TA's public key is (P, R = sP ? G1) and TA's
21850	21863	of which has an IBC key pair. A TA on the immediately above level issues the private part of this key pair. There have recently been a number of different solutions of hierarchy IBC, for examples, , , and . In this document, we focus on only one level TAs. But the solution described in this document can be extended to multiple levels. 3.4 Calculus of TAssThere are a number of
21850	8503	generation and two Tate pairings, where one of them may be precomputed, are used for signature verification. ? Modified Hess’s scheme , which is modified by Chen. ? Paterson’s scheme , where also includes one non-pre-computable pairing and two pre-computable pairings. ? Cha's and Cheon's scheme , where one Weil pairing is used for signing and another is used for
21850	21870	any given message. So the construction of the group of TAs needs to be changed from case to case. There have been a number of papers talking about how to make the shared secret reusable (e.g.,  and ), but none of these schemes is efficient. 2. One authority may be involved in many different groups providing different services. For each group construction, he needs to make an
21850	21872	pairingbased IBE method, Boneh and Franklin in  proposed another secret sharing protocol, in which n TAs each has a share of a secret master key, and by using a t-out-of-n threshold scheme : any t TAs are able to recover the master key, but any t – 1 TAs or less cannot get any information about the master key. 5sBoth of the above solutions stopped a single TA escrowing a user's
21850	21874	scheme , where one Weil pairing is used for signing and another is used for verification. ? Soldera’s scheme , where two Tate pairings are used for signature verification. 2002. Smart  proposed an identity-based authenticated key agreement protocol. After that, Chen and Kudla  modified Smart's scheme to make it more efficient and, more interesting, to make it with the new
4414326	21898	The NPGA is a more efficient technique than traditional multiobjective optimization algorithms, because it only uses a sample of the population to estimate Pareto dominance. Akhtar et al.  proposed a swarm-like based approach solve engineering optimization problems. They simulate soL R Ts cieties that conform a civilization. In each society there is a leader which is followed by the
4414326	21898	???????¦? ? ???¦? ????¤???¦????? ?????????? Table 8. Best solution found for the Welded Beam Design Problem. approaches and (2) comparison of our approach against the Socio-Behavioral approach . Both comparisons are based on two aspects: quality and robustness of the solutions. In the Pressure Vessel problem, our evolution strategy found the best result and it was also the most robust
4414326	21898	Red. ???????????????????? ???????????? ???????????????????? ????????????? ???????????????????? ???????????? Table 6. Comparison of Results Between Our Approach and the Socio Behavioral approach  Details of the best solution found Problem 4 Our approach SB ?????????????? ?????????????? ??? ?????????????? ?????????????? ?¦? ??? ??? ??? ?????????????? ?????????????? ??? ??????????????
4414326	16966	a considerable amount of research and a wide variety of approaches have been suggested in the last few years to incorporate constraints into the fitness function of an evolutionary algorithm . The most common approach adopted to deal with constrained search spaces is the use of penalty functions. When using a penalty function, the amount of constraint violation is used to punish or
4414326	16966	the main one is that they require a careful fine tuning of the penalty factors that accurately estimates the degree of penalization to be applied as to approach efficiently the feasible region . Therefore, other alternative approaches have been suggested. Our approach is based on a simple evolution strategy ¢¤£?¥?§?© - ES and three simple selection rules to guide the evolutionary search
4414326	21901	in problems that have discrete and continuous variables. The obvious drawback of the approach is the need of implementing combined operators for the special encoding adopted. Coello and Mezura  implemented a version of the Niched-Pareto Genetic Algorithm (NPGA)  to handle constraints in single-objective optimization problems. The NPGA is a multiobjective optimization approach in which
4414326	21901	combined to generate just one child. This modified selection process is controlled by a pa? rameter (that is not defined by the user) called Selection (? ? Ratio ). This parameter was introduced in  and it refers to the percentage of selections that will take place only between the current solution and the child generated by all § the parents, based on the three selection criteria previously
4414326	21903	with ? to the feasible region and with ? to the whole search space, then it should be clear that ????? . 3. Previous Work Several authors have used EAs to solve engineering design problems: Deb  proposed a Genetic Adaptive Search (GeneAS) to solve engineering optimization problems. He proposed to use a both, binary and real encoding for each solution. This approach was tested on three
4414326	21904	initialize the societies. 4. Our approach Our approach is based on an Evolution Strategy because this technique has been found not only efficient in solving a wide variety of optimization problems , but also has a strong theoretical background . The motivation of this work is divided in three parts: (1) We hypothesized that the use of an evolution strategy for constrained optimization
4414326	21906	????????¢??¦© is a function that returns TRUE with probability ? ). 5. Test Problems To test our technique we decided to implement four penalty-based approaches: Death penalty, a static penalty , a dynamic penalty  and an adaptive penalty .sWe selected four well known engineering design problems to validate our approach . The full description of each of them is provided below:
4414326	21907	drawback of the approach is the need of implementing combined operators for the special encoding adopted. Coello and Mezura  implemented a version of the Niched-Pareto Genetic Algorithm (NPGA)  to handle constraints in single-objective optimization problems. The NPGA is a multiobjective optimization approach in which individuals are selected through a tournament based on Pareto dominance.
4414326	21908	that returns TRUE with probability ? ). 5. Test Problems To test our technique we decided to implement four penalty-based approaches: Death penalty, a static penalty , a dynamic penalty  and an adaptive penalty .sWe selected four well known engineering design problems to validate our approach . The full description of each of them is provided below: Problem 1: (Design of a
4414326	21910	a considerable amount of research and a wide variety of approaches have been suggested in the last few years to incorporate constraints into the fitness function of an evolutionary algorithm . The most common approach adopted to deal with constrained search spaces is the use of penalty functions. When using a penalty function, the amount of constraint violation is used to punish or
8116	4136	our techniques differ from , since every robot performs local visibility maximization rather than a random walk. 3 Architecture Both techniques proposed in this paper are behavior-based  and have the same architecture. Laser, Vision and Position are the sensors being used. Position is a virtual sensor that includes odometry and compass. Arbitration is used for Fig. 1. System
8116	8113	5, 7, and 9 robots). Figure 3 shows the simulation environment. The simulation engine used in our experiments is Player/Stage, developed at the USC Robotics Research Lab and described in detail in . A trial terminates either when a pre-specified time threshold is exceeded or if the locations of the robots have not changed for a certain amount of time. Fig. 3. (left) The simulation
8116	21913	is analyzed to locate the 'frontiers' between the free and unknown space. Exploration proceeds in the direction of the closest frontier. The multi-robot version of the same problem wassaddressed in . In  an incremental approach for deploying a mobile sensor network was introduced with the assumption that every robot in the network is equipped with an 'ideal' localization sensor. Even though
8116	21914	'ideal' localization sensor. Even though there are inherent similarities between , and , the approaches differ fundamentally in that  uses live sensor data whereas  use stored data.  discusses the problem of deployment of distributed sensors (robots) in the wireless ad hoc network domain. In their setup, the communication ranges between the robots are assumed to be limited and
8116	21914	a map, nor localization in a shared frame of reference. The proposed techniques are adaptive (as opposed to ). Despite the similarity of the idea of dispersion, our techniques differ from , since every robot performs local visibility maximization rather than a random walk. 3 Architecture Both techniques proposed in this paper are behavior-based  and have the same architecture.
8116	21915	(Molecular) slightly outperforms ephemeral identification (Informative). 2 Related Work Exploration and map building by a single robot in an unknown environment has been studied by several authors . The frontier-based approach, described in detail in , concerns itself with incrementally constructing a global occupancy map of the environment. The map is analyzed to locate the 'frontiers'
8116	21915	a mobile sensor network was introduced with the assumption that every robot in the network is equipped with an 'ideal' localization sensor. Even though there are inherent similarities between , and , the approaches differ fundamentally in that  uses live sensor data whereas  use stored data.  discusses the problem of deployment of distributed sensors (robots) in the
8116	21916	(Molecular) slightly outperforms ephemeral identification (Informative). 2 Related Work Exploration and map building by a single robot in an unknown environment has been studied by several authors . The frontier-based approach, described in detail in , concerns itself with incrementally constructing a global occupancy map of the environment. The map is analyzed to locate the 'frontiers'
8116	21916	a mobile sensor network was introduced with the assumption that every robot in the network is equipped with an 'ideal' localization sensor. Even though there are inherent similarities between , and , the approaches differ fundamentally in that  uses live sensor data whereas  use stored data.  discusses the problem of deployment of distributed sensors (robots) in the
8116	21918	is analyzed to locate the 'frontiers' between the free and unknown space. Exploration proceeds in the direction of the closest frontier. The multi-robot version of the same problem wassaddressed in . In  an incremental approach for deploying a mobile sensor network was introduced with the assumption that every robot in the network is equipped with an 'ideal' localization sensor. Even though
8116	21918	this paper differ from the above mentioned approaches in a number of ways. We use neither a map, nor localization in a shared frame of reference. The proposed techniques are adaptive (as opposed to ). Despite the similarity of the idea of dispersion, our techniques differ from , since every robot performs local visibility maximization rather than a random walk. 3 Architecture Both
9391909	21960	issue. The interest in codesign is driven by increasing complexity and the need for early prototypes to validate the specification and provide the customer with feedback during the design process  . Codesign aims to produce an heterogeneous architecture of mixed hardware/software components that implements an initial specification. As shown in figure 1, a typical Rapid System Prototyping
9391909	21960	hardware and software is one of the main issues when dealing with cosimulation and cosynthesis. Some approaches make use of a fixed communication scheme depending on a fixed architectural platform    , in which case the first two problems addressed earlier are easily handled. Although some of these architectures are flexible (e.g. supports a variable number of hardware and software
9391909	21965	primitives that hide the implementation details of the communication units. 1.2. Related Work Several researchers have described frameworks and methodologies for codesign      . Codesign environments differ by the way in which the Hw/Sw are described, abstraction level and communication model used. Most of the previous work have been targeted towards either cosimulation
9391909	21965	software is one of the main issues when dealing with cosimulation and cosynthesis. Some approaches make use of a fixed communication scheme depending on a fixed architectural platform    , in which case the first two problems addressed earlier are easily handled. Although some of these architectures are flexible (e.g. supports a variable number of hardware and software processors)
9391909	20467	synthesized, and the partitioned descriptions are evaluated. In the VULCAN cosynthesis system, the input language is HardwareC and the design system tries to gradually move hardware to software . The Cosyma cosynthesis system (cosynthesis for embedded architectures) also starts with a single specification of the system given in C x (a super-set of the ANSI C standard). The approach is
9391909	20467	some of these architectures are flexible (e.g. supports a variable number of hardware and software processors) they use a fixed communication model to exchange information between protocols   . The COSYMA system makes use of a model of the Sw processor for Hw/Sw communication. In this manner, COSYMA ensures coherence between cosimulation and cosynthesis and provides estimation
9391909	21968	some of these architectures are flexible (e.g. supports a variable number of hardware and software processors) they use a fixed communication model to exchange information between protocols   . The COSYMA system makes use of a model of the Sw processor for Hw/Sw communication. In this manner, COSYMA ensures coherence between cosimulation and cosynthesis and provides estimation of
9391909	21968	and communicates with a hardware simulator through Unix inter-process communication mechanisms, using message passing. The COBRA project uses a prototyping environment based on a fixed architecture . The project makes use of a FPGA based prototyping board called SPARROW. Therefore, it supports standard processor integration under real-time conditions as well as processor emulation. A more
9391909	21969	and cosynthesis and provides estimation of performances. The VULCAN system also uses a processor model for the software component . Therefore it provides a very accurate simulation. In , hardware and software descriptions are treated as separate Unix processes which communicate through BSD (Berkeley Software Distribution) sockets. Since its targeting a specific application, it
9391909	19777	on the Unix IPC layer. The software bus and the elements used to interface VHDL and C modules during simulation where automatically created by a VHDL-C interface generation tool (named VCI)  . The system was described as a VHDL structure to allow the interconnection of Hw and Sw parts. The Sw subsystem (a C-program executed on the workstation) was interconnected to the Hw parts14 CARLOS
58677	21976	testing, intrinsically increasing its reliability. Moreover, engine reuse also makes it economically feasible to use high-cost/highreliability validation such as application of formal methods . However, so far planners are rarely used in on-board control systems for robots. When they are used, the planners are relegated to optimizing high-level task allocation over extended horizon while
58677	21977	All of this is achieved through local reactive planning and plan execution. 3.3 Model description The underlying planning technology used in both IDEA controllers is the EUROPA planning technology , a direct descendent of the Planner/Scheduler that was part of the Remote Agent . The modeling language used for the agent models is the Domain Description Language (DDL) supported by EUROPA.
58677	11840	description The underlying planning technology used in both IDEA controllers is the EUROPA planning technology , a direct descendent of the Planner/Scheduler that was part of the Remote Agent . The modeling language used for the agent models is the Domain Description Language (DDL) supported by EUROPA. Thus, designing a model is equivalent to defining a set of parallel timelines, sets of
58677	21979	a low-level functional layer, often organized as a collection of controllers communicating according to a static routing map, and a high level decision layer using a procedural execution system . Technological diversity among layers is problematic since each layer’s machinery is described with a different computational model and supports different programming languages and methods without
58677	11850	systems for robots. When they are used, the planners are relegated to optimizing high-level task allocation over extended horizon while lower-level control is achieved with procedural execution  or behavior-based control . This situation is partly due to a reaction to early attempts to build plan-based mobile control systems  where planning wassidentified as a principal obstacle to
8830864	21982	Other desktop distributed systems require developers to modify their source code to use custom APIs or simply rely on the application to be “well behaved” and provide weaker security and protection. These solutions are not desirable, since it is not always possible to get access to the application source code (especially for commercially available applications) and, regardless, maintaining
8830864	9230	Testing typically involves assessing the binding affinity of the test molecule to a specific place on a protein in a procedure commonly called docking. Docking codes (e.g., FRED , DLPOLY , and MOE ) are well-matched for distributed computing as each candidate molecule can be evaluated independently. The amount of data required for each molecular evaluation is small–basically the
8830864	21984	large disks (60 to 150 GB disks). Deployed in an enterprise, these systems harvest idle cycles from PCs sitting on hundreds to ten thousands of employees desks. Such distributed computing systems  1 leverage the installed hardware capability (and work well even with low performance PC’s) and thus can achieve a cost per unit computing (or Return-On-Investment) superior to the cheapest
8830864	5714	large disks (60 to 150 GB disks). Deployed in an enterprise, these systems harvest idle cycles from PCs sitting on hundreds to ten thousands of employees desks. Such distributed computing systems  1 leverage the installed hardware capability (and work well even with low performance PC’s) and thus can achieve a cost per unit computing (or Return-On-Investment) superior to the cheapest
8830864	6719	goals. On the other hand, performance is one of the most important requirement in desktop grid systems. Another class of virtual machine systems is exemplified by VMware  and Terra . They allow multiple operating systems to run concurrently on a same hardware resource, providing each OS an isolated virtual machine. A main goal for VMware  is to enable applications
8830864	6719	VM addresses different problems and they are complementary in the sense that VMware technology can enable a wider range of applications being deployed in the desktop grid environment. Terra  has a different design goal than VMware from the observation that different applications may have entirely different security requirements, which fundamentally post very different requirements on
8830864	21992	security and portability goals. On the other hand, performance is one of the most important requirement in desktop grid systems. Another class of virtual machine systems is exemplified by VMware  and Terra . They allow multiple operating systems to run concurrently on a same hardware resource, providing each OS an isolated virtual machine. A main goal for VMware  is to enable
8884949	21993	in that sense that the user can use not only the CSL constructs but also everything that is below them: C++/CSIM, C++, and plain C. More details on CSL can be found in the HP Labs report (Kotov et al. 1998). Kotov 1572 2 THE CSL STRUCTURE There are several levels of the modeling primitives and constructs in the CSL environment. The lowest, CSL BASE level, is formed by data structures, classes, and
8921938	21999	deux. Ce premier étiquetage est ensuite amélioré de façon itérative en utilisant un classifieur qui utilise ses propres sorties et la confiance qui leur est associé pour re-étiqueter les phrases. sApprentissage Numérique pour le Résumé de Texte 3 L’algorithme présente les avantages des méthodes discriminantes, à savoir qu’il est juste nécessaire d’estimer des probabilités a posteriori Ô
8921938	21999	de classes peut être utilisé. Nous avons analysé le comportement de cet algorithme dans le cas de classifieurs linéaires et de classifieurs logistiques sur deux bases de données différentes . Pour chacun des classifieurs nous avons montré sous certaines hypothèses la convergence de l’algorithme vers un maximum local et de la vraisemblance classifiante. Ce critère est beaucoup employé
8921938	9205	qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à s’intéresser aux techniques d’apprentissage pour effectuer des résumés automatiques de textes  . Ces
8921938	22000	nouvelles approches ont commencé à être explorées: 1. des approches qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à s’intéresser aux techniques d’apprentissage
8921938	22003	qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à s’intéresser aux techniques d’apprentissage pour effectuer des résumés automatiques de textes  . Ces
8921938	22004	approches ont commencé à être explorées: 1. des approches qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à s’intéresser aux techniques d’apprentissage pour
8921938	22006	approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à s’intéresser aux techniques d’apprentissage pour effectuer des résumés automatiques de textes  . Ces techniques permettent de s’adapter au corpus traité ou aux demandes particulières de l’utilisateur. Toutefois, toutes ces approches reposent sur de l’apprentissage supervisé, ce qui pour
8921938	22006	de textes. Cette technique a été testée sur deux base deReuters et deSummac. Dans les deux cas, elle présente une augmentation des performances par rapport aux systèmes d’apprentissage classiques . Toutefois, nous n’avons pas considéré le cas où les phrases aient un sens similaire (problème de doublons) qui peuvent être extraites par notre système. Ce traitement est compliqué du fait qu’il
8921938	22008	et la tâche de résumé automatique a suscité de nouvelles vocations. Beaucoup de nouvelles approches ont commencé à être explorées: 1. des approches qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment,
8921938	22009	de nouvelles vocations. Beaucoup de nouvelles approches ont commencé à être explorées: 1. des approches qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à
8921938	14652	de nouvelles vocations. Beaucoup de nouvelles approches ont commencé à être explorées: 1. des approches qui tentent d’utiliser la structure du discours , 2. des approches linguistiques , 3. des approches statistiques , 4. une combinaison de ces deux dernières approches .s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à
8921938	22015	.s2 Massih-Réza Amini, Patrick Gallinari Récemment, différents auteurs ont commencé à s’intéresser aux techniques d’apprentissage pour effectuer des résumés automatiques de textes  . Ces techniques permettent de s’adapter au corpus traité ou aux demandes particulières de l’utilisateur. Toutefois, toutes ces approches reposent sur de l’apprentissage supervisé, ce qui pour
22016	1177	federated database system integrates heterogeneous, autonomous database systems, whereby both local applications and global applications accessing multiple component database systems are supported . Such a federated database system is a complex system of systems which requires a well designed organization at the software architecture level. A problem that federated database systems face, is
22016	1177	must be extended to support the dimensions of distribution, heterogeneity, and autonomy. The generally accepted reference architecture for schemas in federated database systems is presented in . As reported in , this reference schema architecture is generally accepted as the basic structure in federated database systems or at least for comparison with other specific architectures. A
22016	1177	for federated database systems described in the UML notation for class diagrams . In this model, some of the constraints and options for the architecture, which are informally discussed in , are defined by means of the multiplicities at the associations and other notational means. The different schema types in Figure 2 are: Local Schema: A Local Schema is the conceptual schema of a
22016	1177	the Local and Federated Schemas, are optional. Note, that a schema architecture which consists of just 3 one Federated Schema and some Local Schemas concurs with the 5-level schema architecture of . The other levels contain no schemas in this case. ¯ A component database system can participate in more than one federation and continue the operation of local applications. Thus, Local Schemas
8921944	22024	used for either static images, or sequences for object tracking. Other examples include recently proposed segmentation algorithms as the depth-, area- and volumecontrolled continuous watershed , combinations of the level set methods with other techniques Principal Component Analysis (PCA) . An overview of image improvement (PDE-based) operators can be found in . In general, the
8921945	5285	Transform in our case) to a signal decomposes this one with a multiresolution scale factor of two providing at each resolution level one low-resolution approximation (A) and one wavelet detail (D) . Experimentally, we choose the Daubechies-4 wavelet basis (several others basis have been tested and no dependence was pointed out at the exception of the Haar Basis) ; and the wavelet
8921945	22041	within a subset) (Table 1-column 2). In order to evaluate the ability of the proposed method to delineate the osseous interface in strongly corrupted images, we also compute the Signal-to-mse ratio  (using a 5x5 median filter to denoise the image) which corresponds to the classical Signal-to-Noise ration computed in case of additive noise (Table 1-column 3). pixel size is 0.1mm.1mm Table 1:
22044	4377	and the robustness of the best individual fingerprint matcher, and outperforming some simple fusion rules. 1. Introduction Fingerprints are widely used for automatic personal authentication , in order to control the access to limited areas or resources. The person to be “authenticated” submits to the system her/his fingerprint and declares her/his identity. The system matches the input
22044	4377	under the assumption of uncorrelated classifiers , for our experiments, we selected one algorithm for each type. The selected minutiae-based algorithm is commonly referred as “String” algorithm . The ridge bifurcations and endings, usually called “minutiae” (Figure 2), are extracted from the input fingerprint image. Such “minutiae” set is compared with that of the template fingerprint.
22044	4378	shows the ridge flow of the fingerprint. The ridge flow defines the “texture” of fingerprint. Various methods for describing the texture of fingerprints by the orientation field have been proposed . Matchers proposed so far use one of the two above fingerprint representations and produce authentication scores, that is, degrees of similarity between the input and the template fingerprints.
22044	4378	. Figure 2. Fingerprint representation based on the “map” of the minutiae-points locations and orientations. Figure 3. Fingerprint texture representation by the so called “finger=code”. See Reference 3 for further details. The selected texture-based algorithm is also known as “Filter” algorithm . The input fingerprint image is “partitioned” around its centre (the so called
22044	4378	sector is called “finger-code”. The verification score is obtained by computing the Euclidean distance between the finger-code of the input image and the one of the template image. See Reference  for details. Other texture-based algorithms have been proposed, but their use is often limited to the fingerprint classification task , for which the fingercode has shown the best performance
22044	4382	shows the ridge flow of the fingerprint. The ridge flow defines the “texture” of fingerprint. Various methods for describing the texture of fingerprints by the orientation field have been proposed . Matchers proposed so far use one of the two above fingerprint representations and produce authentication scores, that is, degrees of similarity between the input and the template fingerprints.
22044	4382	for details. Other texture-based algorithms have been proposed, but their use is often limited to the fingerprint classification task , for which the fingercode has shown the best performance . In general, the characteristics of these approaches also constitute their main limitations, as the fingerprint description is less detailed with respect to the minutiae-based one. Consequently,
22044	4376	the performances of the best individual matcher. This strategy is called fusion of multiple matchers. Although such strategy appeared to be promising on the basis of recent experimental results , few papers investigated the potentialities of fusing multiple fingerprint matchers. It is well known that the ability in separating genuine and impostor users is the fundamental requirement for an
22044	4376	following transformation to the above scores sm and st: ( m, t ) s s f s = (7) All the fusion rules investigated in this paper can be regarded as the application of a particular transformation rule . - Compare the obtained score value s with a threshold. The claimed identity is classified as “genuine” if: s > th (8) otherwise it is classified as “impostor”. It is easy to see that the above
22044	4384	rules. 5. Experimental results 5.1. The Data Set For our experiments, we used the FVC-DB1 data base that was recently introduced as a benchmark data set for fingerprint verification algorithms . This data set is made up of 800 fingerprint images acquired from a lowcost optical sensor. The image size is 300x300 pixels and the resolution is 500 dpi. The number of identities is 100, and the
8921952	23486	We use Cluster Entropy (CE) to represent the distinctness of a phrase. CE = ? ? where it is defined that 0?log0=0. t D( w) ? D( t) D( w) ? D( t) log D( w) D( w) Phrase Independence According to , a phrase is independent when the entropy of its context is high (i.e. the left and right contexts are random enough). We use IND to measure the independence of phrases. The following is the
8921952	23487	we conclude the paper and give some future works in Section 7. 2. RELATED WORKS The problem of clustering search results has been investigated in a number of previous works. Some of them (e.g. ) apply traditional clustering algorithms which first cluster documents into topically-coherent groups according to content similarity, and generate descriptive summaries for clusters.
8921952	23489	we conclude the paper and give some future works in Section 7. 2. RELATED WORKS The problem of clustering search results has been investigated in a number of previous works. Some of them (e.g. ) apply traditional clustering algorithms which first cluster documents into topically-coherent groups according to content similarity, and generate descriptive summaries for clusters.
8921952	23491	Regression Analysis General Terms Algorithms, Experimentation Keywords Search result organization, document clustering, regression analysis 1. INTRODUCTION Existing search engines such as Google , Yahoo  and MSN  often return a long list of search results, ranked by their relevancies to the given query. Web users have to go through the list and examine the titles and (short)
8921952	23500	we conclude the paper and give some future works in Section 7. 2. RELATED WORKS The problem of clustering search results has been investigated in a number of previous works. Some of them (e.g. ) apply traditional clustering algorithms which first cluster documents into topically-coherent groups according to content similarity, and generate descriptive summaries for clusters.
8921952	23504	Regression Analysis General Terms Algorithms, Experimentation Keywords Search result organization, document clustering, regression analysis 1. INTRODUCTION Existing search engines such as Google , Yahoo  and MSN  often return a long list of search results, ranked by their relevancies to the given query. Web users have to go through the list and examine the titles and (short)
8921952	23505	We use y to sort salient keywords in a descending order, thus the most salient keywords are shown on the top. Several regression models could be used, such as linear regression, logistic regression  and support vector regression . We summarize them in the below and will further compare their effectiveness in the experiments. Linear Regression Linear regression attempts to explain the
8921952	23513	for query &quot;jaguar&quot;, if there is a group named &quot;big cats&quot;, the four relevant results will be ranked high in the corresponding list (as shown in Figure 1). Several previous works  are conducted to develop effective and efficient clustering technology for search result organization. In addition, Vivisimo  is a real demonstration of this technique. Jaguar • Jaguar Cars
8921952	23513	the paper and give some future works in Section 7. 2. RELATED WORKS The problem of clustering search results has been investigated in a number of previous works. Some of them (e.g. ) apply traditional clustering algorithms which first cluster documents into topically-coherent groups according to content similarity, and generate descriptive summaries for clusters. However,
8921952	23514	for query &quot;jaguar&quot;, if there is a group named &quot;big cats&quot;, the four relevant results will be ranked high in the corresponding list (as shown in Figure 1). Several previous works  are conducted to develop effective and efficient clustering technology for search result organization. In addition, Vivisimo  is a real demonstration of this technique. Jaguar • Jaguar Cars
8921952	23514	the paper and give some future works in Section 7. 2. RELATED WORKS The problem of clustering search results has been investigated in a number of previous works. Some of them (e.g. ) apply traditional clustering algorithms which first cluster documents into topically-coherent groups according to content similarity, and generate descriptive summaries for clusters. However,
8921952	23515	process is similar to STC but we further calculate several important properties to identify salient phrases, and utilize learning methods to rank these salient phrases. Some topic finding  or text trend analysis  works are also related to our method. The difference is that we are given titles and short snippets rather than whole documents. Meanwhile, we train regression model
8921952	23516	We use Cluster Entropy (CE) to represent the distinctness of a phrase. CE = ? ? where it is defined that 0?log0=0. t D( w) ? D( t) D( w) ? D( t) log D( w) D( w) Phrase Independence According to , a phrase is independent when the entropy of its context is high (i.e. the left and right contexts are random enough). We use IND to measure the independence of phrases. The following is the
8921952	23518	Regression Analysis General Terms Algorithms, Experimentation Keywords Search result organization, document clustering, regression analysis 1. INTRODUCTION Existing search engines such as Google , Yahoo  and MSN  often return a long list of search results, ranked by their relevancies to the given query. Web users have to go through the list and examine the titles and (short)
8921952	23521	process is similar to STC but we further calculate several important properties to identify salient phrases, and utilize learning methods to rank these salient phrases. Some topic finding  or text trend analysis  works are also related to our method. The difference is that we are given titles and short snippets rather than whole documents. Meanwhile, we train regression model
8921952	23523	we conclude the paper and give some future works in Section 7. 2. RELATED WORKS The problem of clustering search results has been investigated in a number of previous works. Some of them (e.g. ) apply traditional clustering algorithms which first cluster documents into topically-coherent groups according to content similarity, and generate descriptive summaries for clusters.
8921962	22060	costs in communications due to their ability to asynchronously and autonomously without a network connection. 2. AGENTS: THE STATE OF THE ART 2.1. Definition According to Franklin & Graesser, they try to summarize the several definitions of an autonomous agents, with: “An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on
8921962	22060	The various definitions mentioned above involve a host of properties of an agent. These properties may help us further classify agents in meaningful ways. The table (Franklin & Graesser)  that follows lists several of the properties enumerated above: Property Other Names Meaning Reactive Sensing and acting Responds in a timely fashion to changes in the environment autonomous
8921962	22064	Although the majority of the contemporary mobile agent systems are based on the Java language system, we shall also find other languages in use. The most significant languages are Tcl , Scheme and Python. 3.8. MOBILE AGENTS ARCHITECTURES First of all, it would be very useful to know what the MAs architectures are. A computational architecture specifies the organization of a mobile
8921962	10411	service) Protecting agents from hosts: Tampering, Extracting information, Capture / Replay. 3.13. TRENDS OF MOBILE AGENTS AND THE FUTURE OF THE INTERNET There are several trends (Kotz & Gray, 1999) affecting Internet technology: Bandwidth: The telecommunications industry is laying down astonishing amounts of fiber. Although Internet traffic is growing extensively, the bandwidth soon to be
8921962	10411	a MA system but rather as fundamental issues that must be addressed urgently in order to take advantage of a MA system in the mobile world. There are several technical obstacles (Kotz & Gray, 1999) that must be cleared before mobile agents can be widely used. Performance and scalability: Current mobile-agent systems save network latency and bandwidth at the expense of higher loads on the
8921962	10411	a vast adoption of mobile-agent technology. Internet sites must have a strong motivation to overcome apathy, justify the cost of upgrading their systems, and adopt the technology (Kotz & Gray, 1999). While the technological arguments above are significant, they are not sufficient for most site administrators. In the end, the technology will be installed only if it provides substantial
8921962	22074	or broadcast (one-tomany messaging scheme allowing a single agent to send a message to a group of agents). 3.5. MOBILE AGENT STRUCTURE In SeMoA – Secure Mobile Agents (Roth & Jalali-Sohi, 1998), mobile agents are transported as Java Archives (JAR files). The JAR specification of Sun Microsystems extends ZIP archives with support for digital signatures by means of adding appropriate
8921962	22074	cryptographic operations and key management. The structure of an agent's JAR file is represented in the Figure 11: Figure 11: The structure of an agent’s JAR file. Source: (Roth & Jalali-Sohi, 1998). The file agent.properties consists of name/value pairs. The properties as well as the initial classes brought by an agent are covered by the owner's signature, thus any modification of the
8921962	22074	IFT 6261 (WINTER 2003) • An Example in the SeMoA’s Architecture A mobile agent's itinerary in general spans a number of servers which might be run by competing operators (Roth & Jalali-Sohi, 2000). Apart from monitoring, manipulating, and stealing data from mobile agents, malicious hosts might try to abuse passing agents as Trojan Horses in attacks on competing servers while incriminating
8921962	22074	BRAZ - February 21, 2003 45sMOBILE AGENTS FOR WIRELESS E-COMMERCE APPLICATIONS ARTIFICIAL INTELLIGENCE IFT 6261 (WINTER 2003) The security architecture of the SeMoA(Roth & Jalali-Sohi, 2000) server compares to an onion: agents have to pass all of several layers of protection before they are admitted to the runtime system (see Figure 15) and the first class of an agent is loaded into
8921972	22084	is not just a tool to mimic the real-world system for analyzing it, but it has become a popular technique for developing production schedules and dispatch lists in manufacturing environmentss(Mazziotti and Horne 1997, Morito and Lee 1997, Sivakumar 1999). An important aspect of simulation-based scheduling is that it uses actual customer orders and WIP information, and not arrival/demand data estimated from
8921972	22084	update with the real information, simulation offers the advantage of developing a feasible and accurate schedule in shorter computation times compared to some of the other techniques (Kiran 1998, Mazziotti and Horne 1997), even for the job shop scheduling problems which are considered as NP-hard (Pinedo, 1995). Davis (1998) and Sivakumar (2001) report the application of online simulation in complex manufacturing
22147	6956	kind of priority-based scheduling schemes cannot achieve PSD provisioning. Admission control is often used in combination with priority-based scheduling for DiffServ provisioning. For example, in , the authors used classical feedback control theory to achieve overload protection, performance guarantees, and service differentiation in Web servers. The strategy was based on real-time
22147	22148	significantly depending on the requested services. In the server side, a primary focus of DiffServ provisioning has been on priority-based request scheduling for responsive time differentiation . For example, in , the authors addressed strict priority scheduling strategies for controlling CPU utilization in Web content hosting servers. QoS was introduced by assigning priorities to
22147	22149	Recent Internet workload measurements indicate that, for many Web applications, a heavy-tailed distribution is a more accurate model for service time distribution than the exponential distribution . In general, a heavy-tailed distribution is one for which Pr?? ? Ü? ?Üs« ? ?«? , where ? denotes the service time density distribution. The Pareto distribution is a typical heavy-tailed
22147	22149	workload measurements indicate that for many Web applications the exponential distribution is a poor model for service time distribution and that a heavytailed distribution is more accurate . In this paper, we investigate the problem of processing rate allocation for PSD provisioning under a popular heavy-tailed traffic pattern (Bounded Pareto). In , Harchol used slowdown as a
22147	1623	(DiffServ) architecture, which aims to provide different QoS levels among multiple classes of aggregated traffic flows, has been an active research topic since it was formulated by IETF in 1998 . Basically, there are two types of DiffServ scheme. One is absolute DiffServ, in which each class receives an absolute share of resource usages. The other is relative DiffServ, in which a class
22147	22150	processing rate allocation in our work. Note that a task server is an abstract concept in the sense that it can be a child process in a multi-process server, or a thread in a multi-thread server . The structure of the paper is as follows. Section 2 gives a slowdown model for Å??È ? FCFS queues. Section 3 presents the processing rate allocation strategy for PSD provisioning. Section 4
22147	22150	significantly depending on the requested services. In the server side, a primary focus of DiffServ provisioning has been on priority-based request scheduling for responsive time differentiation . For example, in , the authors addressed strict priority scheduling strategies for controlling CPU utilization in Web content hosting servers. QoS was introduced by assigning priorities to
22147	6682	requires that the scheduler contain a number of controllable parameters that are adjustable for the control of quality spacings between classes. The proportional differentiation model  states that certain class performance metrics should be proportional to the differentiation parameters the network operator chooses, independent of the class loads. The model has been accepted as
22147	6682	Differentiation A proportional differentiation model ensures the quality spacing between class ? and class ? to be proportional to certain pre-specified differentiation parameters Æ ? and Æ? ; that is, Õ? Õ? ? Æ? Æ??? ? ?? ? ? Æ? where Õ? and Õ? are the QoS factor of class ? and class ?, respectively. So it is up to applications and clients to select appropriate QoS levels in terms of
22147	6682	decreases; as the upper bound increases, the slowdown increases. ?s5 Related Work The DiffServ provisioning problem was first addressed in the network core. The proportional differentiation model  has been accepted as an important DiffServ model and been applied in the PDD model in packet scheduling . Many algorithms have been designed to achieve the PDD provisioning in the network
22147	6683	chooses, independent of the class loads. The model has been accepted as an important relative DiffServ model and been applied in the proportional delay differentiation (PDD) in packet scheduling . In this model, the network traffic is categorized into Æ classes of service. Each class is assigned a queueing delay differentiation parameter. The packet scheduler of a router gives different
22147	6683	classes. The objective is to keep the ratio of average delay of a higher priority class to that of a lower priority class equal to the pre-specified value. Since the PDD model was formulated in , many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based , and time-dependent prioritysbased [9, 11,
22147	6683	Note that the number of classes in PSD provisioning on the server is usually rather limited. It varied from 2 to 3 in many similar experiments for PDD provisioning in packet scheduling . Each request was sent to the server and stored in a waiting queue according to its class type. Requests from the same class were processed by a task server in a FCFS manner.sSlowdown 100 10 1
22147	6683	problem was first addressed in the network core. The proportional differentiation model  has been accepted as an important DiffServ model and been applied in the PDD model in packet scheduling . Many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based; see BPR  and JoBS  for examples,
22147	6683	in , many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based , and time-dependent prioritysbased . The end-to-end time performance of Internet services is not only due to the packet transmission delay in the network core, but also due to the processing and queueing delay on the servers.
22147	6683	Note that the number of classes in PSD provisioning on the server is usually rather limited. It varied from 2 to 3 in many similar experiments for PDD provisioning in packet scheduling . Each request was sent to the server and stored in a waiting queue according to its class type. Requests from the same class were processed by a task server in a FCFS manner.sSlowdown 100 10 1
22147	6683	rate was reallocated for every thousand time units. Simulation parameters were set as follows. The shape parameter («) of the Bounded Pareto distribution was set equal to 1.5, as suggested in . As indicated in , its lower bound and upper bound were set equal to 0.1 and 100, respectively. We also did experiments for larger upper bound settings to evaluate the influence. We assumed
22147	6683	the PDD provisioning in the network routers. They can be classified into two categories: rate-based; see BPR  and JoBS  for examples, time-dependent priority based; see WTP, PAD, and HPD , adaptive WTP , MDP , and VirtualLength  for examples. Servers play an important role in end-to-end DiffServ provisioning. Those algorithms can be tailored for request scheduling
22147	7048	significantly depending on the requested services. In the server side, a primary focus of DiffServ provisioning has been on priority-based request scheduling for responsive time differentiation . For example, in , the authors addressed strict priority scheduling strategies for controlling CPU utilization in Web content hosting servers. QoS was introduced by assigning priorities to
22147	22152	processing requirement. A high slowdown can also indicate that the system is heavily loaded . Slowdown is being used as a fundamental performance metric of responsiveness on Internet servers . However, few work exists for slowdown differentiation. Existing time-dependent priority based PDD packet scheduling algorithms cannot be tailored to achieve proportional slowdown differentiation
22147	22152	Recent Internet workload measurements indicate that, for many Web applications, a heavy-tailed distribution is a more accurate model for service time distribution than the exponential distribution . In general, a heavy-tailed distribution is one for which Pr?? ? Ü? ?Üs« ? ?«? , where ? denotes the service time density distribution. The Pareto distribution is a typical heavy-tailed
22147	22152	mass function ? Ü ?«? «s«sÜ «? ? ? ?Ü ? ?? (1) and cumulative distribution function ? Ü ? Pr?? ? ? Ü? ?sÜ « . In practice, there is some upper bound on the maximum size of a job. As the work in , throughout this paper, we model job sizes as being generated i.i.d. from a distribution that is heavy-tailed, but has an upper bound. This Bounded Pareto distribution is characterized by three
22147	22152	for every thousand time units. Simulation parameters were set as follows. The shape parameter («) of the Bounded Pareto distribution was set equal to 1.5, as suggested in . As indicated in , its lower bound and upper bound were set equal to 0.1 and 100, respectively. We also did experiments for larger upper bound settings to evaluate the influence. We assumed that all classes had the
22147	22152	workload measurements indicate that for many Web applications the exponential distribution is a poor model for service time distribution and that a heavytailed distribution is more accurate . In this paper, we investigate the problem of processing rate allocation for PSD provisioning under a popular heavy-tailed traffic pattern (Bounded Pareto). In , Harchol used slowdown as a
22147	22154	but also due to the processing and queueing delay on the servers. Therefore, servers are a major force in DiffServ provisioning. Those algorithms can be tailored for PDD provisioning on servers . However, in the server side, an important and interesting performance metric is slowdown, the ratio of a request’s queueing delay to its service time. Both queueing delay and response time are
22147	22154	VirtualLength  for examples. Servers play an important role in end-to-end DiffServ provisioning. Those algorithms can be tailored for request scheduling for PDD provisioning in the server side . However, the algorithms are not applicable to PSD provisioning in the server side because slowdown is not only dependent on a job’s queueing delay but also on its service time, which varies
22147	22154	is maintained below a pre-computed bound. Thus, controltheory approaches, in combination with content adaptation strategies, were formulated to keep server utilization at or below the bound. In , the authors proposed admission control algorithms in combination with time-dependent priority scheduling for proportional queueing-delay differentiation on a Web server. Therefore, this kind of
22147	2673	is that, the slowdown of a class decreases as the shape parameter increases. Intuitively, for the given lower and upper bounds, the smaller the shape parameter «, the burstier the generated traffic . A request may experience larger queueing delay than that from a “smooth” traffic. Formally, by (4), (5), we know that, when the shape parameter decreases, its second moment ??? ? increases, its
22147	22155	in , many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based , and time-dependent prioritysbased . The end-to-end time performance of Internet services is not only due to the packet transmission delay in the network core, but also due to the processing and queueing delay on the servers.
22147	22155	Note that the number of classes in PSD provisioning on the server is usually rather limited. It varied from 2 to 3 in many similar experiments for PDD provisioning in packet scheduling . Each request was sent to the server and stored in a waiting queue according to its class type. Requests from the same class were processed by a task server in a FCFS manner.sSlowdown 100 10 1
22147	22155	in the network routers. They can be classified into two categories: rate-based; see BPR  and JoBS  for examples, time-dependent priority based; see WTP, PAD, and HPD , adaptive WTP , MDP , and VirtualLength  for examples. Servers play an important role in end-to-end DiffServ provisioning. Those algorithms can be tailored for request scheduling for PDD provisioning in
22147	22156	value. Since the PDD model was formulated in , many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based , and time-dependent prioritysbased . The end-to-end time performance of Internet services is not only due to the packet transmission delay in the network core, but also due to
22147	22156	in packet scheduling . Many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based; see BPR  and JoBS  for examples, time-dependent priority based; see WTP, PAD, and HPD , adaptive WTP , MDP , and VirtualLength  for examples. Servers play an important role in end-to-end DiffServ
22147	22157	in , many algorithms have been designed to achieve the PDD provisioning in the network routers. They can be classified into two categories: rate-based , and time-dependent prioritysbased . The end-to-end time performance of Internet services is not only due to the packet transmission delay in the network core, but also due to the processing and queueing delay on the servers.
22147	22157	network routers. They can be classified into two categories: rate-based; see BPR  and JoBS  for examples, time-dependent priority based; see WTP, PAD, and HPD , adaptive WTP , MDP , and VirtualLength  for examples. Servers play an important role in end-to-end DiffServ provisioning. Those algorithms can be tailored for request scheduling for PDD provisioning in the server
22147	6277	a FCFS way. Recently, there has been a renewal of interests in achieving the proportional-share resource scheduling among multiple queues in both operating systems and networks; see GPS , PGPS , and Lottery Scheduling  for examples. They provide a base for the processing rate allocation in our work. Note that a task server is an abstract concept in the sense that it can be a child
22147	6277	formula follows from the fact that Ï and ? are independent from a FCFS queue. 2.2 Slowdown on Internet Servers Based on the proportional-share resource scheduling mechanisms like GPS , PGPS , and Lottery Scheduling , we assume that the processing rate of an Internet server can be proportionally allocated to a number of task servers. Each task server ? (? ? ???Æ) represents a
22147	22158	processing requirement. A high slowdown can also indicate that the system is heavily loaded . Slowdown is being used as a fundamental performance metric of responsiveness on Internet servers . However, few work exists for slowdown differentiation. Existing time-dependent priority based PDD packet scheduling algorithms cannot be tailored to achieve proportional slowdown differentiation
22147	22159	has been a renewal of interests in achieving the proportional-share resource scheduling among multiple queues in both operating systems and networks; see GPS , PGPS , and Lottery Scheduling  for examples. They provide a base for the processing rate allocation in our work. Note that a task server is an abstract concept in the sense that it can be a child process in a multi-process
22147	22159	fact that Ï and ? are independent from a FCFS queue. 2.2 Slowdown on Internet Servers Based on the proportional-share resource scheduling mechanisms like GPS , PGPS , and Lottery Scheduling , we assume that the processing rate of an Internet server can be proportionally allocated to a number of task servers. Each task server ? (? ? ???Æ) represents a processing unit that handles
22147	22161	processing requirement. A high slowdown can also indicate that the system is heavily loaded . Slowdown is being used as a fundamental performance metric of responsiveness on Internet servers . However, few work exists for slowdown differentiation. Existing time-dependent priority based PDD packet scheduling algorithms cannot be tailored to achieve proportional slowdown differentiation
22147	22162	long delays for ”large” requests. Thus, it is desirable that a request’s delay be proportional to its processing requirement. A high slowdown can also indicate that the system is heavily loaded . Slowdown is being used as a fundamental performance metric of responsiveness on Internet servers . However, few work exists for slowdown differentiation. Existing time-dependent
22147	22162	a Web server. Therefore, this kind of admission control itself is not sufficient in PDD provisioning and is not applicable to PSD provisioning. Stretch factor, a variant of slowdown, was adopted in  as the performance metric for DiffServ provisioning in a cluster of Internet servers. The authors proposed a demand-driven DiffServ strategy by adopting an Å?Å? queueing model to guide node-based
22163	22166	linguistic levels is a critical task in current MT technology. It serves different purposes in different researchers’ work. For example, it is utilised to construct statistical translation models  and to acquire examples for example-based machine translation (EBMT). EBMT is one of the most prominent modern MT paradigms whose basic ideas originally evolved from Nagao . However, EBMT
22163	22168	linguistic levels is a critical task in current MT technology. It serves different purposes in different researchers’ work. For example, it is utilised to construct statistical translation models  and to acquire examples for example-based machine translation (EBMT). EBMT is one of the most prominent modern MT paradigms whose basic ideas originally evolved from Nagao . However, EBMT
22163	22171	information (such as sentence length, sentence position, co-occurrence frequency, sentence length ratio in two languages, etc.) to achieve alignment tasks, as illustrated in previous research . We may refer to such approaches as resource-poor approaches. The attraction of these approaches arises from the sharp contrast between their poor resources and their rich outcomes. The sentence
22163	22173	information (such as sentence length, sentence position, co-occurrence frequency, sentence length ratio in two languages, etc.) to achieve alignment tasks, as illustrated in previous research . We may refer to such approaches as resource-poor approaches. The attraction of these approaches arises from the sharp contrast between their poor resources and their rich outcomes. The sentence
22163	22176	information (such as sentence length, sentence position, co-occurrence frequency, sentence length ratio in two languages, etc.) to achieve alignment tasks, as illustrated in previous research . We may refer to such approaches as resource-poor approaches. The attraction of these approaches arises from the sharp contrast between their poor resources and their rich outcomes. The sentence
22203	22204	network congestion may cause more losses due to the fact that congestion windows may grow to veryshigh values; however, it will last less and potentially, it will appear much less frequently . The dynamics of combined wired and wireless errors appears to be far more important issue than the ability to geographically locate the error and apply the well-known techniques. Transient
22203	22205	discussed the major approaches, we highlight in section 6 some issues which are yet to be solved. 2 An approach to calculate the fair-share without any network support was published recently in  2. GOALS, METRICS AND MODELS In the context of our work, we define the following measurement units: The window is a mechanism in the transport layer which limits the number of packets put into the
22203	22205	either an aggressive or a conservative strategy can be chosen. 5. THE BOX IS GREEN: EXPLICIT CALCULATION OF THE FAIR-SHARE 5.1 Bimodal Mechanism Bimodal congestion avoidance and control mechanism  measures the fair-share of the total bandwidth that should be allocated for each flow, at any point, during the system’s execution. If the fair-share were known, then the sources could avoid
22203	22205	capacity but is also dependent upon the number of participating flows, and the transmitting behavior of the sources. So, fair-share can be measured only in an equilibrium state. Authors proposed in  a bimodal mechanism, which is based on the idea that upwards and downwards adjustments need to operate in association with the system state. Action is determined based on whether the system is in
22203	9101	losses . 6. DISCUSSION There was recently a significant effort to distinguish congestionrelated errors from wireless errors. That effort invested mainly in distinguishing the locus of the error , rather than focusing on the dynamics from the combination of both errors. For example, persistent congestion which may be experienced in some router in a wired network may change to a more
22203	6528	authors improved in  furthermore the efficiency, smoothness and fairness of AIMD-FC and proposed a new algorithm named AIMD-FC+. 3.2 Binomial Mechanisms Bansal and Balakrishnan presented in  a new class of nonlinear congestion control algorithms named Binomial Congestion Control Algorithms. Those algorithms are called binomial because their control is based on the involvement of two
22203	6528	proportion to the current window. The second is called SQRT because both its increase is inversely proportional and decrease proportional to the square-root of the current window. Authors in  concluded the following: • A binomial algorithm is TCP-compatible if and only if k + l = 1 and l ? 1 for suitable a and b. • Those algorithms may compete unfairly across a droptail gateway. Use of
22203	19322	measurement-based transport protocols are based on more precise information on network conditions.s4.1 TCP-VEGAS A well-designed, measurement-based congestion avoidance mechanism is TCP Vegas . Vegas defines BaseRTT to be the minimum of all measured RTTs, and ExpectedRate to be the ratio of the congestion window to BaseRTT. The sender measures the ActualRate based on the sample RTTs. If
22203	19322	bound, the congestion window increases linearly during the next RTT; if the difference exceeds an upper bound, TCP Vegas decreases the congestion window linearly during the next RTT. According to , Vegas achieves better transmission rates than Reno and Tahoe. However,  shows that Vegas can not guarantee fairness. Plus, cannot distinguish nature of error. From the research perspective of
22203	22207	network congestion may cause more losses due to the fact that congestion windows may grow to veryshigh values; however, it will last less and potentially, it will appear much less frequently . The dynamics of combined wired and wireless errors appears to be far more important issue than the ability to geographically locate the error and apply the well-known techniques. Transient
22203	14628	is not exclusively a management property of the router or the base station, nor is it exclusively assigned to the transport layer. A widely accepted approach, presented by the end-to-end argument , states that we can only implement a function at a lower layer, if that layer can perform the complete function. Since the lower level cannot have enough information about the application’s
22203	2242	and be decreased when the RTT increases. • The integration of such ideas with a receiver-oriented feedback approach (like TCP-Real ). 5.2 Network-Assisted Congestion Control Red Gateways  drop packets when congestion is about to happen. RED randomly drops packets, triggering multiplicative decrease in some flows when the length of the queue exceeds a predetermined threshold. The
22203	22209	Multiplicative Decrease), which is not stable in Chiu-Jain model, does converge to fair-states under the more realistic assumption of proportional negative feedback. A new Gorinsky’s approach is . In a very recent work  Lahanas and Tsaoussidis described an asynchronous-feedback model, which corresponds to the diverse round-trip times (RTTs) of competing flows within the same
22203	22210	which may not really hold in real networks. We note that some drawbacks of the simplified synchronous model are somewhat canceled experimentally, due to the wide use of long ftp flows. Gorinsky in  shows that the choice of the model has a direct impact on the results and extends further the model of Chiu and Jain to include different RTTs and consequently asynchronous feedback. MIMD
22203	13165	difference exceeds an upper bound, TCP Vegas decreases the congestion window linearly during the next RTT. According to , Vegas achieves better transmission rates than Reno and Tahoe. However,  shows that Vegas can not guarantee fairness. Plus, cannot distinguish nature of error. From the research perspective of the present work it is important to consider that the authors of Vegas
22203	2243	phase. In case of a packet drop, instead of the multiplicative decrease a more conservative tactic is used in TCP-Tahoe. The conges3 This is not an exact equality because the RTT is variable  and increases with the load of the system. However, the definition gives an intuition of the relation between congestion epoch and the number of additive increasesstion window resets and the
22203	22211	network congestion may cause more losses due to the fact that congestion windows may grow to veryshigh values; however, it will last less and potentially, it will appear much less frequently . The dynamics of combined wired and wireless errors appears to be far more important issue than the ability to geographically locate the error and apply the well-known techniques. Transient
22203	22212	faster than AIMD. Authors have made their investigations using a more realistic asynchronous system model (users can have different roundtrip times). A different approach, based on game theory, is . 3. THE BOX IS BLACK: BLIND CONGESTION CONTROL The Additive Increase Multiplicative Decrease (AIMD) algorithm is used to implement TCP window adjustments; based on the analysis of Chiu and Jain the
22203	2997	network congestion may cause more losses due to the fact that congestion windows may grow to veryshigh values; however, it will last less and potentially, it will appear much less frequently . The dynamics of combined wired and wireless errors appears to be far more important issue than the ability to geographically locate the error and apply the well-known techniques. Transient
22203	22215	appears initially difficult to attain. However, if the system is entitled to a prescribed behavior and the entities agree on common transmission tactics, convergence 1 to fairness becomes feasible . AIMD, the traditional congestion control algorithm of the Internet, operates within that scope: it increases additively the rate of 1 Convergence to fairness should be perceived in this paper as
22203	22217	is not a design criterion for RAP, compared to TFRC. 3.4.4 Ideally Scalable Congestion Control (ISCC) Another rate-based congestion control is Ideally Scalable Congestion Control (ISCC) in . ISCC method is based on the idea of ”Ideal Scalability”. A scheme is defined to have ideal scalability, if Sn is constant for all flows. Where parameter Sn specifies how fast packet loss increases
22203	22218	missing packets, since the wave size is published to the receiver. The congestion window is multiplicatively reduced only when a drop is associated with congestion. 4.3 TCP-WESTWOOD In TCP-Westwood , the sender continuously measures the rate of the connection by monitoring the rate of returning ACKs. Upon three duplicate acknowledgments or timeout, the slow start threshold and the congestion
22203	22219	with ECN: TCP performance can be enhanced by means of avoiding losses of data windows due to limited buffer space at the bottleneck router, and congestive collapse can be avoided. Recent work  presents a critical discussion of the performance expectations with RED. An interesting observation about RED and ECN is that they could, somehow, confine future evolution. Imagine a more
22203	6533	AIMD congestion control by parameterizing the additive increase value a and multiplicative decrease ratio b. Authors of  extended the throughput equation for standard TCP, proposed in , to include parameters ?, ?: T a,b (p, RT T, T 0 , b) = ? 2b(1??) RT T ?(1+?) p + T0min(1, 3 4 1 ? (8) (1??2 )b p)p(1 + 32p 2? 2 ) (9) where p is the loss rate; T0 is the retransmission timeout
22203	22220	limit the loss down to one packet. It also reduces persistent queuing delay. RED can function without requiring any change to the current transport level infrastructure. Ramakrishnan and Floyd in  proposed an Explicit Congestion Notification (ECN) to be added to the IP protocol in order to trigger TCP congestion control. Unlike RED, ECN enables routers to probabilistically mark a bit in the
22203	22221	than TCP does). For b=7/8, (3) gives an increase value a=0.31. 3.4.2 TFRC TFRC is also a measurement-based protocol and will be discussed in section 4. 3.4.3 RAP Rate Adaptation Protocol (RAP)  is a rate-based transport protocol friendly to TCP. It employs additive increase and multiplicative decrease algorithm. It decouples network congestion control from application level reliability.
22203	22223	losses . 6. DISCUSSION There was recently a significant effort to distinguish congestionrelated errors from wireless errors. That effort invested mainly in distinguishing the locus of the error , rather than focusing on the dynamics from the combination of both errors. For example, persistent congestion which may be experienced in some router in a wired network may change to a more
22203	22225	to deal with wireless errors: it freezes the timeout and holds still the congestion window without transmitting any data when wireless errors do not allow the probing mechanism to be completed . Other mechanisms measure contention and decouple wireless errors from others. With what precision can we estimate really network conditions? How can we take into account the risk of wrong
22203	22226	attaches the results to the ACKs sent back to the sender. • Measurements based on wave patterns that distinguish the nature of a packet loss (due to congestion or transient wireless errors A wave  consists of a number of fixed-sized data segments sent back-to-back, matching the inherent characteristic of TCP to send packets back-to-back. The receiver computes the data-receiving rate of a
22203	9831	perspective of the present work it is important to consider that the authors of Vegas demonstrated effectively that measurement-based window adjustment is a viable mechanism. 4.2 TCP-REAL TCP-Real  employs a receiver-oriented and measurementbased congestion control mechanism that significantly improves TCP performance over heterogeneous (wired/wireless) networks and over asymmetric paths.
22203	22227	than in traditional AIMD scheme. The system reaches a window-based equilibrium. This mechanism, named ?-AIMD, has an extra adjustive component ? to the additive increase formula of AIMD. In paper  authors compare AIMD and MAIMD (Multiplicative additive increase and multiplicative decrease). They show that the convergence speeds to fair states of AIMD and MAIMD are close to each other.
22203	22228	Control (GAIMD) is a parameterized TCP-friendly protocol which generalizes AIMD congestion control by parameterizing the additive increase value a and multiplicative decrease ratio b. Authors of  extended the throughput equation for standard TCP, proposed in , to include parameters ?, ?: T a,b (p, RT T, T 0 , b) = ? 2b(1??) RT T ?(1+?) p + T0min(1, 3 4 1 ? (8) (1??2 )b p)p(1 + 32p 2? 2
22203	22228	each ACK. The overall throughput of TCP-Friendly (a, b) protocols is bounded by the average throughput of standard TCP (a=1, b=0.5), which means that equation (11), which is derived from (10) (see ) could provide a rough guide to achieve friendliness. Ta,b(p, RT T, T0, b) = T1,0.5(p, RT T, T0, b) (10) Authors of  derive from (1) and (2) a simple relationship for a and b: a = 4(1 ? b2 ) 3
22203	22229	perspective of the present work it is important to consider that the authors of Vegas demonstrated effectively that measurement-based window adjustment is a viable mechanism. 4.2 TCP-REAL TCP-Real  employs a receiver-oriented and measurementbased congestion control mechanism that significantly improves TCP performance over heterogeneous (wired/wireless) networks and over asymmetric paths.
22203	22229	of a flow can be increased when the RTT of its packets decreases and be decreased when the RTT increases. • The integration of such ideas with a receiver-oriented feedback approach (like TCP-Real ). 5.2 Network-Assisted Congestion Control Red Gateways  drop packets when congestion is about to happen. RED randomly drops packets, triggering multiplicative decrease in some flows when the
22203	22230	indeed for streaming media, telephony or other applications requiring a smooth sending rate. However, smoothness has its own price: the protocol becomes less responsive to bandwidth availability . Furthermore, TFRC is designed for applications that use fixed sized packets. In case of applications with a variance in their packet size (eq. some audio applications), TRFC’s congestion control
10482826	22238	corresponds to the number of voxels in the volume. This results in a massive multiple comparison problem that is most often countered by employing random field theory for the statistical inference (Cao and Worsley, 2001). But permutation tests can also be used by constructing the null distribution for the maximum statistics, where the maximum is taken across all the voxel in the statistical parametric image
10482826	22241	will have several spatial modes, and therefore it has been suggested to use Gaussian mixture models (Nielsen, 2001) or kernel density estimators (Nielsen and Hansen, 2002; Turkeltaub et al., 2002; Chein et al., 2002; Wager et al., 2003). When the statistic analysis is performed it is usually in a mass-univariate setting where the number of statistical tests corresponds to the number of voxels in the volume.
10482826	22241	maximum l (in the following called “location”) positioned in Talairach space at xl by a 3-dimensional Gaussian kernel with isotropic variance (Nielsen and Hansen, 2002; Turkeltaub et al., 2002; Chein et al., 2002). p(x|l) = (2??? 2 ) ?3/2 exp ? ? (x ? xl) T (x ? xl) 2?2 ? . (1) When we construct the probability density corresponding to a group of experiments p(x|g) we combine the contributions from all the
10482826	22253	and Grabrieli, 2000). However, in many cases the distribution of the Talairach coordinates will have several spatial modes, and therefore it has been suggested to use Gaussian mixture models (Nielsen, 2001) or kernel density estimators (Nielsen and Hansen, 2002; Turkeltaub et al., 2002; Chein et al., 2002; Wager et al., 2003). When the statistic analysis is performed it is usually in a
10482826	22254	Modeling, Technical University of Denmark, Lyngby ‡ Center of Sensory-Motor Interaction, Aalborg University, Aalborg 1s(Fox and Lancaster, 1994; Fox and Lancaster, 2002) and the Brede database (Nielsen, 2003), and a number of studies have modeled the distribution of the Talairach coordinates, e.g., (Fox et al., 1997; Nielsen and Hansen, 2002). If the Talairach coordinates are restricted to a specific
10482826	22254	pain studies, where the two groups are hot and cold pain. Such studies will typically employ a 45 ? 50 ? C or 0 ? 5 ? C stimulus to the subjects. The studies were added to the Brede database (Nielsen, 2003). Slight variations among the studies appear 3sFrequency Frequency 100 80 60 40 20 Hot pain 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 200 150 100 50 Cold pain 0 1000 2000 3000 4000
10482826	22257	thot = max(vhot,i ? vcold,i) i (7) tcold = max(vcold,i ? vhot,i) i (8) Many of the operations performed in the method described above are implemented in the Brede Neuroinformatics Toolbox (Nielsen and Hansen, 2000). 3 Results and discussion Figure 2 displays the empirical histogram of the null distribution of the maximum statistics t ? . The red lines indicate the maximum statistics of the comparisons of
10482826	22259	any change first occure. We have previously proposed a method that uses a database of experiments to generate a null distribution of the correlation coefficient between two volumes (Nielsen, 2004; Nielsen and Hansen, 2004). That method requires a database of dissimilar experiments to build up a null hypothesis, e.g., a pain experiment is compared with a memory or language experiment. The method we present in this
22269	22275	mechanisms of evolution of socio-technical systems. It is possible to identify a taxonomy of evolution, as a conceptual framework for the analysis of the evolution of socio-technical systems . The evolutionary framework extends over two dimensions, from Evolution in Design to Evolution in Use and from Hard Evolution to Soft Evolution, that define an evolutionary space for
22269	22275	to these technological artefacts. Soft evolution therefore captures the evolution of stakeholder perception of technical systems. Figure 1 shows the evolutionary space for socio-technical systems . Figure 1. The evolutionary space. The identification of a broad spectrum of evolutions in socio-technical systems points out strong contingencies between system evolution and dependability .
22269	22276	solved within industrial settings . The formal extension of solution space transformation (i.e., the Functional Ecology model) provides a framework to model and capture requirements evolution . The basic idea is to provide a formal representation of solutions and problems. The aim of a formal representation is twofold. On the one hand the formalisation of solutions and problems supports
22269	22276	therefore are accessible possibilities or possible worlds in solution spaces available in the production environment. The resulting framework is sufficient to interpret requirements changes . Hence, it is possible to define requirements evolution in terms of sequential solution space transformations. Requirements evolution consists of the requirements specification evolution and the
22269	22276	The modelling of evolutionary dependency highlights that the formal extension of the solution space transformation enables the gathering of evolutionary information at different abstraction levels . Hence, the formally extended solution space transformation allows the reasoning of ripple effects of requirements changes at different abstraction levels. The underlying heterogeneity of the
22269	22277	On the other hand it allows us formally to capture the solution space transformation, hence requirements evolution. The formalisation represents solutions and problems in terms of modal logic . Intuitively, a solution space is just a collection of solutions, which represent the organisational knowledge acquired by the social shaping 7 of technical systems. Solutions therefore are
22269	22278	This enhances our ability to assess requirements correctness and completeness. For instance, the Software Cost Reduction (SCR) consists of a set of techniques for designing software systems . The SCR techniques use formal design techniques, like tabular notation and information hiding, in order to specify and verify requirements. According to information hiding principles, separate
22269	22280	reduces the cost of software development and maintenance, it provides limited mechanisms to deal with requests of requirements changes , hence requirements evolution. Intent Specifications  further support the analysis and design of evolving systems. Intent Specifications extend over three dimensions. The vertical dimension consists of various hierarchical levels that represent the
22269	22280	representation of the Functional Ecology model 6 . Figure 2. Functional Ecology model. 5 Leveson reports the problem caused by “Reversals” in TCAS (Traffic Alert and Collision Avoidance System) : “About four years later the original TCAS specification was written, experts discovered that it did not adequately cover requirements involving the case where the pilot of an intruder aircraft
22269	22284	that are likely to change. Although module decomposition reduces the cost of software development and maintenance, it provides limited mechanisms to deal with requests of requirements changes , hence requirements evolution. Intent Specifications  further support the analysis and design of evolving systems. Intent Specifications extend over three dimensions. The vertical dimension
22269	22285	act . Figure 3. The structure of human activity. 7 The mechanisms underlying the social design and implementation of technology systems are referred to as the Social Shaping of Technology (SST) .sThe model allows the analysis of a multitude of relations within the triangular structure of activity. However, the important aspect is to grasp human activity as a whole. The systemic model
22320	12296	amplitude modulation (M-QAM) is an attractive modulation technique for wireless communications. M-QAM has been recently proposed and studied for various nonadaptive – and adaptive ,  wireless systems. However, the severe amplitude and phase fluctuations inherent to wireless channels significantly degrade the bit-error rate (BER) performance of M-QAM. That is because the
8922011	22333	on the diagrams that can be drawn. That is we only define translations for a subset of UML class diagrams. Other authors (Facon et.al. 1996, Meyer & Souquieres 1999, Meyer & Santen 2000, Nagui-Raiss 1994, Shore 1996) have suggested ways of dealing with the translation of more general forms of class diagrams. However, the structures of B machines that result from these translations are not natural.
22347	22348	research on teachers and teaching has indicated that highly competent teachers exhibit performance characteristics similar to those associated with expert performers in other domains (e.g., Berliner, 1986; Borko & Livingston, 1989; Swanson, O'Connor & Cooney, 1990; Sabers, Cushing & Berliner, 1991; Clarridge & Berliner, 1991). Although the domain of teaching is ill-structured and speci&quot;c performance
22347	23853	and did not include playing for fun and enjoyment. Practice activities were deliberate e!orts to improve, hence the term &&deliberate practice.'' In subsequent work, Ericsson and Charness (1994) argue that it is possible to study and analyze the factors which mediate expert performance in any domain. Many of the domains considered sosT.G. Dunn, C. Shriner / Teaching and Teacher Education
22372	22378	nodes can be routers, Points-ofPresence (PoP), links or even prefixes. A pair of such end nodes is commonly termed an origin-destination (OD) pair. Most recent research , , , , ,  has focused on either PoP-to-PoP traffic matrices, or router-to-router matrices when the number of routers is limited. Prior work in , , , ,  consider the traffic matrix estimation
22372	22381	TMs. If such a TM were not available, ISPs could use our method using the delay-limit input alone. Carrier’s have methods for selecting the set of IGP weights under which they like to operate . In IP backbones today, such weights are not changed very often - perhaps once a week, or even once a month. Weights are often changed either after a failure event or after the addition or removal
22372	22381	have built a tool called METL (IGP MEtric assignment TooL) that finds an optimal set of ISIS/OSPF weights for a given topology and traffic matrix. This tool is based on an algorithm we developed in  and finds a set of weights that minimizes the maximum link utilization. We use these weights for our second topology since we assume that any carrier has their own algorithm for finding optimal
22382	22390	stability against invasion by rare mutants) and convergence stability (i.e. being approachable by a series of gene substitution events), which coincide in games with a linear ? tness function (Geritz et al., 1998). As argued by Bulmer (1994, Appendix 8.1), Selten’s result only applies to situations where evolutionary stability implies convergence stability, which may not be the case in our model. Our
22382	23893	ict settlement. In other words, any strategy that ignores the asymmetry cannot be an ESS. This result was ? rst shown by Maynard Smith & Parker (1976) and later established rigorously by Selten (1980, 1983, 1988). They also demonstrated two aspects of ESSs based on asymmetries that are particularly worth mentioning here. First, the asymmetry can be used in different ways to resolve the con? ict (Maynard
22382	23895	ict settlement. In other words, any strategy that ignores the asymmetry cannot be an ESS. This result was ? rst shown by Maynard Smith & Parker (1976) and later established rigorously by Selten (1980, 1983, 1988). They also demonstrated two aspects of ESSs based on asymmetries that are particularly worth mentioning here. First, the asymmetry can be used in different ways to resolve the con? ict (Maynard
22403	22405	the smartcard tiny die size fixed to 25 mm in the ISO standard , which pushes for more integration. This limited size is due to security considerations (to minimize the risk of physical attack ) and practical constraints (e.g., the chip should not break when the smartcard is flexed). Another solution to relax the storage limit is to extend the smartcard storage capacity with external
22403	22403	this paper, we concentrate on the storage manager and the query manager which are the most impacted by the smarcard constraints. Smartcard-specific transaction manager description can be found in , while traditional techniques can be used for access right manager. When designing the PicoDBMS’s components, we must follow several design rules derived from the smartcard’s properties: •
22403	7740	the indexed attribute varies on a domain, the index’s collection of values can be saved since 3 Compression techniques can be advantageously used in conjunction with DS to increase compactness .sit exactly corresponds to the domain extension. The extra cost incurred by the index is then reduced to the pointers linking index values to tuples. Let us go one step further and get these
22403	7740	query optimizer first generates an “optimal” query execution plan (QEP). The QEP is then executed by the query engine which implements an execution model and uses a library of relational operators . The optimizer can consider different shapes of QEP: left-deep, right-deep or bushy trees (see Figure 4). In a left-deep tree, operators are executed sequentially and each intermediate result is
22403	7740	relations, they are already materialized in stable memory, thus allowing to execute a plan with no RAM consumption. Pipeline execution can be easily achieved using the well known Iterator Model . In this model, 17 each operator is an iterator that supports three procedure calls: open to prepare an operator for producing an item, next to produce an item, and close to perform final cleanup.
22403	22418	size. Current smartcards rely on a well established and slightly out-of-date hardware technology (0.35?m) in order to minimize the production cost (less than five dollars) and increase security . Furthermore, up to now, there was not a real need for large memories in smartcard applications like holder’s identification. According tosmajor smartcard providers, the market pressure generated
22403	22418	and participates in distributed transactions. • Access right manager: provides access rights on base data and on complex user-defined views. 2 Most security holes are the results of software bugs .sThus, the PicoDBMS hosted in the chip provides the minimal subset of functionality that is strictly needed to manage in a secure way the data shared by all onboard applications. Other components
22443	22423	values are ranges can be considered a special case of constrained values in Constraint Databases , or as null variables with local conditions in Incomplete Information Databases . However, no work in these areas that we know of considers constrained values as bounded approximations of exact values stored elsewhere. Furthermore, aggregation queries over a set with uncertain
22443	22423	Q5 from Section 1.1 that asks for the number of links that have latency > 10. Figure 7 shows the classification of tuples into T ? , T ? ,andT + .Since|T + | =1 and |T ? | =2, the bounded COUNT is . The CHOOSE REFRESHCOUNT algorithm is based on the fact that HA ? LA = |T ? |, and that refreshing a tuple in T ? is guaranteed to remove it from T ? . Given these two facts, the optimal CHOOSE
22443	22426	answers to queries. Suppose a bounded answer to a query with aggregalink latency bandwidth traffic refresh weights from to cached precise cached precise cached precise cost W W ? W ?? 1 N1 N2  3  61  98 3 2 10 29.5 2 N2 N4  7  53  116 6 2 10 2 3 N3 N4  13  62  105 6 15 41.5 4 N2 N3  9  68  127 8 25
22443	22426	cannot give a guaranteed bound on the answer to a particular query. Finally, data objects whose values are ranges can be considered a special case of constrained values in Constraint Databases , or as null variables with local conditions in Incomplete Information Databases . However, no work in these areas that we know of considers constrained values as bounded approximations of exact
22443	22427	116 6 2 10 2 3 N3 N4  13  62  105 6 15 41.5 4 N2 N3  9  68  127 8 25 2 5 N4 N5  11  50  95 4 3 20 36.5 6 N5 N6  5  45  103 2 2 15 31.5 tion is computed from cached values, but the answer does not satisfy the user’s precision constraint, i.e., the answer bound is too wide. In this case, some
22443	22427	cannot give a guaranteed bound on the answer to a particular query. Finally, data objects whose values are ranges can be considered a special case of constrained values in Constraint Databases , or as null variables with local conditions in Incomplete Information Databases . However, no work in these areas that we know of considers constrained values as bounded approximations of exact
22443	22427	to refresh in 147 Refresh Monitor Data Sources V 1 = 3 V = 5 2 refresh queryinitiated refresh request User query + precision bounded constraint answer Data Caches Query Processor  =   =  Figure 3: TRAPP system architecture. order to meet the precision constraint of a query at minimum cost. The TRAPP architecture as presented in this paper makes some simplifying
22443	22427	algorithms from Sections 6.2 and 6.3, then setting:  =  HCOUNT In our example,  =  and  = . Thus, the linear algorithm yields . Notice that this bound is indeed looser than the  bound achieved by the O(n · log n) algorithm above. 6.4.2 Choosing Tuples to Refresh
22443	22428	query with aggregalink latency bandwidth traffic refresh weights from to cached precise cached precise cached precise cost W W ? W ?? 1 N1 N2  3  61  98 3 2 10 29.5 2 N2 N4  7  53  116 6 2 10 2 3 N3 N4  13  62  105 6 15 41.5 4 N2 N3  9  68  127 8 25 2 5 N4 N5  11  50  95 4 3 20
22443	22428	cannot give a guaranteed bound on the answer to a particular query. Finally, data objects whose values are ranges can be considered a special case of constrained values in Constraint Databases , or as null variables with local conditions in Incomplete Information Databases . However, no work in these areas that we know of considers constrained values as bounded approximations of exact
22443	22429	precise cost W W ? W ?? 1 N1 N2  3  61  98 3 2 10 29.5 2 N2 N4  7  53  116 6 2 10 2 3 N3 N4  13  62  105 6 15 41.5 4 N2 N3  9  68  127 8 25 2 5 N4 N5  11  50  95 4 3 20 36.5 6 N5 N6  5  45  103 2 2 15 31.5 tion is computed from cached values, but the
22443	22429	147 Refresh Monitor Data Sources V 1 = 3 V = 5 2 refresh queryinitiated refresh request User query + precision bounded constraint answer Data Caches Query Processor  =   =  Figure 3: TRAPP system architecture. order to meet the precision constraint of a query at minimum cost. The TRAPP architecture as presented in this paper makes some simplifying assumptions. First,
22443	22429	a weight to each tuple as shown in the column labeled W ?? in Figure 2. The knapsack optimally “contains” tuples 2 and 4. After refreshing the other tuples TR = {1, 3, 5, 6}, the bounded AVG is . 7 Aggregation Queries with Joins Computing the bounded answer to an aggregation query with a join expression (i.e., with multiple tables in the FROM clause) is no different from doing so with a
22443	22429	Functionality • Expanding the class of aggregation queries we consider. We want to devise algorithms for other aggregation functions, such as MEDIAN (for which we have preliminary results ) and TOPn. In addition, we would like to extend our results to handle grouping on bounded values, enabling GROUP-BY and COUNT UNIQUE queries. We would also like to handle nested aggregation
22443	22431	permission from the Endowment. Proceedings of the 26th VLDB Conference, Cairo, Egypt, 2000. Chris Olston, Jennifer Widom Stanford University {olston, widom}@db.stanford.edu 144 as outlined in , and for many distributed applications exact consistency simply is not a requirement. The World-Wide Web is a very general example of a stale replication system, where master copies of pages are
22443	22431	weights from to cached precise cached precise cached precise cost W W ? W ?? 1 N1 N2  3  61  98 3 2 10 29.5 2 N2 N4  7  53  116 6 2 10 2 3 N3 N4  13  62  105 6 15 41.5 4 N2 N3  9  68  127 8 25 2 5 N4 N5  11  50  95 4 3 20 36.5 6 N5 N6  5  45  103 2 2 15
22443	22432	keep track of a small number of bounds. In other applications a source may provide a large number of objects to multiple caches, in which case a scalable trigger system would be of great benefit .) The Refresh Monitor is responsible for detecting whenever the value of a data object exceeds the bound in some cache, and sending a new bound to the cache (a value-initiated refresh). When the
22443	22434	answers. Early work in this area is reported in . Most of these systems use either precomputation (e.g., ), Figure 2: Sample data for network monitoring example. 146 sampling (e.g., ), or both (e.g., ) to give an answer with statistically estimated bounds, without scanning all of the input data. By contrast, TRAPP systems may scan all of the data (some of which may be
22443	22435	weights from to cached precise cached precise cached precise cost W W ? W ?? 1 N1 N2  3  61  98 3 2 10 29.5 2 N2 N4  7  53  116 6 2 10 2 3 N3 N4  13  62  105 6 15 41.5 4 N2 N3  9  68  127 8 25 2 5 N4 N5  11  50  95 4 3 20 36.5 6 N5 N6  5  45  103 2 2 15
22443	22435	in TRAPP systems performance is improved by reducing the numbersof data objects read from remote sources, rather than by reducing the size of the data representation. In Divergence Caching , a bound is placed on the number of updates permitted to the master copy of a data object before the cache must be refreshed, but there are no bounds on data values themselves. Another body of work
22443	22439	cannot give a guaranteed bound on the answer to a particular query. Finally, data objects whose values are ranges can be considered a special case of constrained values in Constraint Databases , or as null variables with local conditions in Incomplete Information Databases . However, no work in these areas that we know of considers constrained values as bounded approximations of exact
22443	22440	cannot give a guaranteed bound on the answer to a particular query. Finally, data objects whose values are ranges can be considered a special case of constrained values in Constraint Databases , or as null variables with local conditions in Incomplete Information Databases . However, no work in these areas that we know of considers constrained values as bounded approximations of exact
22443	22440	along path N1 ? N2 ? N4 ? N5 ? N6 with R =5, tuples 2 and 5 are “placed in the knapsack” (whose capacity is 5), leaving TR = {1, 6}. The bounded SUM of latency after refreshing tuples 1 and 6 is . 5.2.1 Performance Experiments CHOOSE REFRESHNO SEL/SUM uses the approximation algorithm from  to quickly find a cheap set of tuples TR to refresh such that the precision constraint is
22443	22442	the cache must be refreshed, but there are no bounds on data values themselves. Another body of work that deals with imprecision in information systems is Information Quality (IQ) research, e.g., . IQ systems quantify the accuracy of data at the granularity of an entire data server. Since no bounds are placed on individual data values, queries have no concrete knowledge about the precision
22443	22443	Li(T ) ? ? T?Tr, where Tr is the time of the most recent refresh. The proportionality parameter, which determines the width of the bound, is chosen at run-time. The interested reader is referred to  for details. 4 Query Execution for Bounded Answers Executing a TRAPP/AG query with a precision constraint may involve combining precise data stored on remote sources with bounded data stored in a
22443	22443	Choosing an optimal set of tuples to refresh for a MIN query with a precision constraint is also straightforward, although the algorithm’s justification and proof of optimality is nontrivial (see ). The CHOOSE REFRESH NO SEL/MIN algorithm chooses TR to be all tuples ti ? T such that Li < 1 In this and all subsequent formulas, we define min(?) =+? and max(?) =??. 149 mintk?T (Hk) ? R, whereR
22443	22443	min tk?{1,2,5,6}(Hk) ? R =55? 10 = 45. After refreshing, tuple 5’s bandwidth value turns out to be 50, so the new bounded answer is . The MAX aggregation function is symmetric to MIN. See  for details. 5.2 Computing SUM with No Selection Predicate To compute the bounded SUM aggregate, we take the sum of the values at each extreme:  = ti?T ti?T The smallest
22443	22443	will cause the predicate not to be satisfied. The process of classifying tuples into T ? , T ? ,andT + when the selection predicate involves at least one column with bounded values is detailed in . The most interesting aspect is that filters over T that find the tuples in T + and T ? can always be expressed as simple predicates over bounded value endpoints, and all of our algorithms for
22443	22443	TR to be exactly the tuples ti ? T + ? T ? such that Li < min tk?T +(Hk) ? R. This algorithm is essentially the same as CHOOSE REFRESH NO SEL/MIN,andiscorrect and optimal for the same reason (see ). The only additional case to consider is that refreshing tuples in T ? may move them into T ? . However, such tuples do not contribute to the actual MIN, and thus do not affect the bound of the
22443	22445	representation is not practical. Furthermore, the approach in  does not consider fuzzy sets as approximations of exact values available for acost. In the multi-resolution relational data model , data objects undergo various degrees of lossy compression to reduce the size of their representation. By reading the compressed versions of data objects instead of the full versions, the system
22443	22447	be bounds rather than exact values), to provide guaranteed rather than statistical results. The previous work perhaps most similar to the TRAPP idea is Quasi-copies  and Moving Objects Databases . Like TRAPP systems, these two systems are replication schemes in which cached values are permitted to deviate from master values by a bounded amount. However, unlike in TRAPP systems, these
22448	22449	search methods. These methods perform well as long as a good initial guess for the location of the target object is available, but they do not tend to work without such information. As shown in  the object detection problem can be solved e#ciently if we constrain the distribution over shapes to be of a particular form, in terms of decomposable graphs. This is a promising approach and is
22448	22449	been developed for very restricted sets of models. For example, in  a dynamic programming algorithm was used to detect open deformable curves in images. Dynamic programming was also used in  to match models consisting of a number of sparse landmarks with positions constrained by a decomposable graphical model. E#cient algorithms also exist for the related problem of computing a
22448	22449	finds an optimal map in time O(n|G| 3 ), which is exponentially better than just trying all possible maps. The algorithm uses a technique known as non-serial dynamic programming (see  and ). Typical applications of dynamic programming relies on a chain structure. Non-serial dynamic programming generalizes the standard technique to certain problems defined on decomposable graphs that
22448	22450	et al.  pioneered the use of Markov models to represent the boundaries of non-rigid objects. They demonstrated how these models can be used to detect objects in noisy images. The work in  describes how we can measure similarity between objects in terms of the amount of stretching and bending necessary to turn the boundary of one object into the boundary of another one. One problem
22448	22450	with positions constrained by a decomposable graphical model. E#cient algorithms also exist for the related problem of computing a non-rigid match between two pre-segmented objects (such as  and ). 12 3.1 Matching Let P be a simple polygon corresponding to an object template. An embedding of P in the image plane is defined by a continuous function f : P #R 2 , where f is defined
22448	22451	known as constrained Delaunay triangulations (CDT) we obtain a representation that is closely related to the medial axis transform. A good introduction to the Delaunay triangulation can be found in . To define the CDT we need to introduce 6 Figure 4: The graphical structure of the triangulation, G T , is shown with black nodes and solid edges while the dual graph, D T , is shown with gray
22448	22456	of Grenander . Other important models are described in  and . A few e#cient and provably good matching algorithms have been developed for very restricted sets of models. For example, in  a dynamic programming algorithm was used to detect open deformable curves in images. Dynamic programming was also used in  to match models consisting of a number of sparse landmarks with
22448	17473	the representation based on self-similarities described in . In particular our models capture local symmetries and provide natural decompositions of shapes into parts. Statistical shape theory  has been used to study objects that are defined by a set of labeled landmarks. In this scenario, the space of possible landmark configurations has led to natural notions of distances between
22448	17473	Data Here we review some basic concepts from statistical shape theory. First let us define what we mean by the shape of an object that lives in an Euclidean space. The following definition is from : 9 Figure 8: Di#erent objects with the same shape. Definition 4. Shape is all the geometrical information that remains when location, scale and rotational e#ects are filtered out from an object.
22448	17473	Analysis Before describing our learning technique we review a standard method for estimating the mean shape and typical variation of landmark data known as generalized Procrustes analysis. See  or  for more details. Say we have a set of random configurations {X 1 , . . . , X m } obtained from a meansby a small perturbation and a similarity transformation, X i # +# i . Assuming the
22448	17473	the same algorithm. Note that the optimal triangulation may be di#erent for di#erent choices of priors. A number of di#erent shape priors for landmark data that could be used here are described in . 4.4 Experimental Results One problem with Procrustes analysis (and active shape models) is the assumption that objects from a population can be approximately aligned using similarity
22448	16109	are the parts that make up a shoe? Another problem is that we do not know how to extract generic parts from images in a robust way. On the other hand, models based on pictorial structures (e.g. ) have been successfully used to characterize and detect objects that are described by a small number of simple parts connected in a deformable configuration. In this approach, generic parts are not
22448	22460	polygon representation is closely related to the medial axis transform. We obtain a discrete version of the medial axis similar to the representation based on self-similarities described in . In particular our models capture local symmetries and provide natural decompositions of shapes into parts. Statistical shape theory  has been used to study objects that are defined by a
22448	14062	are commonly used both in the context of image segmentation and object recognition. One example is a popular technique for image segmentation known as active contour models or snakes . Boundary models are also used for generic object recognition. Grenander et al.  pioneered the use of Markov models to represent the boundaries of non-rigid objects. They demonstrated how these
22448	22467	the corpus callosum in each case. The quality of our results is similar to the quality of results obtained using the best available methods for model based segmentation of medical images (such as ). The main advantage of our method is that it does not require any initialization. Figure 10(b) shows a model for maple leaves, constructed from a binary silhouette. The best match of the model to
22448	22470	representations based on the medial axis seem well suited to capture the geometry of generic objects. A model of how the shock graph deforms and changes structure was presented in . As shown in , medial axis models can better capture natural deformations of objects when compared to boundary models. An example of generic object recognition using a representation related to the medial axis
22448	22471	In general, representations based on the medial axis seem well suited to capture the geometry of generic objects. A model of how the shock graph deforms and changes structure was presented in . As shown in , medial axis models can better capture natural deformations of objects when compared to boundary models. An example of generic object recognition using a representation related to
22448	22471	with positions constrained by a decomposable graphical model. E#cient algorithms also exist for the related problem of computing a non-rigid match between two pre-segmented objects (such as  and ). 12 3.1 Matching Let P be a simple polygon corresponding to an object template. An embedding of P in the image plane is defined by a continuous function f : P #R 2 , where f is defined over both
22448	22472	example, there is a diagonal separating each finger from the rest of the hand. A natural way to decompose objects into parts is to split them at places where the boundary has curvature minima (see ). This is because joining two parts together usually creates such minima. Figure 5 illustrates how the CDT always includes diagonals that split a limb from the rest of an object and diagonals that
22448	22475	the variability present in the training set. We also considered a more challenging problem, of modeling the shape of a set of hands. In this case we used a database with 40 pictures of hands from  as the input to the learning procedure. Each picture is annotated with the location of 56 landmarks along the boundary of the hand. Some examples of the objects in this data set are shown in Figure
22448	22478	better capture natural deformations of objects when compared to boundary models. An example of generic object recognition using a representation related to the medial axis transform is described in . Our triangulated polygon representation is closely related to the medial axis transform. We obtain a discrete version of the medial axis similar to the representation based on self-similarities
3895040	22480	can be achieved along with at most 7.5% increase in latency. I. INTRODUCTION Interconnection network fabrics have lately been used in a wide range of communication systems – multiprocessor servers , terabit Internet routers , clusters , server blades , and multiprocessor systems-on-a-chip  . In pursuit for wider bandwidth and higher communication efficiency, traditional buses
3895040	22480	interconnection networks dissipating a significant portion of total system power (£¥¤§¦ in the MIT Raw CMP , £¨£¨¦ in the Avici Internet router , and ©¥?¨¦ in the Alpha 21364 microprocessor ), it is now timely to explore power-aware networks. In board-to-board and multi-chip networks, links are already consuming substantial power. They take up ¤¥??¦ of the power budget of the IBM
3895040	20608	have lately been used in a wide range of communication systems – multiprocessor servers , terabit Internet routers , clusters , server blades , and multiprocessor systems-on-a-chip  . In pursuit for wider bandwidth and higher communication efficiency, traditional buses are being replaced by network fabrics as the principal interconnect that incorporate high speed router
3895040	22484	InfiniBand 12X link is almost identical to its worst-case power). In on-chip networks, several new link architectures are recently proposed for global wires, replacing full-swing repeatered wires  . Due to features such as differential and pulsed current-mode signalling, these links have power profiles that are similarly invariant to utilization. As a result, slowing network traffic has
3895040	22484	and transitions from on to off (sleep) states and vice versa (wake). As on-chip links are able to leverage the low-skew global chip clock as the clocking source at transmitters and receivers  , link calibration time when transitioning from off to on states is likely to be substantially shorter than that in board-to-board links. Preliminary measurements by Shepard et. al. on their
3895040	7884	With faster on/off links where it takes 1,000 cycles to turn off a link and another 1,000 cycles to turn it back on, we show that up to ????? ??¦ link power reduction with SPEC2000 and MediaBench  applications running on a networkon-chip can be achieved. These are accompanied with minimal impact on performance, ranging from ??? £¨¦ to ??? ?¨¦ increase in latency. Even with the large 10,000
3895040	7884	wakes up the link just-in-time enabling link power savings without any performance impact. We run simulations with our oracle predictor using real traffic traces from the SPEC2000 and MediaBench  benchmarks running on the TRIPS CMP and using synthetic self-similar traces on an ????? torus. Section V-A provides a description of the simulation environment. Fig. 2 shows the ideal fraction of
3895040	7884	(exponential backoff) We assumed packets consisting of three 32-bit flits and 1GHz routerscores. Network traces were obtained from simulations of a suite of thirteen SPEC2000 and MediaBench  benchmarks. The traces are in general very bursty – large numbers of packets injected at times, and zero packets at others, which present interesting opportunities for power optimization. The
3895040	7884	percentage total network link power savings and the corresponding network latency penalty for a link transition delay of 10 cycles for the TRIPS CMP running a suite of 13 benchmarks for MediaBench  and SPEC2000 applications. Table IV supplies the rest of the results for 100 and ¥ ¤ ¤ ¤ ?¥?¨¦ ?¨?¥? ¡ ??? ? ?§¦ ¥ ?¨?¥? ¡ How do results here compare with those of Case Study I? Considerably
3895040	22486	of the ammp trace benchmark from the TRIPS CMP over ??????? cycles of simulation time. average, a delay that can be readily hidden by the network interface delay that spans hundreds of cycles , such as that in cluster interconnects like InfiniBand. To the best of our knowledge, this is the first work that explores the design space of power-aware on/off networks. The structure of this
3895040	22486	penalty due to the heavy re-routing needed with so many links off. However, in many board-to-board systems such as blade clusters, the network interface delay is in the order of 100s of cycles , and hence easily mask the 4 to 5 additional cycles introduced on average by the power-aware on/off network. TABLE II POWER-PERFORMANCE OF OUR PROPOSED ON/OFF APPROACHES FOR 8-ARY 2-CUBE TOPOLOGIES
3895040	360	as the name suggests, restricts routes so turns from the west direction (east) are disallowed, as shown in Fig. 8. While it is well-known that such protocols that are based on the Turn Model  ensure deadlock freedom in meshes and tori , for it to be an on/off routing protocol, it needs to be deadlock-free with the ever-changing and limited connectivity of an on/off network. To
3895040	22488	turns from the west direction (east) are disallowed, as shown in Fig. 8. While it is well-known that such protocols that are based on the Turn Model  ensure deadlock freedom in meshes and tori , for it to be an on/off routing protocol, it needs to be deadlock-free with the ever-changing and limited connectivity of an on/off network. To ensure deadlock freedom with the alternating
3895040	22488	approach which updates all affected routing tables upon each on/off decision is still manageable. Our approach bears similarities to prior work in fault-tolerant routing protocols and theories  , steering network traffic when network connectivity is reduced as links shut down to save power. However, fault-tolerant approaches are reactive and merely performance oriented – they respond
3895040	22490	so remnants of a packet can completely leave the current router. 3 The power consumed by counters is ignored, as similar hardware has been shown to consume little CMOS area with negligible power . B C Cs¥s¡ ?¨?¥? ¤ ¤ ¤s?¡¤£¦¥§¥§¨?©s?????? ? ¤s?¡¤£¦¥§¥§¨?©s?????? ? ¤s???¡?£¦¥?¥?¨?©s¢????? ? ? ? ??? ?s?¡?£¦¥?¥?¨?©s?????? ? IV. DESIGN CASE STUDIES The alternatives proposed and explored at each
3895040	22490	variance giving good opportunities to optimize power by rerouting packets around inactive links without impacting throughput. 2) Off-Chip Benchmarks: We used a synthetic workload model proposed in  that exhibits both high temporal and spatial variance, a reflection of real-world communication traffic. The workload consists of a two-level self-similar or long-range dependence (LRD)
3895040	22490	X Y 10 cycle on/off delay 45.1 1.4 30.5 0.8 100 cycle on/off delay 45.9 1.6 30.1 1 ? ? cycle on/off delay 46.7 3.5 35.5 2.7 VI. RELEVANT WORK Power-aware networks were first proposed recently  . Previously, designers only had the option of using lower frequency links throughout the network slowing all network traffic across the board, unless they had available accurate prior knowledge of
3895040	22493	Set X Y 10 cycle on/off delay 45.1 1.4 30.5 0.8 100 cycle on/off delay 45.9 1.6 30.1 1 ? ? cycle on/off delay 46.7 3.5 35.5 2.7 VI. RELEVANT WORK Power-aware networks were first proposed recently  . Previously, designers only had the option of using lower frequency links throughout the network slowing all network traffic across the board, unless they had available accurate prior
3895040	22495	available , albeit with 10s of thousands of cycles on/off delays. DVS and DVS-DLS links, on the other hand, require substantial circuits innovations to extend variablefrequency links  to support fast, voltage and frequency changes while ensuring correct link operation during voltage scaling, thus incurring higher power and area overhead. Clearly, the power savings realizable
894057	22502	used image data in order to choose a steering angle. Learning quickly became a preferred solution for handling the real-world complexity in autonomous vehicle applications (see for example ???). Cristian Dima, Martial Hebert and Anthony Stentz The Robotics Institute Carnegie Mellon University Pittsburgh, PA 15213 Email: @ri.cmu.edu Fig. 1. The two robotic vehicles used
894057	22503	all the systems use supervised learning, which require labeled data. For certain problems obtaining labeled data is inherent to the data collection process and is relative cheap (see for example ). However, the most common approach is to have a human expert manually label data that is representative of the operating environment. Since the outdoor off-road environment is highly
894057	22504	human expert with 10-20 images that are really worth labeling. This is the typical application for active learning techniques. Active learning is a research area that had many success stories (see  and  for short but informative reviews). Some of the better known applications are related to data mining text information  , astronomical data or large company records. Robotics has also
894057	22505	with 10-20 images that are really worth labeling. This is the typical application for active learning techniques. Active learning is a research area that had many success stories (see  and  for short but informative reviews). Some of the better known applications are related to data mining text information  , astronomical data or large company records. Robotics has also seen some
894057	22505	to our method as “unlabeled data filtering”. This contrasts our method with better known active learning techniques such as confidence based query selection or voting based query selection (see , –) which require a small amount of labeled data to begin with and then interactively present more data to the human expert for labeling. Using our method does not however exclude the use
894057	22506	known applications are related to data mining text information  , astronomical data or large company records. Robotics has also seen some important applications, mostly in the control domain ( , ). Our long term goal is to make learning practical for large,sreal-world robotics applications by adapting promising techniques from the data mining field to robotics. The approach we
894057	22509	method as “unlabeled data filtering”. This contrasts our method with better known active learning techniques such as confidence based query selection or voting based query selection (see , –) which require a small amount of labeled data to begin with and then interactively present more data to the human expert for labeling. Using our method does not however exclude the use of some
894057	22510	significantly larger than what a person would agree to label, we would like to be scale up to datasets of millions of images. We are interested in using approaches such as the ones described in ??? in order to improve the speed and robustness with which we can estimate probability density functions. Furthermore, we are currently working on more standard active learning systems that are
894057	22512	larger than what a person would agree to label, we would like to be scale up to datasets of millions of images. We are interested in using approaches such as the ones described in ??? in order to improve the speed and robustness with which we can estimate probability density functions. Furthermore, we are currently working on more standard active learning systems that are
8922039	22536	From my point of view, new generation tools should address the previously quoted problems to overpass the Novak’s rule. Modeling tools should be visual and support sketching as done in DENIM . Easy of use is crucial to make work perceived as a non time-consuming task. An standard in XML representation is needed urgently as a base for tools interchange. Later on, notations and semantics
8922039	22537	where I currently works as a researcher and a software engineer. During the last years I have been involved in the development of a user interface specification model based on conceptual patterns , a tool for supporting it, and code generators to produce UI code to several desktop and web platforms. All of these products are part or the commercial tool suite OlivaNova Model Execution System.
22589	22593	compiled both with and without type information. 1 Introduction Several techniques for implementing Prolog have been devised since the interpreter originally developed by Colmerauer and Roussel , many of them aimed at achieving more speed. A good survey of part of this work can be found in . A rough classification of implementation techniques for Prolog (extensible to other
22589	22594	latest ? The authors have been partially supported by the Spanish MCYT Project TIC 991151 EDIPIA and the EU ESPRIT Project 2001-34717 AmossBimProlog compilers , the Gnu Prolog compiler , and the Mercury.  compiler 1 Each solution has its advantages and disadvantages. Generation of low level code promises faster programs at the expense of using more resources during the
22589	22594	code, easy to translate into machine code for different architectures. But it requires, anyhow, different back-ends for different architectures. Besides, recent performance evaluations  show that well-tuned emulatorbased Prolog systems can beat, at least in some cases, Prolog compilers which generate machine code directly. 1 Although Mercury is not a Prolog compiler, the source
22589	22598	types, instantiation modes, etc. This information is expressed by means of a well-defined assertion language , and provided either by the user or by automatic global analysis tools . For example, wamcc (a Gnu Prolog forerunner), which generated C, did not use extensive analysis information (but it included clever tricks which in practice tied it to a single C compiler, gcc);
22589	22604	been partially supported by the Spanish MCYT Project TIC 991151 EDIPIA and the EU ESPRIT Project 2001-34717 AmossBimProlog compilers , the Gnu Prolog compiler , and the Mercury.  compiler 1 Each solution has its advantages and disadvantages. Generation of low level code promises faster programs at the expense of using more resources during the compilation phase.
22589	22604	machine-dependent options of the C compiler or extensions to the ANSI C language (although machine-dependent optimizations can of course be given to the C compiler). Other systems, as  or , take advantage of machine-dependent and non-portable constructs to obtain very good performance. However, one of the goals of our system is studying optimizations of a fixed compilation scheme
22589	24106	an output requiring little or no additional support to be executed. Ideally, the compiler should generate directly machine code. Examples of this are the Aquarius system , the SICStus Prolog  compiler (for some architectures), the latest ? The authors have been partially supported by the Spanish MCYT Project TIC 991151 EDIPIA and the EU ESPRIT Project 2001-34717 AmossBimProlog compilers
22589	22606	Prolog have been devised since the interpreter originally developed by Colmerauer and Roussel , many of them aimed at achieving more speed. A good survey of part of this work can be found in . A rough classification of implementation techniques for Prolog (extensible to other languages) is the following: – Interpreters (such as C-Prolog  and others), where a slight preprocessing
22589	22606	(and, if this is known, wether it lives in the heap, in the stack, etc.) or not. For the unification of structures, the use of write and read modes is avoided using a two-stream scheme (see  for an explanation and references). This scheme requires explicit control instructions, hence the existence of jump instructions (jump, cjump, and ijump). Jumps are performed to labels, marked as
22589	22606	more difficult and to give less speedup, due to the greater granularity of the bytecode instructions (which aims at reducing the cost of fetching them). The same result has been reported elsewhere , although some recent work tries to improve WAM code by means of local analysis . We expect to be able to use more information (e.g., determinacy information) to improve also clause
22589	22609	the generation of the final C code (and probably also the generation of code in languages of similar level). The deegre of complexity of the low-level code is similar to the one proposed in the BAM . Table 2sno choice Mark that there is no alternative first choice(Arity, Alt) Create a choicepoint middle choice(Arity, Alt) Change the alternative last choice(Arity) Remove the alternative
22620	22623	interest has concerned the application of model-checking methods to infinite-state systems. Several interesting classes of infinite state systems has been shown decidable. For example, Alur et al  showed that timed automata have a decidable reachability problem. Finkel et al in , and Abdulla et al in  have shown that infinite, but monotonic, transition systems (also called
22620	22624	Here are some examples. In , Maler et al study how to solve games defined by timed automata. In , Walukiewicz studies how to solve infinite games defined by push down automata. In , Henzinger et al study symbolic algorithms to solve general infinitestate games. In this paper, we study two-player games played on infinite but monotonic game structures (for a well-quasi
22620	22624	is defined symmetrically. We define CPre 0 1,G (S) as S, for any n ? N, CPre n+1 1,G (S) as CPre1,G(CPre n 1,G(S)) and CPre ? 1,G(S) = ? n?N CPren1,G(S). It is well known, see for example , that the following theorem holds:sTheorem 1. For any reachability game (G, c, F ), for i ? {1, 2}, we have that player i has a winning strategy for the game (G, c, F ) iff c ? CPre ? i,G(F ). Note
22620	22625	Several interesting classes of infinite state systems has been shown decidable. For example, Alur et al  showed that timed automata have a decidable reachability problem. Finkel et al in , and Abdulla et al in  have shown that infinite, but monotonic, transition systems (also called well-structured transition systems) have a decidable coverability problem. For instance,
22620	22627	intensively as traditional verification problems on infinite-state transition systems. Nevertheless, recently there have been several interesting works in that direction. Here are some examples. In , Maler et al study how to solve games defined by timed automata. In , Walukiewicz studies how to solve infinite games defined by push down automata. In , Henzinger et al study
22620	22630	systems. Nevertheless, recently there have been several interesting works in that direction. Here are some examples. In , Maler et al study how to solve games defined by timed automata. In , Walukiewicz studies how to solve infinite games defined by push down automata. In , Henzinger et al study symbolic algorithms to solve general infinitestate games. In this paper, we study
8922065	22634	1993). Data in geographic information systems (GIS) are always based on observations because we want to model the world and observations are the only method to get data about the real world (Frank 2001). In some, very rare cases we directly store the observations, for example if observing air pollution at a specific point. In most cases, however, we only store derived values. We use angles and
22672	22663	of wireless networks. We point out the differences from our work, and highlight our contributions. An early work on pricing and bandwidth adaptation in wireless networks was the TIMELY project . Users benefitting from adaptation were charged and those suffering from adaptation were compensated. However, how the exact charges and credits were calculated, was not specified. In , the
22672	22664	that any extra money will be refunded, should encourage users to set their maximum bids ????¥?? high, and thus increase system revenue. The refund helps to avoid the “winner???s curse” effect . 5. Results The channel time allocation algorithm described in the previous section is a variable price algorithm because the price changes according to the “richness” of the users in set ¤ . In
22672	22666	resource allotment when bidders value the resource differently. This is the case with wireless channel time in a hot spot network, as mentioned in Section 1. Assume an ascending, multi-unit auction  of channel time between the users. Assume each user is allotted a CTP proportional to its bid, i.e. CTP allotted at any instant to a user is equal to the user’s bid divided by the sum of all bids
22672	22667	price and fixed price channel time allocation algorithms. on the downlink of a time-slotted or CDMA-based wireless LAN. They assume that users do not know each others’ utility functions. In , the authors divide the network bandwidth into stable (low bandwidth) and instantaneous (unstable, high bandwidth) classes, and broadcast a priceservice menu for these classes periodically. The
22672	22668	values. The price in this case will ? ????? ??????¥?? be (cents/minute)/%CTP. 3 The price for 1 bps throughput is different for users with different channel qualities. This is also the case in .s4.3. Centralized Auction of Channel Time For the ? ????? ????¥???????????????????? case , the algorithm described in the previous subsection is actually a centralized version of a distributed
22672	22668	project . Users benefitting from adaptation were charged and those suffering from adaptation were compensated. However, how the exact charges and credits were calculated, was not specified. In , the authors discuss price-based resource allocationsAlgorithm Price ((c/min)/%CTP) Revenue (c) Avg. Satisfaction Channel Utilization Users blocked Variable Price Variable 19617 71% 83% 24 Fixed @
22672	22670	the user’s activity. For besteffort flows, ©???¥?????????????? . Such a utility function, defined by the minimum and maximum bandwidth requirements has also been used previously in literature . One of the major contributions of our scheme is that we convert bandwidth requirements into channel time proportion (CTP) requirements. The channel time proportion is the fraction of unit time a
22672	22671	also directly auctions channel time so that users who value it more obtain more of it. There has also been research in the area of price-based resource allocation for wireless ad-hoc networks . In , the authors argue that the shared resource is not a link, as in the case of a wireline network, but a wireless neighborhood clique. On the basis of this, they adapt the concepts of Kelly
22672	2154	exceeds the channel capacity, then there will be severe performance degradation. Bandwidth management and co-ordinated rate control enhances the QoS of the individual applications, as shown in . In this paper, we address this problem of how to distribute bandwidth in a wireless LAN between users with different requirements, different channel qualities, and willing to pay different sums of
22672	2154	The users only communicate with the AP, not directly with each other. In practice, IEEE 802.11 DCF is used even for such a communication model, not just for peer-to-peer wireless transmissions. In , bandwidth management over peer-to-peer wireless LANs is discussed, using the same system architecture described in this paper. Each user ¥§¦¨¤ has a maximum bandwidth requirement ©???¥??????????
22672	2154	the user’s activity. For besteffort flows, ©???¥?????????????? . Such a utility function, defined by the minimum and maximum bandwidth requirements has also been used previously in literature . One of the major contributions of our scheme is that we convert bandwidth requirements into channel time proportion (CTP) requirements. The channel time proportion is the fraction of unit time a
22672	2154	time proportion (CTP) requirements. The reason for this was that each user’s communication with the AP is affected by different and varying levels of medium contention, fading and interference. In , we proposed a simple method to estimate maximum throughput over a wireless link, that considered these time- and location-dependent effects. Using this method, we obtain a different maximum
22672	2154	4. System Architecture and Channel Time Allocation Algorithm The components of the system and its overall architecture are very similar to that of the bandwidth management scheme described in . The price-based channel time allocation algorithm, however, is completely different from the “fair” allocation algorithm in . 4.1. System Architecture The overall architecture of the system is
22672	22672	and thus yielding very low revenue to the system. By setting the price to gorithm finds the largest price for which user satisfaction of all users is 100% and channel utilization is maximum. (See  for proof.) Alternatively, in the trivial case, the network provider may opt to just set ? to ????????????????? , ir????? ????????¥???? , when ????? ????? respective of the ??????¥?? values. In the
22672	22672	as the price our algorithm computes, ? ????? ????¥???????????? when of channel time among the users in the network. When ? ????? ????¥???????????????????? , the CTP allocated to each ??????? . (See  for proof.) This price reflects the true worth user in set ? is limited by its maximum bid, and the CTP allocated to each user in set ? is limited by its maximum requirement. In other words, users
22672	22672	existing user leaves, or some other parameter, such as a user’s maximum bid, changes. The equilibrium price ? is the true worth of CTP among the users. An example of such an auction can be found in . Obviously, such a distributed auction is infeasible in our scenario. The repeated bids from every user, each resulting in a different CTP allotment for all users, constitute a very large overhead.
22672	22672	????¥?? . The channel time allocation algorithm described earlier then simulates the ascending auction. It determines the same equilibrium price and CTP allotment as the distributed auction (see  for proof) in ????????¤?????? time, while the distributed auction would have required several iterations of bidding from each user. One possible problem with an auction-based channel time allocation
22672	2155	fraction allocated. While we do not address this problem of co-operative and collective rate-control, it is an active area of research at various layers of the OSI protocol stack. Fair scheduling  addresses this problem of rate control at the MAC layer. We address the problem from the point of view of the policy in allocating a portion of the overall network bandwidth to each user. The
22672	22673	also directly auctions channel time so that users who value it more obtain more of it. There has also been research in the area of price-based resource allocation for wireless ad-hoc networks . In , the authors argue that the shared resource is not a link, as in the case of a wireline network, but a wireless neighborhood clique. On the basis of this, they adapt the concepts of Kelly
22674	22675	examples show that for a decomposable non-binary constraint satisfaction problem, FC on the binary decomposition is incomparable to algorithms nFC0 and nFC1 in terms of consistency checks. As in , we count n primitive consistency checks to check if an n-tuple of an n?ary constraint is consistent, which means that 2 checks are counted for a binary constraint. Example 1. Consider a ternary
22674	22675	and van Beek compared the forward checking algorithm, nFC0 on non-binary constraints with the forward checking algorithm FC applied to binary encodings that introduce extra (hidden) variables .s148 I. Gent, K. Stergiou, and T. Walsh 7 Conclusions We have performed a detailed theoretical comparison of the effects of binary and non-binary constraint propagation on decomposable non-binary
22674	22676	all remaining values for a variable are removed, a domain wipeout occurs and the algorithm backtracks. Forward checking can be generalized to an algorithm for non-binary constraints (called nFC0 in ) which makes every k-ary constraint with k ? 1 variables instantiated arc-consistent. No pruning is performed on k-ary constraints with less than k ? 1 variables instantiated. As required, this
22674	22676	and there is one problem on which it visits strictly fewer nodes. Algorithm A and B are incomparable if neither A dominates B or vice versa (A ? B). The following identities summarize results from : nFC2 > nFC1 > nFC0, nFC5 > nFC3 > nFC2, nFC5 > nFC4 > nFC2, nFC3 ? nFC4.sDecomposable Constraints 137 3 Forward Checking on Decomposable Constraints We will compare the level of consistency
22674	22676	in the projections for nFC1) and discover that the current subproblem (the constraint projections) admit no satisfying tuples. ??sDecomposable Constraints 139 These results, as well as those from , are summarized in Figure 1. nFC5 nFC4 nFC3 nFC2 nFC1 FC nFC0 incomparable strictly dominates Fig. 1. The performance of the forward checking algorithm, FC on the binary decomposition of a set of
22674	22676	variable, where d is the maximum domain size. If Cc,f is the number of constraints between the current variable and future variables then FC performs O(Cc,f d) consistency checks at each node.  gives upper bounds in the number of consistency checks that algorithms nFC0-nFC5 perform at each node of the search tree. nFC0 forward checks an n?ary constraint when n?2 variables have been
22674	22678	with one another using more complicated examples. 5.2 Arc-Consistency For any non-binary constraint C, specified by a predicate, GAC can be established by the best known algorithm, GAC-schema , with O(d k ) worst-case complexity, where d is the maximum domain size of the variables and k is the arity of the constraint. AC can be enforced on the binary decomposition of a decomposable
22674	22679	A solution is an assignment of values to variables that is consistent with all constraints. Many lesser levels of consistency have been defined for binary constraint satisfaction problems (see  for full references). A problem is (i, j)?consistent iff it has non-empty domains and any consistent instantiation of i variables can be extended to a consistent instantiation involving j
22674	22679	is generalized arcconsistent (GAC) iff for any variable in a constraint and value that it is assigned, there exist compatible values for all the other variables in the constraint . Following , we call a consistency property A stronger than B (A ? B) iff in any problem in which A holds then B holds, and strictly stronger (A >B)iffs136 I. Gent, K. Stergiou, and T. Walsh it is stronger and
22674	22679	B (A ? B) iffA is not stronger than B nor vice versa. Finally, we call a local consistency property A equivalent to B iff A implies B and vice versa. The following identities summarize results from  and elsewhere: strong PC > SAC > PIC > RPC > AC, NIC > PIC, NIC ? SAC, and NIC ? strong PC. Backtracking based algorithms perform depth-first search in a tree of variable assignments. Each node of
22674	22683	(PIC) iff it is (1, 2)-consistent. A problem is neighbourhood inverse consistent (NIC) iff any value for a variable can be extended to a consistent instantiation for its immediate neighbourhood . A problem is restricted path-consistent (RPC) iff it is arc-consistent and if a variable assigned to a value is consistent with just a single value for an adjoining variable then for any other
22674	22684	Even higher levels of consistency can be maintained at each nodes in the search tree. For example, the maintaining arc-consistency algorithm (MAC) enforces AC at each node in the search tree . If enforcing AC removes all remaining values for a variable, a domain wipe-out occurs and the algorithm backtracks. For non-binary constraints, the algorithm that maintains generalized
22674	22685	arc-consistency on the binary decomposition. Indeed, under a simple restriction, it is strictly stronger than path inverse consistency on the binary decomposition. By generalizing the arguments of , these results show that a search algorithm that maintains generalized arc-consistency on decomposable constraints strictly dominates a search algorithm that maintains arc-consistency on the binary
22674	2002	frequency allocation, etc. Many constraint satisfaction problems can be naturally and efficiently modelled using non-binary constraints like the “all-different” and “global cardinality” constraints . Certain classes of these non-binary constraints are “network decomposable”  as they can be represented by binary constraints on the same set of variables. Throughout this paper, we will
22674	22689	frequency allocation, etc. Many constraint satisfaction problems can be naturally and efficiently modelled using non-binary constraints like the “all-different” and “global cardinality” constraints . Certain classes of these non-binary constraints are “network decomposable”  as they can be represented by binary constraints on the same set of variables. Throughout this paper, we will
22674	22689	variables. In this paper, we compare theoretically the levels of consistency which are achieved on non-binary constraints to those achieved on their binary decomposition. We extend the results of  and include material that covers several new topics. To be precise, we present many new results about the level of consistency achieved by the forward checking algorithm and its various
22674	22689	to non-binary constraints, and identify special cases of non-binary decomposable constraints where weaker or stronger conditions, than in the general case, hold. We correct an error in  that suggested that neighborhood inverse consistency on the binary decomposition is an upper bound on the level of consistency achieved by generalized arc-consistency on decomposable non-binary
22674	22689	that enforce even higher levels of consistency than forward checking have been shown to be highly effective at solving binary and non-binary constraint satisfaction problems (see, for example, ). In this section, we characterize the level of consistency achieved by (generalized) AC on decomposable constraints. The following theorem (from ) puts a lower bound on the level of
22674	22689	with respect to the binary decomposition. Theorem 5. Generalized arc-consistency on decomposable constraints is strictly stronger than arc-consistency on the binary decomposition. Proof. See . ???? As we show later on in this section, this lower bound is strict since we can exhibit a large class of problems on which GAC is equivalent to AC on the binary decomposition. In , we claimed
22674	22690	algorithm is used. However, since we are dealing with “not equals” constraints, AC can be achieved with O(k2 ) worst-case complexity. This is a correction on the bound for such constraints given in  and it is based on the following observations: First, for a network of “not equals” constraints, an AC-3 like algorithm will revise each edge at most once. And second, for a “not equals”
9864749	12653	? and white noise respectively. Such a multi-channel model may arise through the deployment of multiple sensors or through fractional sampling when the continuous-time channel has excess bandwidth . A typical reformulation of this problem  involves the vector processes where ? ¤?? ? ¢ ¤ ¡?? ¡??¢???????????? ¡£?¢??£??????? ? ? ? ? ? ? ? ? ¢ ? ¢ ? ¢???? ????? ? ¢???????? ? ? ? ? ¤ ? ?¥¢ ¢ ? ¤
9864749	12653	¦ ? ? ????? ? ¦ ??? ??? (3) Two types of techniques that have received significant attention are the so called subspace based algorithms, pioneered by , and the genre of algorithms initiated by . Both assume that ? has full column rank, and use the SOS of the channel output. Subspace methods have the advantage of not requiring knowledge of the channel input statistics. However, in many
9864749	12653	? ¤??¥? ??¢????¢?????? ? (4) ? to estimate ? ? to within an unitary scaling ? constant. Here indicates conjugate transpose. ? ? The whiteness ? ¢?? assumption on is crucial for the algorithm from . This was noted in  where a modification was proposed in order to deal with weakly correlated sources with unknown correlation (which must then be estimated). Another approach is found in ,
9864749	12653	contained in ??? ? , ? ? ??? ? , . . . ??? , ??? ? with ? ? the corresponding increase in computational complexity, By contrast, our algorithm is a natural extension of the original method from , making use of ??? ??? and ? ? ??? ? only. It ??? is shown via simulations that the new algorithm outperforms the subspace approach of . We denote by ? the square shift matrix with ones in the
9864749	12653	¢???? ? ? Assumption 2 ? is tall and has full column rank. We shall assume ? ¢ ¤ , as the white noise component can ? be subtracted from the output autocorrelation matrices using a standard device, . In this case one has for ??? ? ? ? all ? ? ¤?? ??? ? ? ? ? ? (6) ? ? Our goal is to find an estimate of ? from ? ¤ ? ??? (6) for and from the knowledge of ??? ? and ? ? ??? ? . To this end we ???
8832677	22717	by enumerative algorithms, are often unable to achieve feasible solutions to large problems due to excessive computing requirement. Examples of optimization approaches include Branch-and-Bound (Perregaard and Clausen 1998), and linear programming (Pinedo 1995). Due to the limitation of exact enumerative techniques, approximation methods become a more viable alternative. In these methods, an optimal solution is not
8922090	22724	protein-protein interaction prediction, appearance probability matrix, primary interaction probability 1 Introduction With the accumulation of protein data and the associated data on the Internet , the chance to computationally find the structures and functions of proteins based on the data is greatly increased. More importantly the accumulation of experimental protein-protein interaction
8922090	22725	protein-protein interaction prediction, appearance probability matrix, primary interaction probability 1 Introduction With the accumulation of protein data and the associated data on the Internet , the chance to computationally find the structures and functions of proteins based on the data is greatly increased. More importantly the accumulation of experimental protein-protein interaction
8922090	22725	of Proteins)  database were used to construct a protein family interaction map. In this algorithm, interactions were predicted based on structural information by parsing PDB (Protein Data Bank)  coordinates to determine if each domain pair could make close contacts.s252 Han et al. Recently, predictions of protein interactions are performed in the context of domain-domain interactions at
8922090	22726	Interaction Prediction Method 251 sequence is one approach . Another is to predict protein interactions by analyzing the physicochemical properties or tertiary structure of proteins . Domain based protein-protein interaction prediction is also an approach, and recently it is being actively studied by several research groups . Most domain based protein-protein
8922090	22726	without domain information. A technique using a support vector machine (SVM) based on primary sequence and associated physicochemical properties is developed to predict protein-protein interactions . Gene fusion method calling “Rosetta stone”  is also an computational approach to identify functional relations of proteins rather than to predict physical interactions. In another study
8922090	22730	interaction. Also, it is generally accepted that the unit of protein structure and sequence is domain, and the notion is used in various classification systems such as SCOP, CATH and FSSP . Deng et al.  proposed a probabilistic prediction model for inferring domain interactions from protein interaction data. The maximum likelihood estimation technique is mainly used in their
8922090	22731	used to extract domain information and the MIPS database is used to test their model, but they also take single domain pair as a basic unit of protein interactions. The approach taken by Kim et al.  shares this assumption with Deng et al.  but they both suffer from the low sensitivity and specificity of the predictions. Ng et al.  collected data from three data sources. The first one is
8922090	333	is possible. Moreover, it can be used as basic data in predicting functions of unknown proteins . There are several approaches in computationally predicting protein-protein interactions . Finding and analyzing subsequences affecting the protein-protein interactions from raw proteinsA Protein-Protein Interaction Prediction Method 251 sequence is one approach . Another is to
8922090	333	support vector machine (SVM) based on primary sequence and associated physicochemical properties is developed to predict protein-protein interactions . Gene fusion method calling “Rosetta stone”  is also an computational approach to identify functional relations of proteins rather than to predict physical interactions. In another study , interacting pairs of the Yeast proteins and
8922090	7708	relations of proteins rather than to predict physical interactions. In another study , interacting pairs of the Yeast proteins and domains in the SCOP (Structural Classification of Proteins)  database were used to construct a protein family interaction map. In this algorithm, interactions were predicted based on structural information by parsing PDB (Protein Data Bank)  coordinates
8922090	7708	interaction. Also, it is generally accepted that the unit of protein structure and sequence is domain, and the notion is used in various classification systems such as SCOP, CATH and FSSP . Deng et al.  proposed a probabilistic prediction model for inferring domain interactions from protein interaction data. The maximum likelihood estimation technique is mainly used in their
8922090	22733	is possible. Moreover, it can be used as basic data in predicting functions of unknown proteins . There are several approaches in computationally predicting protein-protein interactions . Finding and analyzing subsequences affecting the protein-protein interactions from raw proteinsA Protein-Protein Interaction Prediction Method 251 sequence is one approach . Another is to
8922090	22733	properties or tertiary structure of proteins . Domain based protein-protein interaction prediction is also an approach, and recently it is being actively studied by several research groups . Most domain based protein-protein interaction prediction methods share the conjecture that protein-protein interaction is the result of domain-domain interaction. Those methods infer domaindomain
8922090	22733	of protein interactions. The approach taken by Kim et al.  shares this assumption with Deng et al.  but they both suffer from the low sensitivity and specificity of the predictions. Ng et al.  collected data from three data sources. The first one is the experimentally derived protein interaction data from DIP . The second one is the intermolecular relationship data from protein
8922090	22738	is evaluated for the interacting set of protein pairs in a Yeast organism and artificially generated non-interacting sets of protein pairs. When 80% of the set of interacting protein pairs in DIP  is used as a learning set of interacting protein pairs, very high sensitivity (86% as average) and moderate specificity (56% as average) are achieved within our framework. This paper is organized
8922090	22738	from the low sensitivity and specificity of the predictions. Ng et al.  collected data from three data sources. The first one is the experimentally derived protein interaction data from DIP . The second one is the intermolecular relationship data from protein complexes and the last one is the computationally predicted data from Rosetta Stone sequences. Then they infer putative
22739	11277	this section we consider a classification by learning classification rules which can be inspected by the physician. The details of the network structure and the learning algorithm can be found in ,. The result of the training procedure are rules of the form (belonging to the core or support rectangle) if variable 1 in (–?, 50) and if variable 2 in (20,40) and if variable 3 in (–?,?) then
22739	11289	dobutrex . 4.1 Diagnosis by growing neural networks The neural network chosen for our classification task is a modified version of the supervised growing neural gas (abbr. SGNG, see ). Compared to the classical multilayer perceptron trained with backpropagation (see ) which has reached a wide public, this network achieved similar results on classification tasks but
22739	22752	(MedLine  listed about 1700 papers for the keywords “artificial neural network” in spring 2001) 3s4 there are many reviews for the use of artificial neural networks in medicine, see e.g. ,,. In this contribution, only the basic principles of neural networks will be presented in the next section in order to set the base for applications like the one in section 4. 3. Basic
22777	22781	in a given state are not known a priori. When the reasoning on specific goal states to be reached becomes not meaningful, the problem solving to be deal with calls for, so called, process-oriented  approach. In other words the goals in the EM planning are also localized on higher abstractions level. For instance, one of such goals (top, maximal preferences) can require to maintain always the
22777	22784	in the specification of E-M'er role. In the IDA system, actions representation is similar to the probabilistic state space operators (PSOs)  an extension of the classical STRIPS operators . A PSO ?? is a triple (?,?,?) where ? e ? are conjunctions of atomic formula (x=v) where x ? X is a variable state and v is one of the possible variable values. ? represents action preconditions
22777	22784	to be able to apply the operator ?, resulting in environment transition to the state described by ? with probability ?. ? is also called postconditions. In practice, ? is a set of STRIPS operators , enriched by a probability value associated to each transition. For instance the complex action of spreading foam on a tank is described as follows: Action: Foam the top ring of a tank when
22777	22784	in decision making. The practical knowledge about utility of the past plans and their adopting to similar emergency situations has motivated our choice of Case-Base reasoning (CBR) techniques. In particular, in the MDP Planner an architecture similar to that of Dyna-Q, described in  is implemented. The Planner provides a policy computed upon the optimization of a value function. If a
22777	22787	(Intelligent Decision Support System) for different emergency domains and operator/manager role were discussed and illustrated in the previous ENEA's papers since 1993, see for example , , , . The main idea is based on the TOGA (Top-down Object-based Goal-oriented Approach) conceptual framework, proposed and theoretically developed by Gadomski since 1989, where an abstract
22777	22787	the UML language has been adopted . 3. Information, Knowledge and Goals 3.1 Information and the Test-Case 4sThe test-case describes the emergency domain from the EU MUSTER project , it is an emergency in an oil port. Every global or local state of the domain communicated to the manager is represented as an information and is available explicitly. The map with the initial
22777	11826	Actual components: objects and resources. They represent current state of the domain. The analytical results obtained here suggest that the temporal intervals, non-monotonic and default reasoning , ,  could also effectively support a generic inference tool employed for information, knowledge and preference management in frame of the IPK architecture. 4.2 Planner agent Emergency
22777	11826	respect to the MDP framework posed also some interesting problems such as how to model complex actions with various duration time and actions that can be executed in parallel, see for example . Concluding, from the personoid perspective, the planning process can use different criteria for internal choices and can be realizable by different planning methods. These criteria were identified
5978	16029	socalled predicator. In figure 1, p1 is the predicator connecting X1 to r1, and p2 the predicator that connects X2 to r2. In the Predicator Set Model, which is an extension of the Predicator Model (, ), a fact type is considered to be a set of predicators. A relation type is therefore considered as an association between predicators, rather than between objects types. Fact types are
5978	16029	For proper specialisation, it is required that subtypes be defined in terms of one or more of their supertypes. Such a decision criterion is referred to as the Subtype Defining Rule (see e.g. ). Identification of subtypes is derived from their supertypes. Specialisation relations are organised in so-called specialisation “hierarchies”. A specialisation hierarchy is in fact not a
5978	21414	a Predicator Set schema. In  a formalised translation mechanism is given. Context-free grammars are generally employed for describing document structures (see for example , , ). The Predicator Set Model has sufficient expressive power to describe such structures elegantly. This is done by interpreting context free grammars in terms of the Predicator Set Model. The
5978	5620	introduced that allow for compact representations of complex objects. 2 Basic Data Modelling Concepts One of the key concepts in data modelling is the concept of relation type or fact type. In ER () and NIAM () a relation type is considered to be an association between object types. In figure 1 the graphical representation of a binary relation type R between object types X1 and X2 in
5978	5620	between these concrete and abstract object types can only be crossed by special binary fact types. These fact types correspond to bridge types in NIAM (, ), and attribute types in ER (). Each entity type must be identifiable in terms of label types. Anothor basic concept of data modelling is specialisation, also referred to as subtyping. Specialisation is a mechanism for
5978	5974	Aided Design (CAD) and Computer Aided Manufacturing (CAM) are also areas in which such complex structures frequently occur. Finally, in the development of so termed Evolving Information Systems (, , , ) where the information structure itself is allowed to change over time as well, there is a need 1This work has been partially supported by SERC project SOCRATES. Software
5978	10702	technique with an expressivity which is based on a set of powerful modelling concepts. In this paper a general data modelling technique is introduced, which has been defined formally in  and . This modelling technique, the Predicator Set Modelling technique (Predicator Set Model for short), indeed is capable of representing complex structures in a natural way. In this paper a number of
5978	24319	structures have to be “flattened”, i.e. represented non-hierarchically, which leads to overspecification. This in turn does not comply with the conceptualisation principle as it is formulated in . Various application domains indeed contain objects with complex structures. Documents (and Hypertexts) are an example in the field of office automation. In  it is estimated that 1% of all
5978	24319	Products (see figure 4). However, these extra object types are not conceptually relevant. Their introduction should therefore be considered as a violation of the Conceptualisation Principle (see ,  or ). ? ? ? ? ?? ?? ??? Car  ? ??? Product ? ? ?? House ? ? ? ?? ???? ?? ? ?? has price is price of has is of has is of Figure 5: Example of Generalisation ? ? ? ? ? ? ? ? ?
5978	10707	Design (CAD) and Computer Aided Manufacturing (CAM) are also areas in which such complex structures frequently occur. Finally, in the development of so termed Evolving Information Systems (, , , ) where the information structure itself is allowed to change over time as well, there is a need 1This work has been partially supported by SERC project SOCRATES. Software
5978	16081	Figure 4: Subtyping instead of Generalisation ? ? number is modelled by means of an encircled U, a so-called uniqueness constraint. For the semantics of complex uniqueness constraints, see . We will point out that this schema suffers from overspecification. Firstly, a special label type (P code) has to be introduced in order to identify Products. Secondly, a special fact type and a
8922107	22820	study undertaken in ten agricultural regions of Ghana, Rwanda and Kenya documented the dynamic evolution of property rights over cropland with increasing population density and market integration (Migot-Adholla, et al. 1991; Place and Hazell 1993). As in much of Sub-Saharan Africa, full ownership rights over land traditionally reside with the community in the study regions, and individuals have a more restricted set
8922119	22837	information at the expense of generality or guaranteed performance. Though in practice they may perform quite well. A novel approach of processing XML queries is being developed for project Timber  which is based on a complete and closed algebra named TAX which issStoring XML Data In a Native Repository 61 a generalization of the current relational algebra for tree structures. The project
8922119	22839	scheme is that it does not provide structural information – relations between nodes must be stored in separate. The best way seems to be to leverage one of existing structural number schemes , ,  which allow very effectively to determine the relation of arbitrary two nodes of the XML tree just from the information contained in their respective identifiers and thus they allow effective
8922119	22839	joins are very hard to distinguish from those which cannot be evaluated this way. Many of practical implementations avoid this problem simply by supporting only a limited set of XML queries , , . The rest is modestly ignored. Unlike other implementations our goal was to support all basic constructs of XPath and XQuery languages, not only path expressions. The
8922119	22839	currently used evaluation techniques use extensive indices built mostly as combinations of structural path summaries , value indexing and tree traversal (Lore ) or identifier schemes (XISS ). However the storage efficiency is often not considered in these approaches. Earlier systems relied on tree traversal techniques and structural indices like DataGuides or T-indices which are very
8922119	22840	of the query away from a repeatedly evaluated expressions) or constraint motion (evaluation of constraints and conditions as soon as possible). The overview of such rewriting rules can be found in . This phase might also include the elimination of common subexpressions of the query. The logically optimized query tree is then passed to a generator of query plans. This module constructs
8922119	24360	to renumber the whole XML tree. At the same time, neither contemporary techniques used for XML data management nor languages specially developed for XML document actualization like XUpdate  give us enough information about possible shapes of inserted subtrees or about scale of modifications to be done on the stored documents. In general, it is possible to create a structural numbering
8922119	22841	is that it does not provide structural information – relations between nodes must be stored in separate. The best way seems to be to leverage one of existing structural number schemes , ,  which allow very effectively to determine the relation of arbitrary two nodes of the XML tree just from the information contained in their respective identifiers and thus they allow effective query
8922119	22841	on the stored documents. In general, it is possible to create a structural numbering scheme which is usable even in case when we know nothing about shapes of inserted trees. However, as proven in , the worst-case maximum size of resulting identifiers assigned to individual nodes is O(n) bits. Occasional renumbering of nodes in some XML subtrees does not imply an insurmountable problem
8922119	22843	or a XML document collection. The currently used evaluation techniques use extensive indices built mostly as combinations of structural path summaries , value indexing and tree traversal (Lore ) or identifier schemes (XISS ). However the storage efficiency is often not considered in these approaches. Earlier systems relied on tree traversal techniques and structural indices like
8922119	22844	used indices used for structural joins can generally exceed the size of the whole source XML tree not giving any additional information besides the transitive ancestor-descendant relationship . A few other indexing schemes like SphinX  or APEX  reduce the size of resulting indices by deliberately not covering all necessary information at the expense of generality or guaranteed
8922119	22846	joins are very hard to distinguish from those which cannot be evaluated this way. Many of practical implementations avoid this problem simply by supporting only a limited set of XML queries , , . The rest is modestly ignored. Unlike other implementations our goal was to support all basic constructs of XPath and XQuery languages, not only path expressions. The implementationsStoring
8922119	22846	indices like DataGuides or T-indices which are very inefficient when they are stored in the external memory. These methods have been surpassed with more modern structural joins (XISS, eXist ) which compose the tree patterns by pairwise matching parent-child and ancestor-descendant relations between candidate XML nodes. However the most commonly used indices used for structural joins
22848	22852	controller used in our evaluation was differentiable with respect to its weights. This property is exploited by policy gradient ascent methods (Sutton et al., 2000). The Hamiltonian MCMC algorithm (Neal, 1993) uses gradient information to determine proposal directions. HINTS could also exploit gradient information (calculated for individual training examples) to obtain its primitive proposals. The
22848	22853	information that may be available. For example, the controller used in our evaluation was differentiable with respect to its weights. This property is exploited by policy gradient ascent methods (Sutton et al., 2000). The Hamiltonian MCMC algorithm (Neal, 1993) uses gradient information to determine proposal directions. HINTS could also exploit gradient information (calculated for individual training examples)
8922126	22870	properties of bit-level permutations in the construction of new ciphers or in strengthening existing ciphers. In particular, we study the cryptographic properties of the group operation GRP , as well as OMFLIP , which were recently identified for possible inclusion in future processor architectures. We investigate the properties of GRP and OMFLIP and consider how their
8922126	22870	As we decrease the size of the subword, we significantly increase the difficulty of achieving all possible permutations since the number of items to be permuted increases. Nevertheless, recent work  has examined architectural solutions that can achieve any arbitrary permutation of both single-bit and multi-bit subwords packed in a register. Cryptographically, bit-level operations are useful in
8922126	22870	slow in software. While the few fixed permutations in DES can be sped up using table lookup techniques in software, it is not feasible to do this for all possible data-dependent permutations. In  the use of OMFLIP to speed up the performance of fixed permutations within DES is explored. More recent proposals for hash functions and encryption functions—including the new AES —have
8922126	22870	X and Y , generating a result Z where all are w-bit words. The word X is rotated left by the amount specified in the lower lg(w) bitsofY . Several new permutation instructions such as PPERM , GRP , CROSS , OMFLIP , and BFLY  have been proposed for arbitrary bit-level permutations. However, we will restrict our attention to GRP and OMFLIP in this chapter. I.4.1
8922126	10323	cryptographic algorithms and the role of bitwise permutations as a contribution to their security. It is typical to classify cryptographic algorithms according to the way they use key information . Public key algorithms use two keys; one is kept secret and the other—as the name implies—is made public. Such algorithms are not our concern here. Other algorithms require that the two
8922126	6029	operations in Section I.4. In Section I.5, we analyze the cryptographic properties of GRP and, as an example, in Section I.6 we explore how one might use GRP in a variant of the block cipher RC5 . Section I.7 concludes the chapter. I.2 Motivation for New Permutation Operations Bit-level permutation operations are very important from both an architectural and cryptographic point of view.
8922126	6029	weaknesses. To help judge how successful such new operations might be, we will use the datadependent rotation (DDR) as a means for comparison. This operation has been used in the block cipher RC5  and it has been widely studied from a cryptographic perspective. Like all the permutations considered in this chapter, the action of DDR is not fixed. Instead, the bits of a control register are
8922126	6029	of these properties. I.6.1 The block cipher RC5 When considering the possible impact of DDR and other permutations in cryptographic algorithms, a natural starting point is the block cipher RC5 . This was designed to be extremely simple and this means that the effect of introducing DDR can be reasonably well measured. We give a very brief description of RC5. The initial secret key is used
8922126	22876	properties of bit-level permutations in the construction of new ciphers or in strengthening existing ciphers. In particular, we study the cryptographic properties of the group operation GRP , as well as OMFLIP , which were recently identified for possible inclusion in future processor architectures. We investigate the properties of GRP and OMFLIP and consider how their
8922126	22876	As we decrease the size of the subword, we significantly increase the difficulty of achieving all possible permutations since the number of items to be permuted increases. Nevertheless, recent work  has examined architectural solutions that can achieve any arbitrary permutation of both single-bit and multi-bit subwords packed in a register. Cryptographically, bit-level operations are useful in
8922126	22876	and Y , generating a result Z where all are w-bit words. The word X is rotated left by the amount specified in the lower lg(w) bitsofY . Several new permutation instructions such as PPERM , GRP , CROSS , OMFLIP , and BFLY  have been proposed for arbitrary bit-level permutations. However, we will restrict our attention to GRP and OMFLIP in this chapter. I.4.1 Definition of
8922126	22876	omega flip flip omega Figure I.2. A 16-bit omega-flip network I.4.3 Basic properties of GRP and OMFLIP GRP can be used to simulate any bit permutation of a w-bit word with at most lg(w) steps . It can also be used for multi-bit subword permutations and is useful for multimedia processing. It can achieve any one of m! permutations of m subwords in at most lg(m) instructions, where m is
8922126	22877	and k is the number of bits in a multi-bit subword. In addition, GRP is very useful for accelerating sorting algorithms, and can achieve a speedup of 10 or more when sorting a small set of integers . OMFLIP has similar properties to GRP in terms of performing permutations of bits or multi-bit subwords that are stored in one word (or register). It can perform an arbitrary permutation of w bits
8922126	22878	formats, and pipeline organizations. Other implementation issues like execution latency and size of the functional unit required are discussed below. A hardware implementation of GRP given in  suggests that it takes slightly longer than a typical ALU (Arithmetic Logical Unit) latency. Since the latter is often 7sI. PERMUTATION OPERATIONS IN BLOCK CIPHERS used to determine the cycle time
8922126	22879	As we decrease the size of the subword, we significantly increase the difficulty of achieving all possible permutations since the number of items to be permuted increases. Nevertheless, recent work  has examined architectural solutions that can achieve any arbitrary permutation of both single-bit and multi-bit subwords packed in a register. Cryptographically, bit-level operations are useful in
8922126	22879	words. The word X is rotated left by the amount specified in the lower lg(w) bitsofY . Several new permutation instructions such as PPERM , GRP , CROSS , OMFLIP , and BFLY  have been proposed for arbitrary bit-level permutations. However, we will restrict our attention to GRP and OMFLIP in this chapter. I.4.1 Definition of GRP The GRP operation will be written as Z =
8922126	22880	permutations in the construction of new ciphers or in strengthening existing ciphers. In particular, we study the cryptographic properties of the group operation GRP , as well as OMFLIP , which were recently identified for possible inclusion in future processor architectures. We investigate the properties of GRP and OMFLIP and consider how their inclusion within a cryptographic
8922126	22880	As we decrease the size of the subword, we significantly increase the difficulty of achieving all possible permutations since the number of items to be permuted increases. Nevertheless, recent work  has examined architectural solutions that can achieve any arbitrary permutation of both single-bit and multi-bit subwords packed in a register. Cryptographically, bit-level operations are useful in
8922126	22880	slow in software. While the few fixed permutations in DES can be sped up using table lookup techniques in software, it is not feasible to do this for all possible data-dependent permutations. In  the use of OMFLIP to speed up the performance of fixed permutations within DES is explored. More recent proposals for hash functions and encryption functions—including the new AES —have
8922126	22880	where all are w-bit words. The word X is rotated left by the amount specified in the lower lg(w) bitsofY . Several new permutation instructions such as PPERM , GRP , CROSS , OMFLIP , and BFLY  have been proposed for arbitrary bit-level permutations. However, we will restrict our attention to GRP and OMFLIP in this chapter. I.4.1 Definition of GRP The GRP operation will be
22915	1957	outperform sum rule fusion, with the multi-label model performing better than the binary model. 1. INTRODUCTION Combinations of classifiers are consistently more accurate than single classifiers . For atlas-based segmentation, multiple independent classifiers arise naturally from the use of multiple atlases derived from different individuals . We evaluate in this work methods to estimate
22915	1957	2.3. Performance-Based Decision Fusion Independent segmentations are combined into a final segmentation using several different decision fusion methods. The simplest method is sum rule fusion , where eachsr?medLip r?medBR l?medLip r?medColl l?medBR l?medColl r?latLip l?latLip r?latColl l?latBR r?latBR l?latColl r?Med r?Lob r?vMB CB l?vMB PL?SOG l?Lob l?Med Fig. 1. Example of a central
22915	1957	Study Using the manual segmentation as a gold standard, we evaluate the segmentation accuracies of both EM methods relative to each other and to classifier combination by sum rule decision fusion . For each of 20 individuals we compute 19 atlas-based segmentations, using each of the remaining 19 individuals as the atlas. These segmentations are combined into a final segmentation using each
22915	22917	more accurate than single classifiers . For atlas-based segmentation, multiple independent classifiers arise naturally from the use of multiple atlases derived from different individuals . We evaluate in this work methods to estimate the performance parameters of multiple atlas-based classifiers. The performance estimates can be used in the classifier combination to assign higher
22915	22920	2.2. Image Registration and Atlas-Based Segmentation An atlas is registered to a given image by first computing an affine  transformation, followed by a free-form deformation based on B-splines  to account for inter-individual shape differences. Our implementation of both methods is highly efficient and takes advantage of SMP multiprocessing . This facilitates repeating in particular
22915	22921	a free-form deformation based on B-splines  to account for inter-individual shape differences. Our implementation of both methods is highly efficient and takes advantage of SMP multiprocessing . This facilitates repeating in particular the (computationally expensive) non-rigid registration with different atlases. 2.3. Performance-Based Decision Fusion Independent segmentations are
22915	22923	interpolation. Each atlas-based segmentations is more or less accurate, and knowledge of each segmentation’s accuracy can be used to weight its contribution in the decision fusion. Warfield et al.  recently described an EM algorithm for estimating the binary segmentation performance of multiple experts in the absence of a ground truth, acronymed STAPLE for Simultaneous Truth and Performance
22915	22924	?? Cj). (2) The second method is based on a multi-label performance parameter model (row-normalized confusion matrix of a Bayesian classifier) that takes into account cross-label misclassifications . The entries of this matrix are the following conditional probabilities: ? (i,j) k = P (ek(x) = j|x ? Ci). (3) Note that the binary model is a special case of the multilabel model with two classes,
22915	22924	and foreground, and p (1) k ? ?(1,1) k and q (1) k ? ?(0,0) k . Analogously to the binary performance model, the parameters of the multi-label model are estimated by an EM algorithm (see Ref.  for details). Given the classifier decisions and the estimated performance parameters of either model, the label probabilities can be computed for each voxel using Bayes’ theorem, e.g., P (x ?
8922143	5540	OTAs by re-wiring their input stage, without adding or re-sizing a single transistor. 1. INTRODUCTION The current-feedback OTA recently appeared in a new classification of operational amplifiers . This classification is based on four-terminal theory instead of two-port theory, and thus contains nine instead of the four classes of operational amplifiers that appeared in earlier
8922143	5540	stages (s, and ¡ –¡ in this paper. The two termiand ¡ , input and output) always have the same impedance level: very high or very low. When fourterminal instead of two-port theory is used, as in , then the two input terminals and the two output terminals are not seen as one port each, but as four independent terminals that can have different impedance levels. As a consequence, hybrid stages
8922143	5540	and the transistor implementations differ from what is discussed in this paper. 3. CFB OTA AND OFA There is an important relation between the amplifiers in Table 1: duality. It was shown in  that thesinput stage is dual to the output stage, and output stage, the ¡ input stage is dual to the ¡s¢ the input stage is dual to ¢ the output stage. This means, among other things, that the CFB
8922143	5540	the opamp into an OFA, i.e. to convert thissoutput into a ¢ output, an additional current-output terminal must ¦ § I-656 be built that reproduces the current of the voltage output. As explained in , this can be done either by sensing and replicating the supply current of the whole opamp, or by doing the same for the output branch of the opamp output voltage buffer. In both cases, two
8922143	22935	that the same functionality can also be described from a completely different theoretical background. One can show that the so-called infinite-gain secondgeneration current conveyor (CCII¨ ) from  is essentially the same as the CFB OTA. The background from which it came is, however, different, the CCII¨ was developed on the transistor level in order to optimise the trade-off between speed
22939	22941	very appealing for studying microbial ecology, they have only rarely been used in this field. Recently, we’ve introduced the idea of using EC techniques to assemble efficient real world ecosystems . With this approach, EC is used to search a set of individual organisms for the subset of organisms that together form an ecosystem that optimally performs a specific predetermined function.
22939	22941	Algorithm and representing ecosystems as bit strings encoding for the presence or absence of the corresponding organisms. The functions we’ve studied include dye degradation , biomass production  and minimal growth . 4 Conclusion We have described the use of EC to assemble efficient real world biological ecosystems. This constitutes a completely new area of research both in the fields of
22939	22941	very appealing for studying microbial ecology, they have only rarely been used in this field. Recently, we’ve introduced the idea of using EC techniques to assemble efficient real world ecosystems . With this approach, EC is used to search a set of individual organisms for the subset of organisms that together form an ecosystem that optimally performs a specific predetermined function.
22939	22941	in the same way, but it also goes further. Using EC, it’s actually possible to obtain ecosystems that perform functions that can not be selected for in nature. An example of this is given in . 2.2 Types of Ecosystem Assembly It’s possible to distinguish three main types of ecosystem assembly, each of which can be optimized using EC. The most straightforward method of assembly is
22939	22941	ecosystems as bit strings encoding for the presence or absence of the corresponding organisms. The functions we’ve studied include dye degradation , biomass production  and minimal growth . 4 Conclusion We have described the use of EC to assemble efficient real world biological ecosystems. This constitutes a completely new area of research both in the fields of EC and ecology. This
8922146	22944	0 1 3 5 7 CPU % 9 Figure 8. Variance of Inter-arrival times. 6. Related Work Quality of Service provisioning for data delivery and real-time applications have received considerable attentions in  and . There are been appreciable progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous
8922146	22945	5 7 CPU % 9 Figure 8. Variance of Inter-arrival times. 6. Related Work Quality of Service provisioning for data delivery and real-time applications have received considerable attentions in  and . There are been appreciable progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous experience in
8922146	22946	provisioning for data delivery and real-time applications have received considerable attentions in  and . There are been appreciable progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous experience in quality of service support , highlights needs to service differentiation even in the
8922146	22949	unavailable service. The performance perceived by the users of a Web service depends on the network infrastructure (possibly QoSenabled) but especially on the management of the servers’ resources . It is thus desirable that network servers (e.g., Web, Video on Demand, and FTP servers) are able to differentiate their services in a variety of classes, replacing the current simple best-effort
8922146	22949	delivery and real-time applications have received considerable attentions in  and . There are been appreciable progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous experience in quality of service support , highlights needs to service differentiation even in the end system. Different
8922146	22952	was used in Rialto for efficient scheduling of time-constrained independent activities . There are also different distributed processing schemes that allow end-to-end QoS support in middleware . In all revised works, resources control was used to increase performances or to provide class differentiation, without considering the lack of availability due to a poor control of communication
8922146	22953	have received considerable attentions in  and . There are been appreciable progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous experience in quality of service support , highlights needs to service differentiation even in the end system. Different architectures have been proposed and
8922146	22954	considerable attentions in  and . There are been appreciable progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous experience in quality of service support , highlights needs to service differentiation even in the end system. Different architectures have been proposed and implemented
8922146	22955	level to the entire process does not ensure real-time communication. In fact, the performance of a communication-bound process mainly depends on the scheduling of its I/O tasks, as indicated in . The architecture we propose is in charge of managing I/O activities of all processes residing on the endsystem. In fact, process I/O tasks consist of a sequence of system call invocations which
8922146	22955	progresses in QoS support separately for Web Server  and Video on Demand  services. Many works like  and  as well as our previous experience in quality of service support , highlights needs to service differentiation even in the end system. Different architectures have been proposed and implemented in order to support QoS guarantees in the end-system. For example, in
8922146	1500	the bandwidth assigned to different class of service. Many kernel extensions have been proposed to provide real-time guarantees for QoS-sensitive applications. For example, capacity reserves  have been used in Mach to allocate processing capacity for multimedia applications , and flexible CPU reservations was used in Rialto for efficient scheduling of time-constrained independent
8922146	1495	used in Mach to allocate processing capacity for multimedia applications , and flexible CPU reservations was used in Rialto for efficient scheduling of time-constrained independent activities . There are also different distributed processing schemes that allow end-to-end QoS support in middleware . In all revised works, resources control was used to increase performances or to
8922146	22959	Internet scenario is concerned, in order to maintain the popularity and reputation of a web site the quality of service perceived by users, especially the service availability, is a success factor . Furthermore, new web applications demand for delivery of multimedia data in real-time (e.g. streaming stored video and audio), and the information transfer via the Internet is becoming one of the
8922146	22960	Guaranteed. Such classes are presented in the following subsections. 3.1. Adaptive class service By adaptive we mean a service class that can be requested without any admission control mechanism . According to this class definition, we allocate CPU shares in a weighted way. This means that we set preliminary n weights, W1 < W2 < ... < Wn, associated to each of n classes (class n has the
8922146	22961	in order to support QoS guarantees in the end-system. For example, in  are proposed some architectural mechanisms to manage communication resources for guaranteed-QoS connections, and in  has been addressed the problem of scheduling real-time applications on general-purpose Operating System in order to provide different classes of communication services. Both architectures did not
22962	22963	There are nowadays many existing models for cryptographic protocol verification. The most well-known are perhaps the Dolev-Yao model (after , see  for a survey) and the spi-calculus of . A lesser known model was introduced by Sumii and Pierce , the cryptographic lambda-calculus. This has certain advantages; notably, higher-order behaviors are naturally taken into account,
22962	22963	here in using logical relations or variants thereof as sound criteria for establishing contextual equivalence of two programs. This is instrumental in defining security properties. As noticed in , a datum M of type # is secret in some term t(M) of type # # if and only if no intruder can say anything about M just by looking at t(M), i.e., if and only if t(M) # # # t(M # ) for any two M and M
22962	22963	equivalence at type # # . We are using #-calculus notions here, following , but the idea of using contextual equivalence to define security properties was pioneered by Abadi and Gordon , where both secrecy and authentication are investigated. We shall define precisely what we mean by contextual equivalence in a calculus without names (Section 3.2), then with names (Section 5.3).
22962	22963	to describe side effects, and in particular name creation, using Stark's insights . Further comparisons will be made in the course of this paper, especially with bisimulations for spi-calculus . This continues the observations pioneered in , where notions of logical relations for various monads were shown to be proper extensions of known notions of bisimulations. The precise relation
22962	22963	to suggest that this really denotes the encryption of V with key K. (That ciphertexts are just modeled as pairs is exactly as in modern versions of the Dolev-Yao model , or in the spi-calculus .) Then, let #bits# be the set of all pairs E(V, K), V # ###, K # #key#. For any set A, let A# be the disjoint sum of A with {#}, where # is an element outside A, and let # be the canonical
22962	22964	not relating any nonobservable key with any key. This is clearly a partial bijection, in fact the identity on the subset fr # of #key#. This is a popular choice: fr # is what Abadi and Gordon  call a frame, up to the fact that frames are defined there as sets of names, not of keys. To define R bits , we may choose any relation sandwiched between R # bits and R # bits . For
22962	22964	not just R fr,# # but also fr # and # # are defined by mutual induction on types. It is interesting, too, to relate the definition of R fr,# # to selected parts of the notion of framed bisimulation . Slightly adapting  again, call a theory (on type bits) any finite binary relation th # on #bits#. By finite, we mean that it should be finite as a set of pairs of values. A frame-theory
22962	22965	from # # #(#) to C C C. If U # (R o ) o## = #_# 0 , then by the uniqueness property of #_# 1 , we must have U # R = #_# 1 , i.e., diagram (3) commutes. As observed in , and extended to CCCs in , when C = Set Set Set, C C C is the product of two CCCs A A A and B B B, and |_| is the functor A A A(1, _) B B B(1, _), (R(#)) # type behaves like a logical relation. It is really a logical
22962	22966	to describe side effects, and in particular name creation, using Stark's insights . Further comparisons will be made in the course of this paper, especially with bisimulations for spi-calculus . This continues the observations pioneered in , where notions of logical relations for various monads were shown to be proper extensions of known notions of bisimulations. The precise relation
22962	22968	lambda-calculus, Subscone 1 Introduction There are nowadays many existing models for cryptographic protocol verification. The most well-known are perhaps the Dolev-Yao model (after , see  for a survey) and the spi-calculus of . A lesser known model was introduced by Sumii and Pierce , the cryptographic lambda-calculus. This has certain advantages; notably, higher-order
22962	22969	Cryptographic lambda-calculus, Subscone 1 Introduction There are nowadays many existing models for cryptographic protocol verification. The most well-known are perhaps the Dolev-Yao model (after , see  for a survey) and the spi-calculus of . A lesser known model was introduced by Sumii and Pierce , the cryptographic lambda-calculus. This has certain advantages; notably,
22962	22969	E(V, K) the pair (V, K), to suggest that this really denotes the encryption of V with key K. (That ciphertexts are just modeled as pairs is exactly as in modern versions of the Dolev-Yao model , or in the spi-calculus .) Then, let #bits# be the set of all pairs E(V, K), V # ###, K # #key#. For any set A, let A# be the disjoint sum of A with {#}, where # is an element outside A,
22962	22970	They define a so-called operational logical relation to establish observational equivalence of nu-calculus expressions. They prove that this logical relation is complete up to first-order types. In , Goubault-Larrecq, Lasota and Nowak define a Kripke logical relation for the dynamic name creation monad, which is extended by Zhang and Nowak in  so that it coincides with Pitts and Stark's
22962	22970	using Stark's insights . Further comparisons will be made in the course of this paper, especially with bisimulations for spi-calculus . This continues the observations pioneered in , where notions of logical relations for various monads were shown to be proper extensions of known notions of bisimulations. The precise relation with hedged and framed bisimulation  remains to
22962	22970	in a value of type # , possibly creating fresh names during the course of computation, is defined semantically by #T ## = T T T ###, where (T T T , # # # ,s, t t t) is the strong monad defined in . T T TA is defined by colim s # A(_ + s # ) : I # Set Set Set. On objects, this is given by T T TAs = colim s # A(s + s # ), i.e., T T TAs is the set of all equivalence classes of pairs (s # , a)
22962	22962	complex models of encryption, where cryptographic primitives may obey algebraic laws. Proofs omitted in the sequel are to be found in the full version of this paper, available as a technical report . Outline. We survey related work in Section 2. We focus on the approach of Sumii and Pierce, in which they define several rather complex logical relations as sound criteria of contextual
22962	22962	at the key type satisfies the basic lemma, hence can be used to establish contextual equivalence. Specializing the prelogical relations(R # ) # type of Theorem 3 (its proof is in the full version ), we get that R key is exactly equality on the set fr = {#t# | # t : key} of definable keys. Similarly, we may define the binary relation # # (K, K # ), for every K,K # # #key# \fr, (i.e., for all
22962	22971	first, we clarify the import of Sumii and Pierce as far as the behavior of logical relations on encryption types is concerned, and simplify it to the point that we reduce it to prelogical relations  and more generally to lax logical relations ; while standard recourses to the latter were usually required because of arrow types, here we require the logical relations to be lax at encryption
22962	22971	The first, and hardest one, is fresh name creation. The second is dealing with encryption and decryption. We shall see that the latter has an elegant solution in terms of prelogical relations , which we believe is both simpler and more general than Sumii and Pierce's proposal ; this is described in Section 3, although we ignore fresh name creation there, for clarity. Dealing with
22962	22971	semantics of case as an exercise to the reader.) The fact that the constants dec, enc, are required to have their denotations, D and E, related to themselves is reminiscent of prelogical relations . These can be defined in a variety of ways. Following , a prelogical relation is any family (R # ) # type of relations such that: 1. for every f, f # # ## 1 # #
22962	22971	subterms of # . The prelogical relation of Proposition 4 is strict at option types, too, provided there is a closed term of type # or ### has no junk. While the point in prelogical relations in  is mainly of being not strict at arrow types, the point here is to argue that it is meaningful either not to be strict at bits types, as in Section 3.2 (in the sense that R bits was not
22962	22973	, A # # are pairs of morphisms #u, v# (u in C, from S to S # , and v in C C C , from A to A # ), making the obvious square commute. Noting that Subscone C C C C is again a CCC (Mitchell and Scedrov  make this remark when C is Set Set Set, and |_| is the global section functor C C C(1, _)), the following purely diagrammatic argument obtains. Assume we are given a functor from # to Subscone C C
22962	22973	representation of CCCs again, from # # #(#) to C C C. If U # (R o ) o## = #_# 0 , then by the uniqueness property of #_# 1 , we must have U # R = #_# 1 , i.e., diagram (3) commutes. As observed in , and extended to CCCs in , when C = Set Set Set, C C C is the product of two CCCs A A A and B B B, and |_| is the functor A A A(1, _) B B B(1, _), (R(#)) # type behaves like a logical relation.
22962	22974	equivalence in a calculus without names (Section 3.2), then with names (Section 5.3). Both notions are standard, the latter being inspired by , only adapted to Moggi's computational #-calculus . In  and some other places, this kind of equivalence, which states that two values (or terms) a and a # are equivalent provided every context of type bool must give identical results on a and
22962	22974	in  so that it coincides with Pitts and Stark's operational logical relation up to first-order types. We continue this work here, relying on the elegance of Moggi's computational #-calculus  to describe side effects, and in particular name creation, using Stark's insights . Further comparisons will be made in the course of this paper, especially with bisimulations for spi-calculus
22962	22974	dispense with fresh name creation. This is most easily done by following Stark , who defined a categorical semantics for a calculus with fresh name creation based on Moggi's monadic #-calculus . We just take his language, adding all needed constants as in Section 4. 5.1 The Moggi-Stark Calculus The Moggi-Stark calculus is obtained by adding a new type former T (the monad), to the types of
22962	22974	)a # (#s # )a for any additional set of new names s ## not free in a. We shall in fact write (#s # )a the equivalence class of (s # , a), to aid intuition. The semantics of let and val is standard . Making it explicit on this particular monad, we obtain: #val t# s# = (##) #t# s# and #let x # t in u# s# = (# s # + s ## )b, where #t# s# = (#s # )a, we assume that # # t : T # and #, x : # # u :
22962	22975	We shall define precisely what we mean by contextual equivalence in a calculus without names (Section 3.2), then with names (Section 5.3). Both notions are standard, the latter being inspired by , only adapted to Moggi's computational #-calculus . In  and some other places, this kind of equivalence, which states that two values (or terms) a and a # are equivalent provided every
22962	22975	in Section 3, although we ignore fresh name creation there, for clarity. Dealing with fresh name creation is harder. The work of Sumii and Pierce  is inspired in this respect by Pitts and Stark , who proposed a #-calculus devoted to the study of fresh name creation, the nu-calculus. They define a so-called operational logical relation to establish observational equivalence of nu-calculus
22962	22975	n 1 , . . . , nm , the only way C can be made to depend on them is to assume that C has m free variables z 1 , . . . , z m of type # # #, which are mapped to n 1 , . . . , nm . (It is more standard  to consider expressions built on separate sets of variables and names, thus introducing the semantic notion of names in the syntax. It is more natural here to consider that there are variables z l
22962	22976	as far as the behavior of logical relations on encryption types is concerned, and simplify it to the point that we reduce it to prelogical relations  and more generally to lax logical relations ; while standard recourses to the latter were usually required because of arrow types, here we require the logical relations to be lax at encryption types. Second, we prove various completeness
22962	22976	Monads Given that terms now take values in some category (Set Set Set I ), not in Set Set Set as in Section 3, the proper generalization of prelogical relations is given by lax logical relations . We introduce this notion as gently as possible. Let # be the set of base types, seen as a discrete category. The simply-typed #- calculus gives rise to the free CCC # # #(#) over # as follows: the
22962	22977	to first-order types. We continue this work here, relying on the elegance of Moggi's computational #-calculus  to describe side effects, and in particular name creation, using Stark's insights . Further comparisons will be made in the course of this paper, especially with bisimulations for spi-calculus . This continues the observations pioneered in , where notions of logical
22962	22977	to some simplification. 5 Name Creation and Lax Logical Relations No decent calculus for cryptographic protocols can dispense with fresh name creation. This is most easily done by following Stark , who defined a categorical semantics for a calculus with fresh name creation based on Moggi's monadic #-calculus . We just take his language, adding all needed constants as in Section 4. 5.1
22962	22977	allows one to describe the creation of fresh names as returning any name outside s. This is most elegantly described by letting the values of terms be taken in the presheaf category Set Set Set I , where I is the category whose objects are finite sets and whose morphisms s i #s # are injections. Given any type # , ### s is intuitively the set of all values of type # in a world where all
22962	22977	in a value of type # , possibly creating fresh names during the course of computation, is defined semantically by #T ## = T T T ###, where (T T T , # # # ,s, t t t) is the strong monad defined in . T T TA is defined by colim s # A(_ + s # ) : I # Set Set Set. On objects, this is given by T T TAs = colim s # A(s + s # ), i.e., T T TAs is the set of all equivalence classes of pairs (s # , a)
22962	22978	protocol verification. The most well-known are perhaps the Dolev-Yao model (after , see  for a survey) and the spi-calculus of . A lesser known model was introduced by Sumii and Pierce , the cryptographic lambda-calculus. This has certain advantages; notably, higher-order behaviors are naturally taken into account, which is ignored in other models (although, at the moment, higher
22962	22978	through the use of well-crafted logical relations, a tool that has been used many times with considerable success in the #-calculus: see , for numerous examples. Sumii and Pierce  in particular define three logical relations # Partially supported by the RNTL project Prouv, the ACI Scurit Informatique Rossignol, the ACI jeunes chercheurs &quot;Scurit informatique, protocoles
22962	22978	here in using logical relations or variants thereof as sound criteria for establishing contextual equivalence of two programs. This is instrumental in defining security properties. As noticed in , a datum M of type # is secret in some term t(M) of type # # if and only if no intruder can say anything about M just by looking at t(M), i.e., if and only if t(M) # # # t(M # ) for any two M and M
22962	22978	and decryption. We shall see that the latter has an elegant solution in terms of prelogical relations , which we believe is both simpler and more general than Sumii and Pierce's proposal ; this is described in Section 3, although we ignore fresh name creation there, for clarity. Dealing with fresh name creation is harder. The work of Sumii and Pierce  is inspired in this respect
22962	22978	stated precisely. 3 Deconstructing Sumii and Pierce's approach The starting point of this paper was the realization that the rather complex family of logical relations proposed by Sumii and Pierce  could be simplified in such a way that it could be described as merely one way of building logical relations that have all desired properties. It turned out that the only property we really need to
22962	22979	in a value of type # , possibly creating fresh names during the course of computation, is defined semantically by #T ## = T T T ###, where (T T T , # # # ,s, t t t) is the strong monad defined in . T T TA is defined by colim s # A(_ + s # ) : I # Set Set Set. On objects, this is given by T T TAs = colim s # A(s + s # ), i.e., T T TAs is the set of all equivalence classes of pairs (s # , a)
22962	22979	category of I, has as objects all morphisms w i #s in I, and as morphisms from w i #s to w # i # #s # all pairs (j, k) of morphisms such that the right diagram commutes. This is in accordance with , where it is noticed that w i j ## s k ## w # i # ## s # (4) Set Set Set I # is the right category to define a Kripke logical relation (but not necessarily lax) that coincides with Pitts and
22962	22979	that w : # # # # u : # and w : # # # # u # : # are derivable, #u# s # w i #s # #u # # s iff #u# s R w i #s # #u # # s. The (non-lax) logical relation of  is defined on # # # by: n R w i #s # # # n # iff n = n # # w. This is exactly what the lax logical relation of Definition 1 is defined as on the # # # type: Lemma 2. Let R w i #s # be the logical
22980	22983	of design decisions remains small. In the domain of automated generation of UIs, a lot of work has already been done, as reported by Paternò . SIERRA  provides a mixed-initiative approach  for selecting the widgets of a GUI and laying them out according to different layout algorithms. While the process is mainly guided by the system, the designer can stop the process, change
22980	22983	and list boxes, building the CloseRst procedure, setting the size of the widgets, and laying the widgets out in their containers. These steps are supported by a mixed-initiative approach  where some values of parameters are suggested by WOLD when possible. These suggested values can then be superseded by values provided by the designer when she want to keep the control over the
22980	22984	families of UIs of particular kinds of interactive systems, when the domain of interest is well defined in scope, several UI design support tools have been developed that show potential results . In this paper, we are interested by UIs of information systems which are characterized by consistent UIs to database access. This family of UIs is usually equipped with simple methods such as
22980	22984	believed that more UI design patterns attached to configurations of this model are expected, especially when the domain model should serve for different computing platforms with some generalization .sThis paper will present WOLD (WIZARD fOr Leveraging the Development of multi-platform user interfaces), a software that helps designers to produce running UIs coupled to data bases of information
22980	22984	the potential UI to be obtained are limited as the set of design decisions remains small. In the domain of automated generation of UIs, a lot of work has already been done, as reported by Paternò . SIERRA  provides a mixed-initiative approach  for selecting the widgets of a GUI and laying them out according to different layout algorithms. While the process is mainly guided by the
22980	22984	abstract UI. This system operates on specific XWEB servers and browsers tuned to the interactive capacities of particular platforms, which communicate thanks to an appropriate XTP protocol. TERESA  produces different UIs for multiple computing platform from a general task model which is progressively refined for the different platforms. Then, various presentation and dialogues techniques are
22980	22985	families of UIs of particular kinds of interactive systems, when the domain of interest is well defined in scope, several UI design support tools have been developed that show potential results . In this paper, we are interested by UIs of information systems which are characterized by consistent UIs to database access. This family of UIs is usually equipped with simple methods such as
22980	22985	parameters and re-explore step by step. Preliminary observations revealed that the mixedinitiative was appreciated but was located at a too low level. In the domain of multiplatform UIs, XWEB  produces UIs for several devices starting from a multi-modal description of the abstract UI. This system operates on specific XWEB servers and browsers tuned to the interactive capacities of
22980	22987	families of UIs of particular kinds of interactive systems, when the domain of interest is well defined in scope, several UI design support tools have been developed that show potential results . In this paper, we are interested by UIs of information systems which are characterized by consistent UIs to database access. This family of UIs is usually equipped with simple methods such as
22980	22987	of the current UI based on the actual values of options. Software engineers typically use domain models for creating UIs: entity-relationship model , class diagram, object-oriented models . Although the use of domain models is not questionable, it is believed that more UI design patterns attached to configurations of this model are expected, especially when the domain model should
22988	22990	integration, they define equivalences among objects, reconciliation of discrepancies, and “covering” supertypes which are collections of instances of different imported types. In the SIMS system , information sharing from multiple relational schema is facilitated through using the LOOM knowledge representation schema to construct a global schema for each application domain. Here, the global
22988	14835	Pegasus , UniSQL/M  and SIMS  support mediator capabilities through a unified global schema which integrates each remote database and resolves conflicts among these remote databases  within this unified schema. These projects made substantial contributions in resolving conflicts among different schema and data models. Scalability was not explicitly addressed, and will pose
22988	22994	applied in the context of heterogeneous DBMS, supporting SQL-like query languages. Alternately, the capability of a mediator is supported by the use of higher-order query languages or meta-models . The language or model provide constructs to resolve conflicts among the sources. Here, too, scalability is a problem, since the higherorder queries or the model have to be significantly changed,
22988	22994	formula can be complex, compared to Prolog unification. Although the research is interesting, INRIAsThe Design of DISCO 21 it is not applicable in a DBMS environment with legacy applications. In , a language for declarative specification of mapping between different object-oriented multidatabases is presented. Finally, the M(DM) meta-model uses meta-level descriptions of schemas to
22988	22996	structures. All these features can be applied while incorporating new data sources, and associating types of objects in the data sources to the types defined in the mediators. In related research , the main objective when integrating multiple data sources was obtaining a single unified type. In contrast, in DISCO we apply these features to the task of providing support for incorporating new
22988	22996	A further extension is functions which map between domains and ranges, and will allow the mediator to resolve mismatch of values in the data sources during query processing. In prior research , there has been much discussion about the mismatch of the data types, formats, values, etc., with respect to data sources and mediator types. In these previous approaches, the DBA resolves all
22988	22996	applied in the context of heterogeneous DBMS, supporting SQL-like query languages. Alternately, the capability of a mediator is supported by the use of higher-order query languages or meta-models . The language or model provide constructs to resolve conflicts among the sources. Here, too, scalability is a problem, since the higherorder queries or the model have to be significantly changed,
22988	22997	logical operator whose comparisons operators match but whose constants do RR n¡ 2704s18 A. Tomasic, L. Raschid & P. Valduriez not match. We believe that a variant of predicate-based caching  will accomplish close matching. While the associated statistics may be somewhat inaccurate, particularly in this case if there is high data skew, we believe that the statistics are still useful. We
22988	8193	wrt some target relational schema. In contrast to the unified global schema which resolves all conflicts among the entities of the local schema, the Garlic system , and research described in , assume a mediator environment based on a common data model. In , the common data model is the ODMG standard object model , which extends the OMG object-oriented data model . Semantic
22988	8193	interfaces. All these equivalences are used for query reformulation. They address the problem of mismatch in the querying capability of the servers, since a query is reformulated using the views . However, they do not focus on scalability issues. Although it is not described in this paper, we assume that there is such semantic knowledge, and it is used in query reformulation. The system
22988	8193	as a (conjunctive) query over the world view relations. However, they are not able to express general integrity constraints in the local interfaces. The reformulation algorithm described in  is limited, since they try to match each global entity in the world view, against the mapping knowledge. Thus, they are not able to match all conjunctive queries expressed over the the world view
22988	8193	an optimized equivalent query. This is especially true in a heterogeneous environment, where the view may be expressed over local information sources, which have dissimilar costs. In comparison to , the OQL query language that we use to express semantic knowledge is much more expressive. We are able to express rewrite rules which replace a view in the MDBMS interface with an OQL query over
22988	23002	wrt some target relational schema. In contrast to the unified global schema which resolves all conflicts among the entities of the local schema, the Garlic system , and research described in , assume a mediator environment based on a common data model. In , the common data model is the ODMG standard object model , which extends the OMG object-oriented data model . Semantic
22988	23002	interfaces. All these equivalences are used for query reformulation. They address the problem of mismatch in the querying capability of the servers, since a query is reformulated using the views . However, they do not focus on scalability issues. Although it is not described in this paper, we assume that there is such semantic knowledge, and it is used in query reformulation. The system
22988	23002	conjunctive queries expressed over the the world view entities, even if there exists a local entity defining this world view query (or a fragment of it). They cite an extension of their algorithm , which is able to answer a larger class of queries, by matching a conjunctive query against a conjunctive view, to produce an equivalent query, and the algorithm is NP-complete. The intent is to
22988	23002	replaced by a rewrite rule. Since the result of query, which is essentially a view, can be used to replace a subquery in the user query, we are able to cover the same space as the the algorithm in , with the caveat that we are reformulating wrt a much more complex and expressive query language. We also note that the space of query reformulation is not necessarily those queries in which we
22988	23002	collections, as described in . However, we are able to eliminate some collections in the query, based on semantic knowledge. This simplification is more general than the minimality criterion of , which does not exploit semantic knowledge. The focus of research in the TSIMMIS project  is the integration of structured and unstructured (schema-less) data sources, techniques for
22988	21126	system, and then describe various features of this architecture. 1.1 Architecturesdatabases must be available. If some As shown in Figure 1, current distributed heterogeneous database systems  deal with scale by adopting a distributed architecture of several specialized components. End users interact with applications (A) written by application programmers. Applications access a uniform
22988	21126	applied in the context of heterogeneous DBMS, supporting SQL-like query languages. Alternately, the capability of a mediator is supported by the use of higher-order query languages or meta-models . The language or model provide constructs to resolve conflicts among the sources. Here, too, scalability is a problem, since the higherorder queries or the model have to be significantly changed,
22988	21126	based on semantic knowledge. This simplification is more general than the minimality criterion of , which does not exploit semantic knowledge. The focus of research in the TSIMMIS project  is the integration of structured and unstructured (schema-less) data sources, techniques for the rapid prototyping INRIAsThe Design of DISCO 23 of wrappers and techniques for implementing
22988	23004	based on semantic knowledge. This simplification is more general than the minimality criterion of , which does not exploit semantic knowledge. The focus of research in the TSIMMIS project  is the integration of structured and unstructured (schema-less) data sources, techniques for the rapid prototyping INRIAsThe Design of DISCO 23 of wrappers and techniques for implementing
22988	23005	the knowledge required to resolve conflicts among the local schema, and mapping or transformation algorithms that support query mediation and interoperation among relational and object databases . Here, too, the emphasis is on resolving conflicts among schema and data models, to support interoperability of the queries. Lefebvre et al (1992) In , F-logic, a second order logic, is used to
22988	23007	the knowledge required to resolve conflicts among the local schema, and mapping or transformation algorithms that support query mediation and interoperation among relational and object databases . Here, too, the emphasis is on resolving conflicts among schema and data models, to support interoperability of the queries. Lefebvre et al (1992) In , F-logic, a second order logic, is used to
23026	23027	transformation techniques. For example it is possible to define an OR compositional semantics, i.e. a semantics which is compositional with respect to the union of theories, in a way analogous to . This leads to techniques for modular transformation of programs . Another application is in improving the accuracy of program analyses . Iterating unfolding of a program k times induces a
23026	23028	for deriving correct and efficient programs. Unfolding is a well-known program transformation strategy which was first formulated in the case of equational programs by Burstall and Darlington  and later introduced in logic programming by Komorowski . The combined effect of unification with simplification is also achieved in  by means of some superposition procedure for
23026	23030	case of equational programs by Burstall and Darlington  and later introduced in logic programming by Komorowski . The combined effect of unification with simplification is also achieved in  by means of some superposition procedure for program synthesis. We want to define an unfolding transformation on equational logic programs which preserves the computed answers substitutions
23026	23032	Logic Programs ? M. Alpuente † M. Falaschi ‡ M.J. Ramis † G. Vidal † Abstract Equational Logic Programming is a programming paradigm which integrates both Equational and Logic Programming (see  for surveys on this area). In this paradigm, an equational logic program can be seen as a Conditional Term Rewriting System (CTRS for short), i.e. a set of conditional equations which are
8922164	24584	49 least-developed countries on the UNCTAD list, 30 of whom are members of WTO. 8 Article 27(3)(b) of TRIPS Agreement, which is currently being reviewed by members of the WTO. 9 Henson-Apollonia (2002) estimates that by October 2002, there were 114 patents with claims to novel plants issued in the United States so far that year. 10 Ibid. Plant varietal or breeders’ rights are examples of
8922164	24584	to analyze the economic effects of PVP legislation deal mainly with the United States and include Perrin et al. (1983), Butler and Marion (1985), Knudson and Pray (1991), and Alston and Venner (2002). Diez (2002) analyzes the situation in Spain.s4 information on the future stream of revenues from selling a variety were complete, plant breeders would simply calculate the present value of the
8922164	24584	does, have strong agro-ecological determinants. 14 Fan and Pardey (1992, 1997) describe developments in Chinese agricultural R&D through to the early 1990s. Fan et al. (2003) and Huang et al. (2002) provide some details of the more recent developments. 15 The amount of public research spending in China is significant in global terms, accounting for about 10 percent of public agricultural R&D
8922164	24584	00/4 00/2 99/12 99/10 99/8 99/6 99/4 year/month Source: Compiled by authors from China Ministry of Agriculture (various issues)s19 Based on an assessment of the early wave of PVP applications, Tong (2002) argued that the number of applications in China was limited because of a lack of appreciation of the role of property rights in a market economy, the high cost of gaining protection, the uncertain
8922164	24584	rights to new seed varieties. Higher valued crops and those with significant market transactions give a greater incentive to acquire and maintain the rights to new varieties.s27 According to ISF (2002), worldwide seed sales are US$30 billion, of which China’s domestic market accounts for about 10 percent. The basis for these figures is not revealed, nor are crop specific values reported. Thus,
8922164	24605	are from China National Bureau of Statistics (2001). Estimates of the sowing rate, seed price, and seed replacement rate are national averages obtained from consulting Chinese experts. Pray et al. (1998) also contain useful information to help calibrate these simulations. a. The ratio of hybrid to non-hybrid is 9.5:0.5 for corn, 5:5 for rice, 4:6 for rapeseed, and 1:7:2(GM) for cotton. b.
8922164	24606	years) Total cost 71,864 8,679 39,704 Agricultural plants 47,089 5,687 26,016 (15 years of protection) Forestry (20 years of protection) 78,264 9,452 43,240 Source: China Ministry of Agriculture (1999) a. The 1999 exchange rate used here was US$1 = 8.28 yuan. b. The 1999 purchasing power parity rate is US$1 = 1.81 yuan World Bank (2001). 30 Similarly, Brazil charges US$348 to establish PVP rights
23073	23087	Fisher et al., 1992; Schurmann & Steffensen, 1994). In addition, it is unclear whether fish can recognize hypoxic water. Some early laboratory studies on stickleback Gasterosteus aculeatus L. (Jones, 1952) and on roach Rutilus rutilis (L.) (Hoeglund, 1961) suggest that hypoxia avoidance by fish was due to oxykinesis alone (i.e. unidirectional, increased activity level in hypoxic water resulting in
6568561	23107	RAND,16) C= OVL ? K master K master = OVL ? C 2.5 Authentication Master Slave RAND FIGURE 5: GENERATING A MASTER KEY Authentication is the process of verifying who is at the other end of the link . Bluetooth authentication is a challenge-response scheme in which a claimant's knowledge of a secret key is checked using a symmetric secret key (K). First, the verifier challenges the claimant to
3155	3133	problem reduces to the design of observers for the hybrid state;  considers the case in which the discrete state is further known and proposes a Luenberger observer for the continuous state;  combines location observers with Luenberger observers to design a hybrid observer that identifies the discrete location in a finite number of steps and converges exponentially to the continuous
3155	3134	first appeared in the seventies; a review of the state of the art as of 1982 can be found in . After a decadelong hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism
3155	3137	hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism are known, the identification problem reduces to the design of observers for the hybrid state;  considers the case
3155	3140	hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism are known, the identification problem reduces to the design of observers for the hybrid state;  considers the case
3155	3142	location observers with Luenberger observers to design a hybrid observer that identifies the discrete location in a finite number of steps and converges exponentially to the continuous state;  proposes a moving horizon estimator that, under some conditions, is asymptotically convergent and can be implemented via mixed-integer quadratic programming. When the model parameters and the
3155	3143	and can be implemented via mixed-integer quadratic programming. When the model parameters and the switching mechanism are unknown, the identification problem becomes even more challenging:  assumes that the number of models is known, and proposes an identification algorithm that combines clustering, regression and classification techniques;  uses mixed-integer quadratic
3155	3143	the hybrid state have been identified, the problem of estimating the switching parameters, e.g. the partition of the state space for PWAS, becomes a simpler problem. We refer interested readers to  for specific algorithms. Remark 5 (MIMO systems) Notice than our algorithm for SISO systems can also be applied to MIMO systems. To see this, let us first consider the case of multiple-input
3155	3144	hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism are known, the identification problem reduces to the design of observers for the hybrid state;  considers the case
3155	3146	hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism are known, the identification problem reduces to the design of observers for the hybrid state;  considers the case
3155	3147	hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism are known, the identification problem reduces to the design of observers for the hybrid state;  considers the case
3155	23117	first appeared in the seventies; a review of the state of the art as of 1982 can be found in . After a decadelong hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism
3155	23118	zK] T ??  T , (7) with I chosen in the degree-lexicographic order; and ? ? ? ? n + K ? 1 n + K ? 1 Mn = = (8) K ? 1 n is the total number of independent monomials. One can show  that the vector h ? RMn is simply a vector representation of the symmetric tensor product of the individual model parameters {bi} n i=1 , i.e. ? b?(1) ? b?(2) ? · · · ? b?(n), (9) ??Sn where Sn is
3155	23118	recover h from the linear system Lnh = 0. However, since the linear system in (10) depends explicitly on n, we cannot estimate h directly without knowing n in advance. The following theorem (see ) shows that the estimation of the number of discrete states n is very much related to the conditions under which the solution for h from (10) is unique (recall that hMn = 1). Theorem 1 (Number of
3155	23118	the parameters that better reconstruct h. Alternatively, one can obtain {zi} n i=1 as points in the data set that minimize a certain distance to the hyperplanes. We refer the interested reader to  for further details. 3.3 Filtering of the hybrid state Given the number of discrete states n and the model parameters {bi} n i=1 , we now show how to reconstruct the hybrid state trajectory {xt,
3155	3151	first appeared in the seventies; a review of the state of the art as of 1982 can be found in . After a decadelong hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism
3155	3152	first appeared in the seventies; a review of the state of the art as of 1982 can be found in . After a decadelong hiatus, the problem has recently been enjoying considerable interest (see  and references therein). Related work has also appeared in the machine learning community (see  and references therein). When the model parameters and the switching mechanism
23120	23121	trees. Walker et al.  introduce an abstract machine to define the operational semantics of ML extended with aspects; Tucker and Krishnamurthi  rely on abstract machines as well. Andrews  models AOP by means of algebraic processes. In the tradition of process calculi, Jagadeesan et al.  propose a calculus of AOP where aspects are primitive abstractions. Such models are a
23120	15975	is applied to the insert and ?I is executed. To end the discussion of the weaver, note that stateful aspects are implementable efficiently using static analysis and transformation techniques (see, ). Interaction analysis. Two distinct aspects are said to interact when they match the same join point. Two aspects are independent if their crosscuts never match the same join point simultaneously.
23120	15975	of execution events as a basic mechanism for the definition of aspects has been proposed independently by several researchers, in particular Filman , Walker et al. , as well as the authors . As to the formalization of aspects and weavers, different approaches have been advocated. Wand et al. propose a denotational semantics for a subset of AspectJ . Lämmel formalizes method-call
23120	23122	application of a crosscut to a join point C j amounts to solving the formula obtained by substituting j for • in C (C j = C). There exists an algorithm to find equations solving such formulas . It is used by the interaction analysis of Section 3.2. We write def (C) for the set of variables (? Vars) occurring as lhs of equations in C. These variables are defined by the equations of C and
23120	23122	a form appropriate for further rewriting. = C ? first.set(1); (A ? 1 ? A ? 2) ? false ? first.set(1); (A ? 1 ? A ? 2) ? false ? skip; (A ? 1 ? A ? 2)  The law  uses the algorithm of  to check if a crosscut has no solution in which case the crosscut is replaced by false. In the example, if the join point does not match C it is not a call; it cannot match call f or call g either.
23120	23123	between conflicting aspects and have to implement conflict resolution code without support for this task. We address these problems based on the generic and formal framework introduced in . This framework is very general: it does not depend on a specific programming language and is expressive enough to allow the definition of stateful aspects. Stateful aspects are defined in terms of
23120	23123	how it facilitates reuse of aspects. Finally, Section 6 presents related work and concludes. 2. PRELIMINARIES In this section, we briefly present the generic framework (introduced by the authors in ) on which the current work is based. Then, we give an overview of the extensions defined in the current article for the analysis of interactions, composition and reuse of stateful aspects. 2.1
23120	23123	weaving is non-deterministic. 2.2 Overview of contributions One main objective of the present work is to generalize the techniques for interaction analysis and conflict resolution introduced in . Furthermore, we are interested in extending the framework by means for the more expressive definition of aspects. Concretely, we present three contributions in the following: introduction of
23120	23123	after each program modification. (A more precise variant of the interaction analysis which takes into account the base program and new behavior introduced by other inserts has been presented in . We expect that the extension by variables carries over smoothly to that case as well).s µa.A = A  (A1 ? A2) ? A3 = A1 ? (A2 ? A3)  (C1 ? I1; A1) ? (C2 ? I2; A2) =
23120	23123	lies between strong independence analysis (which shows that aspects never interact regardless of the base program to which they are applied) and the weak independence analysis introduced in  (which shows that aspects do not interact for a specific base program). By taking into account requirements, the contextual interaction analysis proofs that aspects do not interact for a set of
23120	23124	of execution events as a basic mechanism for the definition of aspects has been proposed independently by several researchers, in particular Filman , Walker et al. , as well as the authors . As to the formalization of aspects and weavers, different approaches have been advocated. Wand et al. propose a denotational semantics for a subset of AspectJ . Lämmel formalizes method-call
23120	23125	of execution events as a basic mechanism for the definition of aspects has been proposed independently by several researchers, in particular Filman , Walker et al. , as well as the authors . As to the formalization of aspects and weavers, different approaches have been advocated. Wand et al. propose a denotational semantics for a subset of AspectJ . Lämmel formalizes method-call
23120	23125	to formally study properties such as aspect interactions. However, despite its importance, very few work has previously been done on aspect interaction and conflict resolution. Douence et al.  present an approach for manual proofs of independence. Sereni et al.  generalize AspectJ’s cflow using regular expressions on the call stack. They focus on optimization but they point out that
23120	23126	of software engineering. For instance, Sihman et al.  use model checking to detect superimposition interactions and a large body of work is devoted to feature interactions (e.g., Felty et al. ). Concerning reuse, aspects are often advocated as reusable pieces of software. It is true that AOP can sometimes avoid duplicating code. However, in order to make them fully reusable, module and
23120	23127	6. RELATED WORK AND CONCLUSION Use of the history of execution events as a basic mechanism for the definition of aspects has been proposed independently by several researchers, in particular Filman , Walker et al. , as well as the authors . As to the formalization of aspects and weavers, different approaches have been advocated. Wand et al. propose a denotational semantics for a
23120	23128	with aspects; Tucker and Krishnamurthi  rely on abstract machines as well. Andrews  models AOP by means of algebraic processes. In the tradition of process calculi, Jagadeesan et al.  propose a calculus of AOP where aspects are primitive abstractions. Such models are a prerequisite to formally study properties such as aspect interactions. However, despite its importance, very
23120	23130	It is true that AOP can sometimes avoid duplicating code. However, in order to make them fully reusable, module and software composition techniques should be adapted to aspects. Kienzle et al.  represent properties of aspects (namely, whether they provide, require andslet A = (C1 ? I1; A1) ? . . . ? (Cn ? In; An) and S = (J1 ? S1)  . . .  (Jm ? Sm) and and CJ CS = = ˆJ where each
23120	23132	despite its importance, very few work has previously been done on aspect interaction and conflict resolution. Douence et al.  present an approach for manual proofs of independence. Sereni et al.  generalize AspectJ’s cflow using regular expressions on the call stack. They focus on optimization but they point out that their technique could also be used to detect interactions. Finally,
23120	23133	they point out that their technique could also be used to detect interactions. Finally, interaction issues also arise in closely related fields of software engineering. For instance, Sihman et al.  use model checking to detect superimposition interactions and a large body of work is devoted to feature interactions (e.g., Felty et al. ). Concerning reuse, aspects are often advocated as
23120	23133	w.r.t. a base program abstraction remove services) with a graph. An aspect can be reused in a configuration when it can be inserted into the corresponding dependencies graph. Sihman et al.  modularize proof obligations for superimpositions and perform checks before a superimposition is applied (i.e., when an aspect is woven). In this article, we have extended our generic formal
23120	23134	are specified by predicates on abstract syntax trees. Walker et al.  introduce an abstract machine to define the operational semantics of ML extended with aspects; Tucker and Krishnamurthi  rely on abstract machines as well. Andrews  models AOP by means of algebraic processes. In the tradition of process calculi, Jagadeesan et al.  propose a calculus of AOP where aspects are
23120	23136	and weavers with execution monitors. De Volder et al.  propose a meta-programming framework based on Prolog where crosscuts are specified by predicates on abstract syntax trees. Walker et al.  introduce an abstract machine to define the operational semantics of ML extended with aspects; Tucker and Krishnamurthi  rely on abstract machines as well. Andrews  models AOP by means of
23120	23137	CONCLUSION Use of the history of execution events as a basic mechanism for the definition of aspects has been proposed independently by several researchers, in particular Filman , Walker et al. , as well as the authors . As to the formalization of aspects and weavers, different approaches have been advocated. Wand et al. propose a denotational semantics for a subset of AspectJ
23120	23138	as well as the authors . As to the formalization of aspects and weavers, different approaches have been advocated. Wand et al. propose a denotational semantics for a subset of AspectJ . Lämmel formalizes method-call interception using a big-step semantics . Douence et al.  model crosscut definitions with execution trace parsers and weavers with execution monitors. De
23139	23140	defined in the Message Understanding Conferences (MUCs) . In recent years, the extraction of protein-protein interactions in biomedical articles and abstracts are reported in many works such as . In this work, the relations to be extracted are binary ones, and the frequently occurring verbs as well as patterns are used in order to construct the template elements of the relations which will
23139	23140	verbs can be semantically categorized. For instance, the verbs such as “activate”, “associate”, and “interact” were used as the key verbs in extracting the protein-protein interactions in . Theoretically, even given a complete lexicon which contains all the lexical entries, the categorization of the verbs in a corpus could still not be solved perfectly, if additional contextual cues
23139	23142	defined in the Message Understanding Conferences (MUCs) . In recent years, the extraction of protein-protein interactions in biomedical articles and abstracts are reported in many works such as . In this work, the relations to be extracted are binary ones, and the frequently occurring verbs as well as patterns are used in order to construct the template elements of the relations which will
23139	23142	verbs can be semantically categorized. For instance, the verbs such as “activate”, “associate”, and “interact” were used as the key verbs in extracting the protein-protein interactions in . Theoretically, even given a complete lexicon which contains all the lexical entries, the categorization of the verbs in a corpus could still not be solved perfectly, if additional contextual cues
23139	23143	defined in the Message Understanding Conferences (MUCs) . In recent years, the extraction of protein-protein interactions in biomedical articles and abstracts are reported in many works such as . In this work, the relations to be extracted are binary ones, and the frequently occurring verbs as well as patterns are used in order to construct the template elements of the relations which will
23139	23144	defined in the Message Understanding Conferences (MUCs) . In recent years, the extraction of protein-protein interactions in biomedical articles and abstracts are reported in many works such as . In this work, the relations to be extracted are binary ones, and the frequently occurring verbs as well as patterns are used in order to construct the template elements of the relations which will
23139	23145	defined in the Message Understanding Conferences (MUCs) . In recent years, the extraction of protein-protein interactions in biomedical articles and abstracts are reported in many works such as . In this work, the relations to be extracted are binary ones, and the frequently occurring verbs as well as patterns are used in order to construct the template elements of the relations which will
23139	23146	is 3 Actually, the domain-specific verbs should not include the general verbs independent of the domain in the scientific papers, such as “analyze”, “indicate”, “observe”, and so on. Spasi? et al.  also discussed this problem. 4 GENIA project, available at http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ 5 In some relation extraction works, inhibitory relation is treated as a kind of
23147	23148	Research continues on how to estimate arrival rate functions either from period counts 4sDate of draft: 29 June 2002 (for example, Massey et al., 1996) or transactional data (for example, Arkin and Leemis, 2000). The relevance of research on estimating continuous arrival rate functions depends, in part, on whether the use of such functions leads to different results than using a piecewise constant arrival
23147	23164	used to evaluate the service level. Many of the results for M()/ t M / ? systems hold for M()/ t G/ ? systems (Eick et al., 1993a), and some results are available for Gt ()/ G/ ? systems as well (Jennings et al., 1996). These results could in principle be used in combination with approximations for stationary M/G/s or G/G/s systems to develop MOL approximations for Mt ()/ G/ st () or Gt ()/ G/ st () systems. As
23147	23169	storage decreases, the first reason should become less compelling. Research continues on how to estimate arrival rate functions either from period counts 4sDate of draft: 29 June 2002 (for example, Massey et al., 1996) or transactional data (for example, Arkin and Leemis, 2000). The relevance of research on estimating continuous arrival rate functions depends, in part, on whether the use of such functions leads
8910334	23191	several platforms and languages to provide flexibility and usability&quot;. Today, many research laboratories are working on the development of measurement tools for software functional size measurement . The design of such tools is necessarily based on: (1) the identification of all the concepts handled in a method’s measurement procedure, as well as the relationships between these concepts
8910334	23191	to help measurers in their task makes the task even more difficult. Today, many research laboratories are working on the development of measurement tools for software functional size measurement . To design such tools the following aspects should be taken into account: (1) the identification of all the concepts handled in a method’s measurement procedure, as well as the relationships
8910334	23193	the following address: http://www.labunix.uqam.ca/~bevo/FPA/ontologies.htm V. RESEARCH PERSPECTIVES An XML, then XMI translation of the presented ontologies is to be produced, using adequate tools . It could be very useful for measurement results analysis & documentation tools, and some aspects of measurement results validation. XML being appropriate for documents exchange, historical
8910334	19584	software being measured. The “static model” of a measurement method should correspond to the domain ontology (« explicit formal specifications of the terms in the domain and relations among them » ) related to the method’s measurement 114 procedure. Very often (if not almost always) in measurement handbooks, the static models appear in the form of a lexicon (list of concepts with their
8922211	23206	that allow them to focus on interesting local features by absorbing distortions. Because of the local view of these models, we introduced the concept of “Local View Models” (MVL) for such models . The drawback of these models is the observation independence hypothesis that limits their global representativeness. This point was reflected in some works in the fact that “HMM correctly
8922211	23206	forms, models such as neural networks are more efficient. That can be explained by their global vision allowing correlation on the whole form; we classified them as “Global View Models” (MVG) . Their drawbacks are an huge sensibility to important distortions and a fixed input size. Because of these limits they are usually used to estimate local observation probabilities ,
8922211	23206	of writing. This model was successfully applied on digit image classification thus it seems adapted to analyze normalized images. We preferred MLP to SVM (Support Vector Machine) previously used  because of their output that approximate a posterior probability : SVMsoutput is difficult to interpret. Section 2 describes the training and testing schemes of this approach. Section 3 gives some
8922211	23206	NSHP-HMM states. At each NSHP-HMM state is associated a column in the normalized image. This column is calculated as the mean value of all the columns gathered in the corresponding NSHP-HMM state . The figure 2 illustrate this method and its effect on a sample of the word “et”. The figure 3 compares this normalization and a classical linear normalization on several samples of the words “et”
8922211	21665	Models” (MVG) . Their drawbacks are an huge sensibility to important distortions and a fixed input size. Because of these limits they are usually used to estimate local observation probabilities , reducing their global efficiency. This analysis of MVL and MVG shows that they are complementary. The aim of this work is to combine efficiently the power of these two kind of models. We propose
8922211	23211	size. Then it can be analyzed by a MVG to estimate global correlation between these features. To validate our approach, we choose the NSHP-HMM (Non-symmetric Half-Plane Hidden Markov Model)  as MVL and a Multi-Layers Perceptron (MLP)asMVG. The NSHP-HMM is an HMM model where the observation probability in the states is estimated by a random field. This ? Markov model is acting directly
8922211	2249	Models” (MVG) . Their drawbacks are an huge sensibility to important distortions and a fixed input size. Because of these limits they are usually used to estimate local observation probabilities , reducing their global efficiency. This analysis of MVL and MVG shows that they are complementary. The aim of this work is to combine efficiently the power of these two kind of models. We propose
9864747	23218	practice, channel order estimation becomes a delicate task , and therefore blind algorithms robust to the effects of channel order overestimation are of clear interest. Recently, Gazzah et al.  have proposed an SOS-based method for channel identification which is able to accurately estimate the channel impulse response when the assumed order is greater than the exact channel order. Their
9864747	23218	? ? ? £ ????? § ? (5) ? ? ? ?s? £ ??? ??? ??????? ? Then from (3) one has ? ?s? £ ? ??? § ? ? ? £ ? ? § ??? (6) ? ? ?s? £ ? ? ? ?s? £ ? § ? (7) ? 4. REVIEW OF GAZZAH’S APPROACH Gazzah’s approach  can be briefly described as follows. Consider ? ? vectors ? ? , ? satisfying ?s? £ § ? ? ? ? ? ? ?s? £ ? ? ? ? ? (8) ? ? Under our assumptions, ? has full column rank, ????? which equals .
9864747	23218	which need not be nonzero. If the zero solution could be avoided, then it is seen that ? ? , ? ? would constitute ZF equalizers with associated delays ? (minimal) and ? ? ? (maximal). To do so,  proposes to maximize the SNR at the equalizer output. Since the signal and noise terms are respectively ? § ??? ? and ? ? ? , this output SNR is seen to be given by ? § ????? ? ? § ? Thus the
9864747	23218	since the dimension of the subspace from which the preequalizer is extracted ? is ? with . ? ? ? £ ?s??? 8. CONCLUSIONS ? , which grows linearly We have shown how the algorithm of Gazzah et al.  for blind identification of FIR SIMO channels can be appropiately modified in order to account for source correlation (assuming that the source statistics are known to the receiver). More
9864747	23220	situations. However, this knowledge is often available, and its use should intuitively improve performance. Among the algorithms that exploit source correlation information one finds those in . One common drawback of these methods is that they require precise knowledge of the channel order. In practice, channel order estimation becomes a delicate task , and therefore blind algorithms
9864747	23220	assump? £ ? § ? (3) ? Assumption 2. Thesof the source process is positive definite. ? ? £ ? ? ? ? ? ? ????????? ? § ? ????? ? ? ? ?? autocorrelation matrix ????????? (4) ¤ Following the approach in , we introduce the Cholesky factorization ? with ? lower triangular with positive diagonal elements (note thats£ ??? due to Assumption 2) which is known to the receiver. ? With this, we can
9864747	23220	a ? phase rotation ambiguity, inherent to the blind nature of the problem) by noting ? that ? ?s? £ ? ? ? ??? ?????? ? ? . §? 5. THE COLORED SOURCE CASE In the general case, one has ??? as seen in , these matrices present a rich structure which can be exploited. In particular, they are related by ? ? ? and ? ?s? ? ? ? © . However, £ where ? ????? ? ? ¦ is the coefficient vector of the ? -
9864747	23221	therein. We consider communication systems in which the channel input statistics are colored but known to the receiver. Correlated sources may arise for instance as a result of channel encoding , or from the use of nonlinear modulation formats such as Supported by the Ramón y Cajal program of the Spanish Ministry of Science and Technology. continuous-phase modulation (CPM) . In either
9864747	23223	(SIMO) channels can be equalized by a bank of FIR filters, whose coefficients can be computed from the channel output SOS. Following , many SOS-based blind methods have been developed; see  and the references therein. We consider communication systems in which the channel input statistics are colored but known to the receiver. Correlated sources may arise for instance as a result of
9864747	12653	Second-order statistics (SOS) based methods are particularly attractive, as SOS can be more accurately estimated from finite data records than their higher-order counterparts. The seminal work in  showed that under certain conditions, finite impulse response (FIR) single-input multiple-output (SIMO) channels can be equalized by a bank of FIR filters, whose coefficients can be computed from
9864747	12653	? ? ????? ¡?¡?¡ ??????? ? ??? ? ¦ ? ??? ? and ? ??? ????? ? is an sists ??????? of ? an vector whose output ? is just generalized Sylvester matrix constructed £ from the channel impulse response . A linear equalizer con????? § ? ????? ? ?? ????? § ? ? § ? § ? ? ? ? ? ??? ? ? ? § Thus the row vector contains the taps of the combined channel-equalizer impulse response. The vector is said to
9864747	12653	which is required). Then, if the transfer functions of the For this, the equalizer length ? must satisfy ??????? available subchannels do not present any common root, a cele? brated result from  states that ? will have full column rank. In that case, it is readily seen that any combined channel-equalizer response ? can be attained by suitably choosing the equalizer vector ? . In
9864747	12653	the source symbols respectively. In the sequel we will assume ? that the noise is zero, as the noise component can be subtracted from the output ?s£ autocorrelation matrices using a standard device : ? note that ? £ ??? ? ? Since it is assumed that ? is strictly tall, the smallest eigenvalue ?s£ of is seen to ? be and therefore it can be estimated. Then we can construct the denoised output
23224	23225	being distributed according to a standard normal distribution. Alternatively the standard factor analysis can be extended by modelling the factors with GMMs as in independent factor analysis (IFA) (Attias, 1999). In IFA the individual factors are modelled by independent 1-dimensional GMMs. This paper introduces an extension to the standard factor analysis which is applicable to HMMs. The model is called
23224	23225	model can be achieved by choosing the state space dimensionality according to k < (p ? 1)/2. Factor analysis has been extended to employ Gaussian mixture distributions for the factors in IFA (Attias, 1999) and the observation noise in SFA (Gopinath et al., 1998). As in the standard factor analysis above, there is a degeneracy present in these systems. The covariance matrix of one state space
23224	23225	the state distribution with zero mean and identity covariance. • By setting the number of observation space distribution components to one, M (o) = 1, FAHMM corresponds to a dynamic version of IFA (Attias, 1999). The only difference to the standard IFA is the independent state vector element (factor) assumption which would require a multiple stream (factorial) HMM (Ghahramani and Jordan, 1997) with
23224	23228	?j, has to be optimised row by row as in SFA (Gopinath et al., 1998). The scheme adopted in this paper follows closely the maximum likelihood linear regression (MLLR) transform matrix optimisation (Gales, 1998). The lth row vector ?jl of the new observation matrix can be written as ?jl = k ? jlG?1 jl where the k by k matrices Gjl and the k dimensional column vectors kjl are defined as follows M Gjl = (o)
23224	23229	in principal component analysis. Unfortunately, it is hard to find a single transform that decorrelates speech feature vectors for all states in an HMM system. Semi-tied covariance matrices (STCs) (Gales, 1999) can be viewed as a halfway solution. A class of states with diagonal covariance matrices can be transformed into full covariance matrices via a class specific linear transform. Systems employing
23224	23229	setting the state space dimensionality equal to the observation space dimensionality, k = p, FAHMM reduces to a semi-tied covariance matrix HMM. The only difference to the original STC model in (Gales, 1999) is that the mean vectors are also transformed in FAHMM. • By setting the observation noise to zero, vt = 0, and setting the state space dimensionality greater than the observation space
23224	23229	an arbitrary number of models at various levels of the model. For example, the observation matrix can be shared globally or between classes of discrete states as in semi-tied covariance HMMs (Gales, 1999). A global observation noise distribution could represent a stationary noise environment corrupting all the speech data. Implementing arbitrary tying schemes is closely related to those used with
23224	23229	number of mixture components was increased by the mixture splitting procedure. Nine full iterations of embedded training were used with 20 within iterations and 20 row by row transform iterations (Gales, 1999). The results are presented on the second row in Table 3, marked STC. The best semitied performance was 3.83% obtained with 5 mixture components. As usual, the performance when using STC is better
23224	23231	distributions are described by the mixture weights {c (x) jn , c (o) jm}, mean vectors {µ (x) jn , µ (o) jm} and diagonal covariance matrices {? (x) jn , ? (o) jm}. Dynamic Bayesian networks (DBN) (Ghahramani, 1998) are often presented in conjunction with the generative models to illustrate the conditional independence assumptions made in a statistical model. A DBN describing a FAHMM is shown in Fig. 1. The
23224	23232	a dynamic version of IFA (Attias, 1999). The only difference to the standard IFA is the independent state vector element (factor) assumption which would require a multiple stream (factorial) HMM (Ghahramani and Jordan, 1997) with 1-dimensional streams in the state space. Effectively multiple streams can model a larger number of distributions but the independence assumption is relaxed in this FAHMM assuming
23224	23233	estimation is similar and both can be viewed as feature space transform schemes. Alternatives to systems based on LDA-like projections are schemes based on factor analysis (Saul and Rahim, 1999; Gopinath, Ramabhadran, and Dharanipragada, 1998). These model the covariance matrix via a linear probabilistic process applied to a simpler lower dimensional representation called factors. Where the LDA can be viewed as a projection scheme the
23224	23233	shared among several observation noise components as in shared factor analysis (SFA). This system is closely related to the “factor analysis invariant to linear transformations of data” (FACILT) (Gopinath et al., 1998) without the global linear transformation. SFA also assumes the factors being distributed according to a standard normal distribution. Alternatively the standard factor analysis can be extended by
23224	23233	space dimensionality according to k < (p ? 1)/2. Factor analysis has been extended to employ Gaussian mixture distributions for the factors in IFA (Attias, 1999) and the observation noise in SFA (Gopinath et al., 1998). As in the standard factor analysis above, there is a degeneracy present in these systems. The covariance matrix of one state space component 5scan be subsumed into the loading matrix and one
23224	23233	matrices and the mean vectors are tied on the same level. The parameter tying is further discussed in Section 3.2. The new observation matrix, ?j, has to be optimised row by row as in SFA (Gopinath et al., 1998). The scheme adopted in this paper follows closely the maximum likelihood linear regression (MLLR) transform matrix optimisation (Gales, 1998). The lth row vector ?jl of the new observation matrix
23224	23233	HMM, and the observation matrix is made redundant because no state vectors will be generated. • By setting the number of state space mixture components to one, M (x) = 1, FAHMM corresponds to SFA (Gopinath et al., 1998). Even though the state space distribution parameters are modelled explicitly, there are effectively 12sTable 1 Standard systems related to FAHMMs. FAHMM can be converted to the systems on the left
23224	23234	hour subset of the Switchboard (Hub5) acoustic training data set was used. 862 sides of the Switchboard1 and 92 sides of the Call Home English were used. The set is described as “h5train00sub” in (Hain, Woodland, Evermann, and Povey, 2000). As with Minitrain, the baseline was a decision-tree clustered tied state triphone HMM system with VTLN, cross word models and GMMs. The 1998 Switchboard evaluation data set was used for testing.
23224	23235	this cross evaluation were less than 2.50%. The performance of FAHMMs using lower dimensional state space is reported in the experiments below. 20s4.2 Minitrain The Minitrain 1998 Hub5 HTK system (Hain, Woodland, Niesler, and Whittaker, 1999) was used as a larger speech recognition task. The baseline was a decision-tree clustered tied state triphone HMM system. Vocal tract length normalisation (VTLN) was used to make the system gender
23224	23235	and McDonough, 1997) containing 398 conversation sides of Switchboard-1 corpus was used as the acoustic training data. The test data set was the subset of the 1997 Hub5 evaluation set used in (Hain et al., 1999). The best performance, 51.0%, was achieved with 12 components which corresponds to ? = 936 parameters per state. The mixture splitting was not continued further since the performance started
23224	23238	addition, FAHMMs allow a variety of configurations and subspaces to be explored. The model complexity has become a standard problem in speech recognition and machine learning over the recent years (Liu, Gales, and Woodland, 2003). For example, Bayesian information criterion has been applied separately to speaker clustering and selecting the number of Gaussian mixture components in (Chen and Gopalakrishnan, 1998). Current
23224	23239	1991). However, implementing decision-tree clustering for FAHMMs is not as straightforward as for HMMs. The clustering based on single mixture component HMM statistics is not optimal for HMMs (Nock, Gales, and Young, 1997). Since the FAHMMs can be viewed as full covariance matrix HMMs, decision-tree clustered single mixture component HMM models may be considered as a sufficiently good starting point for FAHMM
23224	23240	and setting the state space dimensionality greater than the observation space dimensionality, k > p, FAHMM becomes a covariance version of extended maximum likelihood linear transformation (EMLLT) (Olsen and Gopinath, 2002) scheme. FAHMM is based on a generative model which requires every state space covariance matrix being a valid covariance matrix; i.e. positive definite. EMLLT, on the other hand, directly models
23224	23241	an extension to the standard factor analysis which is applicable to HMMs. The model is called factor analysed HMM (FAHMM). FAHMMs belong to a broad class of generalised linear Gaussian models (Rosti and Gales, 2001) which extends the set of standard linear Gaussian models (Roweis and Ghahramani, 1999). Generalised linear Gaussian models are state space models with linear state evolution and observation
23224	23241	? ? jm ? ?j ˆµ (o) ? ? jm ot ˆx ? ? ? ? jmn (t) ot ? ? + ?j ˆµ (o) ? jm ? ? ˆRjmn(t) ? ˆxjmn(t) ? ? ? ? ?j ˆµ (o) ? ? ? jm (15) Detailed derivation of the parameter optimisation can be found in (Rosti and Gales, 2001). A direct implementation of the training algorithm is inefficient due to the heavy matrix computations required to obtain the state vector statistics. An efficient two level implementation of the
23224	23242	model is called factor analysed HMM (FAHMM). FAHMMs belong to a broad class of generalised linear Gaussian models (Rosti and Gales, 2001) which extends the set of standard linear Gaussian models (Roweis and Ghahramani, 1999). Generalised linear Gaussian models are state space models with linear state evolution and observation processes, and Gaussian mixture distributed noise processes. The underlying HMM generates
23224	23243	and HLDA. The parameter estimation is similar and both can be viewed as feature space transform schemes. Alternatives to systems based on LDA-like projections are schemes based on factor analysis (Saul and Rahim, 1999; Gopinath, Ramabhadran, and Dharanipragada, 1998). These model the covariance matrix via a linear probabilistic process applied to a simpler lower dimensional representation called factors. Where
23224	23243	can be viewed as state vectors and the factor analysis as a generative observation process. Each component of a standard HMM system can be replaced with a factor analysed 2scovariance model (Saul and Rahim, 1999). This dramatically increases the number of model parameters due to an individual loading matrix attached to each component. The loading matrix (later referred to as the observation matrix) and the
23224	23243	FAHMM Parameters A maximum likelihood (ML) criterion is used to optimise the FAHMM parameters. It is also possible to find a discriminative training scheme such as minimum classification error (Saul and Rahim, 1999) but for this initial work only ML training is considered. In common with standard HMM training the expectation maximisation (EM) algorithm is used. The auxiliary function for FAHMMs can be written
8832677	24800	For Mahal DaCosta Facultad de Medicina Universidad de Concepción Av. Roosevelt 1550 Concepción, CHILE example, in the area of emergency rooms simulation it is possible to highlight Garcia et al. (1995). They present a simulation model focused on reduction of waiting time in the emergency room of Mercy Hospital in Miami. A similar application is presented in Baesler et al. (1998) where important
2458640	23251	(it can even have negative effects) and vice versa. 4 The Recognition System The recognition system used in this paper was originally developed to work on single words (for a full description, see ), but no modifications were necessary to use it for handwritten texts . The system is based on a sliding window approach: after a normalization step, a fixed width window shifts column by column
23252	382	cameras). The range finders are used to perform scans of the fix obstacles around the robot and the localization is calculates matching those scans with a metric map of the environment . The vision sensors are used to recognize characteristic landmarks subsequently matched within a map  or to find the reference image most similar to the image currently grabbed by the robot
23252	382	omnidirectional vision system as sensor to emulate and enhance the behaviour of range-finder devices. In this work MCL (Monte-Carlo localization) was implemented based on the approach proposed in . We adapted that approach to take into account the new information given by this sensor. Experiments are made in a typical RoboCup environment (a 8x4 m soccer field), characterized by the lack of
23252	382	reading; (3) A re-sampling step is performed: high probability particles are replicated, low probability ones are discarded. The process repeats from the beginning. For more details please refer to . 3.1 Motion model The motion model p(lt|lt?1, at?1) is a probabilistic representation of the robot kinematics, which describes a posterior density over possible successive robot poses. We
23252	382	both metric maps in Fig. 2. The likelihood p(ot|lt) can be calculated as p(ot|lt) = p(ot|o(lt)). In other words, the probability p(ot|o(lt)) models the noise in the scan by the expected scan . When using a sonar or a laser, like in , the expected scan is computed from a metric map of the environment. The expected scan is obtained simulatings(a) (b) Fig. 2. The metric maps used
23252	419	density with the mixture of three probability density of Eq. 2. The numerical values of the parameters in Eq. 2 are calculated with a modified EM algorithm iteratively run on the 2000 images . The resulting mixture, for the green-white transition, is plotted in Fig. 4(b). The three terms in Eq. 2 are respectively: an Erlang probability density, a Gaussian probability density and a
23252	23254	assures a robust and accurate localisation. In Fig. 7(b) is shown the error for a kidnapped robot episode using 1000 samples and different rate of samples uniformly distributed in the environment . With a higher rate of samples scattered in the environment the re-localization is faster (there are more possibility that the the samples are distributed close to the robot position), but the
23252	23255	the rays of a scan is very frequent in the densely crowded environment of the Middle-Size RoboCup field. This rays discrimination allow us to avoid to use other techniques, e.g. distance filters , that can affect negatively the computational performance of the system.s(a) (b) Fig. 4. In (a) the distribution of the measured distances for an expected known distance. There is a peak for the
23252	23255	only around the actual position of the robot. In order to improve the performance of the system, the distances in the environment are discretized in a grid of 5x5 cm cells, in a way similar to . The expected distances for all poses and the probabilities p(oi|g(l, i)) for all g(l, i) can be pre-computed and stored in six (two for each chromatic transition) lookup tables. In this way the
23252	23257	are used to recognize characteristic landmarks subsequently matched within a map  or to find the reference image most similar to the image currently grabbed by the robot (without a map) . In our approach we use an omnidirectional vision system as sensor to emulate and enhance the behaviour of range-finder devices. In this work MCL (Monte-Carlo localization) was implemented based on
23252	23258	are used to recognize characteristic landmarks subsequently matched within a map  or to find the reference image most similar to the image currently grabbed by the robot (without a map) . In our approach we use an omnidirectional vision system as sensor to emulate and enhance the behaviour of range-finder devices. In this work MCL (Monte-Carlo localization) was implemented based on
23252	23259	the localization is calculates matching those scans with a metric map of the environment . The vision sensors are used to recognize characteristic landmarks subsequently matched within a map  or to find the reference image most similar to the image currently grabbed by the robot (without a map) . In our approach we use an omnidirectional vision system as sensor to emulate and
23252	23259	yellow, the robots are black, the robots’ marker are cyan and magentastransition is more robust with respect to colour calibration than to identify the colour of every single pixel, as reported in . We measure the distance of the nearest chromatic transitions of interest along 60 rays as shown in Fig.1. This enable our “range finder” to scan a 360 degree field of view. Our omnidirectional
23252	382	cameras). The range finders are used to perform scans of the fix obstacles around the robot and the localization is calculates matching those scans with a metric map of the environment . The vision sensors are used to recognize characteristic landmarks subsequently matched within a map  or to find the reference image most similar to the image currently grabbed by the robot
23252	382	omnidirectional vision system as sensor to emulate and enhance the behaviour of range-finder devices. In this work MCL (Monte-Carlo localization) was implemented based on the approach proposed in . We adapted that approach to take into account the new information given by this sensor. Experiments are made in a typical RoboCup environment (a 8x4 m soccer field), characterized by the lack of
23252	382	reading; (3) A re-sampling step is performed: high probability particles are replicated, low probability ones are discarded. The process repeats from the beginning. For more details please refer to . 3.1 Motion model The motion model p(lt|lt?1, at?1) is a probabilistic representation of the robot kinematics, which describes a posterior density over possible successive robot poses. We
23252	382	both metric maps in Fig. 2. The likelihood p(ot|lt) can be calculated as p(ot|lt) = p(ot|o(lt)). In other words, the probability p(ot|o(lt)) models the noise in the scan by the expected scan . When using a sonar or a laser, like in , the expected scan is computed from a metric map of the environment. The expected scan is obtained simulatings(a) (b) Fig. 2. The metric maps used
23252	23260	the reflections of the sonar or laser beams against the walls and the fix obstacles. In the RoboCup Middle-Size field, a similar approach was used, very effectively, by the CS Freiburgh Team , until RoboCup 2001. However, when in 2002 the walls surrounding the field were removed, the reliability of this approach was impaired by the lack of fix features detectable by a range-finder
23252	23261	are used to recognize characteristic landmarks subsequently matched within a map  or to find the reference image most similar to the image currently grabbed by the robot (without a map) . In our approach we use an omnidirectional vision system as sensor to emulate and enhance the behaviour of range-finder devices. In this work MCL (Monte-Carlo localization) was implemented based on
23262	23264	and manufacturing techniques places the computational aspects of e-textiles at the forefront of the problem domain. Early prototypes such as the Wearable Motherboard , shape sensing jacket , and piezoelectric glove  provided a glimpse of e-textile capabilities in wearable applications. Design rules derived and learned from the construction of a large scale acoustic beamforming
23262	23264	models that were trained offline. The hidden Markov models were trained over a 7svariety of observation windows and number of classification states to find an optimal model. Farringdon et. al.  developed a badge and jacket that could determine the user’s context. Additionally, the idea of sensing the shape of the body was introduced. This was achieved by using metallic woven strips whose
23262	23264	usually determines what type of information is fundamental, but the task of differentiation is 12stypically left to the algorithms. Popular sensor choices in past examples of context awareness    have been accelerometers, gyroscopes, compasses, light sensors, and other devices capable of sensing spatial or environmental conditions. The number of sensors used is also a variable in
23262	23265	places the computational aspects of e-textiles at the forefront of the problem domain. Early prototypes such as the Wearable Motherboard , shape sensing jacket , and piezoelectric glove  provided a glimpse of e-textile capabilities in wearable applications. Design rules derived and learned from the construction of a large scale acoustic beamforming array  showed that building
23262	23265	design of acoustic e-textile applications and that e-textiles provide a feasible platform for acoustic applications. The use of piezoelectric films as shape sensors in e-textiles was presented in . The piezoelectric films detected the movement and tapping of the fingers in a virtual keyboard application. This work demonstrated the promise of piezoelectric films as sensing components in
23262	23265	and shape of the body. All of the physical measurements such as velocity and position can be derived from acceleration, thus accelerometers were selected as a sensor type. Prior work performed in  demonstrated the feasibility of using piezoelectric films to detect shape. Consequently, piezoelectric films were chosen for sensing the shape of the body. While other sensors choices may be
23262	23266	and piezoelectric glove  provided a glimpse of e-textile capabilities in wearable applications. Design rules derived and learned from the construction of a large scale acoustic beamforming array  showed that building e-textile systems is both feasible and beneficial. The conventional design methodology for these systems included many prototype iterations due to the large number of design
23262	23266	power sources at finer granularities increased fault tolerance. The resulting simulation could also be used in future e-textile simulations to induce faults in the fabric. The STRETCH project   utilized large-scale (30 foot) fabrics to deploy acoustic sensors for beamforming applications such as vehicle detection. The processing nodes were 4sFigure 2.1: Field test of a 30 foot
23262	23266	arrays to calculate location. This project was also one of the first to begin integrating computational devices into the fabric. A field test of the fabric is shown in Figure 2.1. Also presented in  is a set of general design precepts or rules for e-textile systems. Wearable acoustic e-textiles applications, such as source separation and speech processing were explored in . Specifically,
23262	23266	on the legs, feet, and near the center of mass. These qualities fit nicely with the gait analysis and context awareness applications. The e-textile also incorporates the precepts presented in  such as generalized fabric swatches to maintain large scale manufacturing capability, digital communication across the textile network, small-scale sensor nodes, and mechanical attachment. 3.3.1
23262	23266	networks in e-textiles, simulation is the only realistic method for thoroughly exploring the design space. While a full simulation of an e-textile system would be ideal, such as that performed in , which includes the physical fabric, fault tolerant networking, and power, it is unnecessary for investigating many of the design decisions and is beyond the scope of this thesis. A minimal
23262	23270	primary design issues and took steps towards integrating processing elements into the fabric. Modeling and simulation of power distribution/management and fabric fault/tears were investigated in . This simulation allowed the investigation of power management and its effects on fault tolerance. It was shown that including power sources at finer granularities increased fault tolerance. The
23262	23272	Also presented in  is a set of general design precepts or rules for e-textile systems. Wearable acoustic e-textiles applications, such as source separation and speech processing were explored in . Specifically, the arrangement of microphones, sampling rates, and motion effects from wearing the device were investigated. These results show the importance of simulation in the design of
23262	23279	standing up, running, jumping, and idle. Classification of user activity is useful for pervasive computing and health monitoring applications. Several prototype systems such as those in   , attempt to fuse sensor data to form high level abstractions of the user’s activity. Methods for achieving activity awareness include the use of neural networks, decision trees, Markov chains,
23262	23279	algorithm. A push button interface was provided that allowed the user to train the system to recognize a various task or activity. Recognition accuracy ranged from 42%-96%. Randell et. al.  also investigated the use of clustering neural network algorithms for determining user context. Their work examined the validity of using a single accelerometer and minimizing power by using a very
23262	23279	determines what type of information is fundamental, but the task of differentiation is 12stypically left to the algorithms. Popular sensor choices in past examples of context awareness    have been accelerometers, gyroscopes, compasses, light sensors, and other devices capable of sensing spatial or environmental conditions. The number of sensors used is also a variable in the
23262	23279	the context awareness application was generalization 51sFigure 5.5: Effects of body size on sensor output across an entire population. Many previous context awareness applications such as   were trained on a small number of people. Simulation offers the ability to train on a large number and wide variety of people. To minimize design effort the affects of the training population on
23262	23287	and sewn into the final pair of pants. Connections between the fabric and sensor nodes were made using insulation displacement connectors created using off-the-shelf components shown in Figure 3.3 . The connectors were designed with a minimal profile in mind. While these connectors are not the optimal solution, they were satisfactory for a first iteration prototype. The computational
23262	23288	multiple models of computation such as finite state machines, data flow, discrete event, and continuous domains. Additionally, Ptolemy II is open-source, free, and is the basis for the TailorMade  simula27stion environment. The developed simulation must allow integration with the interrupt-driven model of computation developed in  such that future work could encompass the simulation of
23316	23318	clickstream data of online stores; they define “micro-conversion rates” as metrics in web merchandising analysis to understand effectiveness of marketing and merchandising efforts. Heckerman et al.  take a model-based approach and use a mixture model to predict behaviour of user clusters and visualize the classification of users. Mobasher et al.  propose a hybrid approach to real-time web
23316	15397	and merchandising efforts. Heckerman et al.  take a model-based approach and use a mixture model to predict behaviour of user clusters and visualize the classification of users. Mobasher et al.  propose a hybrid approach to real-time web personalization by combining web usage mining and context- based mining, in a try to overcome the inabilities of each approach individually. * This
23316	23319	recording user interactions with a web site. We keep track of both selected hyperlinks and visited pages. The major challenges in tracking are being accurate and unobtrusive. As for accuracy, in  we proposed the utilization of an agent in the client side to detect and log user interactions in place and send the collected data to an agent server for further analysis. This approach enables us
23316	23319	any other suitable attribute. We aggregate each attribute of the path/cluster into its corresponding matrix in the CM-Model of the path/cluster. Moreover, we introduced a new similarity measure in , which suffers from overestimating the similarity between two paths. This is due to the fact that the base sub-paths of the two compared paths that are used for computing their inner product are
23316	23321	the analysis layer scalable, we need to cluster similar paths in order to reason about clusters as opposed to paths. There are a handful of clustering algorithms such as CLARANS, BIRCH, CLUDIS, and K-Means that we can use for this purpose. However, regardless of what clustering algorithm we choose, we have to model 636 both path and cluster, and we have to define a distance function to
23316	23323	the newly added path as well as those of the old paths that had already joined that cluster. This second approach has been implemented within INSITE and will be demonstrated. 2. 3 Interpretation In , we defined a methodology to measure the correlation between the structure of the web site and the user profile by borrowing the concept of channel mutual information from the field of Information
23316	23323	pages by hit count, by viewing time, by time stamps etc.) and target the user with customized information accordingly. In our implementation, we have not used the association rules as suggested in . We rather use more straightforward approach of basing our decisions on the salient features of the clusters. During a single session, a visitor can play multiple roles, each captured by the
23326	23347	the context of LS-LS learners may not be prepared to spend much time. Therefore a much simplified adaptation is used, whereby students select amongst brief descriptions of learning style components . To compensate for the lack of detail, students may indicate that two poles (e.g. thinking/feeling) are both applicable. This results in a less precise learning style descriptor, but it does ensure
23326	23347	and repetition. The range of strategy suggestions, and hence CALL suggestions, are likely to vary since even in a small sample of students (5), total suggested strategies ranged from 6 to 16 . An interesting situation has occurred, whereby an intelligent learning environment (LS-LS) will be recommending largely 'unintelligent' programs to students. LS-LS starts from a quite simple
23326	23358	Chapelle and Mizuno recognize that much CALL assumes learners are already able to regulate their learning effectively, whereas, in fact, they often do not use the most appropriate strategies . Hence the importance of allowing tutors the space to describe for students, the use of these strategies in the particular CALL contexts (see Figure 2). An advantage students who have used LS-LS
23359	24943	of real option theory. The ability to delay the second stage (development) depending on the state makes early investment in the first stage (evaluation) more attractive, as in Bar-Ilan and Strange (1998). But here the real option value due to technological uncertainty actually can advance investment in both stages relative to the case of no technological uncertainty. Bar-Ilan and Stranges27 (table
23359	24944	of real option theory. The ability to delay the second stage (development) depending on the state makes early investment in the first stage (evaluation) more attractive, as in Bar-Ilan and Strange (1998). But here the real option value due to technological uncertainty actually can advance investment in both stages relative to the case of no technological uncertainty. Bar-Ilan and Stranges27 (table
4797601	23387	per-hop latency, dominates rate. Second, the data is much larger than what can be stored in RAM. As access to non-volatile storage is expensive in terms of energy, this often leads to hierarchical  or windowing  approaches to minimize the amount of caching necessary on a receiver. Overcast uses an underlying, single route network to distribute multicast data in Internet-class networks
4797601	4875	lead to distinct issues in naming. Many sensor network applications use basic collection trees . However, as deployments grow in complexity, needing in-network storage  or novel addressing , more complex routing protocols will be needed. Like these approaches, Firecracker depends on any-toany routing within the network. The prevailing approach so far has been to define logical
4797601	6493	or windowing  approaches to minimize the amount of caching necessary on a receiver. Overcast uses an underlying, single route network to distribute multicast data in Internet-class networks . Overcast deals with constructed efficient distribution trees to selective end points, while Firecracker builds on top of an existing routing protocol. The distinct domains (Internet vs. sensor
4797601	2305	very far away, hopefully outside the coordinate space. In order to try to reach this point, the network will route the data to the edge, as far as it can go. In a routing protocol such as GPSR , for example, this would involve routing to a geographic location well outside the area of the network, while in GEM  it would involve routing to a very large depth value. Figure 3(d) shows
4797601	23388	protocol, we analyze Firecracker’s cost and benefit over a purely broadcast approach. 4. EVALUATION To evaluate Firecracker’s cost/rate trade-offs, we incorporated it into the Maté virtual machine . In the Maté programming model, a user writes high level scripts and compiles them to tiny (tens of bytes) VM bytecode programs. The user then sends the programs to a base station node, which
4797601	14015	builds on top of an existing routing protocol. The distinct domains (Internet vs. sensor net) also lead to distinct issues in naming. Many sensor network applications use basic collection trees . However, as deployments grow in complexity, needing in-network storage  or novel addressing , more complex routing protocols will be needed. Like these approaches, Firecracker depends on
4797601	23390	data to the edge, as far as it can go. In a routing protocol such as GPSR , for example, this would involve routing to a geographic location well outside the area of the network, while in GEM  it would involve routing to a very large depth value. Figure 3(d) shows results for picking three random seed points outside the bounds of the network. Instead of constraining the destination
4797601	23390	be needed. Like these approaches, Firecracker depends on any-toany routing within the network. The prevailing approach so far has been to define logical coordinate spaces over a network topology . For the purposes of Firecracker, a key requirement for these coordinate spaces is names indicating some notion of network distance. 6. CONCLUSION By incorporating a routing phase into broadcast
4797601	23391	to operations are inevitable, whether in response to changing needs or environmental events. Broadcasting code too quickly can easily overload the network, causing the broadcast storm problem . Current mote hardware can support approximately forty packets per second: each mote broadcasting once a second can become difficult for moderate densities. Suppression techniques can help in this
4797601	18651	vs. sensor net) also lead to distinct issues in naming. Many sensor network applications use basic collection trees . However, as deployments grow in complexity, needing in-network storage  or novel addressing , more complex routing protocols will be needed. Like these approaches, Firecracker depends on any-toany routing within the network. The prevailing approach so far has been
4797601	23392	dominates rate. Second, the data is much larger than what can be stored in RAM. As access to non-volatile storage is expensive in terms of energy, this often leads to hierarchical  or windowing  approaches to minimize the amount of caching necessary on a receiver. Overcast uses an underlying, single route network to distribute multicast data in Internet-class networks . Overcast deals
4797601	23393	of detection but propagate rapidly. Routing Protocol: The routing protocol must allow nodes to address arbitrary nodes in the network: traditional sensor network collection trees are insufficient.  Because the purpose of the routing phase is to spread data to distant points in the network, a naming scheme that allows nodes to choose such points is helpful. Nodes along the route should be able
23395	23397	is replaced by a task creation so that sub-problem calls can be executed in parallel. Lazy Task Creation (LTC) is a general and efficient method for executing parallel divide-and-conquer algorithms . It has been shown to achieve good performance both on shared memory and distributed memory machines . It primarily needs two kinds of coordination/communication between nodes
23395	23398	joining/leaving resources and fault tolerance. The first category includes MPI implementation supporting communication across LANs , accommodating node failures , and supporting checkpoints . Our main contribution, in contrast to these efforts, is that we proposed a new programming model, necessary for applications that acquire and release resources during computation. As we argued in
23395	23399	model, we can see its simple adaptation to parallel applications in many “task-scheduling” systems. In cluster environments, they include PBS  and LSF . In wide area, they include Nimrod/G  and many Internet-scale computing projects . They have a basic architecture in common; there is a single or a few task servers that pool tasks. Compute resources occasionally get tasks
23395	23399	Phoenix that supports communication across LANs over many restrictions (firewall, DHCP, NAT, etc.). MPI implementations should be able to take advantage of it. The second category includes Nimrod/G  and many other task scheduling tools . There is a single or a few servers that pool tasks and whichever resources are live at that moment get a task from the servers, computes the result,
23395	23400	farming) to support dynamically joining/leaving resources and fault tolerance. The first category includes MPI implementation supporting communication across LANs , accommodating node failures , and supporting checkpoints . Our main contribution, in contrast to these efforts, is that we proposed a new programming model, necessary for applications that acquire and release resources
23395	23401	is a general and efficient method for executing parallel divide-and-conquer algorithms . It has been shown to achieve good performance both on shared memory and distributed memory machines . It primarily needs two kinds of coordination/communication between nodes (processors), namely, load balancing and synchronization. For load balancing, each node maintains its own local task deque
23395	17746	computation model (task farming) to support dynamically joining/leaving resources and fault tolerance. The first category includes MPI implementation supporting communication across LANs , accommodating node failures , and supporting checkpoints . Our main contribution, in contrast to these efforts, is that we proposed a new programming model, necessary for applications
23395	23402	is a general and efficient method for executing parallel divide-and-conquer algorithms . It has been shown to achieve good performance both on shared memory and distributed memory machines . It primarily needs two kinds of coordination/communication between nodes (processors), namely, load balancing and synchronization. For load balancing, each node maintains its own local task deque
23395	23403	that operates on a stream of data produced every day , information retrieval systems (e.g., web crawlers and indexers) that cooperatively gather and index web pages without duplications , many Internet-scale computing projects to support scientific discoveries by harnessing a large number of compute-resources on the Internet , and task scheduling systems that distribute
23395	23404	its resource name. When this occurs, p must abort the transaction (lines 34, 42 and 54). This protocol was modeled by a protocol description language Promela and verified via a model checker SPIN . Although complete verification was not achievable with a 10 nodes experiment (due to out of memory), more than 300M states were examined and no errors were found. 2 We also wrote a simple Phoenix
23395	23405	environment, thus we need a protocol that builds and maintains a routing table over an arbitrary set of possible connections. Elsewhere, we have proposed one such protocol in a similar context . It builds a spanning tree among participating nodes. While it is simple, it unfortunately does not use available bandwidth effectively. We need a protocol in which nodes establish allowed
23395	9121	passing, Phoenix can also take advantage of a large body of work on algorithms, libraries and higher-level abstractions built on top of message passing models such as distributed shared memory  and distributed garbage collections . • It allows nodes to join and leave an application while it is running without relying on non-scalable notifications. More specifically, it defines
23395	9121	solutions to achieving both #1 and #4 at the same time. 2.4 Shared Memory A large body of work studied software shared memory or shared object abstraction implemented on distributed memory machines . Although shared memory models are not commonly used in wide area settings, it has many interesting aspects that naturally solve some of the problems associated with message passing and
23395	3195	of an item with hash key k is destined for whichever node assumes virtual node k at that moment. This is essentially how all recent peer-topeer information sharing systems are designed . For such a mechanism to support transparent migration, we must guarantee that a node assuming virtual node k always has all valid items of hash key k. This requires nodes to migrate hash table
23395	23406	of a large body of work on algorithms, libraries and higher-level abstractions built on top of message passing models such as distributed shared memory  and distributed garbage collections . • It allows nodes to join and leave an application while it is running without relying on non-scalable notifications. More specifically, it defines a simple message delivery semantics with which
23395	24978	scalably and how to notify participating nodes of the new node. A more complex problem arises when a node leaves because it would leave a gap in the node name space. Both PVM  and MPI version 2  allow nodes to join, and PVM also allows nodes to leave. Above problems are, however, largely unsolved, and complications that arise are simply exposed to the programmer. Typical operating
23395	24979	message to accomplish the task of the algorithm. Phoenix virtual node name space mediates them. In this section, we study two array-based applications, integer sort (IS) in NAS Parallel Benchmark  and an LU factorization algorithm. Throughout the section, we assume each processor assumes a single range of virtual nodes, and the size of the range is maintained roughly the same. Given this
23395	20031	aggressively and select a short route to the destination. It is technically close to routing table construction problems studied in the context of IP routing  and mobile ad-hoc network routing . Among many proposed routing table construction protocols, we currently employ the destination-sequenced distancevector routing (DSDV)  originally proposed in the context of mobile ad-hoc
23395	939	of an item with hash key k is destined for whichever node assumes virtual node k at that moment. This is essentially how all recent peer-topeer information sharing systems are designed . For such a mechanism to support transparent migration, we must guarantee that a node assuming virtual node k always has all valid items of hash key k. This requires nodes to migrate hash table
23395	939	models and implementation of Phoenix share many things in common with recent efforts on scalable peer-topeer information sharing systems, such as Pastry , Tapestry , Chord , and CAN . They are all based on a large and fixed name space abstraction mediating communication. They all build a routing infrastructure so that involved nodes can send messages to any name (“key” in
23395	5902	of an item with hash key k is destined for whichever node assumes virtual node k at that moment. This is essentially how all recent peer-topeer information sharing systems are designed . For such a mechanism to support transparent migration, we must guarantee that a node assuming virtual node k always has all valid items of hash key k. This requires nodes to migrate hash table
23395	5902	Information Sharing Systems Both models and implementation of Phoenix share many things in common with recent efforts on scalable peer-topeer information sharing systems, such as Pastry , Tapestry , Chord , and CAN . They are all based on a large and fixed name space abstraction mediating communication. They all build a routing infrastructure so that involved nodes can
23395	943	of an item with hash key k is destined for whichever node assumes virtual node k at that moment. This is essentially how all recent peer-topeer information sharing systems are designed . For such a mechanism to support transparent migration, we must guarantee that a node assuming virtual node k always has all valid items of hash key k. This requires nodes to migrate hash table
23395	943	Systems Both models and implementation of Phoenix share many things in common with recent efforts on scalable peer-topeer information sharing systems, such as Pastry , Tapestry , Chord , and CAN . They are all based on a large and fixed name space abstraction mediating communication. They all build a routing infrastructure so that involved nodes can send messages to any name
23395	23410	for a long time in the face of dynamically joining/leaving resources and occasional failures. Such applications include data-intensive computing that operates on a stream of data produced every day , information retrieval systems (e.g., web crawlers and indexers) that cooperatively gather and index web pages without duplications , many Internet-scale computing projects to support
23395	23411	of a large body of work on algorithms, libraries and higher-level abstractions built on top of message passing models such as distributed shared memory  and distributed garbage collections . • It allows nodes to join and leave an application while it is running without relying on non-scalable notifications. More specifically, it defines a simple message delivery semantics with which
23395	23412	is a general and efficient method for executing parallel divide-and-conquer algorithms . It has been shown to achieve good performance both on shared memory and distributed memory machines . It primarily needs two kinds of coordination/communication between nodes (processors), namely, load balancing and synchronization. For load balancing, each node maintains its own local task deque
23395	23414	automatically connects participating resources together, and it does not assume a connection between every pair of nodes is allowed. Our current implementation also supports tunneling through SSH  without requiring individual application users to manually establish them outside the application. • It allows scalable implementation that does not have an obvious bottleneck. While scalability
23395	5903	of an item with hash key k is destined for whichever node assumes virtual node k at that moment. This is essentially how all recent peer-topeer information sharing systems are designed . For such a mechanism to support transparent migration, we must guarantee that a node assuming virtual node k always has all valid items of hash key k. This requires nodes to migrate hash table
23395	5903	Sharing Systems Both models and implementation of Phoenix share many things in common with recent efforts on scalable peer-topeer information sharing systems, such as Pastry , Tapestry , Chord , and CAN . They are all based on a large and fixed name space abstraction mediating communication. They all build a routing infrastructure so that involved nodes can send messages
23395	23416	passing, Phoenix can also take advantage of a large body of work on algorithms, libraries and higher-level abstractions built on top of message passing models such as distributed shared memory  and distributed garbage collections . • It allows nodes to join and leave an application while it is running without relying on non-scalable notifications. More specifically, it defines
23395	23416	solutions to achieving both #1 and #4 at the same time. 2.4 Shared Memory A large body of work studied software shared memory or shared object abstraction implemented on distributed memory machines . Although shared memory models are not commonly used in wide area settings, it has many interesting aspects that naturally solve some of the problems associated with message passing and
23417	23421	this formal proof yields an easily checkable criterion classifying correct compilation results. This criterion can easily be integrated into the well-established approach of program result checking  (also known as translation validation ) typically used to ensure correctness of compiler results. 2 SSA - Based Intermediate Languages Static single assignment (SSA) form has become the
23417	23421	by the Verifix project  and has also become known as translation validation . For an overview and for results on program checking in optimizing backend transformations cf. . 7 Conclusion In this paper, we have presented a formal semantics for SSA intermediate representations within the theorem prover Isabelle/HOL. Thereby we represented common subexpressions in basic
23417	23422	conditions that classify if a compilation result is correct. Our solution is based on the observation that SSA programs specify imperative, i.e. statebased computations. In a previous work , we have shown that SSA semantics can be captured elegantly and adequately with abstract state machines . Based on this work, we develop a formal SSA semantics within the theorem prover
23417	23423	that SSA programs specify imperative, i.e. statebased computations. In a previous work , we have shown that SSA semantics can be captured elegantly and adequately with abstract state machines . Based on this work, we develop a formal SSA semantics within the theorem prover Isabelle/HOL. The imperative semantics transfers control flow from one basic block to its successor block, 1 We
23417	23425	able to connect the formal proof for the algorithmic correctness of code generation with a concrete compiler implementing it.s6 Related Work Early work on formal correctness proofs for compilers  was carried out in the Boyer-Moore theorem prover considering the translation of the programming language Piton. Recent work has concentrated on transformations taking place in compiler frontends.
23417	882	project  focusing on correct compiler construction:  considers the verification of a compiler for a Lisp subset in the theorem prover PVS. The approach of proof-carrying code  is weaker than ours because it concentrates only on the verification of necessary but not sufficient correctness criteria. The approach of program checking has been proposed by the Verifix project
23417	23429	characterized by the current basic block and by the outcomes of the operations in the previously executed basic blocks. add Memory accesses need special treatment. In the functional store approach , memory read/write nodes are considered as accesses to fields of a global state jump variable memory. A write access modifies this global variable memory and requires that the outcome of this
23417	23429	two associated numbers assigned to it, the value representing the result of the corresponding operation and its identifier. Memory accesses are specified according to the functional store approach , cf. section 2. MEMORY memory identifier represents the state of memory at the beginning of the evaluation of a given basic block; identifier being the identifier of this constant (wrt. a basic
23440	23441	conduct simulation experiments in NS-2 to evaluate the performance of TOMA. We compare TOMA with a scalable application-layer multicast protocol NICE , an IP multicast protocol (Core-Based Tree ), and unicast protocol in various network topologies. We find that TOMA can achieve very competitive performance for large groups with hundreds or even thousands of members. Then we present some
23440	8974	are implemented at end hosts. Data packets are transmitted between end hosts via unicast, and are replicated at end hosts. Examples are End System Multicast ,Yoid , ALMI , and NICE , to name a few. These systems do not require infrastructure support from intermediate nodes (such as routers), and thus can be easily deployed. However, application-layer multicast is generally not
23440	8974	Evaluation In this section, we first conduct simulation experiments in NS-2 to evaluate the performance of TOMA. We compare TOMA with a scalable application-layer multicast protocol NICE , an IP multicast protocol (Core-Based Tree ), and unicast protocol in various network topologies. We find that TOMA can achieve very competitive performance for large groups with hundreds or
23440	8974	generated by TOMA and NICE for a single group when group size varies, since NICE is shown to have relatively low control overhead by organizing group members into a hierarchical overlay topology . Second, we evaluate the effectiveness of aggregated multicast in reducing multicast tree setup and maintenance overhead when there are a large number of co-existing groups. Control overhead for a
23440	8974	multicast has emerged as a new architecture to apply multicast paradigm in the Internet. The proposed schemes can be classified into two categories: structured   , and unstructured     . Structured application layer multicast schemes leverage Distributed Hash Table (DHT)-based overlay networks and build multicast forwarding trees on top of this structured overlay.
23440	8974	to collect membership information, periodically calculate a minimum spanning tree based on the measurement updates received from all members, and distribute this information to group members. NICE , on the other hand, is designed to support applications with very large receiver sets and relatively low bandwidth requirements. It recursively arrange group members into a hierarchical overlay
23440	8976	of homogeneous members, each of which contains a RMX proxy. Each RMX proxy communicates with peer proxies using TCP and uses simple multicast congestion control within its data group.   and  focus on optimizing the end-to-end delay and access bandwidth usage at the Multicast Service Nodes. Shi et al proposes a set of heuristic algorithms to solve minimum-diameter degree-limited
23440	18410	evaluate our architecture, we use two types of network topologies. The first set of network topologies are generated using the Transit-Stub Model developed by Institute of Georgia Technology . These topologies have 50 transit 11 (7)sTotal cost 10000 8000 6000 4000 2000 TOMA NICE Unicast IP Multicast 0 0 200 400 600 800 1000 1200 Group size Figure 3: Tree cost vs. group size in
23440	8978	Recently application layer multicast has emerged as a new architecture to apply multicast paradigm in the Internet. The proposed schemes can be classified into two categories: structured   , and unstructured     . Structured application layer multicast schemes leverage Distributed Hash Table (DHT)-based overlay networks and build multicast forwarding trees on top of
23440	18402	it also brings many advantages. First, an MSON provider can support a variety of group communication applications simultaneously, unlike some other existing multicast overlays (such as , , , and  etc.), with each overlay only supporting one group. This provides an additional incentive for ISPs to adopt TOMA. Second, it is relatively easy for ISPs to manage resources since MSON is
23440	18402	than groups. We refer to this procedure as group-tree matching. 4 The URL-like naming approach has been adopted by many systems, such as CDN (content delivery networks), Yoid , Scattercast , Overcast , etc. 4sHaving discussed the main components (forwarding, member and host proxies) of TOMA and their functionalities, we describe its main design issues in more details as follows.
23440	18402	bandwidth and processing capabilities. A promising solution which divides the group members into a number of homogeneous sub-groups has been adopted in existing overlay multicast architectures . Even though this approach solves user heterogeneity problem, it exacerbates the state scalability problem, because multiple multicast trees are now needed for each multicast group. TOMA, on the
23440	11925	bandwidth and processing capabilities. A promising solution which divides the group members into a number of homogeneous sub-groups has been adopted in existing overlay multicast architectures . Even though this approach solves user heterogeneity problem, it exacerbates the state scalability problem, because multiple multicast trees are now needed for each multicast group. TOMA, on the
23440	11925	distribution trees rooted at a central source. This root is responsible for redirecting a client’s HTTP requests to a Overcast node, from which the client receives data using TCP connection. RMX  provides reliable data delivery to heterogeneous end users by using a set of RMX proxies that are organized into a spanning tree. The end users are split into a number of locally-scoped multicast
23440	11350	forwarding trees on top of this structured overlay. Here we concentrate on unstructured application layer multicast schemes because they are more related to our work. End System Multicast   and Application Level Multicast Infrastructure (ALMI)  are targeted at applications with small and sparse groups, such as audio and video conferences. In End System Multicast, end hosts
23440	7895	approach: multicast-related features are implemented at end hosts. Data packets are transmitted between end hosts via unicast, and are replicated at end hosts. Examples are End System Multicast ,Yoid , ALMI , and NICE , to name a few. These systems do not require infrastructure support from intermediate nodes (such as routers), and thus can be easily deployed. However,
23440	7895	for ISPs, it also brings many advantages. First, an MSON provider can support a variety of group communication applications simultaneously, unlike some other existing multicast overlays (such as , , , and  etc.), with each overlay only supporting one group. This provides an additional incentive for ISPs to adopt TOMA. Second, it is relatively easy for ISPs to manage resources
23440	7895	layer multicast has emerged as a new architecture to apply multicast paradigm in the Internet. The proposed schemes can be classified into two categories: structured   , and unstructured     . Structured application layer multicast schemes leverage Distributed Hash Table (DHT)-based overlay networks and build multicast forwarding trees on top of this structured
23440	23442	model, the price for a multicast group is derived from these two costs. Bandwidth Price Chuang-Sirbu Law states that the cost of a multicast tree varies at the 0.8 power of the multicast group size : Lm Lu = N k m where Lm is the total number of links in the multicast distribution tree, Lu is the average number of links in a unicast path, Nm is the multicast group size, and k is a scaling
23440	23443	T (i.e., the total cost of the links on tree T ), and B(g) is the bandwidth requirement of group g. A group-tree matching algorithm similar to the one proposed in Aggregated QoS Multicast (AQoSM)  can be used to map a group to an aggregated tree, and set up or remove trees accordingly. The basic idea is as follows: the host proxy attempts to map the group to an existing aggregated tree if
23440	23444	after approximately two decades since the inception of IP multicast, it is still far from being widely deployed on the Internet. This is due to many technical reasons as well as marketing reasons . The most critical ones include: the lack of a scalable inter-domain routing protocol, the state scalability issue with a large number of groups, the lack of support in access control, the
23440	8980	are strategically placed to form an overlay network, on top of which multicast distribution trees are built for data delivery. To reduce the 2 From this aspect, we share a similar vision with SON , a service overlay proposed for end-to-end QoS support. 3sGroup Tree Mapping Table Tree (B, T ) 1 Groups g , g 0 1 Group-Tree Matching ( g , g ) 0 1 A Member Access Control Member Proxy
23440	23445	paper, we address all these issues. We propose the TOMA architecture. To solve the state scalability issue when there are a large number of groups, in MSON we adopt aggregated multicast approach , with multiple groups sharing one delivery tree. Outside MSON, we develop efficient member proxy selection mechanisms, and choose a core-based application-layer multicast routing approach for data
23440	23445	groups g0 and g1 share the aggregated tree, denoted by (B, T1). management overhead of a large number of trees and improve the multicast state scalability, we employ aggregated multicast approach , in which multiple groups are forced to share one delivery tree (called aggregated trees) 3 . Data packets are encapsulated at incoming proxies, transmitted on aggregated trees, and decapsulated at
23440	6493	brings many advantages. First, an MSON provider can support a variety of group communication applications simultaneously, unlike some other existing multicast overlays (such as , , , and  etc.), with each overlay only supporting one group. This provides an additional incentive for ISPs to adopt TOMA. Second, it is relatively easy for ISPs to manage resources since MSON is based on
23440	6493	We refer to this procedure as group-tree matching. 4 The URL-like naming approach has been adopted by many systems, such as CDN (content delivery networks), Yoid , Scattercast , Overcast , etc. 4sHaving discussed the main components (forwarding, member and host proxies) of TOMA and their functionalities, we describe its main design issues in more details as follows. 2.2 Member Proxy
23440	6493	the above-mentioned protocols provide very limited support (if any) for large multicast groups. A number of schemes have also been proposed to construct multicast overlay using proxies. Overcast  provides reliable single-source multicast by using a distributed protocol to build data distribution trees rooted at a central source. This root is responsible for redirecting a client’s HTTP
23440	8988	next step is to connect these proxies into a mesh, on top of which the overlay multicast trees will be constructed. We choose to use a so-called “Adjacent Connection” overlay topology proposed in  and . In this approach, an overlay link is established between two overlay nodes if the shortest IPlayer path between them does not go through any other overlay nodes. As a result, the
23440	23450	approach  can be adopted. When a new multicast tree is established, the host proxy computes a backup tree by using some redundant tree fault-tolerance schemes, such as the algorithm described in . When a proxy or link failure occurs, the neighbor proxies will detect the failure from the lack of “heartbeat” message exchange and broadcast this discovery to all other proxy nodes. Then the host
23440	23451	is to connect these proxies into a mesh, on top of which the overlay multicast trees will be constructed. We choose to use a so-called “Adjacent Connection” overlay topology proposed in  and . In this approach, an overlay link is established between two overlay nodes if the shortest IPlayer path between them does not go through any other overlay nodes. As a result, the constructed
23440	7896	features are implemented at end hosts. Data packets are transmitted between end hosts via unicast, and are replicated at end hosts. Examples are End System Multicast ,Yoid , ALMI , and NICE , to name a few. These systems do not require infrastructure support from intermediate nodes (such as routers), and thus can be easily deployed. However, application-layer multicast is
23440	7896	the literature, there are many application-layer multicast protocols. In TOMA, due to the existence of a member proxy node in every cluster, we adopt a core-based approach, which is similar to ALMI , to construct P2P multicast trees. In each cluster, the member proxy acts as a core, storing the group membership information in its cluster scope. Periodically, every end user monitors other users
23440	7896	has emerged as a new architecture to apply multicast paradigm in the Internet. The proposed schemes can be classified into two categories: structured   , and unstructured     . Structured application layer multicast schemes leverage Distributed Hash Table (DHT)-based overlay networks and build multicast forwarding trees on top of this structured overlay. Here
23440	18412	groups of homogeneous members, each of which contains a RMX proxy. Each RMX proxy communicates with peer proxies using TCP and uses simple multicast congestion control within its data group.   and  focus on optimizing the end-to-end delay and access bandwidth usage at the Multicast Service Nodes. Shi et al proposes a set of heuristic algorithms to solve minimum-diameter degree-limited
23440	18411	data groups of homogeneous members, each of which contains a RMX proxy. Each RMX proxy communicates with peer proxies using TCP and uses simple multicast congestion control within its data group.   and  focus on optimizing the end-to-end delay and access bandwidth usage at the Multicast Service Nodes. Shi et al proposes a set of heuristic algorithms to solve minimum-diameter
23440	9002	Work Recently application layer multicast has emerged as a new architecture to apply multicast paradigm in the Internet. The proposed schemes can be classified into two categories: structured   , and unstructured     . Structured application layer multicast schemes leverage Distributed Hash Table (DHT)-based overlay networks and build multicast forwarding trees on
23452	23453	for application level development. For example, web services can be assembled to a composite service or services application with the help of routing based workflow. 4. Related Works Allen et al  separate the dynamic re-configuration behavior of architecture from its non-reconfiguration functionality, and provide a notation to precisely interpret each of these aspects. Through analysis to
23452	23455	based on the Architecture Description Language (ADL) is only passive to know when it’s better to make the reconfiguration and has not addressed the problem of consistency maintaining. In , , Almeida et al in detail analyze the situation when dynamic reconfiguration occurs and subdivide the problems to three concrete requirements: structural integrity, mutually consistent states and
23452	6212	to preserving mutual-consistency, and give a description of the intended changes, automaticallysidentify and forces components which will be affected by the change into a safe state. In , , Kon et al design a Component Configurator to model and ma nge the runtime dependencies of components. Component Configurator just records the dependences of component, but it has no consideration
23452	23461	way between components is also not available. In our approach the design of control dependency is influenced by the concept of Component Configurator. Rather similar idea with us can be found in , Shrivastava et al present a workflow based model for distributed applications. In their model, workflow schema is used to express the structure of tasks in a distributed application and task
23462	23464	Development also occur. Reducing the genome size with developmental mappings was considered as a possible approach to the scalability problem in complex evolutionary design optimizing tasks . However, this issue turned out to be more complex than expected . Several comparative studies between direct encodings and developmental mappings indicated that direct encodings were at least
23462	23466	mappings was considered as a possible approach to the scalability problem in complex evolutionary design optimizing tasks . However, this issue turned out to be more complex than expected . Several comparative studies between direct encodings and developmental mappings indicated that direct encodings were at least as efficient as developmental mappings for the problems studied [13,
23462	23467	solution of the modified function? We can sandwich the Kolmogorov complexity of the new optimum x ? ? z between two values as follows: max(C(x ? ), C(z)) ? min(C(x ? ), C(z)) ? c ??? C(x ? ? z) (4) C(x ? ??? z) ? C(x ? ) + C(z) + c, (5) where c is a small constant. A proof is given in Appendix A. In our case, the optimal solution x ? has very low complexity, so if z has high complexity (i.e.
23462	10261	Development also occur. Reducing the genome size with developmental mappings was considered as a possible approach to the scalability problem in complex evolutionary design optimizing tasks . However, this issue turned out to be more complex than expected . Several comparative studies between direct encodings and developmental mappings indicated that direct encodings were at least
23462	23468	is needed if they are to be used in a practical setting. A A short proof Proposition 1. Let x and y be two binary strings of equal length. Then max(C(x), C(y)) ? min(C(x), C(y)) ? c ? C(x ? y) (6) where c is a (small) constant. C(x ? y) ? C(x) + C(y) + c, (7) Proof. Inequality (7) is trivial: If we have two programs which generate the binary strings x and y independently, then we can apply
23462	23469	. Several comparative studies between direct encodings and developmental mappings indicated that direct encodings were at least as efficient as developmental mappings for the problems studied . There is no consensus in the field as to when developmental mappings should be applied. We therefor believe the following question to be important for further progress in AD: For which classes of
23462	23472	complexity of a binary string consisting of 100 zeros is CLZ(0 100 ) = 12.s2.2 Developmental mappings and fitness functions The literature on AD describes many developmental systems and mappings . They vary considerably, but many can be classified as either a grammatical or a chemical approach. We decided to use the developmental mapping described by Kitano in . This is an example of a
23462	23475	. Several comparative studies between direct encodings and developmental mappings indicated that direct encodings were at least as efficient as developmental mappings for the problems studied . There is no consensus in the field as to when developmental mappings should be applied. We therefor believe the following question to be important for further progress in AD: For which classes of
23462	23476	. Several comparative studies between direct encodings and developmental mappings indicated that direct encodings were at least as efficient as developmental mappings for the problems studied . There is no consensus in the field as to when developmental mappings should be applied. We therefor believe the following question to be important for further progress in AD: For which classes of
23462	23476	and our previous experience with the mapping. The literature often refers to this mapping. For example, Siddiqi and Lucas compared Kitano mapping to direct encoding for evolving neural networks . In our particular implementation of Kitano mapping, the genotype is 357 bits long. See  for more details about our implementation of the mapping. We will contrast Kitano mapping with a direct
23462	23477	that the problem gets harder for developmental mappings but does not change the difficulty for direct encodings. Motivated by previous results on developmental mappings and phenotypic complexity , we hypothesize that one can construct such fitness function modifiers based on phenotypic complexity. Our approach to complexity is based on Kolmogorov complexity . 2 Background We will use
23462	23477	For example, Siddiqi and Lucas compared Kitano mapping to direct encoding for evolving neural networks . In our particular implementation of Kitano mapping, the genotype is 357 bits long. See  for more details about our implementation of the mapping. We will contrast Kitano mapping with a direct encoding. A direct encoding is here simply the identity function from genotype to phenotype.
23462	23477	the density plot can give us any hints about the reason for this. The construction of the Complexity Translate modifier was motivated by results from earlier experiments with the Kitano mapping . Results from that work indicate that the distance-preservation of the Kitano mapping from genotype to phenotype depends on phenotypic complexity. If the phenotype has high complexity we can expect
23462	23477	were described: the Complexity Translate modifier, and the Complexity Filter modifier. Their construction was motivated by previous results on phenotypic complexity and developmental mappings . Experimental results indicate that the Complexity Filter modifier can make a problem harder for direct encoding, while preserving the difficulty of the problem for the Kitano mapping. Furthermore,
23462	13314	and phenotypic complexity , we hypothesize that one can construct such fitness function modifiers based on phenotypic complexity. Our approach to complexity is based on Kolmogorov complexity . 2 Background We will use the following notation. The set of binary strings of length n is denoted {0, 1} n . If x is a binary string, then xi denotes the ith bit of x and x denotes the
23462	13314	Kolmogorov Complexity and Lempel-Ziv Compresssion The fitness modifiers described later are based on complexity. Kolmogorov complexity is quantitative measure of complexity based on Turing machines . (See eg.  to recall the definitions of TMs and partial recursive functions.) Definition 1. The Kolmogorov complexity C(x) of a natural number x (or the corresponding binary string) is defined
23462	23479	of the shortest program which outputs that string on a fixed universal Turing Machine and then halts. This complexity measure is not computable, but we use the Lempel-Ziv compression algorithm  to approximate the Kolmogorov complexity. In the experiments described below, we used the standard compression library zlib . For practical purposes, each bit in a binary string is encoded as
23462	23481	to the phenospace in direct encodings. While developmental mappings are traditionally applied to design optimization problems, they are here applied to pseudo-boolean function optimization . These functions are more convenient for our purpose than design optimization problems. The pseduo-boolean fitness functions chosen were OneMax , DebDeceptive and RoyalRoad , because they
23462	23483	optimization . These functions are more convenient for our purpose than design optimization problems. The pseduo-boolean fitness functions chosen were OneMax , DebDeceptive and RoyalRoad , because they are well-known, simple to implement, and has varying hardness. The function OneMax is relatively simple, while the function DebDeceptive seems harder. These functions are defined in
23462	23486	and this might explain our results with Kitano mapping on Complexity Translate. Furthermore, Downing observed that it was harder to evolve irregular binary strings with his developmental mapping . Furthermore, if Complexity Translate translates > c f 9s10 the optimum to a bitstring outside the image of the Kitano mapping, then the optimum will of course not be found with Kitano mapping. 6
23487	23489	interesting mathematical applications, and in particular, several types of stochastic differential equations driven by fBm have been considered in finite dimensions (see among others ,  or ). The question of infinite dimensional equations has emerged very recently (see , ). The purpose of this article is to provide a detailed study of the existence and regularity properties of
23487	23491	have It ? 2 ? ? t ? ? ?e n 0 (t?s)A ? ? ?en? 2 V K2 (t, s)ds +2 ? ? ? t ? ? t ? ? ? e n 0 s (t?r)A ?en ? e (t?s)A ? ? ?K ? ?en (r, s)dr? ?r ??? = ? (I1(n)+I2(n)) n Using the following inequality (see , Th. 3.2), 1 1 H? H? K(t, s) ? c(H)(t ? s) 2 s 2 the first sum above can be majorized in the following way ? I1(n) ? c(H) ? n n = c(H) ? n ? c(H) ? ? t 0 ? ? 0 ? ? ?e 2(t?s)A ?en, ?en?V (t ? s)
23487	23500	situation occurs for example in the case of the Laplace-Beltrami operator on compact Lie groups; in this situation, with H =1/2, the trace condition with (?A + I) ?2H was proved to be optimal in . This condition is equivalent to conditions presented in work done in  for both the stochastic heat and wave equations in Euclidean space R d with d ? 2; therein, the authors even treat
848156	23504	efficiency requirements of interactive search. We give a detailed description of a number of these types of shape descriptors below, and a more general survey of shape descriptors can be found in . Extended Gaussian Image (EGI) The extended Gaussian image is a shape descriptor that represents a 3D model by a spherical function. The EGI was initially proposed by Horn in  and is obtained
848156	23505	a mapping from the space of models into a fixed-dimensional vector space, and then to define the measure of similarity between two models as the distance between their corresponding descriptors . This approach has the advantage of addressing the correspondence problem on a per-model basis, allowing for the computation of descriptors in an offline process. Thus, correspondences are
848156	23505	problem into two independent 3D optimization problems. Shape Histograms Motivated by the challenge of using shape matching techniques to address the challenge of protein matching, Ankerst et al.  developed three different methods for representing 3D models in terms of the distribution of surface points as a function of distance from the center of mass and spherical angle. When only the
848156	23505	a general method for transforming a rotation-varying descriptor into a rotation-invariant one. In this section we show that the two descriptors designed to be rotation-invariant, namely the Shells  and the D2  distributions, are actually very specific instances of the power spectrum approach. In particular, we show that using the methodology of the power spectrum approach these two
848156	23505	approach converges very efficiently in practice. 5.3 Anisotropy Factoring The method that we propose for anisotropy factoring is a general one that can be applied to any of the many methods  that matches two models by independently representing each one by a shape descriptor, and then defining the measure of ? ?s¨smodel similarity as the difference between the corresponding
848156	23505	case that ¢ ? § , anisotropy information plays no role in the matching and the matching method is invariant to anisotropic scale. If additionally the shape descriptor is itself rotation-invariant  then we obtain a matching method that is invariant to all affine transformations. The advantage of this matching approach is that the shape metric defines similarity as the product of the
848156	23508	establish correspondences between pairs of points on the two models, and then defining the measure of shape similarity as the sum of the squared distances between pairs of points in correspondence . The second method is more general, decomposing a model into constituent parts, and then representing the model as a 5sgraph characterizing the relationship between the different segments [21, 45,
848156	23513	complexity of model comparison. Normalizing for Axial Ambiguity: Another approach is to normalize for the ambiguity by using a consistent method for choosing the direction of the axis. Tal et al.  propose a method for resolving this ambiguity using forward weighting. For each of the three ¡ eigenvectors, ? ? ? ? ? ¥ ¨ , they compute the area of the intersection of the model with the ? ? ¡¤£
848156	23514	establish correspondences between pairs of points on the two models, and then defining the measure of shape similarity as the sum of the squared distances between pairs of points in correspondence . The second method is more general, decomposing a model into constituent parts, and then representing the model as a 5sgraph characterizing the relationship between the different segments [21, 45,
848156	23514	– the collection of amplitudes of the different frequency components – discards phase and therefore is invariant to rotation. Invariance to Axial Rotation: One approach that has been described () for addressing the limitations of PCA alignment uses the fact that in the case that only two of the eigenvectors have the same eigenvalue, the PCA approach can be used to define one of the
848156	23514	to the amplitude. Thus, it represents a two-dimensional spherical function by a one-dimensional array of energy values. 49 The axial rotation-invariant representation (Fourier) described in  is roughly half the size of the initial spherical function because each complex spherical harmonic coefficient is represented by its norm. Exhaustively searching for the optimal rotation for model
848156	23515	a mapping from the space of models into a fixed-dimensional vector space, and then to define the measure of similarity between two models as the distance between their corresponding descriptors . This approach has the advantage of addressing the correspondence problem on a per-model basis, allowing for the computation of descriptors in an offline process. Thus, correspondences are
848156	23515	the database indicate that more of the models are cylindrical, the largest ( © -axis) principal axis gives the best retrieval results. Invariance to General Rotation: Another approach, described in , generalizes the power spectrum notion defined for functions whose domain is a circle to a notion of power spectrum defined for functions whose domain is a sphere. This makes it possible to obtain
848156	23515	approach converges very efficiently in practice. 5.3 Anisotropy Factoring The method that we propose for anisotropy factoring is a general one that can be applied to any of the many methods  that matches two models by independently representing each one by a shape descriptor, and then defining the measure of ? ?s¨smodel similarity as the difference between the corresponding
848156	23515	case that ¢ ? § , anisotropy information plays no role in the matching and the matching method is invariant to anisotropic scale. If additionally the shape descriptor is itself rotation-invariant  then we obtain a matching method that is invariant to all affine transformations. The advantage of this matching approach is that the shape metric defines similarity as the product of the
848156	23518	a mapping from the space of models into a fixed-dimensional vector space, and then to define the measure of similarity between two models as the distance between their corresponding descriptors . This approach has the advantage of addressing the correspondence problem on a per-model basis, allowing for the computation of descriptors in an offline process. Thus, correspondences are
848156	23518	in . Extended Gaussian Image (EGI) The extended Gaussian image is a shape descriptor that represents a 3D model by a spherical function. The EGI was initially proposed by Horn in  and is obtained by 6shaving each triangle vote on the bin corresponding to its normal direction, with a weight equal to the area of the triangle. The extended Gaussian image has several important
848156	23518	approach converges very efficiently in practice. 5.3 Anisotropy Factoring The method that we propose for anisotropy factoring is a general one that can be applied to any of the many methods  that matches two models by independently representing each one by a shape descriptor, and then defining the measure of ? ?s¨smodel similarity as the difference between the corresponding
848156	23518	Because the convergence of the iterative process depends on the distribution of triangle normals over the surface of the model, we believe that it may be possible to use the Extended Gaussian Image  to design a method for directly transforming a 3D surface into an isotropic one. 6.2.4 Shape Descriptors Finally, this thesis has described two general methods for extending existing shape
848156	23521	be moved in order to lie on the other model. summing the asymmetric dot products of two vectors rather than computing Euclidean distances. Consequently, algorithms for nearest neighbor search (e.g. ) cannot be used to provide efficient retrieval, limiting the practical efficacy of this method. In the next section we show that this dot-product representation can be approximated with a shape
848156	23523	a mapping from the space of models into a fixed-dimensional vector space, and then to define the measure of similarity between two models as the distance between their corresponding descriptors . This approach has the advantage of addressing the correspondence problem on a per-model basis, allowing for the computation of descriptors in an offline process. Thus, correspondences are
848156	23523	3D shape is convex, the EGI is an invertible representation. Complex Extended Gaussian Image (CEGI) The complex extended Gaussian image is a generalization of the EGI, proposed by Kang et al. in . Rather than just voting with a real value equal to the area of the triangle, this method votes with a complex number whose amplitude is equal to the area of the triangle and whose complex phase is
848156	23523	approach converges very efficiently in practice. 5.3 Anisotropy Factoring The method that we propose for anisotropy factoring is a general one that can be applied to any of the many methods  that matches two models by independently representing each one by a shape descriptor, and then defining the measure of ? ?s¨smodel similarity as the difference between the corresponding
848156	23524	a circle, or in 2D. The dependence of these methods on the FFT made them hard to generalize to shape descriptors that represented a 3D model with either a spherical function or a function in 3D. In  we presented a method for computing the measure of reflective symmetries for all planes passing through the origin. For a spherical descriptor ofs? © ? ? size (respectively 3D function ofs? © ¥ ?
848156	23525	a circle, or in 2D. The dependence of these methods on the FFT made them hard to generalize to shape descriptors that represented a 3D model with either a spherical function or a function in 3D. In  we presented a method for computing the measure of reflective symmetries for all planes passing through the origin. For a spherical descriptor ofs? © ? ? size (respectively 3D function ofs? © ¥ ?
848156	23526	the database indicate that more of the models are cylindrical, the largest ( © -axis) principal axis gives the best retrieval results. Invariance to General Rotation: Another approach, described in , generalizes the power spectrum notion defined for functions whose domain is a circle to a notion of power spectrum defined for functions whose domain is a sphere. This makes it possible to obtain
848156	23527	the database indicate that more of the models are cylindrical, the largest ( © -axis) principal axis gives the best retrieval results. Invariance to General Rotation: Another approach, described in , generalizes the power spectrum notion defined for functions whose domain is a circle to a notion of power spectrum defined for functions whose domain is a sphere. This makes it possible to obtain
848156	23532	efficiency requirements of interactive search. We give a detailed description of a number of these types of shape descriptors below, and a more general survey of shape descriptors can be found in . Extended Gaussian Image (EGI) The extended Gaussian image is a shape descriptor that represents a 3D model by a spherical function. The EGI was initially proposed by Horn in  and is obtained
848156	23534	the database indicate that more of the models are cylindrical, the largest ( © -axis) principal axis gives the best retrieval results. Invariance to General Rotation: Another approach, described in , generalizes the power spectrum notion defined for functions whose domain is a circle to a notion of power spectrum defined for functions whose domain is a sphere. This makes it possible to obtain
848156	23534	frequency components uniquely, up to rotation. In the past, this problem has been addressed by using algebraic methods to obtain additional rotation-invariants for the different frequency components. 42sFigure 3.7: The minimum distance between two functionssand ¡ , taken over the space of all rotations, is equal to the distance between the two manifolds ¢¤£ and ¢¦¥ , where ¢§£ and ¢¦¥ are
848156	23536	establish correspondences between pairs of points on the two models, and then defining the measure of shape similarity as the sum of the squared distances between pairs of points in correspondence . The second method is more general, decomposing a model into constituent parts, and then representing the model as a 5sgraph characterizing the relationship between the different segments [21, 45,
848156	23537	became available for matching models without explicitly establishing correspondences and the advantage of the canonical parameterization was leveraged in a number of symmetry detection algorithms . These methods used the fact that the covariance ellipsoid of a 3D model rotates with the model, so that a model could only have symmetries where its covariance ellipsoid had 55sthem. Since the
848156	23538	a mapping from the space of models into a fixed-dimensional vector space, and then to define the measure of similarity between two models as the distance between their corresponding descriptors . This approach has the advantage of addressing the correspondence problem on a per-model basis, allowing for the computation of descriptors in an offline process. Thus, correspondences are
848156	23538	representation of the intersection of the model with the shell. The resultant descriptor gives rise to a three-dimensional representation that rotates with the model. Shape Distribution (D2) In , Osada et al. present a generalization of the Shells method by generating a his8stogram of distances between pairs of points on the surface of a model. Similar to the Shells representation, the D2
848156	23538	for transforming a rotation-varying descriptor into a rotation-invariant one. In this section we show that the two descriptors designed to be rotation-invariant, namely the Shells  and the D2  distributions, are actually very specific instances of the power spectrum approach. In particular, we show that using the methodology of the power spectrum approach these two representations can be
848156	23538	approach converges very efficiently in practice. 5.3 Anisotropy Factoring The method that we propose for anisotropy factoring is a general one that can be applied to any of the many methods  that matches two models by independently representing each one by a shape descriptor, and then defining the measure of ? ?s¨smodel similarity as the difference between the corresponding
848156	23538	case that ¢ ? § , anisotropy information plays no role in the matching and the matching method is invariant to anisotropic scale. If additionally the shape descriptor is itself rotation-invariant  then we obtain a matching method that is invariant to all affine transformations. The advantage of this matching approach is that the shape metric defines similarity as the product of the
848156	23540	efficiency requirements of interactive search. We give a detailed description of a number of these types of shape descriptors below, and a more general survey of shape descriptors can be found in . Extended Gaussian Image (EGI) The extended Gaussian image is a shape descriptor that represents a 3D model by a spherical function. The EGI was initially proposed by Horn in  and is obtained
848156	23541	variations that would result in votes being cast into nearby bins. While these limitations could be addressed by using a non-normed measure of similarity, such as the Earth Mover’s Dis16stance , the resulting comparison becomes much more expensive for spherical and 3D histograms and, as a result, matching can no longer be performed in real time. Similarly, the Spherical Extent Function
848156	23544	41, 69]. The second method is more general, decomposing a model into constituent parts, and then representing the model as a 5sgraph characterizing the relationship between the different segments . Correspondences between two models can then be established using graph matching techniques, which simultaneously define the correspondences between the nodes of the two graph representations, and
848156	23544	41, 69]. The second method is more general, decomposing a model into constituent parts, and then representing the model as a 5sgraph characterizing the relationship between the different segments . Correspondences between two models can then be established using graph matching techniques, which simultaneously define the correspondences between the nodes of the two graph representations, and
848156	23546	became available for matching models without explicitly establishing correspondences and the advantage of the canonical parameterization was leveraged in a number of symmetry detection algorithms . These methods used the fact that the covariance ellipsoid of a 3D model rotates with the model, so that a model could only have symmetries where its covariance ellipsoid had 55sthem. Since the
848156	23547	efficiency requirements of interactive search. We give a detailed description of a number of these types of shape descriptors below, and a more general survey of shape descriptors can be found in . Extended Gaussian Image (EGI) The extended Gaussian image is a shape descriptor that represents a 3D model by a spherical function. The EGI was initially proposed by Horn in  and is obtained
848156	23549	a mapping from the space of models into a fixed-dimensional vector space, and then to define the measure of similarity between two models as the distance between their corresponding descriptors . This approach has the advantage of addressing the correspondence problem on a per-model basis, allowing for the computation of descriptors in an offline process. Thus, correspondences are
848156	23549	a two-dimensional representation with improved retrieval performance. Spherical Extent Function Initially described in  though first used in database retrieval applications by Vranic et al. in , this descriptor represents a 3D model by a spherical function giving the maximal distance from the center of mass as a function of spherical angle. Similar to the Sectors descriptor, this function
848156	23549	– the collection of amplitudes of the different frequency components – discards phase and therefore is invariant to rotation. Invariance to Axial Rotation: One approach that has been described () for addressing the limitations of PCA alignment uses the fact that in the case that only two of the eigenvectors have the same eigenvalue, the PCA approach can be used to define one of the
848156	23549	to the amplitude. Thus, it represents a two-dimensional spherical function by a one-dimensional array of energy values. 49 The axial rotation-invariant representation (Fourier) described in  is roughly half the size of the initial spherical function because each complex spherical harmonic coefficient is represented by its norm. Exhaustively searching for the optimal rotation for model
848156	23549	approach converges very efficiently in practice. 5.3 Anisotropy Factoring The method that we propose for anisotropy factoring is a general one that can be applied to any of the many methods  that matches two models by independently representing each one by a shape descriptor, and then defining the measure of ? ?s¨smodel similarity as the difference between the corresponding
848156	23551	the perfect symmetries of a model. Thus if a symmetric model had even a small amount of noise, these methods would fail to identify its symmetries. In order to address this issue, Zabrodsky et al.  defined a continuous measure of symmetry which transformed the discrete question: “Does a model have a given symmetry?” to the continuous question: “How much of a given symmetry does a model have?”
848156	23552	the perfect symmetries of a model. Thus if a symmetric model had even a small amount of noise, these methods would fail to identify its symmetries. In order to address this issue, Zabrodsky et al.  defined a continuous measure of symmetry which transformed the discrete question: “Does a model have a given symmetry?” to the continuous question: “How much of a given symmetry does a model have?”
848156	23553	the perfect symmetries of a model. Thus if a symmetric model had even a small amount of noise, these methods would fail to identify its symmetries. In order to address this issue, Zabrodsky et al.  defined a continuous measure of symmetry which transformed the discrete question: “Does a model have a given symmetry?” to the continuous question: “How much of a given symmetry does a model have?”
848156	23554	establish correspondences between pairs of points on the two models, and then defining the measure of shape similarity as the sum of the squared distances between pairs of points in correspondence . The second method is more general, decomposing a model into constituent parts, and then representing the model as a 5sgraph characterizing the relationship between the different segments [21, 45,
11120431	3928	for surface triangulation. Most of them are based on evaluations of the implicit surface function at the vertices of a uniform grid in some predefined area . There, whenever the function has different sign on the vertices, its intersections with the edges of the cell are found and triangles are appropriately extracted. Sometimes, the intersections may
11120431	3928	for a starting point (a cell where an intersection with the surface exists). Depending on where the intersections lie, neighbor cells are added so that the rest of the surface is found as well . These are known as continuation methods. These methods demand a starting point (i.e. a point that zeroes the implicit function) and so it is difficult to find surfaces with more than one
11120431	23567	been proposed for surface triangulation. Most of them are based on evaluations of the implicit surface function at the vertices of a uniform grid in some predefined area . There, whenever the function has different sign on the vertices, its intersections with the edges of the cell are found and triangles are appropriately extracted. Sometimes,
11120431	23567	to gain more accuracy . Others make an approximation of the curvature and, if needed, a subdivision of the extracted triangles is performed in a second refinement stage . Witkin et al. , solve an optimization problem to acquire samples on the surface. Fingueiredo et al.  propose a physically based approach to polygonizing the surface. Overveld et al.
23568	23570	describe relevant work that utilizes human model-based approaches and related studies involving hand and arm gestures. More complete details can be found in surveys by Watson , Aggarwal and Cai  and Pavlovic et al. . 2.1. Overview of Recognition Methodologies The common methodologies that have been used for motion and gesture recognition are: (1) template matching , (2) neural
23568	23571	that utilizes human model-based approaches and related studies involving hand and arm gestures. More complete details can be found in surveys by Watson , Aggarwal and Cai  and Pavlovic et al. . 2.1. Overview of Recognition Methodologies The common methodologies that have been used for motion and gesture recognition are: (1) template matching , (2) neural networks (also known as the
23568	23572	The common methodologies that have been used for motion and gesture recognition are: (1) template matching , (2) neural networks (also known as the feature-based approach) , (3) statistical  and (4) multimodal probabilistic combination . By far the most popular recognition methods are the neural networks (e.g., ) and the statistical method, hidden Markov models
23568	23578	known as the feature-based approach) , (3) statistical  and (4) multimodal probabilistic combination . By far the most popular recognition methods are the neural networks (e.g., ) and the statistical method, hidden Markov models (HMMs) (e.g., ). Each of these methods has a set of drawbacks which either affect their performance or limit their utilization by
23568	23579	probabilistic combination . By far the most popular recognition methods are the neural networks (e.g., ) and the statistical method, hidden Markov models (HMMs) (e.g., ). Each of these methods has a set of drawbacks which either affect their performance or limit their utilization by users. One of the major drawbacks is that they depend on user-specific training
23568	23579	Baudel and Beaudouin-Lafon designed an application that uses hand and arm gestures for controlling a computer presentation. Kahn et al.  studied pointing operations and Campbell et al.  performed a study involving T’ai Chi gestures. 3. Background Here we give the background for methods that we utilized and integrated in the design of our filter. 3.1. Extended Kalman Filter The
23568	23580	probabilistic combination . By far the most popular recognition methods are the neural networks (e.g., ) and the statistical method, hidden Markov models (HMMs) (e.g., ). Each of these methods has a set of drawbacks which either affect their performance or limit their utilization by users. One of the major drawbacks is that they depend on user-specific training
23568	23581	probabilistic combination . By far the most popular recognition methods are the neural networks (e.g., ) and the statistical method, hidden Markov models (HMMs) (e.g., ). Each of these methods has a set of drawbacks which either affect their performance or limit their utilization by users. One of the major drawbacks is that they depend on user-specific training
23568	23582	probabilistic combination . By far the most popular recognition methods are the neural networks (e.g., ) and the statistical method, hidden Markov models (HMMs) (e.g., ). Each of these methods has a set of drawbacks which either affect their performance or limit their utilization by users. One of the major drawbacks is that they depend on user-specific training
23568	23587	system, into the process of filtering motion capture data of human movements. Model-based approaches using dynamics seems to have first appeared in Pentland and Horowitz  and others (e.g., ). The method has been shown to improve tracking by helping reduce the search space required to determine the position and orientation of a tracked object for the next motion state. This includes
23568	23589	This includes estimating position and orientation when interference between the tracking equipment and tracked object occur . Model-based approaches have been utilized by Zordan and Hodgins , Metaxas  and others for generating motion appropriate for animated characters. Badler  experimented with model-based approaches to simulate human-like virtual actors in a system he
23568	23591	motion appropriate for animated characters. Badler  experimented with model-based approaches to simulate human-like virtual actors in a system he developed called Jack. Wren and Pentland  applied dynamics to a 3D skeletal model of the body for a tracking application. They applied 2D measurements from image features and combined them with the extended Kalman filter to drive the 3D
23568	23592	Kalman filter to drive the 3D model. Their resulting tracking system was able to tolerate temporary image occlusions and the presence of multiple people in the tracked area. In more recent work  they explored the notion that people utilize muscles to actively shape purposeful motion. They were able to extract models of purposeful actions from a system they built. This reenforces the notion
23568	23593	from a system they built. This reenforces the notion that there is important information 2 from the underlying structure of human motion that can be incorporated in the recognition process. Rohr  studied human movements from image sequences by incorporating models from medical motion studies. He specifically analyzed people walking by applying measurements of the body joints and vertical
23568	23595	of the subjects walking in the images. Others have performed similar work, including Hogg . We explored the use of a model-based approach for arm motion recognition performance in earlier work . We were not able to find enough evidence that the approach improved recognition performance at that time. We felt this was due to the lack of sophistication of the model and control system, which
23568	23596	on a simple particle model representing the position of the wrist and its associated dynamics. 2.3. Arm and Hand Gesture Studies The hand has been studied extensively for computer human interaction . However, fewer studies have been performed on gestures involving the arm in addition to the hand. Sturman  developed a system for recognizing gestures for orienting construction cranes. Morita
23568	23598	on a simple particle model representing the position of the wrist and its associated dynamics. 2.3. Arm and Hand Gesture Studies The hand has been studied extensively for computer human interaction . However, fewer studies have been performed on gestures involving the arm in addition to the hand. Sturman  developed a system for recognizing gestures for orienting construction cranes. Morita
23568	23600	on a simple particle model representing the position of the wrist and its associated dynamics. 2.3. Arm and Hand Gesture Studies The hand has been studied extensively for computer human interaction . However, fewer studies have been performed on gestures involving the arm in addition to the hand. Sturman  developed a system for recognizing gestures for orienting construction cranes. Morita
23568	23603	on a simple particle model representing the position of the wrist and its associated dynamics. 2.3. Arm and Hand Gesture Studies The hand has been studied extensively for computer human interaction . However, fewer studies have been performed on gestures involving the arm in addition to the hand. Sturman  developed a system for recognizing gestures for orienting construction cranes. Morita
23568	23605	for recognizing gestures for orienting construction cranes. Morita et al.  shows how to interpret gestures from a musical conductor by tracking the tip of the wand. Baudel and Beaudouin-Lafon designed an application that uses hand and arm gestures for controlling a computer presentation. Kahn et al.  studied pointing operations and Campbell et al.  performed a study involving
23568	19218	T’ai Chi gestures. 3. Background Here we give the background for methods that we utilized and integrated in the design of our filter. 3.1. Extended Kalman Filter The extended Kalman filter (EKF)  estimates both the time sequence of states of an input data stream and a statistical model of that data stream. The EKF differs from the standard Kalman filter  in that it can be used to
23568	23606	velocities. The Lagrangian formulation for the dynamics of a system is ? ??? ? ??? ??? ??????? ¨ ? £???? ¨?? ???????¥????? ? (4) ? ??? where ? is the set of externally applied forces and torques . Solutions to Equation 4 can be found in closed form, which are more efficient and readily parameterizable than the open form derivations generated by the Featherstone algorithm , which is a
23568	23595	to changes in the parameters then it may be considered to be more generalizable and potentially more powerful. We analyzed the sensitivity of a few of the body parameters (summarized in Schmidt ), but did not determine enough meaningful information to make conclusions about the generalizability of our filter. 5.4. Expert User Experiments We set out to verify the effectiveness of the filter
5447184	23641	between individuals in which there is a dilemma over whether to act cooperatively or selfishly. In the single shot PD selfish behaviour is the only rational outcome (Nash 1950,1951). However (Axelrod 1984) proposed iterated interaction in the PD as a mechanism for promoting cooperation. An alternative explanation for the evolution of cooperative behaviour in populations exists in (Hamilton 1964)’s
5447184	23641	stochastic elements, these are limited to calculations from one time step to the next. However, we can still use these calculations to get an impression of the implicit “shadow of the future??? (Axelrod 1984). The shadow of the future is defined as the probability that two agents will interact again at some point in the future. If the shadow of the future is large then iterated interaction is likely,
5447184	23642	of interactions between any two interacting agents was also calculated, as well as the average relatedness of interacting agent pairs. Relatedness was calculated using the following formula from (Collins and Jefferson 1991), where f = frequency of allele 1 (for i example) at locus i, l = number of loci being compared and 0 ? D ? 1. The relatedness of two interacting agents was therefore calculated as 1-D, with l = 5.
5447184	23652	A pair of agents reproduce to combine their genetic material into one offspring agent, whose energy level is initialised appropriately. The genetic operators used in the reproduction process are (Spears and DeJong 1991)???s parameterised crossover operator, and a standard mutation operator. Kin Selection A Necessary Condition for Kin Selection There is one fundamental condition for the operation of kin selection,
8922258	5274	functions enjoy some type of symmetry and periodicity. All the cited works use an algebraic approach, relying on popular signal processing tools like the -transform and polyphase components . Manuscript received February 15, 1997; revised May 29, 1998. This work was supported in part by the U.S. National Science Foundation under Grant MIP-93-21302, the Swiss NSF Award 21-43136.95, and
8922258	5274	AND CONTINUOUS-TIME LOCAL COSINE BASES WITH MULTIPLE OVERLAPPING 3177 Every two channel filter bank can be expressed via the lattice factorization as a sequence of rotations and unit delays , . Every rotation is identified by an angle , and a - length filter bank requires rotations. Collect all the angles in a vector Let be the function giving the th sample of the filter relative to
8922258	23660	The fact that they can be interpreted as a “smooth DCT” make them interesting for compression purposes , . Recently, they have also found application in multitone modulation systems . In discrete time, the first perfect reconstruction (PR) version of the CMFB has been introduced by Princen and Bradley . In such a construction, the filter length is twice the sampling period ,
8922258	23660	Since every is a branch of a PR filter bank, (37) implies that must be the conjugate (in a PR sense) of Window symmetry clearly fulfills such a condition. If LCB’s are used for multitone modulation , the window at the receiver is a distorted version of the window at the transmitter Because of this, we can loose external orthogonality, and this causes intersymbol interference. To measure the
8922258	23661	they have also found application in multitone modulation systems . In discrete time, the first perfect reconstruction (PR) version of the CMFB has been introduced by Princen and Bradley . In such a construction, the filter length is twice the sampling period , giving rise to single overlapping CMFB. The first results on the multiple overlapping case, more precisely for , are due to
8922258	23666	literature. The continuous-time counterpart of the CMFB is known as local cosine bases (LCB), and it has been introduced by Coifman and Meyer . Such a device has been used by Auscher et al.  to construct the Lemarié and Meyer wavelet . Recently, Matviyenko  introduced biorthogonal LCB, showing that the dual is still an LCB but with a different window. All the cited works
8922258	23666	is due to Malvar, which, in , shows that by modulating a raised cosine, we get an orthonormal basis for Bernardini and Kova?evi? in  explore both continuous and discrete time. Inspired by , they approach the problem with a vector space point of view, interpreting PR as a decomposition of  into a direct sum of subspaces of compactly supported signals. The theory presented in
4049426	23697	homolog for macaque area F5, which contains mirror neurons (Rizzolatti et al., 2001). Several action observation and imagery studies have found responses in premotor areas, as well (for review, see Jeannerod, 2001). We plotted on the cortical surface several reported peak activation coordinates from previously published studies that had action observation conditions and that found responses in inferior
23716	23717	(MIMO) systems, recent attention has turned to iterative detection and decoding at the receiver. The well-known bit-interleaved coded modulation technique for singleantenna channels  was used in  with iterative detection on the MIMO channel. Here, a modified version of the sphere decoder  has eased the detection complexity of the multi-antenna signals. In , a LISt-Sequential (LISS)
23716	23717	classical algorithms operating on trees like the stack or the Fano algorithm, we can find the most promising candidate sequence with largest corresponding metric values. While the sphere decoder in  can be interpreted as a variant of the Fano algorithm, we propose a list variant of the stack algorithm which proceeds as follows while keeping track of all visited open nodes in an ordered stack
23716	23718	channels  was used in  with iterative detection on the MIMO channel. Here, a modified version of the sphere decoder  has eased the detection complexity of the multi-antenna signals. In , a LISt-Sequential (LISS) detector was shown to be a good approximation to APP (a posteriori probability) detection at reduced complexity. The LISS detector uses the stack algorithm known from
23716	23718	of the receiver. To avoid numerical problems, the evaluation of the L-value is normally done with logarithmic probabilities which simplify if we assume independence of the bits x. It was shown in  how the decision metric ? defined by the logarithmic probability 5s(ln p(y|x) + ln P (x)) can be rewritten as a cumulative metric for nR ? nT if we have an initial unconstrained estimate ˆs = (H H
23716	14655	to MIMO systems, where the design of multi-dimensional signal constellations lets us gain another degree of freedom. Furthermore, for serially concatenated codes it was stated as a design rule in , that the inner encoder must be a convolutional recursive encoder of rate 1 if an interleaver gain (gain of the iterative system) is desired. We are going to verify this condition for MIMO systems,
23716	23724	will consider both convolutional codes and parallel-concatenated convolutional (PCC or “Turbo”) codes. The interleaver is a random interleaver. Other forms of interleaving are possible, see e.g.,  for design rules of semi-random (S-random) interleavers for PCC codes. For the long frame lengths considered in this thesis, there are no large improvements to be expected from the design of
23716	23725	(ˆsnd+1, . . . , ˆsnT ) (ZF), and the soft bits (¯xnd+1,1, . . . , ¯xnT,M) from the a priori information (AP) resulting in the estimates ¯si = E{si}. These two approaches have been compared in  where the AP path augmentation has proved to give a slight advantage, which might be outweighed by the simpler calculation of the ZF augmentation. If we want to include these incomplete paths in
23716	23726	differential rate-1 pre-coder in the mapper (see Fig. 1). The mapping is no longer a symbol-by-symbol mapping, and the pre-coder operating in GF(2) is defined by the recursive state-space equations  ?k+1 = A?k + Bv (k) , (8) x (k) = C?k + Dv (k) , (9) transforming the length mnT vectors v (k) into vectors x (k) of the same size. The state vector of the recursive pre-coder at time k is
23727	23728	There are several projects being conducted to support inter-operability between a particular security infrastructure and Kerberos, for example supporting a single login for NetWare and Kerberos . This project has similar goals to Legion’s integration with Kerberos—in particular, no changes to Kerberos and no reduction in security in either security realm due to the single login. A
23727	23729	security contexts for each pair of communicating objects is intuitively too expensive, as the number of object-object communications is potentially quite large in the life of a computation. CRISIS  is the security architecture for the WebOS project at UC Berkeley. The WebOS provides many of the same high-level services as Legion. WebOS is fundamentally different than Legion in that while
23727	23730	or more simply just an RSA public key. The X.509 certificate in the LOID is not an X9.57 attribute certificate , but rather an ID certificate (also referred to as a public-key certificate ), and pairs a public key with a person’s name, organization, identification of the public key algorithm, and other information. A certificate may be signed by a certification authority (CA) that
23727	23731	must be used to hold users accountable and to track intruders. However, a solution to the metacomputer security problem requires significant additions over the uniprocessor security solution , particularly because the machines that comprise the metacomputer may not be administered by a single policy-making entity. A single metasystem can span multiple organizations, but only if each
23727	23731	and policies of the host systems. The material in this section is presented primarily to establish the context for the remaining sections. For more details regarding these mechanisms, see . 2.1 Identity Identity is fundamental to higher-level security services such as access control. Every Legion object is identified by a unique, multi-field, location-independent Legion Object
23727	15967	in Section 2, the work of Yialelis and Sloman is CORBA-based and does not address the generals¨ metasystems requirements, such as hardware heterogeneity and multiple administrative domains. Globus  is another metasystem research project, and as such is addressing many of the same issues as Legion. In many instances, convergent evolution has led to similar solutions to these problems. For
23727	23733	each other’s existence. Minsky and Ungureanu address the need of unifying heterogeneous security policies in distributed systems by introducing a formalism that describes various security policies . A unified mechanism is used to enforce the security schemes. This work is important for the construction and analysis of security policies in metasystems, in that the metasystem mechanism must
23727	23734	– including the method that is executed to modify these lists! Usually, these lists are modified through the use of a GUI. Currently, the use of a standardized access control language such as GAA  is being investigated. When a method call is received, the credentials it carB.foo() B Ok A MayI? foo() No! ries are checked by MayI and compared against the access control lists. For example, in
23727	23735	Host” in this section refers to a host that is executing the MIT source code distribution . This paper does not discuss efforts to integrate Kerberos directly with public key cryptography , because this paper focuses on integration with widely-deployed Kerberos systems. Similarly, the use of Proxiable tickets in Kerberos is not discussed, because their usage is not widely supported.
23727	23736	is responsible for remote process management, in much the same manner as the PCD. The manner in which Globus integrates with Kerberos is through use of the Generic Security Services API (GSS-API ). The level of granularity of the GSS-API and the Legion object model are fundamentally different: In GSS-API, two applications such as FTP and FTPD establish a security context and then
23727	376	policies in a single formalism is difficult if not impossible, severely impeding a metasystem’s deployment. Yialelis and Sloman describe a security framework for object-based distributed systems . This project is related to the work in Legion, because it attempts to allow the development of secure distributed applications on operating systems with varying degrees of security mechanisms
23738	23742	energy and power metrics at RTL level is discussed. Section 6 presents results using ITC’99 benchmarks. Finally, section 7 summarizes the conclusions. 2. RTL Test Preparation In a previous paper , the authors showed that test generated at RTL can be rewardingly reused in a production environment to improve the coverage of physical defects. In fact, random pattern-resistant faults, which
23738	23742	partially specified test vectors (masks), which drive the system under test into the functionality visited in a limited set of the input space. We refer to this functionality as dark-corners . Test quality of digital systems is frequently evaluated using the LSA (Line Stuck-At) fault model. However, more accurate fault models are used in this paper. The simulation environment uses a
23738	23742	both for interconnection and cell faults, are included in the VERIDOS tool for CMOS physical implementations . VERIDOS generates RTL fault lists according to the RTL fault models defined in , performs mixed RTL / logic level fault simulation and the WSA (Weighted Switching Activity) computation (the metric for energy / power estimation)  . Additionally, it computes the RTL IFMB
23738	23744	, can be detected with significantly shorter test lengths, if test is derived using RTL information. Then, it dramatically reduces the required energy for the BIST session. In a previous paper , the authors provided evidence that multiple detection of hard to detect RTL explicit and implicit faults leads to the detection of random pattern-resistant realistic faults at logic level, that
23738	23744	the “co-processor0” module (Cp0_ctr) and the “Booth multiplier or adder” module (MOAPpsum) from TORCH. The TPG process is performed in such a way that, after mask generation, as described in , the test pattern V = { v0, v1, ! , vN} is built of N = { N0( PR) + ? Ni(mask mi)} vectors, in which N0 are pseudo-random vectors and, for each mask mi, Ni vectors are generated. The unconstrained
23738	23752	detect faults, which are subsequently detected using weighted random pattern generation    or deterministic approaches . However, high LSA fault coverage does not guarantee high DC . Moreover, hard accessibility to parts of the structural description is expected to result from the synthesis of functional parts seldom exercised. Nevertheless, this information can be obtained at
23738	23757	BIST energy / power consumption can be reduced by means of: (I) vector selection and reduction of the number of vectors applied    , (II) TPG carried out for low-power BIST  , (III) circuit activity reduction during shift in the chain of a test-per-scan architecture  . The proposed BIST strategy consists in the customization of the pseudo-random test vectors,
23738	23758	of the number of vectors applied    , (II) TPG carried out for low-power BIST  , (III) circuit activity reduction during shift in the chain of a test-per-scan architecture  . The proposed BIST strategy consists in the customization of the pseudo-random test vectors, generated on-chip with a LFSR (Linear Feedback Shift Register) for instance, with partially
23738	7139	above, energy and power consumption are evaluated during BIST mask preparation at RTL level. Different methods and models exist to estimate energy / power at this level of description, like   . However, they do not use the specific nature of the problem under consideration. In this paper, the model proposed for test energy / power estimation uses this special characteristic of
23738	23761	energy and power consumption are evaluated during BIST mask preparation at RTL level. Different methods and models exist to estimate energy / power at this level of description, like   . However, they do not use the specific nature of the problem under consideration. In this paper, the model proposed for test energy / power estimation uses this special characteristic of test
23738	19495	near 0.8 µC is observed. This linear shape can be explained as follows: (I) Since the TPG is of type pseudo-random, static probability and transition density of signals are both time invariant . Consequently, the internal nodes of the circuit will present a similar situation as well. According to this, cycle energy Ek will have a stable value if the test length is long enough. These facts
23738	21454	to all internal nodes of the circuit. Even when no detailed information about gates exists, estimation of the number of gates is appropriately made based on the complexity of the functionality  . These types of simulators require of P and D statistics to be defined for each input node. According to this, ? and ? parameters are obtained from expression (4) after modifying inputs P and
23738	7132	to all internal nodes of the circuit. Even when no detailed information about gates exists, estimation of the number of gates is appropriately made based on the complexity of the functionality  . These types of simulators require of P and D statistics to be defined for each input node. According to this, ? and ? parameters are obtained from expression (4) after modifying inputs P and D
23763	919	coupled with a reduction in sending rate to react to congestion, resulting in degraded throughput and increased latency. Internet paths often experience outages lasting several minutes , and endto-end connections that are in the middle of data transfers usually end up aborting when such outages happen. Over the past few years, both routing optimizations at the IP layer [24, 29,
23763	919	that when a problem occurs with the current path or when a better path presents itself, traffic is rerouted appropriately to reduce the observed loss rate. Inspired by the approach used in RON , we focus on a simple but effective overlay routing method that uses at most one intermediate node in the overlay network to forward packets. We analyze fourteen days of probes between 30
23763	919	these techniques generally operate over long time-scales. As a result of current backbone routing’s ignorance of short-term network conditions, the route taken by packets is frequently sub-optimal . Recent network path selection products  attempt to provide more finegrained, measurement-based path selection for single sites. Recent research in overlay networks has attempted to
23763	919	to determine if the remote host is down. The paths are selected based upon the average loss rate over the last 100 probes. These are similar to the parameters used in an earlier evaluation of RON , but the interval between probes is five seconds longer. ¥ ¤¢¤ S ©¢© ¨¢¨ 2?redundant Multipath Routing Best Path from probes § ¦¢¦ £¢£ ¡¢¡ Figure 1: 2-Redundant multipath routing and best path
23763	919	probabilitys? ,s? ? ?¢¡¤£ ?¦¥ ?¨§?©???? ? ¤?s? ? Reactive routing is constrained to the latency of the best path, as well. In general, the path with the best loss rate may not have the best latency . Cost: The cost of all-paths probing and route dissemination is fixed—each host must send and receive £¥¤§¦ ¨?? data. The cost is not dependent upon the amount of traffic in the flow; hence, it can
23763	23764	such applications, we examine loss-resilient routing strategies that do not dramatically increase end-to-end round-trip latencies. Hop-by-hop ARQ schemes can reduce the delay for certain topologies , but require buffering and network support at intermediate nodes. Many ARQ schemes are tuned for certain loss characteristics, and function poorly over channels outside of their design space. While
23763	23765	split the transfer of information over multiple network paths to provide enhanced reliability and performance. Simulation results and analytic studies have shown the benefits of this approach . Chen evaluated the use of parallel TCP flows to improve performance, but did not examine failures, or real Internet paths . In addition, researchers have suggested combining redundant coding
23763	23766	between sites , and the designers of the Opus overlay system have proposed the future use of redundant transmission in an overlay, but, to our knowledge, have not yet evaluated this technique .s2.4 Sources of shared failures Multi-path and alternate-path routing schemes make generous assumptions about path independence that may not hold when considering typical Internet paths, as we show
23763	7031	or real Internet paths . In addition, researchers have suggested combining redundant coding with dispersity routing to improve the reliability and performance of both parallel downloads  and multicast communication . Akamai is reported to use erasure codes to take advantage of multiple paths between sites , and the designers of the Opus overlay system have proposed the
23763	6324	or missing . FEC is commonly used in wireless systems to protect against bit corruption , and more recently in multicast and content distribution systems to protect against packet loss . The latter applications require packet—as opposed to bit—level FEC. We consider packet-level FEC in this paper. Sending redundant data along the same Internet path is rarely completely effective
23763	23767	and analytic studies have shown the benefits of this approach . Chen evaluated the use of parallel TCP flows to improve performance, but did not examine failures, or real Internet paths . In addition, researchers have suggested combining redundant coding with dispersity routing to improve the reliability and performance of both parallel downloads  and multicast communication
23763	23770	the same physical location . We also recently observed that many failures manifest themselves near the network edge, where routing protocols are less likely to be able to route around them . Seemingly unrelated network prefixes often exhibit similar patterns of unreachability because of their shared infrastructure . Network failures are not only caused by external factors, but may
23763	23771	causes of loss in the wired Internet. FEC adds redundant information to a stream, allowing the stream to be reconstructed at the receiver even if some of the information is corrupted or missing . FEC is commonly used in wireless systems to protect against bit corruption , and more recently in multicast and content distribution systems to protect against packet loss . The latter
23763	6493	of content delivery for specific applications such as HTTP and streaming video. Overcast and other application level multicast projects attempt to optimize routes for bandwidth or latency . 2.3 Multi-path routing The success of traffic engineering and overlay routing indicates the presence of redundant routes between many pairs of Internet hosts. A variety of approaches have been
23763	7178	coupled with a reduction in sending rate to react to congestion, resulting in degraded throughput and increased latency. Internet paths often experience outages lasting several minutes , and endto-end connections that are in the middle of data transfers usually end up aborting when such outages happen. Over the past few years, both routing optimizations at the IP layer [24, 29,
23763	7178	be exacerbated by link failures, routing problems, or both. Labovitz et al. show that routers may take tens of minutes to stabilize after a fault, and that packet loss is high during this period . They also note that route availability is not perfect, causing sites to be unreachable some fraction of the time . Paxson notes that packets are often subject to routing loops and other
23763	7969	to stabilize after a fault, and that packet loss is high during this period . They also note that route availability is not perfect, causing sites to be unreachable some fraction of the time . Paxson notes that packets are often subject to routing loops and other pathologies . 2.1 Reliable transmission The traditional way to mask losses in packetized data transfer is to use packet
23763	7970	also note that route availability is not perfect, causing sites to be unreachable some fraction of the time . Paxson notes that packets are often subject to routing loops and other pathologies . 2.1 Reliable transmission The traditional way to mask losses in packetized data transfer is to use packet diversity through retransmissions, forward error correction (FEC), or a combination of the
23763	20453	of back-to-back packets is high when the packets are closely spaced (¡ tional loss probability when the gap is ¡¤£¦¥§¥ ms. Similarly, Paxson examined TCP bulk transfers between 35 sites in 1997 . In this work, he found that the conditional loss probability of data packets that were queued together was 10–20 times higher than the base loss rate. We compare our loss probabilities with those
23763	20453	probe types, including back-to-back direct packets. £ datasets, we examined a wider Bolot  examined packets separated by 8 ms, and found that their conditional loss probability was 60%. Paxson  examined TCP packets that queued together at a router, finding their conditional loss probability to be about 50%. In our experiments, back-toback packets had a higher conditional loss
23763	6338	many pairs of Internet hosts. A variety of approaches have been developed to leverage the existence of multiple, simultaneous paths through multi-path routing. Dispersity routing  and IDA  split the transfer of information over multiple network paths to provide enhanced reliability and performance. Simulation results and analytic studies have shown the benefits of this approach [5,
23763	23778	or missing . FEC is commonly used in wireless systems to protect against bit corruption , and more recently in multicast and content distribution systems to protect against packet loss . The latter applications require packet—as opposed to bit—level FEC. We consider packet-level FEC in this paper. Sending redundant data along the same Internet path is rarely completely effective
23763	23778	the noloss case—the so called standard codes. Reed-Solomon erasure codes are a standard FEC method that provide a framework with which to apply variable amounts of redundancy to groups of packets . As a simpler case, packets can simply be duplicated and sent along multiple paths, as is done in mesh routing . We restrict our evaluation to using this simple encoding over two paths,
23763	6499	these techniques generally operate over long time-scales. As a result of current backbone routing’s ignorance of short-term network conditions, the route taken by packets is frequently sub-optimal . Recent network path selection products  attempt to provide more finegrained, measurement-based path selection for single sites. Recent research in overlay networks has attempted to
23763	23780	end up aborting when such outages happen. Over the past few years, both routing optimizations at the IP layer  and overlay networks layered on top of the Internet routing substrate  have been proposed as ways to improve the resilience of packet delivery to these problematic conditions. These approaches either probe to find a single best path through the Internet, or send data
23763	23780	failure of all other paths from the source to the destination. Mesh routing is the simplest way to add redundant packets to the data stream by duplicating all of the packets along different paths . In this scheme, the overhead is due to redundant packets, but the scheme does not require additional probing. When its paths are disjoint, mesh routing is resilient to the failure of any proper
23763	23780	In addition, researchers have suggested combining redundant coding with dispersity routing to improve the reliability and performance of both parallel downloads  and multicast communication . Akamai is reported to use erasure codes to take advantage of multiple paths between sites , and the designers of the Opus overlay system have proposed the future use of redundant transmission
23763	23780	routing Redundant multi-path routing sends redundant data down multiple paths, such that a certain fraction of lost packets can be recovered. In this study, we consider 2-redundant mesh routing , in which each packet is sent to the receiver twice, one on each distinct paths. In the most basic scheme, the first packet is sent directly over the Internet, and the second is sent through a
23763	23780	framework with which to apply variable amounts of redundancy to groups of packets . As a simpler case, packets can simply be duplicated and sent along multiple paths, as is done in mesh routing . We restrict our evaluation to using this simple encoding over two paths, so-called 2-redundant routing, since we believe the number of truly loss-independent paths between two points on the
23782	23783	range-dependent spectral characteristics leading to a poor estimate of the CM at the considered range. Among methods proposed to perform the rangedependence compensation, the method described in  is able to perform an exact range compensation for any range and for any monostatic (MS) or bistatic (BS) configuration. This method is based on the registration, prior to averaging, of the clutter
23782	23783	BS scenario and range. Furthermore, with the exception of the MS sidelooking configuration (and some particular BS configurations), the shape of the clutter PS locus varies with the BS range , . By stacking clutter PS loci corresponding to increasing BS ranges, one obtains a surface that helps un?ssderstand the estimation method. This surface is illustrated in Fig. 2 for two
23782	23783	estimation accuracy is also evaluated. Subsequently, the end-to-end performance of the parameter estimation method combined with a variant of the range-dependence compensation method proposed in  is reported. 4.1. Parameter estimation performance Figure 7 represents cuts in the clutter PS locus surface showing the true clutter PS locus and the estimated clutter PS locus. In the case of
23782	23783	of the PS of the clutter snapshots. On the other hand, the CM estimate whose PS is depicted in Fig. 11(b) was obtained using a variant of the rangedependence compensation method proposed in  combined with the configuration-parameter estimation method proposed in this paper. Clearly, the PS of the latter estimate closely follows the true clutter PS locus. The parameter estimation method
23782	23783	of the PS of the clutter snapshots. On the other hand, the CM estimate whose PS is depicted in Fig. 13(b) was obtained using a variant of the range-dependence compensation method proposed in  combined with the configuration-parameter estimation method proposed in this paper. Clearly, the PS of the latter estimate closely follows the true clutter PS locus. Figure 14 shows the SINR loss
23782	23785	a global minimum search is performed, resulting in the simultaneous estimation of all the parameters. More details about this method and the other guided minimizations examined can be found in . ?s (b) 4. Results In this section, the performance of the parameter estimation method are first evaluated in terms of accuracy of the clutter PS locus estimation. The metric used to this end is
23782	23787	coordinates. On the other hand, some particular pulse-to-pulse and element-to-element signal decorrelation  such as ICM also causes a jitter in the angle-Doppler location of the peaks. In , the proposed method was shown to be also essentially insensitive to ICM. The reason was that, as long as the distribution of the location jitter is symmetric around the true (mean) location value,
8922269	23792	(PseudoBase) for RNA pseudoknots has been constructed . Unfortunately, it is known that cfg cannot represent pseudoknot structure and a few grammars have been proposed to represent pseudoknots . However, the relation among the expressive (generative) power of these grammars and/or other grammars in formal language theory beyond cfg has not been clarified. The authors have proposed a class
8922269	23792	G with dim(G) ? m and rank(G) ? r. Also, let m-MCFL= ? r?1 (m, r)-MCFL and MCFL=?m?1 m-MCFL. The following shows inclusion relation among the classes of languages. MCFL ? 2-MCFL ? (2,2)-MCFL ? REL  ? HL  = TAL  ? ESL-TAL  ? (SL-TAL  ? CFL) REL is the class of languages generated by grammars introduced in . Especially, {a n 1 an 2 an 3 an 4 an 5 0}?MCFL\2-MCFL by Lemma 3.3 of
8922269	23793	? CFL) REL is the class of languages generated by grammars introduced in . Especially, {a n 1 an 2 an 3 an 4 an 5 0}?MCFL\2-MCFL by Lemma 3.3 of , L6,2 ?2-MCFL\(2,2)-MCFL by Theorem 1 of , {am 1 am2 bn1 bn2 cm1 cm2 dn1 dn2 | m, n ? 0} ?REL\HL (or TAL) by Lemma 4.15 of , {anbncn | n ? 0} ? SL-TAL\CFL by Proposition 2 of , {?ak 1bk1 ?al2 bl2 ?am3 bm3 ?an4 bn4 ? | k, l, m, n ? 0}
23800	13119	of thes494 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 12, NO. 3, JUNE 2004 energy required for receiving. For example, Stemm and Katz measure that the idle:receive:send ratios are 1:1.05:1.4 , while the Digitan wireless LAN module (IEEE 802.11/2 Mb/s) specification shows idle:receive:send ratios is 1:2:2.5 . Most sensor networks are designed to operate for long time, and nodes will
23800	13119	and is mainly built on the research protocol MACAW . It is widely used in ad hoc wireless networks because of its simplicity and robustness to the hidden terminal problem. However, recent work  has shown that the energy consumption using this MAC is very high when nodes are in idle mode. This is mainly due to the idle listening. 802.11 has a power-save mode, and we will discuss it
23800	4875	multihop communications to conserve energy . Most communications will occur between nodes as peers, rather than to a single base station. In-network processing is critical to network lifetime , and implies that data will be processed as whole messages in a store-and-forward fashion. Packet or fragment-level interleaving from multiple sources only increases overall latency. Finally, we
23800	23803	overhearing. An important feature of wireless sensor networks is the in-network data processing. It greatly reduces energy consumption compared to transmitting all the raw data to the end node ???. Techniques such as data aggregation can reduce traffic, while collaborative signal processing can reduce traffic and improve sensing quality. In-network processing requires store-and-forward
23800	5746	sensor nodes, Motes, 1 developed by the University of California, Berkeley, and manufactured by Crossbow Technology, Inc. 2 The mote runs on a very small event-driven operating system called TinyOS . We evaluated S-MAC design tradeoffs on this platform. The contributions of this paper are as follows. An implemented low-duty-cycle scheme in multihop networks that significantly reduces energy
23800	5746	all the energy-conserving features of S-MAC. We use Motes as our development platform and testbed. The motes are running TinyOS, an efficient event-driven operating system for tiny sensor nodes3 . A. First Implementation on Rene Motes An early implementation of S-MAC is on Rene motes, which has the Atmel AT90LS8535 microcontroller  and the TR1000 radio transceiver from RF Monolithics,
23800	5010	technique of adaptive listening that greatly reduces the latency caused by periodic sleeping. Use of in-channel signaling to avoid energy waste on overhearing, extending the work of PAMAS . Applying message passing to reduce application-perceived latency and control overhead. Experimental measurement and evaluation of S-MAC performance on energy, latency and throughput over
23800	5010	that the energy consumption using this MAC is very high when nodes are in idle mode. This is mainly due to the idle listening. 802.11 has a power-save mode, and we will discuss it shortly. PAMAS  made an improvement on energy savings by trying to avoid the overhearing among neighboring nodes. Our paper also exploits the same idea. The main difference of our work with PAMAS is that we do not
23800	5010	As a result, each node overhears many packets that are not directed to itself. It is a significant waste of energy, especially when node density is high and traffic load is heavy. Inspired by PAMAS , S-MAC tries to avoid overhearing by letting interfering nodes go to sleep after they hear an RTS or CTS packet. Since DATA packets are normally much longer than control packets, the approach
23800	15310	latency and control overhead. Experimental measurement and evaluation of S-MAC performance on energy, latency and throughput over sensor-net hardware. The early work of S-MAC was published in . This paper includes significant extensions in the protocol design, implementation, and experiments. II. RELATED WORK MAC is a broad research area, including work in the new area of low-power and
23800	18632	extensions in the protocol design, implementation, and experiments. II. RELATED WORK MAC is a broad research area, including work in the new area of low-power and wireless sensor networks ???. Current MAC design for wireless sensor networks can be broadly divided into contention-based and TDMA protocols. a) Contention-Based MACs: The standardized IEEE 802.11 distributed
23800	18632	fairness, and even trades it off for further energy savings. Finally, we look at some work on low-duty-cycle operation of nodes, which are closely related to S-MAC. The first example is Piconet , which is an architecture designed for low-power ad hoc wireless networks. Piconet also puts nodes into periodic sleep for energy conservation. However, there is no coordination and synchronization
23800	18633	time slot assignment. So its scalability is normally not as good as that of a contention-based protocol. For example, Bluetooth may have at most eight active nodes in a cluster. Sohrabi and Pottie  proposed a self-organization protocol for wireless sensor networks. Each node maintains a TDMA-like frame, called a super frame, in which the node schedules different time slots to communicate with
23800	10128	and there is no contention-introduced overhead and collisions. However, using TDMA protocol usually requires the nodes to form real communication clusters, like Bluetooth , , and LEACH . Most nodes in a real cluster are restricted to communicate within the cluster. Managing inter-cluster communication and interference is not an easy task. Moreover, when the number of nodes within
23800	20039	extensions in the protocol design, implementation, and experiments. II. RELATED WORK MAC is a broad research area, including work in the new area of low-power and wireless sensor networks ???. Current MAC design for wireless sensor networks can be broadly divided into contention-based and TDMA protocols. a) Contention-Based MACs: The standardized IEEE 802.11 distributed coordination
23800	20039	of the scheme is its low bandwidth utilization. For example, if a node only has packets to be sent to one neighbor, it cannot reuse the time slots scheduled to other neighbors. Woo and Culler  examined different configurations of carrier sense multiple access (CSMA) and proposed an adaptive rate control mechanism, whose main goal is to achieve fair bandwidth allocation to all nodes in a
23800	20039	However, we suggest that algorithms without requiring overhearing may be a better match to energy-limited networks. For example, S-MAC uses explicit data acknowledgments rather than implicit ones . B. Message Passing This subsection describes how to efficiently transmit a long message in both energy and latency. A message is the collection of meaningful, interrelated units of data. The
23800	5023	a) Contention-Based MACs: The standardized IEEE 802.11 distributed coordination function (DCF)  is an example of the contention-based protocol, and is mainly built on the research protocol MACAW . It is widely used in ad hoc wireless networks because of its simplicity and robustness to the hidden terminal problem. However, recent work  has shown that the energy consumption using this MAC
23800	5023	the 802.11 does a very good job on collision avoidance. S-MAC follows similar procedures, including virtual and physical carrier sense, and the RTS/CTS exchange for the hidden terminal problem . There is a duration field in each transmitted packet that indicates how long the remaining transmission will be. If a node receives a packet destined to another node, it knows how long to keep
23800	3696	of the radio is reduced and there is no contention-introduced overhead and collisions. However, using TDMA protocol usually requires the nodes to form real communication clusters, like Bluetooth , , and LEACH . Most nodes in a real cluster are restricted to communicate within the cluster. Managing inter-cluster communication and interference is not an easy task. Moreover, when the
23800	18635	to S-MAC is that Fig. 1. Periodic listen and sleep. the PS mode in 802.11 is designed for a single-hop network, where all nodes can hear each other, simplifying the synchronization. As observed by , in multihop operation, the 802.11 PS mode may have problems in clock synchronization, neighbor discovery and network partitioning. In fact, the 802.11 MAC in general is designed for a single-hop
23800	18635	that all nodes are synchronized together. Finally, although 802.11 defines PS mode, it provides very limited policy about when to sleep. Whereas in S-MAC, we define a complete system. Tseng et al.  proposed three sleep schemes to improve the PS mode in the IEEE 802.11 for its operation in multihop networks. Among them the one named periodically-fully-awakeinterval is the most closest to the
23800	5749	note that in some cases overhearing is indeed desirable. Some algorithms may rely on overhearing to gather neighborhood information for network monitoring, reliable routing or distributed queries . If desired, S-MAC can be configured to allow application specific overhearing to occur. However, we suggest that algorithms without requiring overhearing may be a better match to energy-limited
23800	23806	first field. In this way, each packet buffer includes all header fields from all lower layers. Therefore, it avoids memory copies across layers. Details of our stack implementation are described in . Some important parameters are listed in Table I. We use Manchester code as the channel coding scheme. It is a robust DC-balanced code, and has a overhead of 1:2. That is, each data bit becomes two
23811	23812	Stanford University, CA 94305-4065. 1sThe smaller µ(P ) is, the faster the Markov chain converges to its stationary distribution. For more rigorous statements and background, see, e.g.,  and references therein. The second-largest eigenvalue modulus can also be expressed as the spectral norm of P restricted to the subspace 1 ? , i.e., the subspace of all vectors whose components
23811	23813	see, e.g.,  for pointers to the literature. Thus the analysis and design of fast mixing Markov PSfrag chains, replacements with given stationary distribution, has become a research area. In , we show how to numerically find the fastest mixing Markov chain (i.e., the one with smallest secondlargest eigenvalue modulus) on a given underlying graph using tools of convex optimization, in
23811	23813	Stanford University, CA 94305-4065. 1sThe smaller µ(P ) is, the faster the Markov chain converges to its stationary distribution. For more rigorous statements and background, see, e.g.,  and references therein. The second-largest eigenvalue modulus can also be expressed as the spectral norm of P restricted to the subspace 1 ? , i.e., the subspace of all vectors whose components
23811	23815	Stanford University, CA 94305-4065. 1sThe smaller µ(P ) is, the faster the Markov chain converges to its stationary distribution. For more rigorous statements and background, see, e.g.,  and references therein. The second-largest eigenvalue modulus can also be expressed as the spectral norm of P restricted to the subspace 1 ? , i.e., the subspace of all vectors whose components
23811	23817	fastest mixing Markov chain (i.e., the one with smallest secondlargest eigenvalue modulus) on a given underlying graph using tools of convex optimization, in particular, semidefinite programming . The present note presents a simple, self contained example where the optimal Markov chain can be identified analytically. We consider a random walk on a path with n ? 2 nodes, labeled 0, 1, . . .
23839	23840	of queries for specific values of certain properties of the files. More recent approaches that embody this idea are Dourish et al’s Placeless Documents  and Ricardo Baeza-Yates et al’s PACO . These approaches, however, shift the burden from classifying the documents to the need for the user to remember the names and possible values of an arbitrary set of properties. We argue that
23839	23841	Several approaches have appeared that try to make use of additional information for document organization and retrieving. For some, time is the most relevant factor. It is the case of Lifestreams , in which all documents are organized on a temporal stream that can be navigated or filtered. In the Timescape system , the desktop displays collections of objects bound to a certain time
23839	23841	directories whose contents are the result of queries for specific values of certain properties of the files. More recent approaches that embody this idea are Dourish et al’s Placeless Documents  and Ricardo Baeza-Yates et al’s PACO . These approaches, however, shift the burden from classifying the documents to the need for the user to remember the names and possible values of an
23839	23842	have the disadvantage that, by giving time a special role, they can disregard other potentially useful information. More general are the approaches based on Gifford’s Semantic File Systems . In them, there are no directories or folders per se. Instead, the user faces virtual directories whose contents are the result of queries for specific values of certain properties of the files.
23839	23847	For some, time is the most relevant factor. It is the case of Lifestreams , in which all documents are organized on a temporal stream that can be navigated or filtered. In the Timescape system , the desktop displays collections of objects bound to a certain time period, and can be moved to past or future states. These approaches have the disadvantage that, by giving time a special role,
23839	23848	just storing them in undifferentiated collections (‘piles’) andsresorting to additional clues, such as their location, to find them. More recently, while looking at the usage of email tools , Whittaker et al witnessed that some people use those tools not just to read and send email, but overloading them with other functions for which those tools were not designed for, such as managing
23850	23851	in military organizations, where lessons have been captured and stored for more than ten years . Nonetheless, these repository systems are not yet promoting knowledge reuse. Recent work  associated reuse impediments with lesson distribution, format, and collection. Using an intelligent user interface strategy, LET overcomes the reuse obstacles imposed by the collection methods
23850	23853	the remainder of this paper, we describe LET’s architecture and methods. LESSON ELICITATION TOOL (LET) LET was designed to implement the lesson collection process. Like some information extraction  methods, it focuses on obtaining the values for a template. However, instead of extracting values and expressions from text, LET coaxes information from users. LET asks questions, starts sentences,
23850	23856	capture, validation, storage, distribution, and reuse of organizational lessons. LLS are ubiquitous in military organizations, where lessons have been captured and stored for more than ten years . Nonetheless, these repository systems are not yet promoting knowledge reuse. Recent work  associated reuse impediments with lesson distribution, format, and collection. Using an intelligent
23897	23906	is the field of algorithm animation. Some of the most famous examples are Ronald Baecker’s motion picture, “Sorting Out Sorting” , Marc Brown’s research systems BALSA  and ZEUS , and John Stasko’s Tango . The original 8smotivation for algorithm animation was to explain an algorithm to an audience for educational purposes. Since then it has been applied to a number
23897	23907	flow. Because monitoring employs a programming paradigm that has been heavily studied, many coding techniques developed for graphical user interface programming, such as the use of callbacks , are applicable to monitors. Several of the example EMs in subsequent chapters use a callback model to take advantage of a higher-level monitoring abstraction available by means of a library
23897	23914	or to continuously update (animate) a graphic display to show dynamic behavior as program execution commences. Examples of such tools are the MemMon system for dynamic storage visualization  and the Incense data structure visualization tool . The best-known area of program visualization is the field of algorithm animation. Some of the most famous examples are Ronald Baecker’s
23897	23914	instrumentation. The run-time system instrumentation is an extension and generalization of an earlier special-purpose monitoring facility oriented around dynamic memory allocation and reclamation . It also supercedes the language’s built-in procedure tracing mechanism . 15sEvent masks — Monitor control over target program execution is coupled with the concept of filtering
23897	23914	from the heap string and block regions, including size and type information. This instrumentation is based on earlier instrumentation added to Icon for a memory monitoring and visualization system . ? Garbage collections including the storage region being collected (Icon has separate regions for strings and data structures), the memory layout after compaction, and the completion of garbage
23897	23914	with the objective of providing information for improving the implementation. Nevertheless, prior research in the monitoring and visualization of memory usage led to improved allocation heuristics , and observation of EMs under MT Icon also suggested improvements to the implementation. For example, monitoring of list-creation events led to a change in list concatenation with the result that
23897	23914	is in the area of garbage collection monitoring. The MemMon system is able to provide very detailed information about Icon’s marking and compaction algorithms through a file-based event stream . This information has proven useful in practice, but there is no way to safely report events during a garbage collection in MT Icon. An event report causes transfer of control and execution in an
23897	23920	relevant definitions, a description of the programming interface and underlying interpreter instrumentation are given. Additional programmer’s reference material is available for these facilities . 5.1 Terminology The terminology used in discussing execution monitoring relates to events and the linguistic features associated with them. These terms are used throughout the rest of the
23897	23920	or manipulate its value, etc. 5.2 Obtaining events A standard library is available for use by EMs in order to provide a means of obtaining events. The library is described more completely in . Programs wishing to use the standard library include a link declaration such as link evinit. Setting up an event stream An EM first sets up a source of events; the act of monitoring then consists
23897	23920	procedures are useful in EMs. This section presents those library procedures that are used in the examples in the rest of this dissertation; the rest are described in the evinit library reference . 37sLocation decoding and encoding procedures are useful in processing location change event values, but they are also useful in other monitors in which two-dimensional screen coordinates must be
23897	23920	Codes The following list of event codes is provided in order to give a general indication of the extent of instrumentation discussed in Chapter 5. More information on these codes is presented in . Classes of events AllocMask Memory allocation events AssignMask Assignment events TypeMask Events related to Icon data types ConvMask Type conversion events ListMask List operation events
23897	23923	a co-expression that may be from a task other than the one being executed. Functions that generate can produce more than one result from a given call. There are other inter-task access functions;  serves as a reference for MT Icon programming. 28sglobalnames(C) generates the names of C’s global variables. keyword(s, C) produces keyword s in C. localnames(C) generates the names of C’s local
23897	23926	monitoring systems. The method used to obtain information is limited by and often motivates the execution model adopted by a monitoring system. The most common methods are run-time instrumentation , manual instrumentation , interpreter instrumentation , and instrumenting compilers . In addition to various methods of instrumentation, some systems
23897	23926	as more general profiling and monitoring systems that modify the code at run-time. 2.4.1 Dbx Dbx is representative of conventional source level debuggers, the most common form of execution monitor . Source-level debuggers vary widely in their capabilities, but the features of dbx are illustrative of this class of monitors: ? The basic interface is textual in nature. The user specifies both
23897	23926	a subset of the source language) in order to determine whether the debugger should be invoked. Unfortunately, conditional breakpoints are “so slow that using this capability is often not practical” . ??? Program variables can be displayed along with their values; in the case of structures, elements can be displayed and traversed. ? The procedure call chain can be displayed, including parameters
23897	23934	debugging code could be written separately and compiled in with programs when debugging was needed. 11s2.6.2 PECAN PECAN is an integrated programming environment for an extended dialect of Pascal . It employs multiple views of the static aspects of the program from a single underlying abstract syntax tree. PECAN also includes execution monitoring facilities and can display the current line
23897	23935	After a general discussion of monitor coordinators, an example monitor coordinator is presented that implements a generalization of the selective broadcast communication paradigm advocated by Reiss . Other paradigms of monitor coordination are possible within the framework. In addition, other generalizations of selective broadcast proposed in the literature may prove complementary to the one
23897	23938	run-time system events; further, Masnavi notes SeePS suffers from efficiency problems. 12s2.6.6 Dynascope Dynascope is a tool for directing the execution of C language programs using event streams . Event streams are not at the source-level, but rather at the level of the machine instruction for an hypothetical processor 1 . Events are produced during the interpretation of code by a virtual
23897	23940	a low-level architecture typical of current RISC chips. 13s2.7.3 SMLD The debugger for standard ML, SMLD, is based upon extensive, automatic instrumentation of the program code during compilation . Compiler optimizations reduce the slowdown and code size blowup implied by the instrumenting compiler technique. The instrumentation supports relatively standard debugging features such as setting
23943	12102	semantic information, this tool provides just a syntactic transformation. The key challenge is to map the XML data used by traditional Web Services to classes in an ontology. Currently, Patil et al  are also working on matching XML schemas to ontologies in the Web Services domain. They use a combination of lexical and structural similarity measures. They assume that the user’s intention is not
23943	23945	that the user’s intention is not to annotate similar services with one common ontology, rather they also address the problem of choosing the right domain ontology among a set of ontologies. Sabou  addresses the problem of creating suitable domain ontologies in the first place. She uses shallow natural language processing techniques to assist the user in creating an ontology based on software
23943	23946	it is already very helpful for a human annotator if he or she would have to choose only between a small number of ontological concepts rather than from the full domain ontology. In previous work  we have shown that the category of a services can be reliably predicted, if we stipulate merely that the correct concept be one of the top few (e.g., three) suggestions. Terminology. Before
23943	23946	is to exploit the fact that there are dependencies between the category of a Web Service, the domains of its operations and the datatypes of its input and output parameters. In previous work , we exploited these dependencies in a Bayesian setting and evaluated it on Web forms. In this paper, we present an iterative classification algorithm similar to the one introduced by Neville and
23943	23946	complex types were added to the text used by the datatype classifier classifying the complex type itself. In Fig. 2 we denote this as “add”. Note that this appears to contradict our earlier results , where we claimed that simply adding text from child nodes does not help. In , we where classifying the category level only, and the bag of words for the domain and datatype classifiers
23943	23946	by hand for each of the three groups. The partitions generated by OATS were compared to these reference partitions. In our evaluation, we used the definition of precision and recall proposed by  to measure the similarity between two partitions. We ran a number of tests on each domain. We systematically vary the HAC termination threshold, from one extreme in which each element is placed in
23943	23947	these dependencies in a Bayesian setting and evaluated it on Web forms. In this paper, we present an iterative classification algorithm similar to the one introduced by Neville and Jensen in . Like any classification system, our algorithm is based on a set of features of the services, operations and parameters. Following , we distinguish between intrinsic and extrinsic features. The
23943	4762	In their approach, one single classifier is trained on all (intrinsic and extrinsic) features. In a variety of tasks, ensembles of several classifiers have been shown to be more effective (e.g., ). For this reason, we train two separate classifiers, one on the intrinsic features (“A”) and one on the extrinsic features (“B”), and vote together their predictions. Another advantage of
23943	23950	with a confidence value of 96% according to a Wilcoxon Test. For both significance tests we performed a 20-fold random split. Related work. We already mentioned the algorithm by Neville and Jensen , but iterative classification algorithms were also used for link-based hypertext classification by Lu and Getoor . Relational learning for hypertext classification was also explored by Slattery
23943	23952	classification algorithms were also used for link-based hypertext classification by Lu and Getoor . Relational learning for hypertext classification was also explored by Slattery et al., e.g. . A difference between their problem setting and ours is that the links in our dataset are only within one Web Services, where in the hypertext domain potentially all documents can link to each
23943	23953	classification algorithms were also used for link-based hypertext classification by Lu and Getoor . Relational learning for hypertext classification was also explored by Slattery et al., e.g. . A difference between their problem setting and ours is that the links in our dataset are only within one Web Services, where in the hypertext domain potentially all documents can link to each
23943	23954	each operation with the related arguments should ensure that the instance data of related elements will closely correspond, increasing the chances of identifying matching elements. As in ILA , this probe-based approach is based on the assumption that the operations overlap, i.e, there exists a set of real-world entities that are covered by all of the sources to be aggregated. For
23943	23954	are well suited to scenarios such as Web Service aggregation where instance data can be actively requested. OATS has been influenced specifically by two prior research efforts: LSD  and ILA . LSD uses similarities computed at both the schema and the instance level. LSD treats schema matching as a classification task – a number of trained learner-modules predict labels for each element
23943	23955	element is placed in its own cluster, to the other extreme in which all elements are merged into one large cluster. The ensemble of distance metrics was selected from Cohen’s SecondString library . We chose eight representative metrics, consisting of a variety of csF1 0.6 0.5 0.4 0.3 0.2 0.1 address city state fullstate zip areacode lat long icao 110 135th Avenue New York NY New York 11430
23943	23959	methods are well suited to scenarios such as Web Service aggregation where instance data can be actively requested. OATS has been influenced specifically by two prior research efforts: LSD  and ILA . LSD uses similarities computed at both the schema and the instance level. LSD treats schema matching as a classification task – a number of trained learner-modules predict labels for
8922282	13159	when the TCP algorithm is Reno-like, i.e., employs additive-increase-multiplicative-decrease strategy. 1 Algorithm The design rationale of REM (Random Exponential Marking) is explained in . The REM algorithm at each link (queue) l in each update period t is given by: pl(t + 1) = max {0, pl(t) + ?(?l(bl(t) ? b ? l ) + xl(t) ? cl} (1) prob(t + 1) = 1 ? ? ?pl(t+1) The variables are (see
8922282	2237	= min {0.15, max {0, prob(t) + 0.005?(bl(t + 1) ? (?lb ? l + (1 ? ?l)bl(t)))}} This version involves only queue lengths and not rates or capacity. It is equivalent to the PI controller developed in . We have retained the original version of the REM algorithm in ns-2 to provide more flexibility in choosing queue target b ? l of probability. and experimenting with other TCP algorithms that allow
8521970	19322	or round trip delay, or explicitly through AQM. Different schemes adopt different measures of congestion, e.g., TCP Reno (Jacobson 1988, Stevens 2000) measures congestion by packet loss, TCP Vegas (Brakmo and Peterson 1995) by queuing (excluding propagation) delay (Low et al. 2001b), RED (Random Early Detection) (Floyd and Jacobson 1993) by queue length, and REM (Random Exponential Marking) (Athuraliya et al. 2001)
8521970	2242	(Jacobson 1988, Stevens 2000) measures congestion by packet loss, TCP Vegas (Brakmo and Peterson 1995) by queuing (excluding propagation) delay (Low et al. 2001b), RED (Random Early Detection) (Floyd and Jacobson 1993) by queue length, and REM (Random Exponential Marking) (Athuraliya et al. 2001) by a quantity that is decoupled with performance measures such as loss or delay. With RED or REM these quantities get
8521970	2354	probability, the dual variable. 1 INTRODUCTION Congestion control is a distributed algorithm to share network resources among competing sources. An optimal rate allocation problem is formulated in (Kelly 1997) where the goal is to choose source rates so as to maximize aggregate source utility subject to capacity constraints. This problem is solved using a penalty function approach in (Kelly et al. 1998,
8521970	2355	Size Figure 2: Window Hence (7) implies that the unique utility function of TCP Reno is ? 3/2 Us(xs) = tan ?1 ? ? ?s xs ? (8) 3/2 ?s This utility function for TCP Reno seems to appear first in (Kelly 1999, Low 2000). We make two remarks. First, the relation (7) between equilibrium source rate and loss probability reduces to the well known relation (see e.g. (Lakshman and Madhow 1997, Mathis et al.
8521970	6529	Reno seems to appear first in (Kelly 1999, Low 2000). We make two remarks. First, the relation (7) between equilibrium source rate and loss probability reduces to the well known relation (see e.g. (Lakshman and Madhow 1997, Mathis et al. 1999) ): xs = ?s a ? p when the probability p is small, or equivalently, when the window ?sxs is large compared with ? 3/2. The value of the constant a, around 1, has been found
8521970	8602	1998, Kunniyur and Srikant 2000, Golestani et al. 1998), and extended in, e.g., (Mo and Walrand 2000, Massoulie and Roberts 1999, La and Anantharam 2000). It is solved using a duality approach in (Low and Lapsley 1999) leading to a basic algorithm whose convergence has been proved in an asynchronous environment. A practical implementation of this algorithm is studied in (Athuraliya and Low 2000). This set of
8521970	8602	in TCP Vegas is queuing delay, which evolves according to (again ignoring the nonnegativity constraint): ?pl = 1 (yl(t) ? cl) =: Gl (10) cl Note that this is similar to the basic algorithm in (Low and Lapsley 1999) with ?? replaced by 1/cl. To describe the rate adjustment Fs, let xs(t) = U ??1 s (p(t)) = ?sds p(t) be the target rate chosen based on the end-to-end queuing delay p(t) and the marginal utility U
8521970	8605	capacity constraints. This problem is solved using a penalty function approach in (Kelly et al. 1998, Kunniyur and Srikant 2000, Golestani et al. 1998), and extended in, e.g., (Mo and Walrand 2000, Massoulie and Roberts 1999, La and Anantharam 2000). It is solved using a duality approach in (Low and Lapsley 1999) leading to a basic algorithm whose convergence has been proved in an asynchronous environment. A practical
8521970	8605	and whether delayed acknowledgment is implemented. Equating U ? s (xs) with p, the utility function of TCP Reno becomes: Us(xs) = ? a2 ? 2 s xs This version is used in (Kunniyar and Srikant 2000, Massoulie and Roberts 1999). 2.2 TCP Vegas The model (F, G) and utility function Us of TCP Vegas has been derived and validated in (Low et al. 2001b) We now summarize the results. The utility function of TCP Vegas is Us(xs)
8521970	2360	utility subject to capacity constraints. This problem is solved using a penalty function approach in (Kelly et al. 1998, Kunniyur and Srikant 2000, Golestani et al. 1998), and extended in, e.g., (Mo and Walrand 2000, Massoulie and Roberts 1999, La and Anantharam 2000). It is solved using a duality approach in (Low and Lapsley 1999) leading to a basic algorithm whose convergence has been proved in an
22211	2243	Control Protocol (TCP) in source and destination computers involved in data transfers. The congestion control algorithm in the current TCP, which we refer to as Reno, was developed in 1988  and has gone through several enhancements since, e.g., , , . It has performed remarkably well and is generally believed to have prevented severe congestion as the Internet scaled up by six
22211	2243	flow-level difficulties because they must be addressed by different means. A. Packet and flow level modeling The congestion avoidance algorithm of TCP Reno and its variants have the form of AIMD . The pseudo code for window adjustment is: Ack: w ?? w + 1 w Loss: w ?? w ? 1 2 w This is a packet-level model, but it induces certain flow-level properties such as throughput, fairness, and
22211	2243	IEEE INFOCOM 2004sQueue Delay C D delay loss (a) Binary signal: oscillatory R Window Queue Delay delay loss F (b) Multi-bit signal: stabilizable Fig. 2. Operating points of TCP algorithms: R: Reno , HSTCP , STCP ; D: DUAL ; C: CARD ; T : TFRC ; F : Vegas , FAST. • ui(wi,Ti): the choice of the marginal utility function ui mainly determines equilibrium properties such as
8922292	9284	? R minimizes the number of route-repair events seen by the routing protocol at the expense of significantly increasing the number of forwarding nodes per route. In most on-demand routing protocols  for ad hoc networks there are route-discovery and route-maintenance phases. Route-discovery is responsible for finding new routes between source-destination pairs whereas route-maintenance is
8922292	9284	management in cellular and mobile networks is concerned with the rate of cellular/mobile nodes crossing cell boundaries. In most MANET routing protocols, mobility analysis relies on simulations  due to the lack of a mobility model for this environment. For the specific case of route discovery, the work by  shows that the inherited spacewaste involved while flooding the network with
8922292	23981	unless a different method for discovering and maintain routes that departs from common transmission range broadcast technique is used. Recently, there has been some initial work in this area    that provides variable-range transmission support for routing protocol operation. 01 01 Fig. 9. Disadvantages of Common-range Transmissions Most ad hoc network designs simply borrowed MAC
8922292	23981	aspects of the problem. We address these issues in this paper. In , the authors present several link cost functions that take into account the power reserves of mobile nodes. The work in   intuitively suggest that a variablerange transmission approach can outperform a common-range transmission approach in terms of power savings, however, no definite analytical results are provided.
8922292	23981	wirelessenabled nodes discover energy-efficient routes to neighboring nodes and then use the shortest path Bellman-Ford algorithm to discover routes to other nodes in the network. The PARO protocol , uses redirectors to break longer-range transmissions into a set of smaller-range transmissions. Mobility management in cellular and mobile networks is concerned with the rate of cellular/mobile
8922292	23982	min com) ? 16? A B. Variable-Range Transmission than required to maintain Rmin com are likely to get lost rather than reaching the final destination node. This may lead to network partitions. In , Gupta and Kumar (1998) found an asymptotic expression to characterize the dependence of the common transmission range for asymptotic connectivity (Rcom) in wireless networks. They found that when
8922292	23982	converges to one as the number of nodes n goes to infinity if and only if kn ? +?. Then the critical transmission range for connectivity of n randomly placed nodes in A square meters isshowntobe, R min ? A ln n com > (1 + ?) ; ?>0 (1) ?n Definition: A tree T with node set V is called a spanning tree of V if each node of V is incident to at least one edge of T . Definition: A minimum
8922292	23982	using dynamic power control . VI. RELATED WORK In what follows, we discuss how our contribution discussed in this paper contrasts to the related work in the area. The work by Gupta and Kumar   on the mathematical foundations of common-range transmission in wireless ad hoc networks represents the seminal related research in this area. In this paper, we take a similar approach to Gupta
8922292	23985	to avoid. Fortunately there are some new proposals in MAC design that overtake this limitation and take full advantage of the spectral reuse potential acquired when using dynamic power control . VI. RELATED WORK In what follows, we discuss how our contribution discussed in this paper contrasts to the related work in the area. The work by Gupta and Kumar   on the mathematical
8922292	15313	a different method for discovering and maintain routes that departs from common transmission range broadcast technique is used. Recently, there has been some initial work in this area    that provides variable-range transmission support for routing protocol operation. 01 01 Fig. 9. Disadvantages of Common-range Transmissions Most ad hoc network designs simply borrowed MAC protocols
8922292	15313	MANET protocols  usually assume homogeneously distributed nodes. As discussed earlier, such a regime raises a number of concerns and is an impractical assumption in real networks. The authors in  and  discuss this problem and propose different methods to control the transmission power levels in order to control the network topology. The work in  and  is concerned with
8922292	3706	“no”, unless a different method for discovering and maintain routes that departs from common transmission range broadcast technique is used. Recently, there has been some initial work in this area    that provides variable-range transmission support for routing protocol operation. 01 01 Fig. 9. Disadvantages of Common-range Transmissions Most ad hoc network designs simply borrowed MAC
8922292	3706	aspects of the problem. We address these issues in this paper. In , the authors present several link cost functions that take into account the power reserves of mobile nodes. The work in   intuitively suggest that a variablerange transmission approach can outperform a common-range transmission approach in terms of power savings, however, no definite analytical results are
8922292	23988	it does not provide a mathematical description of the problem space, and ignores the power savings and traffic-carrying capacity aspects of the problem. We address these issues in this paper. In , the authors present several link cost functions that take into account the power reserves of mobile nodes. The work in   intuitively suggest that a variablerange transmission approach can
8922292	23990	usually assume homogeneously distributed nodes. As discussed earlier, such a regime raises a number of concerns and is an impractical assumption in real networks. The authors in  and  discuss this problem and propose different methods to control the transmission power levels in order to control the network topology. The work in  and  is concerned with controlling the
23991	1957	They observed an increase of the recognition rates from 93 percent (best out of four individual classifiers) to 98.6 percent for several of their combination methods. Similarly, Kittler et al.  combined four classifiers for optical character recognition (OCR) of uppercase letters and digits, and found the classification rate to improve from 95 percent (best individual classifier) to over
23991	4750	classifier) to over 98 percent for a combined classifier. A common problem, however, is the construction of independent classifiers. A general solution for this problem has been proposed by Breiman  under the name “bootstrap aggregation”, or simply “bagging”. If the training of a classifier is unstable, that is, sensitive to the training set, multiple classifiers can be generated by using
23991	4750	present paper is merely one example of a technique that we have empirically found to be accurate and computationally efficient. 2.3 Bagging of Atlas-Based Classifiers The principle idea of bagging  is to generate multiple independent classifiers by exploiting instability of classifier learning under changes to the learning set. In addition, one can exploit instability under different internal
23991	22920	the normalized mutual information (NMI) image similarity measure . The coordinate mapping between image and atlas is computed by a non-rigid registration algorithm introduced by Rueckert et al. . The transformation model is a free-form deformation  defined on a uniformly spaced grid of discrete control points. Between the control points, a smooth deformation field is interpolated using
23991	23996	. The coordinate mapping between image and atlas is computed by a non-rigid registration algorithm introduced by Rueckert et al. . The transformation model is a free-form deformation  defined on a uniformly spaced grid of discrete control points. Between the control points, a smooth deformation field is interpolated using approximating third-order B-splines. To improve
23991	22917	results in a different classifier, even when applied to the same atlas. 3. Evaluation Study We evaluate bagging of multiple atlas-based classifiers by segmenting three-dimensional biomedical images  Recognition Rate 0.985 0.980 0.975 0.970 0.965 0.960 0.955 0.950 1 2 3 4 5 6 7 Segmented Subject Figure 1: Recognition rates of individual vs. combined classifiers using three different atlases.
23991	22917	The main advantage of a classifier view of atlas-based segmentation is that it opens the field to the application of multi-classifier decision fusion techniques. We have shown in previous work  that segmentation accuracy can be significantly improved when more than a single atlas is used. However, multiple atlases are not always available, since their generation is time consuming and
23991	23998	results in a different classifier, even when applied to the same atlas. 3. Evaluation Study We evaluate bagging of multiple atlas-based classifiers by segmenting three-dimensional biomedical images  Recognition Rate 0.985 0.980 0.975 0.970 0.965 0.960 0.955 0.950 1 2 3 4 5 6 7 Segmented Subject Figure 1: Recognition rates of individual vs. combined classifiers using three different atlases.
23991	23998	The main advantage of a classifier view of atlas-based segmentation is that it opens the field to the application of multi-classifier decision fusion techniques. We have shown in previous work  that segmentation accuracy can be significantly improved when more than a single atlas is used. However, multiple atlases are not always available, since their generation is time consuming and
23991	23999	algorithms based on computing an equilibrium between internal forces (i.e., image similarity) and external forces (i.e., a physics-based constraint such as elasticity or volume preservation) . The obvious disadvantage of the suggested procedure is the repeated application of a computationally expensive non-rigid registration step. We have therefore limited our consideration in this
23991	22921	In the future, with ever increasing computer speeds, we consider registration times to be not too serious of an issue. Furthermore, non-rigid registration can be parallelized with very low overhead , and multiple independent registrations can easily be performed on a cluster of inexpensive computation nodes. Moving past the combination of multiple atlas-based classifiers, our framework
23991	18338	for combining atlas-based segmentations with segmentations obtained from fundamentally different methods. They could be combined, for example, with results generated by level set techniques , and with such originating from active contour methods . After all, the more dissimilar the methods combined, the more likely their errors are independent. Finally, more advanced methods for
23991	24000	obtained from fundamentally different methods. They could be combined, for example, with results generated by level set techniques , and with such originating from active contour methods . After all, the more dissimilar the methods combined, the more likely their errors are independent. Finally, more advanced methods for classifier combination become applicable. We are currently
23991	22924	Finally, more advanced methods for classifier combination become applicable. We are currently working on techniques to estimate the individual classifier performance in the absence of ground truth . Given these estimates, we hope to be able to identify more accurate individual classifiers and disregard less accurate ones in the decision fusion. These algorithms have a similar aim as the
24001	6469	biological classes (natural kinds) and instantiations, and of granular partitions, as well as respecting the more general demands on good ontology recognized by the philosophical community. BFO is thus ideal as a framework for mapping external ontologies, terminologies, and databases onto LinKBase® in a way that is designed to provide for successful integration, and as a useful guide
9338661	25598	can not be recognized reliably by our OCR engine. Although there exist many good techniques for extracting text lines from a printed document page scanned by a traditional flatbed scanner (e.g., ), it seems none of them can fully achieve the goal for our specific application here, mainly because of the following unique characteristics a pen scanner image might have: ¯ Due to the small
9338661	24014	can not be recognized reliably by our OCR engine. Although there exist many good techniques for extracting text lines from a printed document page scanned by a traditional flatbed scanner (e.g., ), it seems none of them can fully achieve the goal for our specific application here, mainly because of the following unique characteristics a pen scanner image might have: ¯ Due to the small
9338661	24014	are typically the fragmented characters, thus are removed permanently. The patches of black pixels touched to the boundaries are identified by using a run-length smoothing algorithm (RLSA)  and are also removed permanently. In the remaining CCs, we identify a subset of special CCs that might include large noise speckles, underlines, border lines of tables and forms, graphics, images,
24023	24025	links two BGP routers which belong to the same AS. Traditionally, all BGP routers in one AS form a full mesh via IBGP sessions, which is not scalable in large networks . Route reflection  is a widely used technique to solve the scalability problem in IBGP. The basic idea of route reflection is to divide the BGP routers into multiple clusters. In each cluster, there are one or more
24023	14549	will cause a router to hang. Chang et al. show that BGP router failures (even cascading failures) are resulted due to large BGP table injection which makes a router run out of memory. Therefore, routers with larger amount of resources (CPU power and memory) are more robust for handling BGP operations. Based on the above discussion, we can draw two conclusions: (1) It is neither
24023	14549	example of finding the range of nr based on certain degree constraints. The condition (a) requires the curve ?k i=1 c? i be above the curve k2 ?2k+n, which in turn gives the feasible interval nr ? . Meanwhile, the curve c ? k should be above the curve k ? 1, resulting in another interval nr ? . Combining these two intervals, we have that the range of nr should be . This result
24023	24030	are more reliable and thereby more likely to survive in some stressful situations. Also, physical links may D A (c) Eshave different failure rates and some links are much more stable than others . Three major issues need to be considered in designing a reliable IBGP route reflection topology: (1) How many clusters are needed? The use of too few reflectors may cause single point of failure
24023	24030	BGP message delays or losses, such as physical connectivity failures or transport layer instability (severe congestion), may further cause the related BGP sessions to be reset. Iannaccone et al.  show that link failures occur as part of everyday operation, and about 50% of the failures last longer than one minute. Although IGP re-routing and TCP retransmission can recover lost packets in
24023	24030	are generated, which have 50, 80, 120 and 150 nodes respectively. In each topology, 20% of the nodes are selected as border routers. If node i is a border router, ci is randomly selected from ; otherwise ci is generated randomly from . We assume that the minimum hop-count routing is used in IGP. The impacts of link failures and router failures on IBGP sessions are:
24023	14032	may lead to failures of BGP sessions which use this link. Similarly, severe network congestion, which delays the delivery of BGP packets, maystrigger IBGP session failures in some scenarios . In order to model the reliability of IBGP sessions, we assume that the event of failures (including severe congestion) of each physical link follows Poisson process, and denote wij as the failure
24023	24023	{ci} is decided based on the reliability and work load of each router. To solve the RR-ELT and RR-SSL problems, many design issues have to be considered, such as the number of clusters. In , we investigate two special cases, full mesh networks and circle networks, and show that the optimum reflector number is influenced by the redundancy of physical network. In a network with large
24023	24023	conditions for the reflection topology design problems to be solvable are (a) ?k i=1 c? i ? k2 ? 2k + n, ? 2i ? 3,i=1, 2,...,n}. where k = max{i | c ? i (b) c ? n ? 1. Proof: Please refer to . ? Discussions: (1) Theorem 1 provides a convenient method to check if the given degree bounds are feasible. First, each degree bound should be no less than 1. Second, sort the bounds decreasingly,
24036	24039	split (and merge), k-flow has the obligation to split. The motivation for enforced splitting comes from communication applications: a multiroute channel is more tolerant against link failures (cf. ). The celebrated Max-Flow Min-Cut theorem  occupies a central position in classicalsow theory. Is there an analogue for the multiroutesow? Menger's theorem immediately provides a necessary and
24036	24040	cuts (edge capacities depend to some extend on a varying parameter). A discrete multicommodity variant of the multiroutesow problem, namely the k Disjoint Flow Problem, was studied by Bagchi et al. . In this note, we present a simple combinatorial proof of the duality of multiroute ows and cuts. As a by product of the duality theorem we get a characterization of multiroutesows in terms of
24036	24041	split (and merge), k-flow has the obligation to split. The motivation for enforced splitting comes from communication applications: a multiroute channel is more tolerant against link failures (cf. ). The celebrated Max-Flow Min-Cut theorem  occupies a central position in classicalsow theory. Is there an analogue for the multiroutesow? Menger's theorem immediately provides a necessary and
24036	24043	split (and merge), k-flow has the obligation to split. The motivation for enforced splitting comes from communication applications: a multiroute channel is more tolerant against link failures (cf. ). The celebrated Max-Flow Min-Cut theorem  occupies a central position in classicalsow theory. Is there an analogue for the multiroutesow? Menger's theorem immediately provides a necessary and
24036	24046	split (and merge), k-flow has the obligation to split. The motivation for enforced splitting comes from communication applications: a multiroute channel is more tolerant against link failures (cf. ). The celebrated Max-Flow Min-Cut theorem  occupies a central position in classicalsow theory. Is there an analogue for the multiroutesow? Menger's theorem immediately provides a necessary and
24036	24047	and Aggarwal and Orlin  did. Open problems A lot of attention has been paid to max-flow min-cut theorems for multicommodity ows in the last ten years (cf. an excellent survey by Shmoys ). This immediately raises the question: Is there an analogous max k-flow min k-cut theorem for multicommodity multiroutesows? Though there is a clear relation between single commodity
24048	2289	of on-demand and proactive routing for ad-hoc wireless networks. I. INTRODUCTION On-demand route discovery is based on route request (RREQ) and route reply (RREP) messages (e.g., AODV  and DSR ). The way in which these messages are handled may differ among different protocols, but their functionality remains the same: a request is relayed until it reaches a node with a valid
24048	2722	of on-demand and proactive routing for ad-hoc wireless networks. I. INTRODUCTION On-demand route discovery is based on route request (RREQ) and route reply (RREP) messages (e.g., AODV  and DSR ). The way in which these messages are handled may differ among different protocols, but their functionality remains the same: a request is relayed until it reaches a node with a valid route to the
24048	24050	Extensive work has been done on finding a good approximation of MCDS in terms of small approximation ratio. A protocol with a constant approximation ratio of eight has been proposed by Wan et. al. . However, their approach requires that a spanning tree be constructed first in order to select the dominating nodes (forwarders), and only after that a broadcast can be performed. To improve the
24048	24051	of both protocols indicate that AODV-EDP incurs a few more collisions because it delivers more packets. Because the scenarios we have used to evaluate our approach differ from those presented in , and because we implemented our solution together with a neighbor and routing protocol, we do not know how our solution compares to TDP and PDP. The relation between the savings of pruning (too
24048	24052	one-hop neighbors for building the two-hop neighborhood, which issthe minimum requirement for the connected dominating set algorithm under consideration. The Optimized Link State Routing (OLSR)   is closely related to our work, because it uses a similar mechanism for reducing duplicate control traffic retransmissions. Each nodesin the network selects a set of nodes among its symmetrical
24048	2734	to the destination.sIV. SIMULATIONS AND PERFORMANCE RESULTS To compare AODV with EDP (AODV-EDP) against other protocols, we use traffic and mobility models similar to those previously reported in . We implemented AODV-EDP in ????? Qualnet , and compare it against AODV-DP (AODV with Dominant Pruning), AODV with no hello messages and with because they represent some of the most referenced
24048	2734	is room for more improvement (i.e., there is some more redundancy that can be eliminated). OLSR performs better than AODV-DP for large pause times (after ??????? pause time). As pointed out in , the possibility of link failures is low with low mobility, but due to the node movement model (random waypoint) nodes usually get clustered. This situation is responsible for congestion in those
8922299	24055	general starting clause, bottomup approaches generalise a very specific bottom clause. Again we illustrate the main ideas by means of a simple example. Consider the following four ground facts: a(,,). a(,,). a(,,) a(,,). Upon inspection we may conjecture that these ground facts are pairwise related by one recursion step, i.e., the following two
8922299	24055	a much larger set. Suppose now that we know which head literals to choose, but not which body literals. One approach is to simply lump all literals together in the bodies of both ground clauses: a(,,):a(,,),a(,,), a(,,),a(,,). a(,,):a(,,),a(,,), a(,,),a(,,). Since bodies of
8922299	24055	possible pairs of body literals, keeping in mind the variables that were introduced when anti-unifying the heads. Thus, the body of the resulting clause consists of 16 literals: a(,C,):a(,,),a(,C,),sa(W,C,X),a(,,), a(,K,),a(,,), a(Q,,Q),a(,K,),a(N,K,O), a(M,,M),a(,,),a(G,K,L),
8922299	24055	generalise this further by allowing sets and multisets. 4 Further examples illustrating the Escher representation can be found in . A higherorder decision tree learner is described in , and a higher-order evolutionary progamming system in . 4 Concluding remarks This paper has provided an introduction to ILP for non-machine learners. We have also given an example of a learning
8922299	24056	learning and computational logic. Recent years have seen a steady increase in ILP research, as well as numerous applications to practical problems like data mining and scientific discovery – see  for an overview of such applications. Successful ILP applications include drug design , protein secondarysstructure prediction , mutagenicity prediction , carcinogenesis prediction
8922299	24056	general starting clause, bottomup approaches generalise a very specific bottom clause. Again we illustrate the main ideas by means of a simple example. Consider the following four ground facts: a(,,). a(,,). a(,,) a(,,). Upon inspection we may conjecture that these ground facts are pairwise related by one recursion step, i.e., the following two
8922299	24056	a much larger set. Suppose now that we know which head literals to choose, but not which body literals. One approach is to simply lump all literals together in the bodies of both ground clauses: a(,,):a(,,),a(,,), a(,,),a(,,). a(,,):a(,,),a(,,), a(,,),a(,,). Since bodies of
8922299	24056	possible pairs of body literals, keeping in mind the variables that were introduced when anti-unifying the heads. Thus, the body of the resulting clause consists of 16 literals: a(,C,):a(,,),a(,C,),sa(W,C,X),a(,,), a(,K,),a(,,), a(Q,,Q),a(,K,),a(N,K,O), a(M,,M),a(,,),a(G,K,L),
8922299	24056	Furthermore, all ground facts are lumped together, whereas it is generally possible to partition them according to the examples (e.g., the facta(,,) has clearly nothing to do with the facta(,,)). Both restrictions are lifted in Muggleton’s current ILP system Progol . Essentially, Progol constructs a bottom clause for a selected example by adding its negation to the
8922299	24057	GOING WEST Fig. 1. The ten train East-West challenge. Each train consists of 2-4 cars; the cars have attributes like shape (rectangular, oval, u-shaped, ...), length (long, short), number of wheels (2, 3), type of roof (none, peaked, jagged, ...), shape of load (circle, triangle, rectangle, ...), and number of loads (1-3). A possible rule distinguishing between eastbound and westbound trains is ‘a
8922299	19763	of an individualsand the predicate(s) used to refer to parts of the individuals. This connection between term structure and hypothesis construction is obviated by using a strongly typed language . The following representation uses a Haskell-like language called Escher, which is a higher order logic and functional programming language . eastbound :: Train->Bool; type Train = ; type
8922299	19763	of which could be a list of tuples). Higher-order representations generalise this further by allowing sets and multisets. 4 Further examples illustrating the Escher representation can be found in . A higherorder decision tree learner is described in , and a higher-order evolutionary progamming system in . 4 Concluding remarks This paper has provided an introduction to ILP for
8922300	6648	BalkaNet project is given in . The detailed presentation of the Romanian wordnet, part of the BalkaNet multilingual lexical ontology, is given in . The EuroWordNet is largely described in . Depending on the approach in building the monolingual wordnets included into a multilingual lexical semantic network and on the idiosyncratic properties of each language, the semantic alignment of
8922300	19804	semantic distance between ILI records. Our system uses Siddharth Patwardhan and Ted Pedersen’s WordNetSimilarity PERL module, a WN plug-in implementation of the five semantic measures described in . 3 Interlingual Validation Based on Parallel Corpus Evidence If we take the position according to which word senses (language specific) represent language independent meanings, abstracted by ILI
24067	24070	by introducing an auxiliary process governing the regime switching of the hidden process. The introduction of an auxiliary process to model the non stationarity has already been treated before. In , the authors introduce auxiliary data to modify the transition probabilities of the Markov hidden process. In , the author present a model based on the superposition of two Markov chains. A
24067	24071	auxiliary process to model the non stationarity has already been treated before. In , the authors introduce auxiliary data to modify the transition probabilities of the Markov hidden process. In , the author present a model based on the superposition of two Markov chains. A non-homogeneous discrete process is described by a set of transition matrices. At each n , the choice of the active
24067	24074	let V = ( X , U ) . As T is Markovian, the process ( V , Y ) is a Pairwise Markov Chain (PMC) and we can formulate and calculate the distribution of ( Vn , Y ) using Forward-Backward recursions . This means that the distributions p ( x , y) = p( x , u , y) = p( v , y) also are comn ? u ?? n n n ? u ?? putable. Finally, although the distribution of ( X , Y ) is not necessarily a Markov one,
24067	24074	Z = ( X , Y ) is a Markov chain ; (ii)for each 2 ? n ? N , p u z , z ) = p( u z ) ; ( n n n? 1 n n (iii)for each 1 ? n ? N , p ( u n z) = p( u n z n ) are equivalent. The Pairwise Markov Chain (PMC ) model, in which Z is assumed to be a Markov chain, is then a particular case of the TMC obtained by taking ? = ? and U = X . Furthermore, according to the Proposition 2.1 above TMC is strictly
24067	6657	1 6 ( ? 2 , ?3 ? = ? , ? ) , ? = ) , ? = ) . 4 ( 2 2 3 ( 1 2 ( 1) ( 2) ( 1) ( 2) The processes U , U , X , X are then converted, via a Hilbert-Peano scan, into a bi-dimensional set as described in  and their realizations are presented in Figure 1. ( 1) U = ( 1) X = ( 1) Y = u x y ( 1) ( 1) ( 1) ( 1) ( 1) X ˆ = xˆ (HMC), ? = 26% ( 2) U = ( 2) X = ( 2) Y = u x y ( 2) ( 2) ( 2) ( 2) ( 2) X ˆ =
8922303	24080	paper, we present a unified framework for modeling intrinsic properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient
8922303	24080	performance of face recognition under varying illumination conditions. 1 Introduction Quotient image designed for dealing with illumination changes in face recognition by Shashua and Riklin-Raviv  , is a simple yet practical algorithm for extracting illumination invariant representation. It has been shown that the quotient image, i.e. image ratio between a test image and linear combination
8922303	24080	needed for the estimation of the lighting direction because the lighting fields of I and ? I are similar. (3) the self-quotient image is good at removing shadows; whereas in the previous approaches -, the shadow problem was either ignored or was solved by complex 3D rendering. (4) Lighting sources can be any type. Note that the property of Q is dependant on the kernel size. If the kernel
8922303	24080	have obvious effects on both of our two new methods. 5 Conclusion A generalized QI framework based on previous works is presented. This unified framework explains the essence of previous QI-based , Retinex-based ,-] and image ratio-based  algorithms without any assumption of illumination type and absence of shadow. Under this framework, we derive two new algorithms, NPL-QI and
8922303	24082	properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to deal with non-point light sources by
8922303	24082	face recognition under various illuminations. However their application range is limited by needing 3D face model. Similar to the image ratio technique in QI, Nayar and Rolle , and Jacobs et al  also introduce similar algorithms for face image intrinsic property extraction. However both of the two groups only analyze this approach by Lambertian model without shadow. Based on Land’s Retinex
8922303	24082	under all cases, including shading region, shadow region and edge region. Furthermore we introduce a simple edge preserving filter for getting smooth version of the original image. Jacobs etc  also introduce a similar kind of QI, which is the ratio of two images. They show that for point sources and objects with Lambertian reflectance, the ratio of two images from the same object is
8922303	24082	Conclusion A generalized QI framework based on previous works is presented. This unified framework explains the essence of previous QI-based , Retinex-based ,-] and image ratio-based  algorithms without any assumption of illumination type and absence of shadow. Under this framework, we derive two new algorithms, NPL-QI and S-QI. These algorithms extend the original QI from point
8922303	24083	for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to deal with non-point light sources by modeling non-point
8922303	24083	in Nayar’s approach, Jacobs’s method only considers the Lambertian model without shadow and the object surface is smooth, i.e. the partial differential exists. The Retinex theory motivated by Land  deals with illumination effects on images. According to his experiments, human vision can discriminate color under different lighting conditions. Jobson, et al  present a multi-scale version
8922303	24083	for the estimation of the lighting direction because the lighting fields of I and ? I are similar. (3) the self-quotient image is good at removing shadows; whereas in the previous approaches -, the shadow problem was either ignored or was solved by complex 3D rendering. (4) Lighting sources can be any type. Note that the property of Q is dependant on the kernel size. If the kernel size
8922303	24083	on both of our two new methods. 5 Conclusion A generalized QI framework based on previous works is presented. This unified framework explains the essence of previous QI-based , Retinex-based ,-] and image ratio-based  algorithms without any assumption of illumination type and absence of shadow. Under this framework, we derive two new algorithms, NPL-QI and S-QI. These
8922303	24084	properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to deal with non-point light sources by
8922303	24084	cast light on robust face recognition under various illuminations. However their application range is limited by needing 3D face model. Similar to the image ratio technique in QI, Nayar and Rolle , and Jacobs et al  also introduce similar algorithms for face image intrinsic property extraction. However both of the two groups only analyze this approach by Lambertian model without shadow.
8922303	24084	sources are generally not of point; 3D face shapes of different people are not the same in general; the shadow can exist; and accurate alignment is still an unsolved problem by now. Nayar and Rolle  also advance one kind of QI. This kind of QI is the ratio of an image point with its neighboring (2) 2 points. According to their analysis under the assumption of Lambertian model, they deduce that
8922303	24084	Conclusion A generalized QI framework based on previous works is presented. This unified framework explains the essence of previous QI-based , Retinex-based ,-] and image ratio-based  algorithms without any assumption of illumination type and absence of shadow. Under this framework, we derive two new algorithms, NPL-QI and S-QI. These algorithms extend the original QI from point
8922303	24083	for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to deal with non-point light sources by modeling non-point
8922303	24083	also introduce similar algorithms for face image intrinsic property extraction. However both of the two groups only analyze this approach by Lambertian model without shadow. Based on Land’s Retinex , Jobson et al  and Gross and Brajovie  develop reflectance estimation method by the ratio of original image and its smooth version. The difference of the two Retinex-based algorithms is
8922303	24083	in Nayar’s approach, Jacobs’s method only considers the Lambertian model without shadow and the object surface is smooth, i.e. the partial differential exists. The Retinex theory motivated by Land  deals with illumination effects on images. According to his experiments, human vision can discriminate color under different lighting conditions. Jobson, et al  present a multi-scale version
8922303	24088	between images for QI calculation. Besides the QI method, many others techniques have been proposed for face representation under various illumination conditions in recent years. Belhumeur etc.  prove that face images 1 with the same pose under different illumination conditions form a convex cone, called illumination cone. Ramamoorthi  and Basri etc.  independently
8922303	24090	between images for QI calculation. Besides the QI method, many others techniques have been proposed for face representation under various illumination conditions in recent years. Belhumeur etc.  prove that face images 1 with the same pose under different illumination conditions form a convex cone, called illumination cone. Ramamoorthi  and Basri etc.  independently
8922303	24090	kernel. The formation of this filter is shown in Figure 4. 4 Experiments and Discussion Experiments are performed to evaluate NPL-QI and S-QI for face recognition, using Yale face B database  and CMU PIE face database . Frontal face images with lighting variations are selected from the two face databases to reduce the image changes only due to lighting variations. There are 68
8922303	24091	framework for modeling intrinsic properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to
8922303	24091	conditions in recent years. Belhumeur etc.  prove that face images 1 with the same pose under different illumination conditions form a convex cone, called illumination cone. Ramamoorthi  and Basri etc.  independently apply the spherical harmonic representation to explain the low dimensionality of differently illuminated face images. The synthesis and recognition results of
8922303	24091	shadow. However the ordinary face images are always illuminated with non-point light sources and shadow will present unless the face is illuminated by frontal lighting.sAccording to Ramamoorthi  and Basri ’s spherical harmonic representation, face image I can be represented by I = ?H • o (10) where H =  is the spherical harmonic bases and o is the harmonic light.
8922303	24092	framework for modeling intrinsic properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to
8922303	24092	conditions in recent years. Belhumeur etc.  prove that face images 1 with the same pose under different illumination conditions form a convex cone, called illumination cone. Ramamoorthi  and Basri etc.  independently apply the spherical harmonic representation to explain the low dimensionality of differently illuminated face images. The synthesis and recognition results of
8922303	24092	shadow. However the ordinary face images are always illuminated with non-point light sources and shadow will present unless the face is illuminated by frontal lighting.sAccording to Ramamoorthi  and Basri ’s spherical harmonic representation, face image I can be represented by I = ?H • o (10) where H =  is the spherical harmonic bases and o is the harmonic light.
8922303	24092	distribution for 64 dimensions Because these bases must be calculated with known 3D geometry, the application range of this representation is limited. According to Ramamoorthi’s analysis , there is 4 linear relationship between PCA eigenvectors and spherical harmonic bases. U = BT (12) where U =  is the eigenvector matrix of a face under all lighting conditions and
8922303	24094	for modeling intrinsic properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI , Spherical Harmonic, , Image Ratio and Retinex . Under this framework, we generalize these previous works into two new algorithms: (1) Non-Point Light Quotient Image (NPL-QI) extends QI to deal with
8922303	24094	Belhumeur etc.  prove that face images 1 with the same pose under different illumination conditions form a convex cone, called illumination cone. Ramamoorthi  and Basri etc.  independently apply the spherical harmonic representation to explain the low dimensionality of differently illuminated face images. The synthesis and recognition results of illumination cone and
8922303	24094	ordinary face images are always illuminated with non-point light sources and shadow will present unless the face is illuminated by frontal lighting.sAccording to Ramamoorthi  and Basri ’s spherical harmonic representation, face image I can be represented by I = ?H • o (10) where H =  is the spherical harmonic bases and o is the harmonic light. According to their
24099	24101	infinite, and it had been shown in  that there is no finite axiomatization of the behavioural equality in first-order logic. In the framework of initial algebras, an algorithm was given in  for constructing a finite set of contexts, that allows to describe the whole set of observable contexts. It applies to specifications represented by a left-linear system. We consider this
24099	24103	Moreover, if the candidate relation is not a hidden congruence, then users have to find another candidate to complete the proof. The same problems appear in the approach of Bidoit and Hennicker  where users have to provide partial congruences. Several other proof tools have been developed to aid coinductive proofs, but all of them require the user to supply an appropriate relation which
24105	24106	data provider module by the AppAOMManager class, using its getEntityType method. An alternative implementation for the adaptation data provider module may explore the idea presented in another work , using an application-specific object responsible for obtaining dynamic data for adaptations and making them available through its operations. However, with this approach, besides losing in reuse,
24105	24106	Figure 4), which actually characterizes the adaptive behavior. Its implementation with aspects  is interesting as a dynamic way (using pointcuts) of identifying subject changes. • Another work  describes some practices incorporated to this pattern and presents the implementation of some adaptive concerns into J2ME applications using AspectJ, comparing them to pure Object-Oriented
24105	24109	It can also be called by the aspects to obtain information about the context. Its implementation can be based on a variation of the Observer pattern , or on its implementation with aspects . In this way, new mechanisms for accessing the context can be easily supported without significant impact on the application (see the Example section).s• Adaptation Data Provider: Classes
24105	24109	which actions should take place at those moments. We propose the implementation of this module using the Observer pattern, and specially its implementation using aspects, proposed in another work , and adapted to the observation of the context, which increases the flexibility of this module in relation to new environment elements to be observed. This is illustrated by Figure 4. Although this
24105	24109	these aspects and the application classes is illustrated by Figure 7. The Context Manager module will follow the structure shown in Figure 4. The code for the ObserverProtocol is shown elsewhere . Our SpecificAdaptationProtocol aspect will be the LanguageAdaptationProtocol, which is shown below: 1: public aspect LanguageAdaptationProtocol extends ObserverProtocol { 2: declare parents:
24105	24109	and the auxiliary code. In this structure, the aspects part functions as glue between the other two. Another example is the implementation of the Observer aspect using aspects proposed by Hanneman  which we modify here in the Context Manager module. 12 See Also • The Reflection architectural pattern , which provides a mechanism for changing structure and behavior of software systems
24105	24109	• The Observer pattern  is very useful in the implementation of the Context Manager component (see Figure 4), which actually characterizes the adaptive behavior. Its implementation with aspects  is interesting as a dynamic way (using pointcuts) of identifying subject changes. • Another work  describes some practices incorporated to this pattern and presents the implementation of some
24105	24111	a Dictionary application for a cellular phone and an album for pictures, also for small devices. Besides that, some parts of the patterns have already been referred to. An example of this is a work  presenting some experiments using aspects. One of the conclusions is that it is an interesting architecture to separate an AOP system in three parts: the base code, the aspects and the auxiliary
24105	24112	This architectural pattern is intended to show how to use aspects  in order to better structure adaptive applications, which are able to change their behavior in response to context changes , such as the device new localization or its resources state. 2 Context Adaptability has became a common requirement  and its implementation usually affects many parts of the code. Most
24105	24113	of the adaptive behavior data because AOMs are known for their ability to represent classes, attributes,relationships, and behavior as metadata . • The PADA(Pattern for Distribution Aspects) , even dealing with Distribution, is also related to this work as it provides a solution to the lack of modularity and maintainability using AOP. • The Observer pattern  is very useful in the
24105	3374	of concerns. It is composed of known and novel patterns organized so as to provide good maintainability and modularity. 1 1 Intent This architectural pattern is intended to show how to use aspects  in order to better structure adaptive applications, which are able to change their behavior in response to context changes , such as the device new localization or its resources state. 2
24105	3374	devices, such as cellular phones, to enterprise applications. • The kind of contextual information might change and this should not cause a significant impact on the system. 5 Solution Use aspects  to make the application adaptive in a modularized and not invasive way. With Aspect-Oriented Programming we can cleanly capture some implementation aspects that affect many parts of a system, as it
24105	24114	auxiliary classes access the objects of the adaptation data provider, which can be based on the Metadata and Active Object-Model pattern language , also called Adaptive Object-Model Architecture . • The application is then changed according to this dynamic data.sThe sequence diagram shown in Figure 2 shows the interaction between the pattern elements. Instead of representing objects, each
24105	24114	to increase the complexity of the system more than our solution and even to be less efficient. • Adaptive Object-Model systems represent their attributes, classes, and relationships as metadata . AOMs can be used to represent all the system structure orsjust what changes in the system. In our pattern, we took the latter approach and explored AOP as a non-invasive way to include AOMs. We
24105	24115	that the same context change can lead to different behaviors in different moments according to the data provided by this module. These classes can be organized as an Adaptive Object-Model (AOM) . These elements and their inter-relation are shown in Figure 1. They are represented there using the UML package notation. Each package represents a logical part of the code, but each of these
24105	24115	such as XML files that can be obtained locally or remotely (the Bridge  pattern is indicated for providing this flexibility). Such an idea is adopted by the Adaptive Object-Model architecture . The use of AOMs. is not mandatory for the development of this module, but presents the advantage that, once implemented, one of its parts can be reused by many systems; only the interpreter of
24105	24115	the Adaptation Data Provider module, as the provider of the adaptive behavior data because AOMs are known for their ability to represent classes, attributes,relationships, and behavior as metadata . ??? The PADA(Pattern for Distribution Aspects) , even dealing with Distribution, is also related to this work as it provides a solution to the lack of modularity and maintainability using AOP. •
8922305	24121	a common interface to adapt the output of the media codec to a format suitable for transmission over different types of networks, including bandwidth constrained and error-prone wireless links . NAL units can be directly encapsulated into RTP packets according to the respective RTP payload specification . Therefore, the length of the NAL unit directly influences the length of the IP
8922305	24121	Slices within one video frame are independently coded and therefore provide spatially distinct resynchronization points within the video data. Advanced error concealment can be applied in this case . The number of macroblocks encoded within one frame is arbitrary and therefore, this mode allows to generate NAL units of similar length. In this way, drastic changes in the IP packet size due to
8922305	24121	of these intra macroblocks is best done in a rate-distortion optimized way where the distortion should reflect an expectation of the decoder distortion including the channel characteristics , . Hence, we propose not only to adjust the packet length for a given scenario, but also the macroblock intra update ratio in case of infrequent, but still present packet loss. The encoder is
8922305	24121	and network adaptation purpose the maximum NAL unit length in bytes, NNALU , and the macroblock intra update ratio specified by the percentage of expected NAL unit losses, PNALU , can be selected . By applying this simple, but nevertheless effective type of cross-layer design between application and transmission parameters, we achieve good performance at each point of operation in the system
8922305	24123	of these intra macroblocks is best done in a rate-distortion optimized way where the distortion should reflect an expectation of the decoder distortion including the channel characteristics , . Hence, we propose not only to adjust the packet length for a given scenario, but also the macroblock intra update ratio in case of infrequent, but still present packet loss. The encoder is
6080603	16062	whose instances have a direct denotation such as strings, numbers, etc. The types which cannot be directly denoted are the non-value types. This double dichotomy is discussed in more detail in . Relationship types are build from roles. Let be the set of all such roles in the conceptual schema. The fabric of the conceptual schema is then captured by two functions and two predicates. The
6080603	16062	hierarchy, and it does not follow from a disjunction constraint that they are disjunctive. For a definition of this relationship in more basic concepts of an ORM schema, please refer to  or . In ORM some object types may be specialised or be polymorphic (in the current version of InfoModeler only specialisation applies). Sometimes we need to access the so-called root types of a given
6080603	16062	with one element only) for ORM models developed in InfoModeler. Again, for a more detailed definition of such a function in terms of more basic concepts of an ORM schema, please refer to  or . Finally, since all instances of the population of a type must be identifiable in terms of a combination of values, we presume that each non value type has associated an identification scheme (or
6080603	10702	in InfoModeler for the specification of derivation rules and constraints. The definition of this language is a restriction, and a slight extension at the same time, of the existing language LISA-D (, , ). In ConQuer, a central role is played by the so-called path-expressions. In its most elementary form, a path-expression describes a path through the conceptual schema. 1sThe most
6080603	10702	subtype hierarchy, and it does not follow from a disjunction constraint that they are disjunctive. For a definition of this relationship in more basic concepts of an ORM schema, please refer to  or . In ORM some object types may be specialised or be polymorphic (in the current version of InfoModeler only specialisation applies). Sometimes we need to access the so-called root types of
6080603	10702	sets (sets with one element only) for ORM models developed in InfoModeler. Again, for a more detailed definition of such a function in terms of more basic concepts of an ORM schema, please refer to  or . Finally, since all instances of the population of a type must be identifiable in terms of a combination of values, we presume that each non value type has associated an identification
6080603	10702	expressions, and equality. As an illustration of the choices involved, we provide some examples of different choices: 4 Multisets if NULL In this section, which is completely based on a section in , the concept of multiset is introduced formally. Multisets (), also known as multiple membership sets (), or bags (), differ from ordinary sets in that a multiset may contain
6080603	10702	would like to say: For this purpose, the confluence operation is used. Since SQL-92 is not able to deal with nested relations, we have changed the definition slightly as opposed to the one used in . If are attributes, path-expressions, and , then the new definition is: where 32sEach time no is provided, will be used as a default. The ’s are used to connect the ’s path-expressions to the . The
6080603	20215	population. 6.1 Linear path-expressions The first class of path-expressions we introduce are the linear path-expressions. These are the linear paths as they may result from a point to point query () or query 20sby navigation (). The linear path-expressions are called linear as they always correspond to a single path through the conceptual schema. Linear path-expression have two
24134	24134	in a tree is explored, e.g., in , achieving optimal routing. It requires ? O(1) bits for local tables and ? O(1) bits for headers. Due to space limitation, some proofs have been moved in . 1 For stretch-1, a stronger lower bound of ?(n log d) holds for every maximum degree d graphs, 3 ? d ? ?n . 2 The bound relies to a conjecture proved for k = 1, 2, 3, 5. The bound holds only
24134	24134	hash functions given by Lemma 4.4. Let T = (U, r, E, ?) be a rooted tree with r ? U ? V and |U| = m. Sorting the nodes in U by their unique name n(), we denote U as the ith largest node in U, U = maxv?U{n(v)} and for 1 < i ? m define U = maxv?U{n(v) | n(v) < U}. In addition to their given name n(v), we give each node v ? T three more names. First, we give v its name in the
24134	24135	tree designer ? n ?log n? 3  s = 1 unweighted tree fixed n ?log n?  s = 1 unweighted tree fixed log 2 n/ log log n log 2 n/ log log n  s < 3 unweighted tree designer n log n Name-Indep.  Table 1: Lower bounds on the local memory requirements. Bounds of Line 1,4,5,6,7 are known to be optimal up to a constant factor, Line 2 and 3 are optimal up to a polylogarithmic factor. with the
24134	24136	has stretch O(?) When D is polynomial in n, the scheme has asymptotically optimal stretch factor, as proven by . With the same memory bound, the best previous results obtained stretch O(? 2 ) . Then, in Section 4, we consider the problem of routing messages from a distinguished node, the source, to all the other nodes. Single source routing problem with small local storage can also be
24134	24136	routing with ? O(n 3/2 ) total memory. Awerbuch and Peleg  presented a scheme that for any k, requires ? O(k 2 n 1/k log D) bits per node and routes on paths of stretch O(k 2 ). Arias et al.  present a slight improvement that uses the same memory bounds but improves the constant in the O(k 2 ) stretch by a factor of 4. All known name-independent schemes that are “combinatorial” and do
24134	24137	by a factor of 4. All known name-independent schemes that are “combinatorial” and do not rely on the normalized diameter, D, in their storage bound have exponential stretch factor. Awerbuch et al.  achieve with ? O(n 1/k ) memory stretch O(9 k ), and  improved to stretch O(2 k ) 2sstretch graphs port model memory address reference All-to-All s < 1.4 any unweighted designer n log n log n
24134	24137	? O( ? n ) memory Arias et al. provide stretch 5. Recently, Abraham et al. , achieve optimal stretch 3 with ? O( ? n ). A weaker variant of the routing problem, labeled routing, was initiated in . In this problem model, the algorithm’s designer can choose the network addresses of nodes (and of course, use node names to store information about their location in the graph). This paradigm does
24134	24141	certain building blocks devised in the context of labeled routing schemes). Indeed, optimal compact schemes for labeled routing are known. The first non trivial stretch-3 scheme was given by Cowen  with ??? O(n 2/3 ) memory. Later, Thorup and Zwick  improved the memory bound to only ? O( ? n ) bits. They also gave an elegant generalization of their scheme, achieving stretch 4k ?5 (and
24134	24142	< 1.4 any unweighted designer n log n log n  1 s < 3 any unweighted designer n n  s < 2k + 1 any weighted fixed n 1/k n 1/k  2 Single-Source s = 1 unweighted tree designer ? n ?log n? 3  s = 1 unweighted tree fixed n ?log n?  s = 1 unweighted tree fixed log 2 n/ log log n log 2 n/ log log n  s < 3 unweighted tree designer n log n Name-Indep.  Table 1: Lower bounds on the
24134	24142	using only ? O(n 1/k ) bits. Additionally, there exist various labeled routing schemes suitable only for certain restricted forms of graphs. For example, routing in a tree is explored, e.g., in , achieving optimal routing. It requires ? O(1) bits for local tables and ? O(1) bits for headers. Due to space limitation, some proofs have been moved in . 1 For stretch-1, a stronger lower
24134	24142	only for fixed port model and weighted graphs with polynomial weights. 3 If slightly larger than ?log n? bit addresses are allowed, namely (1 + o(1)) log n, then O(log n) bits suffice for memory . 3s2 Preliminaries We denote an undirected weighted graph by G = (V, E, ?), where V is the set of nodes, E the set of links, and ? : E ? R + a link-cost function. For any two nodes u, v ? V let
24134	24142	< U}. In addition to their given name n(v), we give each node v ? T three more names. First, we give v its name in the labeled tree-routing of Thorup & Zwick  and Fraigniaud & Gavoille : Lemma 3.4  For every weighted tree T with n nodes there exists a labeled routing scheme that, given any destination label, routes optimally on T from any source to the destination. The
24134	24142	v ? V , we denote by ?T (v) the tree-routing label of v in T , which is used for the routing in T from source s to destination v. The length of each of these labels is O(log 2 n/ log log n) bits . Overview of the scheme. The basic idea of the scheme is to use indirection: the keys of P are mapped to the nodes of T in a balanced way, typically with no more than ? O(1) keys per node. Then the
8922309	6956	to a large data record to make their predictions. Several dynamic resource allocation techniques have been proposed for data centers that use modeling techniques to achieve resource guarantees . These techniques use measurement and prediction techniques to reallocate resources among applications based on their varying workload. Such techniques are complementary to our work and can be
8922309	24177	prototype, we now describe the application we hosted on the data center and the workload generator we used on the clients to generate the Web workload. As our hosted application, we used RUBiS  ??? an open-source Web application benchmark that emulates an auctioning website like Ebay . RUBiS is a multi-tiered application that uses a Web server, an application server and a database
8922309	24178	excessive power usage during the normal operational period. 1sRecent studies have shown the statistical multiplexing benefits of dynamic resource allocation over over-provisioning in a data center . These drawbacks of over-provisioning make it lucrative to use dynamic resource allocation to handle the relatively uncommon events of flash crowd conditions. As a result, several dynamic resource
8922309	24178	or service differentiation could be employed during the transient allocation period, and they can be relaxed once the requisite number of servers become available for service. Recent studies  have examined the effect of allocation parameters on resource provisioning gains in data center environments. These studies are different from our work in several respects. First of all, our work
8922309	24180	to handle the relatively uncommon events of flash crowd conditions. As a result, several dynamic resource allocation schemes have been proposed to better utilize the resources in such scenarios . These dynamic allocation schemes react to changing application loads by reallocating resources to overloaded applications, borrowing these resources from other under-utilized applications if
8922309	24180	Since data centers host multiple applications on a common server platform, they can dynamically reallocate resources among different applications. Several allocation schemes have been proposed  that perform reallocation on such platforms. Most of these schemes use a measure-detect-allocate cycle for reallocation, wherein they monitor the application workloads, detect any overload
8922309	24180	to a large data record to make their predictions. Several dynamic resource allocation techniques have been proposed for data centers that use modeling techniques to achieve resource guarantees . These techniques use measurement and prediction techniques to reallocate resources among applications based on their varying workload. Such techniques are complementary to our work and can be
8922309	24181	excessive power usage during the normal operational period. 1sRecent studies have shown the statistical multiplexing benefits of dynamic resource allocation over over-provisioning in a data center . These drawbacks of over-provisioning make it lucrative to use dynamic resource allocation to handle the relatively uncommon events of flash crowd conditions. As a result, several dynamic resource
8922309	24181	effectiveness of a dynamic resource allocation scheme depends on a large number of factors, such as the resource allocation algorithm, the granularity of allocation, the allocation overheads, etc. . In our study, we take an algorithm-independent approach in describing a dynamic allocation scheme. We define a set of orthogonal parameters to characterize an allocation scheme, which we describe
8922309	24181	or service differentiation could be employed during the transient allocation period, and they can be relaxed once the requisite number of servers become available for service. Recent studies  have examined the effect of allocation parameters on resource provisioning gains in data center environments. These studies are different from our work in several respects. First of all, our work
8922309	24182	to handle the relatively uncommon events of flash crowd conditions. As a result, several dynamic resource allocation schemes have been proposed to better utilize the resources in such scenarios . These dynamic allocation schemes react to changing application loads by reallocating resources to overloaded applications, borrowing these resources from other under-utilized applications if
8922309	24182	Since data centers host multiple applications on a common server platform, they can dynamically reallocate resources among different applications. Several allocation schemes have been proposed  that perform reallocation on such platforms. Most of these schemes use a measure-detect-allocate cycle for reallocation, wherein they monitor the application workloads, detect any overload
8922309	24182	vary in their underlying architecture, resulting in a difference in the allocation mechanisms they employ. For instance, Oceano  allocates whole server machines among applications, while MUSE  allows multiple applications to be co-hosted on the same server. Cluster-On-Demand  is a hybrid scheme that allows clusters to be allocated to virtual data centers that further share the
8922309	24182	In this paper, we focused on dedicated data center architectures . However, several other data center architectures have been proposed. These include shared architectures such as MUSE  that can host multiple applications on shared servers, and hybrid architectures like Cluster-on-Demand  that hierarchically allocate virtual clusters to a group of applications. Another
8922309	24183	to allocate servers with high allocation overheads. There have been several proposals for managing overload in Web workloads. Many of these approaches employ different forms of admission control , traffic shaping  and scheduling . These approaches use the techniques of workload shaping and service differentiation to meet the application performance needs. Our approach of using
8922309	24184	to a large data record to make their predictions. Several dynamic resource allocation techniques have been proposed for data centers that use modeling techniques to achieve resource guarantees . These techniques use measurement and prediction techniques to reallocate resources among applications based on their varying workload. Such techniques are complementary to our work and can be
8922309	24186	to allocate servers with high allocation overheads. There have been several proposals for managing overload in Web workloads. Many of these approaches employ different forms of admission control , traffic shaping  and scheduling . These approaches use the techniques of workload shaping and service differentiation to meet the application performance needs. Our approach of using
8922309	24187	allocation overheads. There have been several proposals for managing overload in Web workloads. Many of these approaches employ different forms of admission control , traffic shaping  and scheduling . These approaches use the techniques of workload shaping and service differentiation to meet the application performance needs. Our approach of using dynamic allocation under
8922309	24189	duration of flash crowds is difficult to predict for long-term provisioning, and the system has to be largely reactive to the arrival of a flash crowd relying on extremely short-term predictions . To handle flash crowds, data centers can either resort to high levels of over-provisioning, or employ dynamic resource allocation, wherein resources can be borrowed from unused servers or
8922309	24189	scheme) or by allocating servers often (as done by a fast reactive scheme). 5 Related Work A dynamic allocation technique to handle unexpected workload surges has been proposed in a recent work . This technique performs dynamic allocation using short-term load prediction coupled with early pre-allocation of resources. Our work differs from this work in the following aspects. First of all,
8922309	24193	this distribution as TPC-W is a popular e-commerce benchmark specification and emulates closely the user behavior for e-commerce applications). Finally, we replayed these traces using httperf  — an open-source Web workload generator — running on the client machines, to generate the workload during the experiments. Httperf allows varying various workload parameters such as the rate of
8922309	24194	load balancing scheme, where load was defined as the number of connections open to each Web server. We used such a load balancing scheme as opposed to localityaware schemes such as LARD , because our workload consisted predominantly of dynamic requests and was thus less sensitive to caching effects. In addition to distributing incoming sessions equally among the back-end servers,
8922309	24195	to handle the relatively uncommon events of flash crowd conditions. As a result, several dynamic resource allocation schemes have been proposed to better utilize the resources in such scenarios . These dynamic allocation schemes react to changing application loads by reallocating resources to overloaded applications, borrowing these resources from other under-utilized applications if
8922309	24195	application performance under varying allocation parameters, while also allowing us to systematically vary the workload parameters. In this paper, we focused on dedicated data center architectures . However, several other data center architectures have been proposed. These include shared architectures such as MUSE  that can host multiple applications on shared servers, and hybrid
8922309	24198	have been several proposals for managing overload in Web workloads. Many of these approaches employ different forms of admission control , traffic shaping  and scheduling . These approaches use the techniques of workload shaping and service differentiation to meet the application performance needs. Our approach of using dynamic allocation under overload scenarios is
8922309	24201	to allocate servers with high allocation overheads. There have been several proposals for managing overload in Web workloads. Many of these approaches employ different forms of admission control , traffic shaping  and scheduling . These approaches use the techniques of workload shaping and service differentiation to meet the application performance needs. Our approach of using
24211	2102	? = 6 for rich scattering metropolitan and indoor environments. Direct transmission as well as a MIMO system with two transmit and one receive antenna using the well known Alamouti space time code  for transmission (transmit diversity system) are depicted for performance comparison. Both operate at R=1 bit/s/Hz since they are not subject to the orthogonality constraint. We observe that in a
24211	13938	relay to source and destination and we do not retain a fixed transmission rate. We are currently investigating the performance of related protocols with considerably lower implementation complexity . The application of channel coding to our transmission schemes is a further path for investigation, although we may already state that, eventually, by usage of capacity-approaching codes, the
24211	13938	16-QAM, a = 1 and b = 4/5 from which Pb is easily obtained. A3. BER for Diversity Links For the sake of compactness, we only outline the solution, a more detailed derivation has been developed in . We again average (21) over the fading statistics, now using the pdf of two i.i.d. Rayleigh channel fading coefficients, which after partial integration and simplifications yields: Pb = a ? 2 1 ? ?
24211	13937	various layers. A frequently considered concept is the use of relay nodes to help transmit information from a source node to its destination. As an extension of this approach, cooperative diversity  or virtual antenna arrays  exploit the inherent spatial diversity of the relay channel by allowing mobile terminals to cooperate. More generally, by taking the original signal copy sent by the
24211	13937	exploit useful side information that conventional relaying systems unnecessarily discard as noise. For statistically independent channels between all nodes in a single relay system, it was shown in  that full 2 nd order diversity can be achieved asymptotically, i.e. the outage probability as a function of the SNR decays with slope 2 (? = 2) on a logarithmic scale: p out ? 1 , SNR ? 1. (1) ?
24211	13937	sequences with variance N0. For the path loss, we assume a log-distance model, i.e. the received power decreases linearly with distance, on a logarithmic scale. An interesting approach taken in  suggests to model the effects of path loss into the variance of the fading variables by observing that the SNR at a specific node j obtained by transmission from node i can be written as: SNRi,j =
24211	13937	However, since only long term statistics (the average path loss) need to be available, this is only a low increase in complexity. As another modification of the original adaptive protocol , we refrain from letting the source repeat its message whenever the relay decides not to forward it, since repetition coding over a quasi-static channel does not yield significant benefits but
24211	13937	as the quotient of the SNRs required by non-cooperative and cooperative transmission to attain the same outage probability. The outage probability for direct transmission can be formulated as  p out D = 2R ? 1 Solving for SNRD yields , SNRD ? 1. (11) SNRD SNRD = 2R ? 1 pout , SNRD ? 1. (12) D and applying the same to the outage probability of the detached protocol results in ? g SNRDADF
24211	24216	probability as a function of the SNR decays with slope 2 (? = 2) on a logarithmic scale: p out ? 1 , SNR ? 1. (1) ? SNR While this constitutes a very promising result, it has been demonstrated in  that for symmetric network constellations 1 the performance of cooperative schemes is inferior to that of direct transmission in the low SNR regime as well as for high spectral efficiencies. The
24211	24216	repetition coding during the second time slot. Obviously, the source must have knowledge of the relay’s decoding decision, which can for example be obtained via a dedicated feedback channel. In  it was shown that preventing error propagation at the relay is crucial for achieving high performance and that under practical assumptions regarding the knowledge of channel state information,
24211	24216	to a transmit diversity system, compared to a prediction of 5.5 dB from  and matching the results for the traditional adaptive decode-and-forward protocol in practical systems obtained in . By placing the relay halfway between source and destination, the novel protocol is able to fully exploit the nonlinear properties of path loss. The “cost of repetition” is greatly reduced and the
24239	5023	a challenging problem. CSMA/CA-based MAC protocols have been proposed, , whereas others have additionally employed handshake mechanisms like the Ready-ToSend/Clear-To-Send (RTS/CTS) mechanism, , , , , to avoid the hidden/exposed terminal problem. TDMA-based MAC protocols have also been proposed for ad-hoc networks. S-TDMA, proposed by Kleinrock and Nelson, , is capable of
24239	19421	problem. CSMA/CA-based MAC protocols have been proposed, , whereas others have additionally employed handshake mechanisms like the Ready-ToSend/Clear-To-Send (RTS/CTS) mechanism, , , , , to avoid the hidden/exposed terminal problem. TDMA-based MAC protocols have also been proposed for ad-hoc networks. S-TDMA, proposed by Kleinrock and Nelson, , is capable of providing
24239	24243	collision-free scheduling based on the exploitation of noninterfering transmissions in the network. Other collision-free protocols, mechanisms or algorithms have been proposed recently, , , , , , . Topology-Unaware TDMA MAC schemes, under which the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct
24239	24244	collision-free scheduling based on the exploitation of noninterfering transmissions in the network. Other collision-free protocols, mechanisms or algorithms have been proposed recently, , , , , , . Topology-Unaware TDMA MAC schemes, under which the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct
24239	15352	scheduling based on the exploitation of noninterfering transmissions in the network. Other collision-free protocols, mechanisms or algorithms have been proposed recently, , , , , , . Topology-Unaware TDMA MAC schemes, under which the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct transmission
24239	24245	scheduling based on the exploitation of noninterfering transmissions in the network. Other collision-free protocols, mechanisms or algorithms have been proposed recently, , , , , , . Topology-Unaware TDMA MAC schemes, under which the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct transmission is
24239	9855	schemes, under which the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct transmission is possible), have also been proposed, , , , , . In particular, Farago proposed the Deterministic Policy, , whereas the Probabilistic Policy has been proposed and analyzed in ,  and . This analysis has
24239	9855	transmission u ? v when topology control is applied (corresponding either to S S u?v or S P u?v); in view of the above, |S T u?v| ?|S O u?v|. IV. SCHEDULING POLICIES Under the policy proposed in , each node u ? V is randomly assigned a unique polynomial fu of degree k with coefficients from a finite Galois field of order q (GF (q)). Polynomial fu is represented as fu(x) = ?k i=0 aixi (mod
24239	9855	s, (s =0, 1, ..., q ? 1) is given by fu(s)mod q, . Let the set of time slots assigned to node u be denoted as ?u. Consequently, |?u| = q. The deterministic transmission policy, proposed in  and , is the following. The Deterministic Policy: Each node u transmits in a slot i only if i ? ?u, provided that it has data to transmit. The assignment of the unique polynomials, or
24239	24247	under which the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct transmission is possible), have also been proposed, , , , , . In particular, Farago proposed the Deterministic Policy, , whereas the Probabilistic Policy has been proposed and analyzed in ,  and . This analysis has shown
24239	24247	node u ? V is randomly assigned a unique polynomial fu of degree k with coefficients from a finite Galois field of order q (GF (q)). Polynomial fu is represented as fu(x) = ?k i=0 aixi (mod q), , where ai ?{0, 1, 2, ..., q ? 1}; parameters q and k are calculated based on N and D, according to the algorithm presented either in  or . For both algorithms it is satisfied that k ? 1 and
24239	24247	with a frame consisted of q2 time slots. If the frame is divided into q subframes s of size q, then the time slot assigned to node u in subframe s, (s =0, 1, ..., q ? 1) is given by fu(s)mod q, . Let the set of time slots assigned to node u be denoted as ?u. Consequently, |?u| = q. The deterministic transmission policy, proposed in  and , is the following. The Deterministic Policy:
24239	24247	It is obvious that for k > 1, |RO u?v | ? qD. Consequently, the number of non-assigned eligible slots may be quite significant for the cases where k > 1 (this case corresponds to “large networks,” ). Even for the case where k =1, |RO u?v |?0, that is, |RO u?v | can still be significantly greater than zero, as it may be seen from Theorem 1. Theorem 1: It is satisfied that |RO u?v| ?q2 ?q(|Sv|+
24239	24247	ST u?v = SS u?v ? SO u?v. Four different networks of 100 nodes are considered during the simulations, for D = 10 and |S|/D = 0.212, 0.424, 0.614 and 0.866, respectively. The algorithm presented in  is used to derive the sets of scheduling slots and the system throughput is calculated averaging the simulation results over 100 frames. Unique polynomials, that correspond to time slot sets ??,
24239	24249	the assignment of time slots to nodes does not consider the time slots assigned to the neighbor nodes (nodes that a direct transmission is possible), have also been proposed, , , , , . In particular, Farago proposed the Deterministic Policy, , whereas the Probabilistic Policy has been proposed and analyzed in ,  and . This analysis has shown that the
24239	24249	non-assigned eligible time slots for transmission u ? v, that if used by transmission u ? v, the probability of success for the particular transmission would be increased. As it was shown in , , |RO u?v | ? q(k ? 1)D. It is obvious that for k > 1, |RO u?v | ? qD. Consequently, the number of non-assigned eligible slots may be quite significant for the cases where k > 1 (this case
24239	24249	be left by neighboring nodes under non-heavy traffic conditions. On the other hand, the 5sprobabilistic transmission attempts induce interference to otherwise collision-free transmissions, , . The Probabilistic Policy is capable of utilizing the nonassigned eligible time slots under topology control, and potentially benefit more than under traditional omnidirectional antennas since |R T
24239	24249	Fig. 5. System throughput (P ) as a function of p. psFrom Equation (8) it can be concluded that P O P is influenced by |S O u?v| in an exponential manner. This has been extensively studied in , , , and an efficient topology density metric, capable of capturing the density of the topology, was introduced. The topology density is denoted by |S|/D, where |S| = 1 N ? ?u?V |SO u?v|, v ? Su.
24239	24249	applies for smart antennas as depicted in Figure 7(c). In general, under topology control the effect of the mobility factor increases. Mobility was not considered in previous related work, , , , and it is interesting to describe how the network behaves after a link failure under topology control or under no topology control. Consider the traditional omni-directional antenna case
24239	24251	for ad-hoc networks that exploit the capabilities of smart antennas. The majority of them is based on random access schemes (i.e. ALOHA or CSMA/CA) and enhancements of the RTS/CTS mechanism, , , . Power control may also be used for topology control. The transmission power is possible to be adjusted according to the location of the receiver and reduce the interference caused to
24239	24254	used for topology control. The transmission power is possible to be adjusted according to the location of the receiver and reduce the interference caused to neighbor nodes by the transmitting node, , , , , . Resolution is an important factor of topology control. The higher the resolution of the topology control, the narrower the transmission beam of the smart antennas and/or
24239	24255	topology control. The transmission power is possible to be adjusted according to the location of the receiver and reduce the interference caused to neighbor nodes by the transmitting node, , , , , . Resolution is an important factor of topology control. The higher the resolution of the topology control, the narrower the transmission beam of the smart antennas and/or the
24239	24256	control. The transmission power is possible to be adjusted according to the location of the receiver and reduce the interference caused to neighbor nodes by the transmitting node, , , , , . Resolution is an important factor of topology control. The higher the resolution of the topology control, the narrower the transmission beam of the smart antennas and/or the smaller
24239	24257	control. The transmission power is possible to be adjusted according to the location of the receiver and reduce the interference caused to neighbor nodes by the transmitting node, , , , , . Resolution is an important factor of topology control. The higher the resolution of the topology control, the narrower the transmission beam of the smart antennas and/or the smaller the
24239	24258	The transmission power is possible to be adjusted according to the location of the receiver and reduce the interference caused to neighbor nodes by the transmitting node, , , , , . Resolution is an important factor of topology control. The higher the resolution of the topology control, the narrower the transmission beam of the smart antennas and/or the smaller the
24180	6956	that employs the proposed queuing model. An important feature of our optimization-based approach is that it can handle non-linearity in system behavior unlike some approaches that assume linearity . Determining resource shares of applications using such an online approach is crucially dependent on an accurate estimation of the application workload characteristics. A second contribution of our
24180	6956	techniques are applicable to many scenarios where the underlying system or resource can be abstracted using a GPS server. Some adaptive systems employ a control-theoretic adaptation technique . Most of these systems (with the exception of ) use a pre-identified system model. In contrast, our technique is based on online workload characterization and prediction. Further, these
24180	24267	in the presence of varying workloads. 5 Related Work Several research efforts have focused on the design of adaptive systems that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource
24180	20223	our study—synthetic and trace-driven. Our synthetic workloads use Poisson request arrivals and deterministic request sizes. Our tracedriven workload is based on the World Cup Soccer ’98 server logs ???a publiclysavailable web server trace. Here, we present results based on a 24-hour long portion of the trace that contains a total of 755,705 requests at a mean request arrival rate of 8.7
24180	24268	server. This effort is similar to our approach of modeling the resource to relate the QoS metrics and resource shares. Other techniques for dynamic resource allocation have also been proposed in . Our work differs from these techniques in some significant ways. First of all, we define an explicit model to derive the relation between the QoS metric and resource requirements, while a linear
24180	6963	research efforts have focused on the design of adaptive systems that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource with multiple class-specific queues and presented
24180	24269	model where service differentiation between different customers for the same application may be desirable.sdoes not make steady-state assumptions about the system (unlike some previous approaches ) and adapts to changing application behavior. To achieve a feasible resource allocation even in the presence of transient overloads, we employ a non-linear optimization technique that employs the
24180	24269	have focused on the design of adaptive systems that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource with multiple class-specific queues and presented techniques for dynamic resource allocation;
24180	24269	employs a non-linear model derived using the queuing dynamics of the system, and further, we update the model parameters with changing workload. Other approaches for resource sharing in web servers  and e-business environments  have used a queuing model with non-linear optimization. The primary difference between these approaches and our work is that they use steady-state queue behavior to
24180	24180	techniques. Due to space constraints, we omit results related to our prediction technique and those based on longer portions of the trace. More detailed results can be found in a technical report . 4.2 Dynamic Resource Allocation In this section, we evaluate our dynamic resource allocation techniques. We conduct two experiments, one with a synthetic web workload and the other with the trace
24180	24182	server. This effort is similar to our approach of modeling the resource to relate the QoS metrics and resource shares. Other techniques for dynamic resource allocation have also been proposed in . Our work differs from these techniques in some significant ways. First of all, we define an explicit model to derive the relation between the QoS metric and resource requirements, while a linear
24180	24182	a modified scheduling scheme to achieve dynamic resource allocation, while our scheme achieves the same goal with existing schedulers using high-level parameterization. The approach described in  uses an economic model similar to our utility-based approach. This approach employs a greedy algorithm coupled with a linear system model for resource allocation, while we employ a non-linear
24180	8655	allocated to each application depends on the expected workload and the QoS requirements of the application. The workload of web applications is known to vary dynamically over multiple time scales  and it is challenging to estimate such workloads a priori (since the workload can be influenced by unanticipated external events—such as a breaking news story—that can cause a surge in the number
24180	4077	a share ?i = wi ? j wj (i.e., allocated (?i ·C) units of the resource capacity when all queues are backlogged). Several practical instantiations of GPS exist—such as weighted fair queuing (WFQ) , self-clocked fair queuing , and start-time fair queuing —and any such scheduling algorithm suffices for our purpose. We note that these GPS schedulers are work-conserving—in the event a
24180	4077	queues; the experiments reported in this paper specifically model the network interface on a shared server. Requests across various queues are scheduled using weighted fair queuing ???a practical instantiation of GPS. Our simulator is based on the NetSim library  and DASSF simulation package ; together these components support network elements such as queues, traffic
24180	24184	changes in workload, while the queuing theoretic approach attempts to schedule requests based on the steady-state workload. A model-based resource provisioning scheme has been proposed recently  that performs resource allocation based on thesperformance modeling of the server. This effort is similar to our approach of modeling the resource to relate the QoS metrics and resource shares.
24180	24273	capacity when all queues are backlogged). Several practical instantiations of GPS exist—such as weighted fair queuing (WFQ) , self-clocked fair queuing , and start-time fair queuing —and any such scheduling algorithm suffices for our purpose. We note that these GPS schedulers are work-conserving—in the event a queue does not utilize its allocated share, the unused capacity is
24180	22154	have focused on the design of adaptive systems that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource with multiple class-specific queues and presented techniques for dynamic resource allocation;
24180	7057	that employs the proposed queuing model. An important feature of our optimization-based approach is that it can handle non-linearity in system behavior unlike some approaches that assume linearity . Determining resource shares of applications using such an online approach is crucially dependent on an accurate estimation of the application workload characteristics. A second contribution of our
24180	7057	include the network interface bandwidth, the CPU and in some cases, the disk bandwidth, while software resource include socket accept queues in a web server servicing multiple virtual domains .s2.2 Problem Definition Consider a shared server that runs multiple third-party applications. Each such application is assumed to specify a desired quality of service (QoS) requirement; here we
24180	7057	have focused on the design of adaptive systems that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource with multiple class-specific queues and presented techniques for dynamic resource allocation;
24180	7057	techniques are applicable to many scenarios where the underlying system or resource can be abstracted using a GPS server. Some adaptive systems employ a control-theoretic adaptation technique . Most of these systems (with the exception of ) use a pre-identified system model. In contrast, our technique is based on online workload characterization and prediction. Further, these
24180	24275	techniques are applicable to many scenarios where the underlying system or resource can be abstracted using a GPS server. Some adaptive systems employ a control-theoretic adaptation technique . Most of these systems (with the exception of ) use a pre-identified system model. In contrast, our technique is based on online workload characterization and prediction. Further, these
24180	6277	1.2 Research Contributions In this paper, we present techniques for dynamic resource allocation in shared web servers. We model various server resources using generalized processor sharing (GPS)  and assume that each application is allocated a certain fraction of a resource. Using a combination of online measurement, prediction and adaptation, our techniques can dynamically determine the
24180	6970	include the network interface bandwidth, the CPU and in some cases, the disk bandwidth, while software resource include socket accept queues in a web server servicing multiple virtual domains .s2.2 Problem Definition Consider a shared server that runs multiple third-party applications. Each such application is assumed to specify a desired quality of service (QoS) requirement; here we
24180	6970	have focused on the design of adaptive systems that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource with multiple class-specific queues and presented techniques for dynamic resource allocation;
24180	24195	that can react to workload changes in the context of storage systems , general operating systems , network services , web servers  and Internet data centers . In this paper, we focused on an abstract model of a server resource with multiple class-specific queues and presented techniques for dynamic resource allocation; our model and allocation
24180	24195	workloads at short time granularities of upto a few minutes and to respond quickly to transient overloads. Two recent efforts have focused on workload-driven allocation in dedicated data centers . In these efforts, each application is assumed to run on some number of dedicated servers and the goal is to dynamically allocate and deallocate (entire) servers to applications to handle workload
24278	24282	aggregated into bursts. Bursts are typically released into the optical layer before the acknowledgement of a successful lightpath reservation, except in the special case of wavelength-routed OBS , . Several such oneway reservation schemes have been proposed of which the just-enough-time (JET)  reservation protocol has received the most attention. In JET/OBS, a control packet is
24278	24282	into bursts. Bursts are typically released into the optical layer before the acknowledgement of a successful lightpath reservation, except in the special case of wavelength-routed OBS , . Several such oneway reservation schemes have been proposed of which the just-enough-time (JET)  reservation protocol has received the most attention. In JET/OBS, a control packet is released
24278	24290	We apply the new analysis to evaluate the benefit of deploying deflection routing and wavelength reservation in a sample OBS network. I. INTRODUCTION Optical burst switching (OBS) ,  may be a suitable switching paradigm for the envisaged Optical Internet. In OBS, data packets, including internet protocol (IP), arriving at the same source node (edge node), with a common
24278	24290	of a successful lightpath reservation, except in the special case of wavelength-routed OBS , . Several such oneway reservation schemes have been proposed of which the just-enough-time (JET)  reservation protocol has received the most attention. In JET/OBS, a control packet is released into the optical layer, which consists of optical cross-connects (OXCs) interconnected through a
24278	24290	packet failing to make a wavelength reservation. Two methods proposed to resolve wavelength contention are fibre delay lines 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004s(FDLs) , ,  and deflection routing , , , . The former relies on FDLs to temporarily buffer the burst in the optical domain until a reservation can be made on the link that is in contention. The
24278	24291	with deflection routing and wavelength reservation. We have presented the details of a simpler reduced load Erlang fixed point approximation for OBS networks without deflection routing, see . Erlang fixed point approximations of circuit switched networks have been extensively considered, for example see . In Section V, we apply the new analysis to evaluate the benefit of deploying
24278	24291	topology comprises of 13 OXCs and 32 directed links containing one fibre, each comprising of 120 wavelengths. We consider the same 12 SD pairs and corresponding set of primary routes defined in . The selected primary routes represent a variety of lengths, link sharing degrees and mixtures of external and on-route traffic processes. All deflection routes are chosen as shortest hop routes
24278	24293	Two methods proposed to resolve wavelength contention are fibre delay lines 0-7803-8356-7/04/$20.00 (C) 2004 IEEE IEEE INFOCOM 2004s(FDLs) , ,  and deflection routing , , , . The former relies on FDLs to temporarily buffer the burst in the optical domain until a reservation can be made on the link that is in contention. The latter is, however, a more viable method
1628315	24299	more robust to channel impairments in a video communication system. In addition, an I-frame MPEG video stream facilitates manyvideo editing operations, e.g. DCT-domain image processing algorithms  can be applied to each frame of the sequence to achieve the same e ect on the entire sequence. Temporal mode conversions can be used to accommodate particular rate/robustness pro les or IPB
1628315	24300	and backward motion vectors. We then appropriately reorder the data, perform rate control, and update the header information. A detailed description of the reverse-play algorithm is given in . The resulting MPEG stream will be slightly larger than the original stream because of the extra data that results from the P ! I frame conversion. If the increase in data rate is not acceptable,
24341	12296	gain than MRC for all the adaptive policies, as expected. Although the results derived herein are Shannon bounds, similar analysis has been applied to adaptive MQAM modulation without diversity . Thus, we expect that the same general trends will be observed on any adaptive modulation method, although the spectral efficiency will be smaller. APPENDIX A EVALUATION OF (14) We evaluate the
24341	12207	path between the transmitter and receiver antennas, as well as to ionospheric  and tropospheric scatter  channels. Our analyses can also be generalized to Nakagami fading channels . The Shannon capacity of a channel defines its theoretical upper bound for the maximum rate of data transmission at an arbitrarily small BER, without any delay or complexity constraints. Therefore,
24341	20198	an optimistic bound for practical communication schemes, and also serves as a benchmark against which to compare the spectral efficiency of all practical adaptive transmission schemes . In , the capacity of a single-user flatfading channel with channel measurement information at the transmitter and receiver was derived for various adaptive transmission policies. In this paper, we
24341	20198	AND RATE ADAPTATION Given an average transmit power constraint, the channel capacity of a fading channel with received CNR distribution and optimal power and rate adaptation ( ) is given in  as where  is the channel bandwidth and is the optimal cutoff CNR level below which data transmission is suspended. This optimal cutoff must satisfy To achieve the capacity (7), the channel fade
24341	20198	inversion) tends to zero. IV. OPTIMAL RATE ADAPTATION WITH CONSTANT TRANSMIT POWER With optimal rate adaptation to channel fading with a constant transmit power, the channel capacity  becomes ,  (29) was previously introduced by Lee  as the average channel capacity of a flat-fading channel, since it is obtained by averaging the capacity of an AWGN channel (30) over the
24341	20198	CHANNEL INVERSION WITH FIXED RATE The channel capacity when the transmitter adapts its power to maintain a constant CNR at the receiver (i.e., inverts the channel fading) was also investigated in . This technique uses fixed-rate modulation and a fixed code design, since the channel after channel inversion appears as a time-invariant AWGN channel. As a result, channel inversion with fixed
24341	20198	assuming good channel estimates are available at the transmitter and receiver. The channel capacity with this technique ) is derived from the capacity of an AWGN channel and is given in  as (46) Channel inversion with fixed rate suffers a large capacity penalty relative to the other techniques, since a large amount of the transmitted power is required to compensate for the deep
24360	15142	a number of schemes have been proposed to detect or recover from transient errors in processor computations. All these techniques are either based on space redundancy or time redundancy. DIVA  employs a simple “checker” to verify the results of instructions ready to be committed by the high performance core. The checker is a standard five-stage in-order processor designed with
24360	24369	instructions. Pipelining improves performance by increasing instruction level parallelism (ILP). Five to eight stage pipelines are quite common, and some recent designs use twenty or more stages . Such designs are commonly referred to as superpipelined designs. Our study focuses on soft errors, which are also called transient faults or single-event upsets (SEUs). These are errors in
24360	3868	that can be placed between two latches in a single pipeline stage. We use the FO4 metric because it allows us to characterize pipeline depth in a way that is largely independent of device scaling . During the last twelve years technology has scaled from 1000nm to 130nm and the amount of logic per pipeline stage has decreased from 84 to 12 FO4 contributing to a total of 60-fold increase in
24360	7878	to a total of 60-fold increase in clock frequency in the Intel family of processors, and aggressive pipelining could further reduce this to as few as 6 over the next five to seven years . For a given degree of pipelining, the number of gates in the pipeline stage is the largest number that does not exceed the total delay of the corresponding FO4 chain. CLKBAR INPUT OUTPUT Passgate
24360	24377	have both a spatial and temporal gap they will not be affected by the temporal or spatial locality of the particles. ARSMT , SRT , and the Out-Of-Order Reliable Superscalar (O3RS) approach  all execute instructions redundantly and then check that the results match before committing the result to architected state. Both ARSMT  and SRT  use a hardware mechanism called
24360	15141	caused by the high performance core. Since the recomputations have both a spatial and temporal gap they will not be affected by the temporal or spatial locality of the particles. ARSMT , SRT , and the Out-Of-Order Reliable Superscalar (O3RS) approach  all execute instructions redundantly and then check that the results match before committing the result to architected state. Both
24360	15150	effect caused by the high performance core. Since the recomputations have both a spatial and temporal gap they will not be affected by the temporal or spatial locality of the particles. ARSMT , SRT , and the Out-Of-Order Reliable Superscalar (O3RS) approach  all execute instructions redundantly and then check that the results match before committing the result to architected
24360	24381	3. The values shown are for NMOS devices, but are essentially equivalent to PMOS devices. Note: The data presented in Figure 11 differs somewhat from that contained in our earlier conference paper . This is due to a minor problem in our technique for determining ¡ ? ? ??? ¡ ? ??? ? ? ??? ¡¥? ¡ ? ¡ ¡ ? ¢¥¤§¦©¨ which overstated values whenever was less than . Fortunately, this error has
24360	3889	is affected by the input(s) to the gate. We use a form of the Horowitz gate delay model that allows the switching gate and the following gate to have different switching voltages, as described in . This model is given by Equation 5 (for a rising input) and Equation 6 (for a falling input).s¢¡¤£¦¥¢§¢¨?©?????? ?????s?¡¤£?¥?§ ??????? ? ? ? ? ????????????????? ? ??????????????? ¨?©?????? ? ? ?
24386	25999	in terms of retrieval performance using the Princeton Benchmark data set. The idea for the semantic layer and knowledge base draws on previous work at Southampton and semantic web technology. Various other groups have reported the use of ontologies for image annotation.s640 S. Goodall et al. 3 Identifying User Needs The ARTISTE project combined metadata based retrieval with both
24386	24390	facilities to meet some specific needs of the participating museums. These included, for example, sub-image location, low quality query image handling and canvas crack analysis and retrieval. The ARTISTE system was the starting point for our work in SCULPTEUR and the new needs of the museums were identified, particularly in relation to their increasing use of 3-D digital
24386	24391	described. 2 Related Work The SCULPTEUR project is related to and builds on previous work in several fields. A major source of inspiration comes from previous work on the ARTISTE and MAVIS projects and a large body of other published work on content based image retrieval systems. 3-D model capture is an important element and one of the partners, GET-ENST, has developed techniques for
24386	15607	in several fields. A major source of inspiration comes from previous work on the ARTISTE and MAVIS projects and a large body of other published work on content based image retrieval systems. 3-D model capture is an important element and one of the partners, GET-ENST, has developed techniques for accurately generating 3-D models from multiple views. Various authors have published
24386	23538	using a variety of feature vectors extracted from mesh based 3-D representations based on for example, 3-D Hough transforms, 3-D moments and surface features such as chord distributions and radial axis distributions. The work from Princeton has been particularly influential in this area and a recent paper on 3-D benchmarking compares a range of matching algorithms in terms
24386	23538	and integrated with retrieval facilities in the first prototype to provide 3-D CBR. These include the D2 shape distribution descriptors from the Princeton Shape Retrieval and Analysis Group, the histogram descriptors from Paquet and Rioux and the Area to Volume Ratio descriptor which is a single statistic giving the ratio of the surface area of the model to its enclosed
24386	24394	and radial axis distributions. The work from Princeton has been particularly influential in this area and a recent paper on 3-D benchmarking compares a range of matching algorithms in terms of retrieval performance using the Princeton Benchmark data set. The idea for the semantic layer and knowledge base draws on previous work at Southampton and semantic web
24386	24394	respectively. Each of these was implemented as a 16 bin histogram. Before including them in the prototype, an evaluation was made of the five 3-D algorithms for CBR. Using the Princeton Benchmark base dataset, (training group) consisting of 907 models representing about 90 object classes, precisionrecall graphs of the five algorithms were created. They are shown in figure 2 together with
24386	24395	authors have published algorithms for 3-D model matching using a variety of feature vectors extracted from mesh based 3-D representations based on for example, 3-D Hough transforms, 3-D moments and surface features such as chord distributions and radial axis distributions. The work from Princeton has been particularly influential in this area and a recent paper on 3-D benchmarking
24416	24417	to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus was limited to the simplest and most used rules, like majority vote, linear combination of classifier
24416	24417	improvement achievable by simple averaging the outputs of an ensemble of classifiers, was developed by Tumer and Ghosh . In previous works, we extended this framework to weighted average , and to classification with reject option . In these works, we focused on the comparison between the performance of simple and weighted average. In particular, we provided analytical results
24416	24417	the improvement achievable by weighted average over simple average, for classifier ensembles made up of three classifiers, and under the hypothesis of unbiased and uncorrelated estimation errors . We also provided a preliminary analysis of the effects of correlated estimation errors. In this work, we extend the above analysis of linear combiners. In particular, we determine the conditions
24416	24417	factor of the variance component is equal to N, if the errors are unbiased and uncorrelated, while it is lower (higher) than N if the errors are positively (negatively) correlated . In , we extended the above framework to analyze and compare the performance of simple and weighted average (WA). We considered the case in which the estimate ˆ P( ? i | x) of the a posteriori
24416	24417	it turns out that SA minimises Eadd only if the individual m classifiers exhibit the same performance (i.e., equal values of Eadd ), and equal values of ? mn , ? m, n. Otherwise, WA is needed. In , we exploited the above results to analyse the performance improvement SA WA achievable by WA over SA, i.e., Eadd ? Eadd , for ensembles of three classifiers ( N = 3). Note that the difference
24416	24418	outputs of an ensemble of classifiers, was developed by Tumer and Ghosh . In previous works, we extended this framework to weighted average , and to classification with reject option . In these works, we focused on the comparison between the performance of simple and weighted average. In particular, we provided analytical results which showed how the difference between the
24416	1956	to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus was limited to the simplest and most used rules, like majority vote, linear combination of classifier
24416	1957	to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus was limited to the simplest and most used rules, like majority vote, linear combination of classifier
24416	16140	designing multiple classifier systems based on linear combiners. 1 Introduction Recently, some works started to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus
24416	24420	designing multiple classifier systems based on linear combiners. 1 Introduction Recently, some works started to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus
24416	24422	to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus was limited to the simplest and most used rules, like majority vote, linear combination of classifier
24416	24422	improvement achievable by simple averaging the outputs of an ensemble of classifiers, was developed by Tumer and Ghosh . In previous works, we extended this framework to weighted average , and to classification with reject option . In these works, we focused on the comparison between the performance of simple and weighted average. In particular, we provided analytical results
24416	24422	the improvement achievable by weighted average over simple average, for classifier ensembles made up of three classifiers, and under the hypothesis of unbiased and uncorrelated estimation errors . We also provided a preliminary analysis of the effects of correlated estimation errors. In this work, we extend the above analysis of linear combiners. In particular, we determine the conditions
24416	24422	factor of the variance component is equal to N, if the errors are unbiased and uncorrelated, while it is lower (higher) than N if the errors are positively (negatively) correlated . In , we extended the above framework to analyze and compare the performance of simple and weighted average (WA). We considered the case in which the estimate ˆ P( ? i | x) of the a posteriori
24416	24422	it turns out that SA minimises Eadd only if the individual m classifiers exhibit the same performance (i.e., equal values of Eadd ), and equal values of ? mn , ? m, n. Otherwise, WA is needed. In , we exploited the above results to analyse the performance improvement SA WA achievable by WA over SA, i.e., Eadd ? Eadd , for ensembles of three classifiers ( N = 3). Note that the difference
24416	24424	designing multiple classifier systems based on linear combiners. 1 Introduction Recently, some works started to analyze the theoretical foundations of techniques for combining multiple classifiers . Some works also provided analytical comparisons between the performance of different techniques . Because of the complexity of developing analytical models of combining rules, the focus
24416	24424	on linear combiners. A theoretical framework for evaluating the performance improvement achievable by simple averaging the outputs of an ensemble of classifiers, was developed by Tumer and Ghosh . In previous works, we extended this framework to weighted average , and to classification with reject option . In these works, we focused on the comparison between the performance of
24416	24424	and multiple classifiers combined by simple average (SA), as a function of bias, variance and pair-wise correlation of estimation errors that affect the a posteriori probability estimates . It turns out that the added error, for a given class boundary, is proportional to the sum of the variance and the squared bias of the estimation errors. For classifiers combined by SA, the
24416	24424	the reduction factor of the variance component is equal to N, if the errors are unbiased and uncorrelated, while it is lower (higher) than N if the errors are positively (negatively) correlated . In , we extended the above framework to analyze and compare the performance of simple and weighted average (WA). We considered the case in which the estimate ˆ P( ? i | x) of the a posteriori
557801	24426	diagnosis , A is the set of integer values with total order ?, and U returns for each mode assignment the number fault mode assignments. In probabilistic diagnosis , A is the interval  with total order ?, and U associates a probability value with each mode assignment. In subset-minimal diagnosis , A is the lattice of subsets of Z with partial order ?, and each mode
557801	24426	Diagnoses where the preference criterion is the probability of mode assignments, and the goal is to maximize the probability of a mode assignment, can be obtained by choosing the semiring Sp = (, max, ·, 0, 1). For probabilistic diagnosis, the objective function being ×-decomposable corresponds to the assumption that failures are conditionally independent of each other. 5 DECOMPOSITION AND
557801	24426	not necessarily idempotent, which means that constraint propagation cannot be applied in a “chaotic” way anymore. Research that aims at extending the notion of local consistency to soft constraints  has therefore focused on directional consistency, where constraints are propagated in an organized way following a hierarchical (tree) scheme. However, arbitrary constraint networks are not
557801	24426	as semi-join, is the step that establishes directional consistency between a node and its parent. It is a generalization of directional arc consistency for CSPs  to the case of soft constraints . The restriction operator | b ? “prunes” tuples of a constraint by setting their value to 0 if it is worse than b. Formally, fj | b ? returns a function f ? j where f ? j(t) = fj(t) if fj(t) ?S b,
557801	24427	Intelligence can be framed as optimization problems where the task is to find a best assignment to a set of variables, such that a set of constraints is satisfied. Formalisms for soft constraints  aim at more closely integrating constraint satisfaction and optimization. Soft constraints extend hard constraints by defining preference levels for the constraints, such that assignments are
557801	24427	diagnoses are o1=B, o2=G, o3=G, a1=G, a2=G with value {o1}, o1=G, o2=G, o3=G, a1=B, a2=G with value {a1}, and o1=G, o2=B, o3=G, a1=G, a2=B with value {o2, a2}. 3 SEMIRING-CSPS Semiring-CSPs  are a framework for “soft” constraints where the constraints are extended to include a preference level. SemiringCSPs subsume many other notions of preferences in constraints, such as fuzzy CSPs
557801	24427	b) + (a × c)). For instance, Sb = ({0, 1}, ?, ?, 0, 1) forms a c-semiring. The idempotency of the + operation induces a partial order ?S over A as follows: a ?S b iff a + b = b (for Sb, 0 ?S 1). In  it is shown that (A, ?S) forms a lattice. The partial order defines levels of preference and allows to select the “best” solutions for constraints defined over a c-semiring. 2 Definition 4
557801	24427	For model-based diagnosis, non-trivially ×-composed objective functions correspond to the assumption that faults or sets of faults occur independently of each other. Together with the results in , Theorem 1 establishes a oneto-one correspondence between lattice preference structures over “hard” constraints (i.e., {?, ?} functions) and semiring-CSPs. Up to now, we have two separate types of
557801	24427	end if end for Figure 4. Bottom-up phase for solving a tree-structured semiring-CSP through dynamic programming ?S b. This exploits the property that in a c-semiring, the ×-operator is extensive , i.e., (a × b) ?S a for all a, b ? A. Values for solutions can be found by calling solve(root(T )), where root(T ) is the root node of T . After completion of the algorithm, the best value of the
557801	11832	tree-structured evaluation scheme. Methods for decomposition of constraint networks  can be extended to turn semiring-CSPs into equivalent, tree-structured instances. Expanding on previous work , we present algorithms for solving semiring-CSPs based on tree decompositions and directional consistency (an instance of dynamic programming) that 1 MIT CSAIL, Cambridge, MA, USA email:
557801	11832	global objective function and to define preference levels locally, i.e., per each constraint, such that the ranking of solutions is still preserved. This builds on conditions that were defined in  in the context of cost-based optimization in tree-structured CSPs. We illustrate how these conditions correspond to assumptions commonly made in model-based diagnosis. Definition 6 (Composed
557801	11832	(acyclic) instances, possibly by aggregating constraints together. Decomposition was developed in the context of hard constraints, but the idea can be naturally extended to constraint optimization . Structural decomposition is based on the hypergraph H of a constraint system (X, D, F ), which associates a node with each variable xi ? X, and a hyperedge with each constraint fj ? F . Figure 2
557801	11832	This step can be viewed as a search that is guided by an exact heuristic, and is therefore backtrackfree. Previous work on constraint optimization based on decomposition and dynamic programming  has focussed on the task of computing best values for individual variables or a single best assignment to all variables. We extend this work to address important requirements of the diagnosis
557801	5566	tree-structured evaluation scheme. Methods for decomposition of constraint networks  can be extended to turn semiring-CSPs into equivalent, tree-structured instances. Expanding on previous work , we present algorithms for solving semiring-CSPs based on tree decompositions and directional consistency (an instance of dynamic programming) that 1 MIT CSAIL, Cambridge, MA, USA email:
557801	5566	This step can be viewed as a search that is guided by an exact heuristic, and is therefore backtrackfree. Previous work on constraint optimization based on decomposition and dynamic programming  has focussed on the task of computing best values for individual variables or a single best assignment to all variables. We extend this work to address important requirements of the diagnosis
557801	5566	of variables in a tree node (called the tree width), and its space complexity is exponential in the maximal number of variables that are shared between two tree nodes (called the separator size) . Hence, the benefit of tree decomposition is that it breaks down the complexity from being exponential in the number of all variables to being exponential in the number of variables per tree
557801	1995	extended to include a preference level. SemiringCSPs subsume many other notions of preferences in constraints, such as fuzzy CSPs , probabilistic CSPs , or partial constraint satisfaction . Definition 3 () A c-semiring is a tuple (A, +, ×, 0, 1) such that 1. A is a set and 0, 1 ? A; 2. + is a commutative, associative and idempotent (i.e., a ? A implies a + a = a) operation with
557801	24433	that local consistency is still applicable, except that it has to be organized as directional consistency in a tree-structured evaluation scheme. Methods for decomposition of constraint networks  can be extended to turn semiring-CSPs into equivalent, tree-structured instances. Expanding on previous work , we present algorithms for solving semiring-CSPs based on tree decompositions
557801	24433	are propagated in an organized way following a hierarchical (tree) scheme. However, arbitrary constraint networks are not necessarily treestructured. The goal of structural decomposition methods  is to turn arbitrary constraint networks into equivalent, tree-structured (acyclic) instances, possibly by aggregating constraints together. Decomposition was developed in the context of hard
557801	24433	), which associates a node with each variable xi ? X, and a hyperedge with each constraint fj ? F . Figure 2 shows the hypergraph for the boolean polycell circuit. Definition 8 (Tree Decomposition ) A tree decomposition for a constraint system (X, D, F ) is a triple (T, ?, ?), where T = (V, E) is a rooted tree, and ?, ? are labeling functions associating with each node v ? V two sets ?(v) ? X
557801	24433	cut-off parameters b, which permits better control over the number of diagnoses generated. TREE* has been combined with a decomposition method for hard constraints called hypertree decomposition . For hard constraints, hypertree decomposition is a more powerful decomposition method because unlike tree decomposition, it allows for reusing constraints in different nodes of the tree. However,
557801	24435	fault mode assignments. In probabilistic diagnosis , A is the interval  with total order ?, and U associates a probability value with each mode assignment. In subset-minimal diagnosis , A is the lattice of subsets of Z with partial order ?, and each mode assignment is mapped to the subset of variables that represent a fault mode assignment. One can think of further instances, for
557801	24436	However, note that updating the values can become exponential in Z even if the task is only to find a single best diagnosis. Efficient data-structures, such as algebraic decision diagrams (ADDs) , exist for constraints (functions) over c-semirings where A is a subset of the real numbers (as is the case for Sc and Sp). For larger constraints and larger Z, it is therefore more efficient to
557801	24436	type presented in Section 5 do not exist for this diagnostic problem. The algorithms presented in this paper have been implemented using a (modified) version of algebraic decision diagrams (ADDs)  to represent semiring-constraints. We are currently experimenting with random examples and real-world applications from the spacecraft domain. Current and future work includes incorporating AI and
557801	24437	are a framework for “soft” constraints where the constraints are extended to include a preference level. SemiringCSPs subsume many other notions of preferences in constraints, such as fuzzy CSPs , probabilistic CSPs , or partial constraint satisfaction . Definition 3 () A c-semiring is a tuple (A, +, ×, 0, 1) such that 1. A is a set and 0, 1 ? A; 2. + is a commutative, associative
557801	24438	not necessarily idempotent, which means that constraint propagation cannot be applied in a “chaotic” way anymore. Research that aims at extending the notion of local consistency to soft constraints  has therefore focused on directional consistency, where constraints are propagated in an organized way following a hierarchical (tree) scheme. However, arbitrary constraint networks are not
557801	24438	as semi-join, is the step that establishes directional consistency between a node and its parent. It is a generalization of directional arc consistency for CSPs  to the case of soft constraints . The restriction operator | b ? “prunes” tuples of a constraint by setting their value to 0 if it is worse than b. Formally, fj | b ? returns a function f ? j where f ? j(t) = fj(t) if fj(t) ?S b,
557801	2004	Intelligence can be framed as optimization problems where the task is to find a best assignment to a set of variables, such that a set of constraints is satisfied. Formalisms for soft constraints  aim at more closely integrating constraint satisfaction and optimization. Soft constraints extend hard constraints by defining preference levels for the constraints, such that assignments are
557801	24439	semiring-CSPs efficiently based on tree decompositions and an instance of dynamic programming. Finally, in Section 6 we show that SAB and TREE*, two diagnosis algorithm for tree-structured systems , can be understood as special instances of semiring-based constraint optimization. 2 DIAGNOSIS AS CONSTRAINT OPTIMIZATION OVER LATTICES Definition 1 (Constraint System) A constraint system over {?,
557801	24439	as stated in Fig. 5 requires that the ×-operator of the semiring is idempotent or has an inverse. This is the case for all three semirings Sc, Ss, and Sp. 5 6 SAB AND TREE* SAB  and TREE*  are two diagnostic algorithms for treestructured systems. SAB is a dynamic programming algorithm based on “weighting” assignments to mode variables. A correct assignment has weight 0, whereas an
557801	24439	the context of soft constraints, this advantage diminishes because multiple occurrences of the same constraint clash with the possible nonidempotency of the constraint combination operator . In  it has been empirically shown that TREE* can outperform SAB, an effect that can be mainly attributed to the use of a cut-off in TREE*. 7 CONCLUSION This work builds on recent research in constraint
557801	24440	, fm} is a set of constraints. The constraints fj are functions defined over var(fj) where allowed tuples have value ? and disallowed tuples have value ?. For example, the boolean polycell circuit  shown in Fig. 1 can be framed as a constraint system with variables X = {a, b, c, d, e, f, g, x, y, z, o1, o2, o3, a1, a2}. Variables a to z are boolean variables with domain {0, 1}, whereas
8832677	8668	allowing for the highest-degree of scalability. 1 INTRODUCTION Fundamental to the process of network protocol design and operations are a variety of performance analysis techniques (Jain 1991; Floyd and Paxson 2001). These techniques have been used by researchers in a variety of different contexts: analytic models (e.g., TCP models, Padhye et al. 2000; self-similar models, Leland et al. 1994; and topology
8832677	8668	studying feature interactions between protocols. Broadly, we may summarize the objective as a quest for general invariant relationships between network parameters and protocol dynamics (Jain 1991; Floyd and Paxson 2001; Floyd and Kohler 2003). If we already have so many tools, it is natural to ask, why are more sophisticated tools required? Sally Floyd and Vern Paxson (2001) pinpoint a number of reasons why
8832677	24445	(e.g., ns-2, Breslau et al. 2000; SSFNet <www.ssfnet.org>; and GloMoSim <pcl. cs.ucla.edu/projects/glomosim>), prototyping platforms (e.g., MIT Click Router toolkit, Kohler et al. 2000; XORP, Handley et al. 2003), tools for systematic design of experiments and exploring parameter state spaces (e.g., Recursive Random Search, Ye and Kalyanaraman 2003); STRESS, Helmy et al. 2000), experimental Murat Yuksel
8832677	9744	al. 2002), simulation platforms (e.g., ns-2, Breslau et al. 2000; SSFNet <www.ssfnet.org>; and GloMoSim <pcl. cs.ucla.edu/projects/glomosim>), prototyping platforms (e.g., MIT Click Router toolkit, Kohler et al. 2000; XORP, Handley et al. 2003), tools for systematic design of experiments and exploring parameter state spaces (e.g., Recursive Random Search, Ye and Kalyanaraman 2003); STRESS, Helmy et al. 2000),
8832677	2673	(Jain 1991; Floyd and Paxson 2001). These techniques have been used by researchers in a variety of different contexts: analytic models (e.g., TCP models, Padhye et al. 2000; self-similar models, Leland et al. 1994; and topology models, Tangmunarunkit et al. 2002), simulation platforms (e.g., ns-2, Breslau et al. 2000; SSFNet <www.ssfnet.org>; and GloMoSim <pcl. cs.ucla.edu/projects/glomosim>), prototyping
8832677	24448	Department Rensselaer Polytechnic Institute 110 8th Street Troy, NY 12180, U.S.A. emulation platforms (e.g., Emulab, White et al. 2002), real-world overlay deployment platforms (e.g., Planetlab, Peterson et al. 2002), and real-world measurement and data-sets (<www.cadia.org>). The high-level motivation behind the use of these tools is simple: to gain varying degrees of qualitative and quantitative
8832677	24452	Murat Yuksel Shivkumar Kalyanaraman Electrical and Computer Systems Engineering Department Rensselaer Polytechnic Institute 110 8th Street Troy, NY 12180, U.S.A. emulation platforms (e.g., Emulab, White et al. 2002), real-world overlay deployment platforms (e.g., Planetlab, Peterson et al. 2002), and real-world measurement and data-sets (<www.cadia.org>). The high-level motivation behind the use of these
8832677	24454	(e.g., MIT Click Router toolkit, Kohler et al. 2000; XORP, Handley et al. 2003), tools for systematic design of experiments and exploring parameter state spaces (e.g., Recursive Random Search, Ye and Kalyanaraman 2003); STRESS, Helmy et al. 2000), experimental Murat Yuksel Shivkumar Kalyanaraman Electrical and Computer Systems Engineering Department Rensselaer Polytechnic Institute 110 8th Street Troy, NY 12180,
24468	24470	over Bloom filters as it incurs no extra round-trip time, and the compression ratio is independent of the size of the final intersection. 5.2.3 Adaptive Set Intersection Adaptive set intersection  exploits structure in the posting lists to avoid having to transfer entire lists. For example, the intersection {1, 3, 4, 7} ? {8, 10, 20, 30} requires only one element exchange, as 7 < 8 implies
24468	24471	that users will terminate their queries early will be increased if the incremental results are prioritized based on a good ranking function. To achieve this effect, Fagin’s algorithm (FA)  is used in conjunction with a ranking function to generate incremental ranked results. The posting lists are sorted based on the ranking function, and the top ranked docIDs are incrementally
24468	24472	tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The most ambitious known evaluation of such a system  demonstrated good full-text keyword search performance with about 100,000 documents. Again, this is a tiny fraction of the size
24468	24472	among the peers. Each peer stores the posting list for the word(s) it is responsible for. A DHT would be used to map a word to the peer responsible for it. A number of proposals work this way . A query involving multiple terms requires that the postings for one or more of the terms be sent over the network. For simplicity, this discussion will assume a two-term query. It is cheaper to
24468	24473	tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The most ambitious known evaluation of such a system  demonstrated good full-text keyword search performance with about 100,000 documents. Again, this is a tiny fraction of the size
24468	7762	documents together based on their term occurrences. By assigning adjacent docIDs to similar documents, the posting lists are made burstier. We use Probabilistic Latent Semantic Analysis (PLSA)  to group all the MIT Web documents into 100 clusters. Documents within the same cluster are assigned contiguous docIDs. Clustering improves the compression ratio of adaptive set intersection with
24468	24474	assumes that the intersection is empty and that the two posting lists have similar sizes.scompression ratio is increased to 40 with four rounds of Bloom filter exchange 2 . Compressed Bloom filters  give a further 30% improvement, resulting in a compression ratio of approximately 50. 5.2.2 Gap Compression Gap compression  is effective when the gaps between sorted docIDs in a posting list
24468	6542	is often large, search engines usually present only the most highly ranked documents. Systems typically combine many ranking factors; these may include the importance of the documents themselves , the frequency of the search terms, or how close the terms occur to each other within the documents. 3 Fundamental Constraints Whether a search algorithm is feasible depends on the workload, the
24468	939	Web. Another class of P2P systems achieve scalability by structuring the data so that it can be found with far less expense than flooding; these are commonly called distributed hash tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The
24468	19240	tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The most ambitious known evaluation of such a system  demonstrated good full-text keyword search performance with about 100,000 documents. Again, this is a tiny fraction of the size
24468	19240	among the peers. Each peer stores the posting list for the word(s) it is responsible for. A DHT would be used to map a word to the peer responsible for it. A number of proposals work this way . A query involving multiple terms requires that the postings for one or more of the terms be sent over the network. For simplicity, this discussion will assume a two-term query. It is cheaper to
24468	19240	lists in advance. Precomputing for all term pairs is not feasible as it would increase the size of the inverted index significantly. Since the popularity of query terms follows a Zipf distribution , it is effective to precompute only the intersections of all pairs of popular query terms. If 7.5 million term pairs (3% of all possible term pairs) from the most popular terms are precomputed for
24468	19240	without sacrificing result quality. 5.2.1 Bloom Filters A Bloom filter can represent a set compactly, at the cost of a small probability of false positives. In a simple two-round Bloom intersection , one node sends the Bloom filter of its posting list. The receiving node intersects the Bloom filter and its posting list, and sends back the resulting list of docIDs. The original sender then
24468	19240	leads us into the softer realm of accepting compromises to gain performance. 6.1 Compromising Result Quality Reynolds and Vahdat suggest streaming results to users using incremental intersection . Assuming users are usually satisfied with only a partial set of matching results, this will allow savings in communication as users are likely to terminate their queries early. Incremental
24468	943	Web. Another class of P2P systems achieve scalability by structuring the data so that it can be found with far less expense than flooding; these are commonly called distributed hash tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The
24468	24475	tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The most ambitious known evaluation of such a system  demonstrated good full-text keyword search performance with about 100,000 documents. Again, this is a tiny fraction of the size
24468	24477	four rounds of Bloom filter exchange 2 . Compressed Bloom filters  give a further 30% improvement, resulting in a compression ratio of approximately 50. 5.2.2 Gap Compression Gap compression  is effective when the gaps between sorted docIDs in a posting list are small. To reduce the gap size, we propose to periodically remap docIDs from 160-bit hashes to dense numbers from 1 to the
24468	5903	Web. Another class of P2P systems achieve scalability by structuring the data so that it can be found with far less expense than flooding; these are commonly called distributed hash tables (DHTs) . DHTs are well-suited for exact match lookups using unique identifiers, but do not directly support text search. There have been recent proposals for P2P text search  over DHTs. The
24478	24479	of items for a particular user based on the rating information for the same set of items given by many other users. In the past years, many collaborative filtering algorithms have been developed . Generally, they can be categorized into two classes: memory-based algorithms and model-based algorithms . To obtain a prediction for a particular user (i.e., a test user), the memory-based
24478	24479	Michigan State University East Lansing, MI 48824 jchai@cse.msu.edu Luo Si School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu similarity based approach , and the extended generalized vector space model . Model-based approaches group together different users in the training database into a small number of classes based on their rating patterns.
24478	24479	user into one of the predefined user classes and use the rating of the predicted class on the targeted item as the prediction. Algorithms within this category include Bayesian network approaches  and the aspect model . Compared to the memory-based approaches, the model-based approaches have an advantage that only the profiles of models need to be stored. However, the memory-based
24478	24479	shown in Herlocker et al. , weighting items using their rating variance leads to slightly worse results than no weighting. In addition to variance, other weights such as inverse user frequency ,sentropy, and mutual information  have been studied in the previous literature. The results in  indicate that few weighting schemes for items are able to improve the performance of
24478	24479	? R ) t yt x?X ( y)^ X ( yt ) 2 2 ( Ry ( x) ? Ry ) ( Ry ( x) ? R ) t yt x?X ( y)^ X ( yt ) x?X ( y)^ X ( yt ) while the VS method defines the similarity as: wyt , y = More details can be found in . Ry ( x) Ry ( x) t x?X ( y)^ X ( yt ) 2 2 Ry ( x) Ry ( x) t x?X ( y) x?X ( yt ) 2.2 Model-based Approaches Two popular model-based algorithms are the aspect model (AM)  and the Personality
24478	9133	of items for a particular user based on the rating information for the same set of items given by many other users. In the past years, many collaborative filtering algorithms have been developed . Generally, they can be categorized into two classes: memory-based algorithms and model-based algorithms . To obtain a prediction for a particular user (i.e., a test user), the memory-based
24478	9133	from the training database that are most similar to this user in terms of the rating patterns, and then combine those ratings together. This category includes the Pearson-correlation based approach , the vector Joyce Y. Chai Dept. of Computer Science and Engineering Michigan State University East Lansing, MI 48824 jchai@cse.msu.edu Luo Si School of Computer Science Carnegie Mellon University
24478	24482	of items for a particular user based on the rating information for the same set of items given by many other users. In the past years, many collaborative filtering algorithms have been developed . Generally, they can be categorized into two classes: memory-based algorithms and model-based algorithms . To obtain a prediction for a particular user (i.e., a test user), the memory-based
24478	24482	user classes and use the rating of the predicted class on the targeted item as the prediction. Algorithms within this category include Bayesian network approaches  and the aspect model . Compared to the memory-based approaches, the model-based approaches have an advantage that only the profiles of models need to be stored. However, the memory-based approaches are usually much
24478	24482	details can be found in . Ry ( x) Ry ( x) t x?X ( y)^ X ( yt ) 2 2 Ry ( x) Ry ( x) t x?X ( y) x?X ( yt ) 2.2 Model-based Approaches Two popular model-based algorithms are the aspect model (AM)  and the Personality Diagnosis model (PD) . Aspect model is a probabilistic latent space model, which models individual preferences as a convex combination of preference factors . The
24478	24482	y p( R ( x) = r) ? p( R | R ) e yt yt y y 2 2 ?( R ( x) ?r) 2? Previous empirical studies have shown that the PD method is able to outperform several other approaches for collaborative filtering , including the PCC method, VS method and the Bayesian network approach. 3. Related Work Automatically assigning appropriate weighting to items has not been well studied in the previous literatures.
24478	24485	of items for a particular user based on the rating information for the same set of items given by many other users. In the past years, many collaborative filtering algorithms have been developed . Generally, they can be categorized into two classes: memory-based algorithms and model-based algorithms . To obtain a prediction for a particular user (i.e., a test user), the memory-based
24478	24485	by only a small number of users are usually insufficient to create reliable clusters of users. To combine the strength of both approaches, hybrid methods such as ‘Personality Diagnosis’ approach  is developed, which outperforms several model-based and memory-based approaches. Because of the simplicity and robustness, the memory-based approaches have been widely used in many real world
24478	24485	x?X ( y)^ X ( yt ) 2 2 Ry ( x) Ry ( x) t x?X ( y) x?X ( yt ) 2.2 Model-based Approaches Two popular model-based algorithms are the aspect model (AM)  and the Personality Diagnosis model (PD) . Aspect model is a probabilistic latent space model, which models individual preferences as a convex combination of preference factors . The latent class variable z ? Z = z , z ,....., z }
24478	24486	items using their rating variance leads to slightly worse results than no weighting. In addition to variance, other weights such as inverse user frequency ,sentropy, and mutual information  have been studied in the previous literature. The results in  indicate that few weighting schemes for items are able to improve the performance of collaborative filtering. One of the reasons,
24478	24486	this may not be a good idea since items rated by fewer training users may not necessarily be useful in distinguishing users of different interests. In the empirical study conducted by Yu et al. , the IUF weighting has degraded the performance of the PCC method. The second approach weights different items proportionally to their variance in ratings given by different users . It is based
24478	24486	1.085 2 1.174 1.115 1.090 0.1 1.163 1.114 1.087 0.5 1.162 1.106 1.075 2 1.168 1.114 1.089 baseline method that does not use any weighting for items. This is actually consistent with the founding in . In contrast, our new weighting scheme is able to boost the prediction accuracy for Weight 0.2 0.15 0.1 0.05 0 0.6 0.4 0.2 -0.4 1 201 401 601 801 Movie Index Figure 1: The distribution of weights
24478	24487	details can be found in . Ry ( x) Ry ( x) t x?X ( y)^ X ( yt ) 2 2 Ry ( x) Ry ( x) t x?X ( y) x?X ( yt ) 2.2 Model-based Approaches Two popular model-based algorithms are the aspect model (AM)  and the Personality Diagnosis model (PD) . Aspect model is a probabilistic latent space model, which models individual preferences as a convex combination of preference factors . The
24478	24487	x) p( z | y) z?Z where p(z|y) stands for the likelihood for the user y to be the class z and p(r|z,x) stands for the likelihood of assigning the item x with the rating r by the class z of users. In , the ratings of each user are normalized to be norm distribution with zero mean and one variance. A Gaussian distribution is used for the parameter p(r|z,x) and a multinomial distribution for
24478	24488	of many different users, and thus may lose the diversity of users. Finally, model-based approaches tend to perform worse than the memory-based approaches when the number of training users is small . This is because ratings by only a small number of users are usually insufficient to create reliable clusters of users. To combine the strength of both approaches, hybrid methods such as
24478	10728	does not provide ratings for the items that most users share a similar opinion. In order to incorporate the weights of items into the probabilistic model, we use the conditional exponential model  to describe the likelihood p( yi | y j ) , i.e. M 1 p( yi | y j ) exp wk v j, k vi, k Z j k= 1 = (3) The variable v i, k denotes the normalized rating for an item x k by a user y i . It has a
24478	24490	items by assigning them much higher weights. In effect, various studies of classification problems have shown that in general a discriminative model performs better than a generative model . The rest of this paper is arranged as follows: Section 2 provides a brief description of several major approaches for collaborative filtering. Section 3 discusses the related work on item
8922345	24498	that are not usually addressed in other document visualization systems. A more detailed description of VIEWER including architectural aspects and more Web session examples can be found in (Berenci et al., 1998). terms, secondary topic keyword, and non-categorical terms. 6sFigure 1: VIEWER visualization of Web retrieval results for the topic: “scientific accuracy of Bible predictions”. 4. Experiment 1:
8922345	24498	recall points, and the differences were statistically significant (with a combined p value of 5.35E-05). These results confirm and extend earlier findings obtained on two small test collections (Berenci et al., 1998), and offer strong evidence that VIEWER can be effectively used by a user to reorder the documents returned by Web search engines, at least for short queries. This is a useful starting point to
8922345	24501	measures of batch retrieval to the interactive context by taking also into account the dynamics of retrieval sessions (Carpineto and Romano, 1996; Buckley et al., 1999) and the user’s opinions (Brajnik et al., 1996). We focus on precision (i.e., the ratio of number of items retrieved and relevant to the number of items retrieved), because this is usually the primary concern for users engaged in on-line
8922345	24502	displayed distribution of query terms within each document to locate its relevant parts, and Ulysses showed a lattice of terms and documents that can be searched in various and integrated ways (Carpineto and Romano, 1996). Most of these systems, however, cannot be applied to Web-based retrieval because they are either computationally expensive, or require sophisticated graphical facilities, or do not scale well, or
8922345	24502	and measures. In particular, it seems useful to try to extend conventional measures of batch retrieval to the interactive context by taking also into account the dynamics of retrieval sessions (Carpineto and Romano, 1996; Buckley et al., 1999) and the user’s opinions (Brajnik et al., 1996). We focus on precision (i.e., the ratio of number of items retrieved and relevant to the number of items retrieved), because
8922345	24502	by the user, but this approach has the disadvantage that we might be measuring the users judgement more than the effect of the retrieval method. A more typical choice (e.g., Tague-Sutcliffe, 1992; Carpineto and Romano, 1996) is to rate a document as a retrieved document as soon as its description (the summary, in our case) is recovered, without considering the user’s judgement. In this experiment we have taken the
8922345	24505	space using three-dimensional visualization schemes, InfoCrystal (Spoerri, 1994) used a particular visual representation of a Venn diagram to suggest how to refine Boolean queries, TileBars (Hearst, 1995) displayed distribution of query terms within each document to locate its relevant parts, and Ulysses showed a lattice of terms and documents that can be searched in various and integrated ways
8922345	24506	the set of query terms to indicate which terms cover which aspects of the query, e.g., the constraint (A OR B) AND (C OR D) specifies two aspects of the query, each represented by two keywords (Hearst, 1996). These approaches may be useful to solve specific aspects of the exact partial match problem but the formulation of the additional information that they require lends itself to improper partial
8922345	24513	problem with batch measures, including precision, is that they are based on the notion of retrieved document, which is often difficult to define in an interactive setting. One approach (Veerasamy and Heikes, 1997) is to consider as retrieved documents only the documents that have been seen and judged to be relevant by the user, but this approach has the disadvantage that we might be measuring the users
24514	11372	traffic, algorithms have been found for both bi-directional rings ,  and unidirectional rings . Heuristic algorithms for general (non-uniform) traffic have also been presented in , , , . The port requirement in a network can be further reduced by using electronic switches (e.g. SONET cross-connects) to more efficiently groom the offered traffic , .
24514	11390	transceiver. Many researchers have studied the static traffic grooming problem, and in general average transceiver savings that exceed 50% has been obtained for various traffic scenarios , , , . All of the previous works have assumed that the transceivers are fixed tuned. However, as mentioned earlier, using tunable transceivers can help reduce the number of transceivers
24514	24521	algorithms have been found for both bi-directional rings ,  and unidirectional rings . Heuristic algorithms for general (non-uniform) traffic have also been presented in , , , . The port requirement in a network can be further reduced by using electronic switches (e.g. SONET cross-connects) to more efficiently groom the offered traffic , . However, these
24514	24521	Many researchers have studied the static traffic grooming problem, and in general average transceiver savings that exceed 50% has been obtained for various traffic scenarios , , , . All of the previous works have assumed that the transceivers are fixed tuned. However, as mentioned earlier, using tunable transceivers can help reduce the number of transceivers significantly.
24514	24522	have been found for both bi-directional rings ,  and unidirectional rings . Heuristic algorithms for general (non-uniform) traffic have also been presented in , , , . The port requirement in a network can be further reduced by using electronic switches (e.g. SONET cross-connects) to more efficiently groom the offered traffic , . However, these switches
24514	24524	hardware. This work compliments work on reconfigurable WDM networks, where tunable components are used to change the virtual topology in response to traffic variations or for protection purposes , . Reconfiguration is generally thought of as occurring on a much slower time-scale than than that considered here. It is also related to work on optical burst or packet switching , where
24514	24525	This work compliments work on reconfigurable WDM networks, where tunable components are used to change the virtual topology in response to traffic variations or for protection purposes , . Reconfiguration is generally thought of as occurring on a much slower time-scale than than that considered here. It is also related to work on optical burst or packet switching , where
24536	26161	can be measured from a shadow profit or a shadow cost function. The shadow profit function framework was developed by Lau and Yotopoulous (1971), and then extended by Atkinson and Halvorsen (1980), and Lovell and Sickles (1983). The shadow cost function approach was first introduced by Toda (1976) and later applied by Atkinson and Halvorsen (1984). Recently, some economists have used the
24554	24559	to the implementation of distributed control architectures. Such tools represent enabling technologies for the development of home automation networks , human-robot interactive applications =, real-time collaborative telemedicine systems  and, more generally, distributed measurement systems (DMS) both for educational and industrial purposes - . Unfortunately, many of the
24570	24573	is highly parallel. Auslander and Tsao  and Lederman, Tsao, and Turnbull  use multiply based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel  use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also ). These methods are similar to what we propose in that their focus is on
24570	24573	as well) are needed, the most competitive serial algorithm is the QRalgorithm . Matrix multiply methods tend to require many more flops and sometimes encounter accuracy problems as well . Although matrix tearing methods may have lower flops counts, they require finding all the eigenvectors and hence are only useful when all the eigenvectors are required. Furthermore, there are
24570	24573	based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel  use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also ). These methods are similar to what we propose in that their focus is on scalability. On the other hand, they have higher flop counts and suffer drawbacks in accuracy. Dongarra and Sidani
24570	24574	based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel  use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also ). These methods are similar to what we propose in that their focus is on scalability. On the other hand, they have higher flop counts and suffer drawbacks in accuracy. Dongarra and Sidani
24570	24575	based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel  use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also ). These methods are similar to what we propose in that their focus is on scalability. On the other hand, they have higher flop counts and suffer drawbacks in accuracy. Dongarra and Sidani
24570	24576	matrix (1 × 1 and 2 × 2 blocks along the main diagonal). We can assume that H is upper Hessenberg because the reduction to Hessenberg form is well understood and can be parallelized .sPARALLEL QR ALGORITHM 287 Francis HQR Step e = eig(H(N ? 1:N,N ? 1:N)) Let x =(H ? e(1)IN) ? (H ? e(2)IN)e1 Let P0 ?? N×N be a Householder matrix such that P0x is a multiple of e1. H ? P0HP0 for i
24570	24577	tradeoffs involved in using more bulges than necessary. The matrix is decomposed into L × L blocks, which are parceled out to the processors by a two-dimensional (block cyclic) torus wrap mapping . We refer to the block size of the cyclic mapping as L. The bulges in our algorithm must be separated by at least a block, and remain synchronized, to ensure that each row/column of processors
24570	24577	To one familiar with parallel linear algebra, the first instinct is to dismiss this overhead or to include a small “fudge factor” in some modeling. In cases such as a parallel LU decomposition , for example, there is a pipeline start-up when doing the horizontal broadcast of the multipliers around a ring. The key difference, however, is that in that case all the nodes are busy while the
24570	24577	? is kept small. 5. Accuracy results. Our implementation of the parallel multishift QRalgorithm can be found in the current version of the general purpose parallel linear algebra package ScaLAPACK . ScaLAPACK is built on top of a portable communications layer called the BLACS . BLACS have been implemented under PVM  and MPI , among many other communication standards. As far as
24570	24579	based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel  use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also ). These methods are similar to what we propose in that their focus is on scalability. On the other hand, they have higher flop counts and suffer drawbacks in accuracy. Dongarra and Sidani
24570	24584	matrix (1 × 1 and 2 × 2 blocks along the main diagonal). We can assume that H is upper Hessenberg because the reduction to Hessenberg form is well understood and can be parallelized .sPARALLEL QR ALGORITHM 287 Francis HQR Step e = eig(H(N ? 1:N,N ? 1:N)) Let x =(H ? e(1)IN) ? (H ? e(2)IN)e1 Let P0 ?? N×N be a Householder matrix such that P0x is a multiple of e1. H ? P0HP0 for i
24570	24586	11, 5, 7]). These methods are similar to what we propose in that their focus is on scalability. On the other hand, they have higher flop counts and suffer drawbacks in accuracy. Dongarra and Sidani  introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems that are solved independently and then glued back together with a
24570	24595	local leading dimension, respectively). This is impossible for Hankel mappings on multiple nodes. In addition to this problem, the algorithms resulting from all these works were only iso-efficient . That is, you could get 100% efficiency, but only if the problem size was allowed to scale faster than memory does. Nevertheless, these were the first algorithms ever to achieve theoretically
24570	24597	of the data, in which the row transforms are unevenly distributed. For scalability reasons, distributed memory linear algebra computations often require a two-dimensional block wrap torus mapping . To maximize the distribution of this computation, we could wrap our two-dimensional block wrap mapping as tightly as possible with a block size of one (this would create other problems which we
24570	24598	to tridiagonal form (see also ). These might require fewer flops but they are plagued by instability. Against this competition, blocked versions of the implicit double shift QRalgorithm  appear promising. One serious drawback of the double implicit shift QRalgorithm is that its core computation is based on Householder reflections of size 3. This is a drawback for several reasons:
24570	8310705	These positive results may lead to even better methods in the future, since combining the use of HQR for finding eigenvalues with new GEMM-based inverse iteration methods for finding eigenvectors  might lead to completing the spectrum significantly faster than the results in Table 2. Furthermore, there are better load balancing properties to the eigenvalue-only code on a Cartesian mapping.
24570	24570	compilers). Note that these results, currently found in ScaLAPACK version 1.6, are better than those previously released in ScaLAPACK version 1.2 ALPHA (which corresponded to the technical note ). The serial performance of this code is around 10 Mflops, and efficiency results are reported against this and not DLAHQR. Had they been reported against DLAHQR, the 64-node job would have shown a
24570	24605	the most attention recently have been based on matrix multiplication. The reason is clear: large matrix multiplication is highly parallel. Auslander and Tsao  and Lederman, Tsao, and Turnbull  use multiply based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel  use similar matrix multiply techniques using the matrix sign function to split the
24570	24606	bulges (e.g., degree 6). The first attempts at parallelizing the implicit double shift QRalgorithm were unsuccessful. See Boley and Maier , Geist et al. , Eberlein , and Stewart . More successful methods came from vector implementations . Usually, the key problem is how to distribute the work evenly given its sequential nature. A major step forward in work distribution
24570	24606	at least O( ? p), where p is the number of processors. Here we pursue the idea of using M shifts to form and chase M/2 bulges in parallel. The idea of chasing multiple bulges in parallel is not new . However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence . What is new  (having seen ) is the idea of
24570	24613	and is how it is implemented in LAPACK ), the algorithm has convergence problems caused by roundoff errors if the value of M is too large. This was discussed by Dubrulle  and Watkins . Because of this, a multishift size of M = 6 was implemented in LAPACK. It is not clear whether this is faster than the double implicit shift QR when the latter is blocked . Because of the
24570	24614	are taken to be the eigenvalues of the lower 2 × 2 submatrix. This is inexpensive and works well as long as only one bulge at a time is being chased. The convergence rate is usually quadratic . However, this strategy has the following shortcoming for parallel computing: The correct shifts for the next iteration cannot be calculated until the bulge for the current iteration has been
24570	24614	has enough shifts to chase M/2 bulges in either serial or parallel fashion before having to go back for more shifts. This strategy also results (usually) in quadratic convergence, as was proved in  and has been observed in practice. We refer to each cycle of computing M shifts and chasing M/2 bulges as a superiteration. The question of how to determine the number of bulges S = M/2 per
7527165	26267	if these structures can be attached to ssDNA. It should also be pointed out that the sequence of the DNA can be chosen and the molecules can be obtained from a variety of commercial sources . 3. Attachment of DNA to gold surfaces It is now important to review the methodology to attach DNA molecules to surfaces. The most widely used attachment scheme utilizes the covalent bond between
